""Reference works "",,,,""A reference work is a book or periodical (or its electronic equivalent) to which one can refer for confirmed facts. The information is intended to be found quickly when needed. Reference works are usually referred to for particular pieces of information, rather than read beginning to end. The writing style used in these works is informative; the authors avoid use of the first person, and emphasize facts. Many reference works are compiled by a team of contributors whose work is coordinated by one or more editors rather than by an individual author. Indices are commonly provided in many types of reference work. Updated editions are usually published as needed, in some cases annually (e.g. Whitaker's Almanack, Who's Who). Reference works include dictionaries, thesauruses, encyclopedias, almanacs, bibliographies, and catalogs (e.g. catalogs of libraries, museums or the works of individual artists). Many reference works are available in electronic form and can be obtained as application software, CD-ROMs, DVDs, or online through the Internet.

""
""General conference proceedings "",,,,""The Militia Act of 1903 (32 Stat. 775), also known as "The Efficiency in Militia Act of 1903", also known as the Dick Act,  was legislation enacted by the United States Congress which codified the circumstances under which the National Guard could be federalized. It also provided federal funds to the National Guard to pay for equipment and training, including annual summer encampments. In return, the National Guard began to organize its units along the same lines as the regular Army, and took steps to meet the same training, education and readiness requirements as active duty units.

""
""Biographies "",,,,""A biography (or simply bio) is a detailed description of a person's life. It involves more than just the basic facts like education, work, relationships, and death, but also portrays a subject's experience of these life events. Unlike a profile or curriculum vitae (résumé), a biography presents a subject's life story, highlighting various aspects of his or her life, including intimate details of experience, and may include an analysis of the subject's personality.
Biographical works are usually non-fiction, but fiction can also be used to portray a person's life. One in-depth form of biographical coverage is called legacy writing. Works in diverse media, from literature to film, form the genre known as biography.
An authorized biography is written with the permission, cooperation, and at times, participation of a subject or a subject's heirs. An autobiography is written by the person himself or herself, sometimes with the assistance of a collaborator or ghostwriter.""
""General literature "",,,,""The Committee of General Literature and Education was a British publishing organisation set up by the Society for the Promotion of Christian Knowledge in 1832 to produce school books. It also published The Saturday Magazine.

""
""Empirical studies "",,,,""Empirical research is research using empirical evidence. It is a way of gaining knowledge by means of direct and indirect observation or experience. Empiricism values such research more than other kinds. Empirical evidence (the record of one's direct observations or experiences) can be analyzed quantitatively or qualitatively. Through quantifying the evidence or making sense of it in qualitative form, a researcher can answer empirical questions, which should be clearly defined and answerable with the evidence collected (usually called data). Research design varies by field and by the question being investigated. Many researchers combine qualitative and quantitative forms of analysis to better answer questions which cannot be studied in laboratory settings, particularly in the social sciences and in education.
In some fields, quantitative research may begin with a research question (e.g., "Does listening to vocal music during the learning of a word list have an effect on later memory for these words?") which is tested through experimentation. Usually, a researcher has a certain theory regarding the topic under investigation. Based on this theory some statements, or hypotheses, will be proposed (e.g., "Listening to vocal music has a negative effect on learning a word list."). From these hypotheses predictions about specific events are derived (e.g., "People who study a word list while listening to vocal music will remember fewer words on a later memory test than people who study a word list in silence."). These predictions can then be tested with a suitable experiment. Depending on the outcomes of the experiment, the theory on which the hypotheses and predictions were based will be supported or not, or may need to be modified and then subjected to further testing.""
""Measurement "",,,,""Measurement is the assignment of a number to a characteristic of an object or event, which can be compared with other objects or events. The scope and application of a measurement is dependent on the context and discipline. In the natural sciences and engineering, measurements do not apply to nominal properties of objects or events, which is consistent with the guidelines of the International vocabulary of metrology published by the International Bureau of Weights and Measures. However, in other fields such as statistics as well as the social and behavioral sciences, measurements can have multiple levels, which would include nominal, ordinal, interval, and ratio scales.
Measurement is a cornerstone of trade, science, technology, and quantitative research in many disciplines. Historically, many measurement systems existed for the varied fields of human existence to facilitate comparisons in these fields. Often these were achieved by local agreements between trading partners or collaborators. Since the 18th century, developments progressed towards unifying, widely accepted standards that resulted in the modern International System of Units (SI). This system reduces all physical measurements to a mathematical combination of seven base units. The science of measurement is pursued in the field of metrology.""
""Evaluation "",,,,""Evaluation is a systematic determination of a subject's merit, worth and significance, using criteria governed by a set of standards. It can assist an organization, program, project or any other intervention or initiative to assess any aim, realisable concept/proposal, or any alternative, to help in decision-making; or to ascertain the degree of achievement or value in regard to the aim and objectives and results of any such action that has been completed. The primary purpose of evaluation, in addition to gaining insight into prior or existing initiatives, is to enable reflection and assist in the identification of future change.
Evaluation is often used to characterize and appraise subjects of interest in a wide range of human enterprises, including the arts, criminal justice, foundations, non-profit organizations, government, health care, and other human services. It is long term and done at the end of a period of time.""
""Experimentation "",,,,""An experiment is a procedure carried out to verify, refute, or validate a hypothesis. Experiments provide insight into cause-and-effect by demonstrating what outcome occurs when a particular factor is manipulated. Experiments vary greatly in goal and scale, but always rely on repeatable procedure and logical analysis of the results. There also exist natural experimental studies.
A child may carry out basic experiments to understand gravity, while teams of scientists may take years of systematic investigation to advance their understanding of a phenomenon. Experiments and other types of hands-on activities are very important to student learning in the science classroom. Experiments can raise test scores and help a student become more engaged and interested in the material they are learning, especially when used over time. Experiments can vary from personal and informal natural comparisons (e.g. tasting a range of chocolates to find a favorite), to highly controlled (e.g. tests requiring complex apparatus overseen by many scientists that hope to discover information about subatomic particles). Uses of experiments vary considerably between the natural and human sciences.
Experiments typically include controls, which are designed to minimize the effects of variables other than the single independent variable. This increases the reliability of the results, often through a comparison between control measurements and the other measurements. Scientific controls are a part of the scientific method. Ideally, all variables in an experiment are controlled (accounted for by the control measurements) and none are uncontrolled. In such an experiment, if all controls work as expected, it is possible to conclude that the experiment works as intended, and that results are due to the effect of the tested variable.
^ Stohr-Hunt, Patricia (1996). "An Analysis of Frequency of Hands-on Experience and Science Achievement". Journal of research in science teaching 33. doi:10.1002/(SICI)1098-2736(199601)33:1<101::AID-TEA6>3.0.CO;2-Z.""
""Estimation "",,,,""Estimation (or estimating) is the process of finding an estimate, or approximation, which is a value that is usable for some purpose even if input data may be incomplete, uncertain, or unstable. The value is nonetheless usable because it is derived from the best information available. Typically, estimation involves "using the value of a statistic derived from a sample to estimate the value of a corresponding population parameter". The sample provides information that can be projected, through various formal or informal processes, to determine a range most likely to describe the missing information. An estimate that turns out to be incorrect will be an overestimate if the estimate exceeded the actual result, and an underestimate if the estimate fell short of the actual result.
^ C. Lon Enloe, Elizabeth Garnett, Jonathan Miles, Physical Science: What the Technology Professional Needs to Know (2000), p. 47.
^ Raymond A. Kent, "Estimation", Data Construction and Data Analysis for Survey Research (2001), p. 157.
^ James Tate, John Schoonbeck, Reviewing Mathematics (2003), page 27: "An overestimate is an estimate you know is greater than the exact answer".
^ James Tate, John Schoonbeck, Reviewing Mathematics (2003), page 27: "An underestimate is an estimate you know is less than the exact answer".""
""Design "",,,,""Design is the creation of a plan or convention for the construction of an object or a system (as in architectural blueprints, engineering drawings, business processes, circuit diagrams and sewing patterns). Design has different connotations in different fields (see design disciplines below). In some cases the direct construction of an object (as in pottery, engineering, management, cowboy coding and graphic design) is also considered to be design.
Designing often necessitates considering the aesthetic, functional, economic and sociopolitical dimensions of both the design object and design process. It may involve considerable research, thought, modeling, interactive adjustment, and re-design. Meanwhile, diverse kinds of objects may be designed, including clothing, graphical user interfaces, skyscrapers, corporate identities, business processes and even methods of designing.
Thus "design" may be a substantive referring to a categorical abstraction of a created thing or things (the design of something), or a verb for the process of creation, as is made clear by grammatical context.

""
""Performance "",,,,""A performance, in the performing arts, generally comprises an event in which a performer or group of performers present one or more works of art to an audience. Usually the performers participate in rehearsals beforehand. Afterwards audience members often applaud. After a performance, performance measurement sometimes occurs. Performance measurement is the process of collecting, analyzing and reporting information regarding the performance of an individual, group, organization, system or component.
The means of expressing appreciation can vary by culture. Chinese performers will clap with the audience at the end of a performance; the return applause signals "thank you" to the audience. In Japan, folk performing-arts performances commonly attract individuals who take photographs, sometimes getting up to the stage and within inches of performer's faces.
Sometimes the dividing line between performer and the audience may become blurred, as in the example of "participatory theatre" where audience members get involved in the production.
Theatrical performances can take place daily or at some other regular interval. Performances can take place at designated performance spaces (such as a theatre or concert hall), or in a non-conventional space, such as a subway station, on the street, or in somebody's home.
^ Brown, Ju; Brown, John (2006). China, Japan, Korea Culture and Customs. North Charleston: BookSurge. p. 55. ISBN 1-4196-4893-4. 
^ Thornbury, B. (1997). The Folk Performing Arts: Traditional Culture in Contemporary Japan. Albany: State University of New York. p. 12. ISBN 0-7914-3255-6.""
""Digital signal processing "",,,,""Digital signal processing (DSP) is the numerical manipulation of signals, usually with the intention to measure, filter, produce or compress continuous analog signals. It is characterized by the use of digital signals to represent these signals as discrete time, discrete frequency, or other discrete domain signals in the form of a sequence of numbers or symbols to permit the digital processing of these signals.
Theoretical analyses and derivations are typically performed on discrete-time signal models, created by the abstract process of sampling. Numerical methods require a digital signal, such as those produced by an analog-to-digital converter (ADC). The processed result might be a frequency spectrum or a set of statistics. But often it is another digital signal that is converted back to analog form by a digital-to-analog converter (DAC). Even if that whole sequence is more complex than analog processing and has a discrete value range, the application of computational power to signal processing allows for many advantages over analog processing in many applications, such as error detection and correction in transmission as well as data compression.
Digital signal processing and analog signal processing are subfields of signal processing. DSP applications include audio and speech signal processing, sonar and radar signal processing, sensor array processing, spectral estimation, statistical signal processing, digital image processing, signal processing for communications, control of systems, biomedical signal processing, seismic data processing, among others. DSP algorithms have long been run on standard computers, as well as on specialized processors called digital signal processors, and on purpose-built hardware such as application-specific integrated circuit (ASICs). Currently, there are additional technologies used for digital signal processing including more powerful general purpose microprocessors, field-programmable gate arrays (FPGAs), digital signal controllers (mostly for industrial applications such as motor control), and stream processors, among others.
Digital signal processing can involve linear or nonlinear operations. Nonlinear signal processing is closely related to nonlinear system identification and can be implemented in the time, frequency, and spatio-temporal domains.""
""Beamforming "",,,,""Beamforming or spatial filtering is a signal processing technique used in sensor arrays for directional signal transmission or reception. This is achieved by combining elements in a phased array in such a way that signals at particular angles experience constructive interference while others experience destructive interference. Beamforming can be used at both the transmitting and receiving ends in order to achieve spatial selectivity. The improvement compared with omnidirectional reception/transmission is known as the directivity of the element.
Beamforming can be used for radio or sound waves. It has found numerous applications in radar, sonar, seismology, wireless communications, radio astronomy, acoustics, and biomedicine. Adaptive beamforming is used to detect and estimate the signal-of-interest at the output of a sensor array by means of optimal (e.g., least-squares) spatial filtering and interference rejection.""
""Noise reduction "",,,,""Noise reduction is the process of removing noise from a signal.
All recording devices, both analog and digital, have traits that make them susceptible to noise. Noise can be random or white noise with no coherence, or coherent noise introduced by the device's mechanism or processing algorithms.
In electronic recording devices, a major form of noise is hiss caused by random electrons that, heavily influenced by heat, stray from their designated path. These stray electrons influence the voltage of the output signal and thus create detectable noise.
In the case of photographic film and magnetic tape, noise (both visible and audible) is introduced due to the grain structure of the medium. In photographic film, the size of the grains in the film determines the film's sensitivity, more sensitive film having larger sized grains. In magnetic tape, the larger the grains of the magnetic particles (usually ferric oxide or magnetite), the more prone the medium is to noise.
To compensate for this, larger areas of film or magnetic tape may be used to lower the noise to an acceptable level.""
""Sensors and actuators "",,,,""AS-Interface (Actuator Sensor Interface, AS-i) is an industrial networking solution (physical layer, data access method and protocol) used in PLC, DCS and PC-based automation systems. It is designed for connecting simple field I/O devices (e.g. binary ON/OFF devices such as actuators, sensors, rotary encoders, analog inputs and outputs, push buttons, and valve position sensors) in discrete manufacturing and process applications using a single 2-conductor cable.
AS-Interface is an 'open' technology supported by a multitude of automation equipment vendors. According to AS-International Association there are currently (2013) over 24 Million AS-Interface field devices installed globally, growing at about 2 million per year.
AS-Interface is a networking alternative to the hard wiring of field devices. It can be used as a partner network for higher level fieldbus networks such as Profibus, DeviceNet, Interbus and Industrial Ethernet, for whom it offers a low-cost remote I/O solution. It is used in automation applications, including conveyor control, packaging machines (e.g. Schubert's), process control valves, bottling plants, electrical distribution systems, airport baggage carousels, elevators, bottling lines and food production lines (e.g. 2SFG).
AS-Interface provides a basis for Functional Safety in machinery safety/emergency stop applications. Safety devices communicating over AS-Interface follow all the normal AS-Interface data rules. The required level of data verification is provided by dynamic changes in the data. This technology is called Safety at Work and allows safety devices and standard, non-safe devices to be connected to the same network. Using appropriate safe input hardware (e.g. light curtains, e-stop buttons, and door interlock switches), AS-Interface can provide safety support up to SIL (Safety Integrity Level) 3 according to EN 62061, CAT 4 according to EN954-1 as well as Performance Level e (PL e) according to EN ISO 13849-1.
The AS-Interface specification is managed by AS-International, a member funded non-profit organization located in Gelnhausen/Germany. Several international daughter organizations exist around the world.""
""External storage "",,,,""This article is about external storage devices. For external storage in the context of data structures, see Reference (computer science)#External and internal storage.
In computing, external storage comprises devices that temporarily store information for transporting from computer to computer. Such devices are not permanently fixed inside a computer.
Semiconductor memories are not sufficient to provide the whole storage capacity required in computers. The major limitation in using semiconductor memories is the cost per bit of the stored information. So to fulfill the large storage requirements of computers, magnetic disks and optical disks are generally used.""
""Networking hardware "",,,,""Networking hardware, also known as network equipment or computer networking devices, are physical devices which are required for communication and interaction between devices on a computer network. Specifically, they mediate data in a computer network. Units which are the last receiver or generate data are called hosts or data terminal equipment.""
""Sensor devices and platforms "",,,,""The Player Project (formerly the Player/Stage Project) is a project to create free software for research into robotics and sensor systems. Its components include the Player network server and the Stage robot platform simulators. Although accurate statistics are hard to obtain, Player is one of the most popular open-source robot interfaces in research and post-secondary education. Most of the major intelligent robotics journals and conferences regularly publish papers featuring real and simulated robot experiments using Player and Stage.
^ Gerkey, B., Vaughan, R., and Howard, A. (2003) The Player/Stage Project: Tools for Multi-Robot and Distributed Sensor Systems. Proceedings of the International Conference on Advanced Robotics 317-323
^ Collet, T. H. J., MacDonald, B. A., and Gerkey, B. (2005) Player 2.0: Toward a practical robot programming framework. Proceedings of the Australasian Conference on Robotics and Automation (ACRA)""
""Sound-based input / output "",,,,""The Yamaha DX7 is an FM synthesis based digital synthesizer manufactured by the Yamaha Corporation from 1983 to 1989. It was the first commercially successful digital synthesizer. Its distinctive sound can be heard on many recordings, especially pop music from the 1980s. The monotimbral, 16-note polyphonic DX7 was the moderately priced model of the DX series keyboard synthesizers that included the larger and more elaborate DX1 and DX5; the feature-reduced DX9; and the smaller and not directly compatible DX100, DX11, and DX21. Over 200,000 of the original DX7 were made, and it remains one of the best-selling synthesizers of all time.""
""Touch screens "",,,,""A touchscreen is an input device normally layered on the top of an electronic visual display of an information processing system. A user can give input or control the information processing system through simple or multi-touch gestures by touching the screen with a special stylus and/or one or more fingers. Some touchscreens use ordinary or specially coated gloves to work while others use a special stylus/pen only. The user can use the touchscreen to react to what is displayed and to control how it is displayed; for example, zooming to increase the text size.
The touchscreen enables the user to interact directly with what is displayed, rather than using a mouse, touchpad, or any other intermediate device (other than a stylus, which is optional for most modern touchscreens).
Touchscreens are common in devices such as game consoles, personal computers, tablet computers, electronic voting machines, and smartphones. They can also be attached to computers or, as terminals, to networks. They also play a prominent role in the design of digital appliances such as personal digital assistants (PDAs)es and some books (E-books).
The popularity of smartphones, tablets, and many types of information appliances is driving the demand and acceptance of common touchscreens for portable and functional electronics. Touchscreens are found in the medical field and in heavy industry, as well as for automated teller machines (ATMs), and kiosks such as museum displays or room automation, where keyboard and mouse systems do not allow a suitably intuitive, rapid, or accurate interaction by the user with the display's content.
Historically, the touchscreen sensor and its accompanying controller-based firmware have been made available by a wide array of after-market system integrators, and not by display, chip, or motherboard manufacturers. Display manufacturers and chip manufacturers worldwide have acknowledged the trend toward acceptance of touchscreens as a highly desirable user interface component and have begun to integrate touchscreens into the fundamental design of their products.""
""Haptic devices "",,,,""Haptic or kinesthetic communication recreates the sense of touch by applying forces, vibrations, or motions to the user. This mechanical stimulation can be used to assist in the creation of virtual objects in a computer simulation, to control such virtual objects, and to enhance the remote control of machines and devices (telerobotics). Haptic devices may incorporate tactile sensors that measure forces exerted by the user on the interface.
Most researchers distinguish three sensory systems related to sense of touch in humans: cutaneous, kinesthetic and haptic. All perceptions mediated by cutaneous and/or kinesthetic sensibility are referred to as tactual perception. The sense of touch may be classified as passive and active, and the term "haptic" is often associated with active touch to communicate or recognize objects.
Haptic technology has made it possible to investigate how the human sense of touch works by allowing the creation of carefully controlled haptic virtual objects.
The word haptic, from the Greek: ἁπτικός (haptikos), means "pertaining to the sense of touch" and comes from the Greek verb ἅπτεσθαι haptesthai, meaning "to contact" or "to touch".

""
""Scanners "",,,,""Scanners is a 1981 Canadian science-fiction horror film written and directed by David Cronenberg and starring Jennifer O'Neill, Steven Lack, Michael Ironside, and Patrick McGoohan. In the film, "scanners" are people with unusual telepathic and telekinetic powers. ConSec, a purveyor of weaponry and security systems searches out scanners to use them for its own purposes. The film's plot concerns the attempt by Darryl Revok, a renegade scanner, to wage a war against ConSec. Another scanner, Cameron Vale, is dispatched by ConSec to stop Revok.""
""Wireless devices "",,,,""Wireless communication is the transfer of information between two or more points that are not connected by an electrical conductor.
The most common wireless technologies use radio. With radio waves distances can be short, such as a few meters for television or as far as thousands or even millions of kilometers for deep-space radio communications. It encompasses various types of fixed, mobile, and portable applications, including two-way radios, cellular telephones, personal digital assistants (PDAs), and wireless networking. Other examples of applications of radio wireless technology include GPS units, garage door openers, wireless computer mice, keyboards and headsets, headphones, radio receivers, satellite television, broadcast television and cordless telephones.
Somewhat less common methods of achieving wireless communications include the use of other electromagnetic wireless technologies, such as light, magnetic, or electric fields or the use of sound.
The term wireless has been used twice in communications history, with slightly different meaning. It was initially used from about 1890 for the first radio transmitting and receiving technology, as in wireless telegraphy, until the new word radio replaced it around 1920. The term was revived in the 1980s and 1990s mainly to distinguish digital devices that communicate without wires, such as the examples listed in the previous paragraph, from those that require wires. This is its primary usage today.
LTE, LTE-Advanced, Wi-Fi, Bluetooth are some of the most common modern wireless technologies.""
""Wireless integrated network sensors "",,,,""Wireless integrated network sensors (WINS) provide distributed network and Internet access to sensors, controls, and processors that are deeply embedded in equipment, facilities, and the environment. The WINS is a new monitoring and control capability for applications in transportation, manufacturing, health care, environmental monitoring, and safety and security,border security. WINS combine microsensor technology, low power signal processing, low power computation, and low power, low cost wireless networking capability in a compact system. WINS networks provide sensing, local control, and embedded intelligent systems in structures, materials, and environments.
WINS technology was announced by the House Science Committee as one of the nation's technology breakthroughs in the "Great Advances in Scientific Discovery During the 105th Congress" on 24 September 1998. More information available at http://www.pdf-search-engine.com/border-security-using-wireless-integrated-network-sensor-pdf.html""
""3D integrated circuits "",,,,""In electronic engineering, a through-silicon via (TSV) is a vertical electrical connection (via) passing completely through a silicon wafer or die. TSVs are a high performance interconnect technique used as an alternative to wire-bond and flip chips to create 3D packages and 3D integrated circuits, compared to alternatives such as package-on-package, because the density of the vias is substantially higher, and because the length of the connections is shorter.
Using TSVs it is possible to achieve connections through the FEOL.""
""Metallic interconnect "",,,,""Tungsten(VI) fluoride, also known as tungsten hexafluoride, is the inorganic compound of tungsten and fluorine with the formula WF6. This corrosive, colorless compound is a gas under standard conditions, with a density of about 13 g/L (roughly 11 times heavier than air.), WF6 is one of the heaviest known gases under standard conditions. WF6 gas is most commonly used in the production of semiconductor circuits and circuit boards through the process of chemical vapor deposition – upon decomposition, molecules of WF6 leave a residue of metallic tungsten. This layer serves as low-resistive metallic "interconnect".
^ Roucan, J.-P.; Noël-Dutriaux, M.-C. Proprietes Physiques des Composes Mineraux. Ed. Techniques Ingénieur. p. 138. 
^ Gas chart
^ "Tungsten Hexafluoride MSDS" (pdf). 
^ 
^ "Tungsten and Tungsten Silicide Chemical Vapor Deposition". CVD Fundamentals. TimeDomain CVD.""
""Radio frequency and wireless interconnect "",,,,""A wireless network is any type of computer network that uses wireless data connections for connecting network nodes.
Wireless networking is a method by which homes, telecommunications networks and enterprise (business) installations avoid the costly process of introducing cables into a building, or as a connection between various equipment locations. Wireless telecommunications networks are generally implemented and administered using radio communication. This implementation takes place at the physical level (layer) of the OSI model network structure.
Examples of wireless networks include cell phone networks, Wi-Fi local networks and terrestrial microwave networks.

""
""Dynamic memory "",,,,""Memory management is the act of managing computer memory at the system level. The essential requirement of memory management is to provide ways to dynamically allocate portions of memory to programs at their request, and free it for reuse when no longer needed. This is critical to any advanced computer system where more than a single process might be underway at any time.
Several methods have been devised that increase the effectiveness of memory management. Virtual memory systems separate the memory addresses used by a process from actual physical addresses, allowing separation of processes and increasing the effectively available amount of RAM using paging or swapping to secondary storage. The quality of the virtual memory manager can have an extensive effect on overall system performance.""
""Static memory "",,,,""Static memory allocation is the allocation of memory at compile-time before the associated program is executed, unlike dynamic memory allocation or automatic memory allocation where memory is allocated as required at run-time.
An application of this technique involves a program module (e.g. function or subroutine) declaring static data locally, such that these data are inaccessible in other modules unless references to it are passed as parameters or returned. A single copy of static data is retained and accessible through many calls to the function in which it is declared. Static memory allocation therefore has the advantage of modularising data within a program design in the situation where these data must be retained through the runtime of the program.
The use of static variables within a class in object oriented programming enables a single copy of such data to be shared between all the objects of that class.
Object constants known at compile-time, such as string literals, are usually allocated statically. In object-oriented programming, the virtual method tables of classes are usually allocated statically. A statically defined value can also be global in its scope ensuring the same immutable value is used throughout a run for consistency.

""
""Non-volatile memory "",,,,""Non-volatile memory, nonvolatile memory, NVM or non-volatile storage is computer memory that can retrieve stored information even after having been power cycled (turned off and back on). Examples of non-volatile memory include read-only memory, flash memory, ferroelectric RAM (F-RAM), most types of magnetic computer storage devices (e.g. hard disk drives, floppy disks, and magnetic tape), optical discs, and early computer storage methods such as paper tape and punched cards.
Non-volatile memory is typically used for the task of secondary storage, or long-term persistent storage. The most widely used form of primary storage today is a volatile form of random access memory (RAM), meaning that when the computer is shut down, anything contained in RAM is lost. However, most forms of non-volatile memory have limitations that make them unsuitable for use as primary storage. Typically, non-volatile memory costs more, provides lower performance, or has worse write endurance than volatile random access memory.
Non-volatile data storage can be categorized in electrically addressed systems (read-only memory) and mechanically addressed systems (hard disks, optical disc, magnetic tape, holographic memory, and such). Electrically addressed systems are expensive, but fast, whereas mechanically addressed systems have a low price per bit, but are slow. Non-volatile memory may one day eliminate the need for comparatively slow forms of secondary storage systems, which include hard disks.
Several companies are working on developing non-volatile memory systems comparable in speed and capacity to volatile RAM.""
""Read-only memory "",,,,""Read-only memory (ROM) is a type of non-volatile memory used in computers and other electronic devices. Data stored in ROM can only be modified slowly, with difficulty, or not at all, so it is mainly used to store firmware (software that is closely tied to specific hardware and unlikely to need frequent updates) or application software in plug-in cartridges.
Strictly, read-only memory refers to memory that is hard-wired, such as diode matrix and the later mask ROM (MROM) which cannot be changed after manufacture. Although discrete circuits can be altered in principle, integrated circuits (ICs) cannot, and are useless if the data is bad or requires an update. That such memory can never be changed is a disadvantage in many applications, as bugs and security issues cannot be fixed, and new features cannot be added.
More recently, ROM has come to include memory that is read-only in normal operation, but can still be reprogrammed in some way. Erasable programmable read-only memory (EPROM) and electrically erasable programmable read-only memory (EEPROM) can be erased and re-programmed, but usually this can only be done at relatively slow speeds, may require special equipment to achieve, and is typically only possible a certain number of times.""
""Transistors "",,,,""A transistor is a semiconductor device used to amplify or switch electronic signals and electrical power. It is composed of semiconductor material with at least three terminals for connection to an external circuit. A voltage or current applied to one pair of the transistor's terminals changes the current through another pair of terminals. Because the controlled (output) power can be higher than the controlling (input) power, a transistor can amplify a signal. Today, some transistors are packaged individually, but many more are found embedded in integrated circuits.
The transistor is the fundamental building block of modern electronic devices, and is ubiquitous in modern electronic systems. First conceived by Julius Lilienfeld in 1926  and practically implemented in 1947 by American physicists John Bardeen, Walter Brattain, and William Shockley, the transistor revolutionized the field of electronics, and paved the way for smaller and cheaper radios, calculators, and computers, among other things. The transistor is on the list of IEEE milestones in electronics, and Bardeen, Brattain, and Shockley shared the 1956 Nobel Prize in Physics for their achievement.

""
""Logic circuits "",,,,""In electronics, a logic gate is an idealized or physical device implementing a Boolean function; that is, it performs a logical operation on one or more logical inputs, and produces a single logical output. Depending on the context, the term may refer to an ideal logic gate, one that has for instance zero rise time and unlimited fan-out, or it may refer to a non-ideal physical device (see Ideal and real op-amps for comparison).
Logic gates are primarily implemented using diodes or transistors acting as electronic switches, but can also be constructed using vacuum tubes, electromagnetic relays (relay logic), fluidic logic, pneumatic logic, optics, molecules, or even mechanical elements. With amplification, logic gates can be cascaded in the same way that Boolean functions can be composed, allowing the construction of a physical model of all of Boolean logic, and therefore, all of the algorithms and mathematics that can be described with Boolean logic.
Logic circuits include such devices as multiplexers, registers, arithmetic logic units (ALUs), and computer memory, all the way up through complete microprocessors, which may contain more than 100 million gates. In modern practice, most gates are made from field-effect transistors (FETs), particularly MOSFETs (metal–oxide–semiconductor field-effect transistors).
Compound logic gates AND-OR-Invert (AOI) and OR-AND-Invert (OAI) are often employed in circuit design because their construction using MOSFETs is simpler and more efficient than the sum of the individual gates.
In reversible logic, Toffoli gates are used.""
""Asynchronous circuits "",,,,""An asynchronous circuit, or self-timed circuit, is a sequential digital logic circuit which is not governed by a clock circuit or global clock signal. Instead they often use signals that indicate completion of instructions and operations, specified by simple data transfer protocols. This type is contrasted with a synchronous circuit in which changes to the signal values in the circuit are triggered by repetitive pulses called a clock signal. Most digital devices today use synchronous circuits. However asynchronous circuits have the potential to be faster, and may also have advantages in lower power consumption, lower electromagnetic interference, and better modularity in large systems. Asynchronous circuits are an active area of research in digital logic design.""
""Design modules and hierarchy "",,,,""Software design is the process by which an agent creates a specification of a software artifact, intended to accomplish goals, using a set of primitive components and subject to constraints. Software design may refer to either "all the activities involved in conceptualizing, framing, implementing, commissioning, and ultimately modifying complex systems" or "the activity following requirements specification and before programming, as ... [in] a stylized software engineering process."
Software design usually involves problem solving and planning a software solution. This includes both low-level component and algorithm design and high-level, architecture design.
^ Ralph, P. and Wand, Y. (2009). A proposal for a formal definition of the design concept. In Lyytinen, K., Loucopoulos, P., Mylopoulos, J., and Robinson, W., editors, Design Requirements Workshop (LNBIP 14), pp. 103–136. Springer-Verlag, p. 109 doi:10.1007/978-3-540-92966-6_6.
^ Freeman, Peter; David Hart (2004). "A Science of design for software-intensive systems". Communications of the ACM 47 (8): 19–21 [20]. doi:10.1145/1012037.1012054.""
""Finite state machines "",,,,""A finite-state machine (FSM) or finite-state automaton (plural: automata), or simply a state machine, is a mathematical model of computation used to design both computer programs and sequential logic circuits. It is conceived as an abstract machine that can be in one of a finite number of states. The machine is in only one state at a time; the state it is in at any given time is called the current state. It can change from one state to another when initiated by a triggering event or condition; this is called a transition. A particular FSM is defined by a list of its states, and the triggering condition for each transition.
The behavior of state machines can be observed in many devices in modern society that perform a predetermined sequence of actions depending on a sequence of events with which they are presented. Simple examples are vending machines, which dispense products when the proper combination of coins is deposited, elevators, which drop riders off at upper floors before going down, traffic lights, which change sequence when cars are waiting, and combination locks, which require the input of combination numbers in the proper order.
Finite-state machines can model a large number of problems, among which are electronic design automation, communication protocol design, language parsing and other engineering applications. In biology and artificial intelligence research, state machines or hierarchies of state machines have been used to describe neurological systems. In linguistics, they are used to describe simple parts of the grammars of natural languages.
Considered as an abstract model of computation, the finite state machine is weak; it has less computational power than some other models of computation such as the Turing machine. That is, there are tasks that no FSM can do, but some Turing machines can. This is because the FSM memory is limited by the number of states.
FSMs are studied in the more general field of automata theory.""
""Sequential circuits "",,,,""Sequential Circuits Inc. (SCI) was a California-based synthesizer company that was founded in the early 1970s by Dave Smith, and sold to Yamaha Corporation in 1987. Throughout its lifespan, Sequential pioneered technologies and design principles that have served as a foundation for the development of modern music technology. Sequential was also pivotal in the planning, design, and support of 1982's groundbreaking innovation in electronic music, MIDI.
Following the purchase of Sequential Circuits by Yamaha, Dave Smith continued to develop musical instruments through Dave Smith Instruments. In January 2015, he reacquired the rights to use the Sequential brand name from Yamaha president Takuya Nakata.""
""Hardware accelerators "",,,,""In computing, hardware acceleration is the use of computer hardware to perform some functions faster than is possible in software running on a more general-purpose CPU. Examples of hardware acceleration include blitting acceleration functionality in graphics processing units (GPUs) and regular expression hardware acceleration for spam control in the server industry.
Normally, processors are sequential, and instructions are executed one by one. Various techniques are used to improve performance; hardware acceleration is one of them. The main difference between hardware and software is concurrency, allowing hardware to be much faster than software. Hardware accelerators are designed for computationally intensive software code. Depending upon granularity, hardware acceleration can vary from a small functional unit to a large functional block (like motion estimation in MPEG-2).
The hardware that performs the acceleration, when in a separate unit from the CPU, is referred to as a hardware accelerator, or often more specifically as a 3D accelerator, cryptographic accelerator, etc. Those terms, however, are older and have been replaced with less descriptive terms like video card or network adapter.
In the hierarchy of general-purpose processors such as CPUs, more specialized processors such as GPUs, fixed-function implemented on FPGAs, and fixed-function implemented on ASICs; there is a tradeoff between flexibility and efficiency, with efficiency increasing by orders of magnitude when any given application is implemented higher up that hierarchy.""
""High-speed input / output "",,,,""The AN/FSQ-32 SAGE Solid State Computer (AN/FSQ-7A before December 1958, colloq. "Q-32") was a planned military computer central for deployment to Super Combat Centers in nuclear bunkers and to some above-ground military installations. In 1958, Air Defense Command planned to acquire 13 Q-32 centrals for Air Divisions/Sectors at Ottawa, St Louis, San Antonio, Raleigh, Syracuse, Chicago, Spokane, Minot, Portland, Phoenix, Miami (above-ground), Albuquerque (above-ground), and Shreveport (above-ground). (During 1959 SAGE/FAA "boundary alignments", the total was reduced to 12.**)

""
""Programmable logic elements "",,,,""A programmable logic device (PLD) is an electronic component used to build reconfigurable digital circuits. Unlike a logic gate, which has a fixed function, a PLD has an undefined function at the time of manufacture. Before the PLD can be used in a circuit it must be programmed, that is, reconfigured.""
""Programmable interconnect "",,,,""Field Programmable Nanowire Interconnect (often abbreviated FPNI) is a new computer architecture developed by Hewlett-Packard. This is a defect-tolerant architecture, using the results of the Teramac experiment.
Details: The design combines a nanoscale crossbar switch structure with conventional CMOS to create a hybrid chip that is simpler to fabricate and offers greater flexibility in the choice of nanoscale devices. The FPNI improves on a field-programmable gate array (FPGA) architecture by lifting the configuration bit and associated components out of the semiconductor plane and replacing them in the interconnect with nonvolatile switches, which decreases both the area and power consumption of the circuit -- while providing up to eight times the density at less cost. This is an example of a more comprehensive strategy for improving the efficiency of existing semiconductor technology: placing a level of intelligence and configurability in the interconnect can have a profound effect on integrated circuit performance, and can be used to significantly extend Moore's Law without having to shrink the transistors.""
""Reconfigurable logic applications "",,,,""Reconfigurable computing is a computer architecture combining some of the flexibility of software with the high performance of hardware by processing with very flexible high speed computing fabrics like field-programmable gate arrays (FPGAs). The principal difference when compared to using ordinary microprocessors is the ability to make substantial changes to the datapath itself in addition to the control flow. On the other hand, the main difference with custom hardware, i.e. application-specific integrated circuits (ASICs) is the possibility to adapt the hardware during runtime by "loading" a new circuit on the reconfigurable fabric.""
""3D integrated circuits "",,,,""In electronic engineering, a through-silicon via (TSV) is a vertical electrical connection (via) passing completely through a silicon wafer or die. TSVs are a high performance interconnect technique used as an alternative to wire-bond and flip chips to create 3D packages and 3D integrated circuits, compared to alternatives such as package-on-package, because the density of the vias is substantially higher, and because the length of the connections is shorter.
Using TSVs it is possible to achieve connections through the FEOL.""
""Data conversion "",,,,""Data conversion is the conversion of computer data from one format to another. Throughout a computer environment, data is encoded in a variety of ways. For example, computer hardware is built on the basis of certain standards, which requires that data contains, for example, parity bit checks. Similarly, the operating system is predicated on certain standards for data and file handling. Furthermore, each computer program handles data in a different manner. Whenever any one of these variables is changed, data must be converted in some way before it can be used by a different computer, operating system or program. Even different versions of these elements usually involve different data structures. For example, the changing of bits from one format to another, usually for the purpose of application interoperability or of capability of using new features, is merely a data conversion. Data conversions may be as simple as the conversion of a text file from one character encoding system to another; or more complex, such as the conversion of office file formats, or the conversion of image and audio file formats.
There are many ways in which data is converted within the computer environment. This may be seamless, as in the case of upgrading to a newer version of a computer program. Alternatively, the conversion may require processing by the use of a special conversion program, or it may involve a complex process of going through intermediary stages, or involving complex "exporting" and "importing" procedures, which may include converting to and from a tab-delimited or comma-separated text file. In some cases, a program may recognize several data file formats at the data input stage and then is also capable of storing the output data in a number of different formats. Such a program may be used to convert a file format. If the source format or target format is not recognized, then at times a third program may be available which permits the conversion to an intermediate format, which can then be reformatted using the first program. There are many possible scenarios.""
""Clock generation and timing "",,,,""In electronics and especially synchronous digital circuits, a clock signal is a particular type of signal that oscillates between a high and a low state and is utilized like a metronome to coordinate actions of digital circuits.
A clock signal is produced by a clock generator. Although more complex arrangements are used, the most common clock signal is in the form of a square wave with a 50% duty cycle, usually with a fixed, constant frequency. Circuits using the clock signal for synchronization may become active at either the rising edge, falling edge, or, in the case of double data rate, both in the rising and in the falling edges of the clock cycle.""
""Analog and mixed-signal circuit optimization "",,,,""A list of electronic design automation (EDA) companies.""
""Radio frequency and wireless circuits "",,,,""Radio frequency (RF) is any of the electromagnetic wave frequencies that lie in the range extending from around 7003300000000000000♠3 kHz to 7011300000000000000♠300 GHz, which include those frequencies used for communications or radar signals. RF usually refers to electrical rather than mechanical oscillations. However, mechanical RF systems do exist (see mechanical filter and RF MEMS).
Although radio frequency is a rate of oscillation, the term "radio frequency" or its abbreviation "RF" are used as a synonym for radio – i.e., to describe the use of wireless communication, as opposed to communication via electric wires. Examples include:
Radio-frequency identification
ISO/IEC 14443-2 Radio frequency power and signal interface""
""Wireline communication "",,,,""A second responder is a worker who supports "first responders" such as police, fire, and emergency medical personnel. They are involved in preparing, managing, returning services, and cleaning up sites during and after an event requiring first responders. These sites may include crime scenes and areas damaged by fire, storm, wind, floods, earthquakes, or other natural disasters. These types of services may include utility services (shutdown or reinstatement of electrical, gas, sewage, and/or water services), wireless (3G/4G/WiFi) or wireline communication services, specialty construction (i.e. shelter construction), hazardous waste cleanup, road clearing, crowd control, emergency services (i.e. Red Cross), first aid, food services, security services, social services (i.e., trauma counselors), and sanitation.
The overriding objective of these designated professionals is to quickly enable people to get back to work and ensure the viability, continuity and recovery of economic life for public and private-sector organizations. It is believed that in a major event there might be as many as 3 to 10 second responders for every first responder. Coordinating the activities of all second responders is a communications intensive activity usually the responsibility of the on-site Incident Commander. The guidelines and responsibilities of the Incident Commander are described in the U.S. Federal Emergency Management Agency National Incident Management System (NIMS) training program.
Because identifying and empowering second responders helps make the difference between lingering disruption and the necessary and timely restoration of daily life, many state and local governments are adopting the unique “second responder” identification protocol known as the Corporate Emergency Access System (CEAS).
CEAS was developed in New York State during the 1990s by the Business Network of Emergency Resources, a not-for-profit organization which pioneered this emergency-identification-card-based capability.
^ http://training.fema.gov/EMIWeb/IS/is100b.asp
^ Business Network of Emergency Resources, Inc""
""Analog and mixed-signal circuit synthesis "",,,,""Magma Design Automation was a software company in the electronic design automation (EDA) industry. The company was founded in 1997 and maintained headquarters in San Jose, California, with facilities throughout North America, Europe, Japan, Asia and India. Magma software products were used in major elements of chip development, including: synthesis, placement, routing, power management, circuit simulation, verification and analog/mixed-signal design.
Magma was acquired by Synopsys in a merger finalized February 22, 2012 at a cash value of about $523 million, or $7.35 per Magma share.
^ "Synopsys Completes Acquisition of Magma Design Automation,” February 22, 2012 [1]""
""Application specific integrated circuits "",,,,""An application-specific integrated circuit (ASIC) /ˈeɪsɪk/, is an integrated circuit (IC) customized for a particular use, rather than intended for general-purpose use. For example, a chip designed to run in a digital voice recorder or a high-efficiency Bitcoin miner is an ASIC. Application-specific standard products (ASSPs) are intermediate between ASICs and industry standard integrated circuits like the 7400 or the 4000 series.
As feature sizes have shrunk and design tools improved over the years, the maximum complexity (and hence functionality) possible in an ASIC has grown from 5,000 gates to over 100 million. Modern ASICs often include entire microprocessors, memory blocks including ROM, RAM, EEPROM, flash memory and other large building blocks. Such an ASIC is often termed a SoC (system-on-chip). Designers of digital ASICs often use a hardware description language (HDL), such as Verilog or VHDL, to describe the functionality of ASICs.
Field-programmable gate arrays (FPGA) are the modern-day technology for building a breadboard or prototype from standard parts; programmable logic blocks and programmable interconnects allow the same FPGA to be used in many different applications. For smaller designs or lower production volumes, FPGAs may be more cost effective than an ASIC design even in production. The non-recurring engineering (NRE) cost of an ASIC can run into the millions of dollars.

""
""Application specific instruction set processors "",,,,""An application-specific instruction set processor (ASIP) is a component used in system-on-a-chip design. The instruction set of an ASIP is tailored to benefit a specific application. This specialization of the core provides a tradeoff between the flexibility of a general purpose CPU and the performance of an ASIC.
Some ASIPs have a configurable instruction set. Usually, these cores are divided into two parts: static logic which defines a minimum ISA (instruction-set architecture) and configurable logic which can be used to design new instructions. The configurable logic can be programmed either in the field in a similar fashion to an FPGA or during the chip synthesis.
ASIPs can be used as an alternative of hardware accelerators for baseband signal processing or video coding. The traditional hardware accelerators for the baseband or multimedia suffer from inflexibility. It is very difficult to reuse the hardware datapath with handwritten finite-state machines (FSM). The retargetable compilers of ASIPs help the designer to update the program and reuse the datapath. Typically, the ASIP design is more or less dependent on the tool flow because designing a processor from the scratch can be very complicated. There are some commercial tools to design ASIPs, for example, Processor Designer from Synopsys. There is an open source tool as well, TTA-based codesign environment (TCE).
^ Shahabuddin, Shahriar et al., "Design of a transport triggered vector processor for turbo Decoding", in Springer Journal of Analog Integrated Circuits and Signal Processing, March 2014.
^ Hautala, Ilkka, et al. "Programmable Low-Power Multicore Coprocessor Architecture for HEVC/H.265 In-Loop Filtering" in IEEE Transactions on Circuits and Systems for Video Technology, November 2014""
""Network on chip "",,,,""Network on chip or network on a chip (NoC or NOC) is a communication subsystem on an integrated circuit (commonly called a "chip"), typically between intellectual property (IP) cores in a system on a chip (SoC). NoCs can span synchronous and asynchronous clock domains or use unclocked asynchronous logic. NoC technology applies networking theory and methods to on-chip communication and brings notable improvements over conventional bus and crossbar interconnections. NoC improves the scalability of SoCs, and the power efficiency of complex SoCs compared to other designs.""
""System on a chip "",,,,""A system on a chip or system on chip (SoC or SOC) is an integrated circuit (IC) that integrates all components of a computer or other electronic system into a single chip. It may contain digital, analog, mixed-signal, and often radio-frequency functions—all on a single chip substrate. SoCs are very common in the mobile electronics market because of their low power consumption. A typical application is in the area of embedded systems.
The contrast with a microcontroller is one of degree. Microcontrollers typically have under 100 kB of RAM (often just a few kilobytes) and often really are single-chip-systems, whereas the term SoC is typically used for more powerful processors, capable of running software such as the desktop versions of Windows and Linux, which need external memory chips (flash, RAM) to be useful, and which are used with various external peripherals. In short, for larger systems, the term system on a chip is hyperbole, indicating technical direction more than reality: a high degree of chip integration, leading toward reduced manufacturing costs, and the production of smaller systems. Many systems are too complex to fit on just one chip built with a processor optimized for just one of the system's tasks.
When it is not feasible to construct a SoC for a particular application, an alternative is a system in package (SiP) comprising a number of chips in a single package. In large volumes, SoC is believed to be more cost-effective than SiP since it increases the yield of the fabrication and because its packaging is simpler.
Another option, as seen for example in higher end cell phones is package on package stacking during board assembly. The SoC chip includes processors and numerous digital peripherals, and comes in a ball grid package with lower and upper connections. The lower balls connect to the board and various peripherals, with the upper balls in a ring holding the memory buses used to access NAND flash and DDR2 RAM. Memory packages could come from multiple vendors.""
""Platform-based design "",,,,""Platform-based design is defined in Taxonomies for the Development and Verification of Digital Systems as: "an integration oriented design approach emphasising systematic reuse, for developing complex products based upon platforms and compatible hardware and software virtual component, intended to reduce development risks, costs and time to market".
^ Brian Bailey, Grant Martin and Thomas Anderson, Taxonomies for the Development and Verification of Digital Systems, Springer (2005) ISBN 0-387-24019-5""
""Hard and soft IP "",,,,""In electronic design a semiconductor intellectual property core, IP core, or IP block is a reusable unit of logic, cell, or chip layout design that is the intellectual property of one party. IP cores may be licensed to another party or can be owned and used by a single party alone. The term is derived from the licensing of the patent and/or source code copyright that exist in the design. IP cores can be used as building blocks within ASIC chip designs or FPGA logic designs.""
""Design rules "",,,,""Design Rule Checking or Check(s) (DRC) is the area of Electronic Design Automation that determines whether the physical layout of a particular chip layout satisfies a series of recommended parameters called Design Rules. Design rule checking is a major step during Physical verification signoff on the design, which also involves LVS (Layout versus schematic) Check, XOR Checks, ERC (Electrical Rule Check) and Antenna Checks. For advanced processes some fabs also insist upon the use of more restricted rules to improve yield.

""
""Economics of chip design and manufacturing "",,,,""Processor design is the design engineering task of creating a microprocessor, a component of computer hardware. It is a subfield of electronics engineering and computer engineering. The design process involves choosing an instruction set and a certain execution paradigm (e.g. VLIW or RISC) and results in a microarchitecture described in e.g. VHDL or Verilog. This description is then manufactured employing some of the various semiconductor device fabrication processes. This results in a die which is bonded onto some chip carrier. This chip carrier is then soldered onto some printed circuit board (PCB).
The mode of operation of any microprocessor is the execution of lists of instructions. Instructions typically include those to compute or manipulate data values using registers, change or retrieve values in read/write memory, perform relational tests between data values and to control program flow.""
""Full-custom circuits "",,,,""Cadence Design Systems, Inc (NASDAQ: CDNS) is an American electronic design automation (EDA) software and engineering services company, founded in 1988 by the merger of SDA Systems and ECAD, Inc. The company produces software and hardware for designing integrated circuits, systems on chips (SoCs) and printed circuit boards.""
""VLSI design manufacturing considerations "",,,,""Design closure is the process by which a VLSI design is modified from its initial description to meet a growing list of design constraints and objectives.
Every step in the IC design (such as static timing analysis, placement, routing, and so on) is already complex and often forms its own field of study. This article, however, looks at the overall design closure process, which takes a chip from its initial design state to the final form in which all of its design constraints are met.""
""On-chip resource management "",,,,""Waste management is all those activities and action required to manage waste from its inception to its final disposal. This includes amongst other things, collection, transport, treatment and disposal of waste together with monitoring and regulation. It also encompasses the legal and regulatory framework that relates to waste management encompassing guidance on recycling etc.
The term usually relates to all kinds of waste, whether generated during the extraction of raw materials, the processing of raw materials into intermediate and final products, the consumption of final products, or other human activities, including municipal (residential, institutional, commercial), agricultural, and special (health care, household hazardous wastes, sewage sludge). Waste management is intended to reduce adverse effects of waste on health, the environment or aesthetics.
Waste management practices are not uniform among countries (developed and developing nations); regions (urban and rural area), and sectors (residential and industrial).""
""Standard cell libraries "",,,,""Full-custom design is a methodology for designing integrated circuits by specifying the layout of each individual transistor and the interconnections between them. Alternatives to full-custom design include various forms of semi-custom design, such as the repetition of small transistor subcircuits; one such methodology is the use of standard cell libraries (standard cell libraries are themselves designed using full-custom design techniques).
Full-custom design potentially maximizes the performance of the chip, and minimizes its area, but is extremely labor-intensive to implement. Full-custom design is limited to ICs that are to be fabricated in extremely high volumes, notably certain microprocessors and a small number of ASICs.
The main factor affecting the design and production of ASICs is the high cost of mask sets and the requisite EDA design tools. The mask sets are required in order to transfer the ASIC designs onto the wafer.""
""Input / output styles "",,,,""In economics, an input–output model is a quantitative economic technique that represents the interdependencies between different branches of a national economy or different regional economies.
The model depicts inter-industry relationships within an economy, showing how output from one industrial sector may become an input to another industrial sector. In the inter-industry matrix, column entries typically represent inputs to an industrial sector, while row entries represent outputs from a given sector. This format therefore shows how dependent each sector is on every other sector, both as a customer of outputs from other sectors and as a supplier of inputs. Each column of the input–output matrix shows the monetary value of inputs to each sector and each row represents the value of each sector's outputs.
Wassily Leontief (1906–1999) is credited with developing this type of analysis and earned the Nobel Prize in Economics for his development of this model.""
""Package-level interconnect "",,,,""Electromigration is the transport of material caused by the gradual movement of the ions in a conductor due to the momentum transfer between conducting electrons and diffusing metal atoms. The effect is important in applications where high direct current densities are used, such as in microelectronics and related structures. As the structure size in electronics such as integrated circuits (ICs) decreases, the practical significance of this effect increases.""
""Temperature monitoring "",,,,""A temperature data logger, also called temperature monitor, is a portable measurement instrument that is capable of autonomously recording temperature over a defined period of time. The digital data can be retrieved, viewed and evaluated after it has been recorded. A data logger is commonly used to monitor shipments in a cold chain and to gather temperature data from diverse field conditions.""
""Temperature simulation and estimation "",,,,""Process simulation is used for the design, development, analysis, and optimization of technical processes such as: chemical plants, chemical processes, environmental systems, power stations, complex manufacturing operations, biological processes, and similar technical functions.""
""Temperature control "",,,,""Temperature control is a process in which change of temperature of a space (and objects collectively there within) is measured or otherwise detected, and the passage of heat energy into or out of the space is adjusted to achieve a desired average temperature.

""
""Temperature optimization "",,,,""Mycoprotein, also known as fungal protein, is defined in the Oxford English Dictionary as "the albuminoid which is the principal constituent of the protoplasm of the cell." "Myco" is from the Greek word for "fungus". Mycoprotein is a form of single-cell protein and was first produced in the early 1980s.
Mycoprotein means protein from fungi. The main mycoprotein on sale in Europe and North America is called Quorn. It was originally developed as a food source to combat food shortages. All Quorn products contain mycoprotein derived from the fungus Fusarium venenatum. The fungus is grown in vats using glucose syrup as food. A fermentation vat is filled with the growth medium and then inoculated with the fungal spores. The F. venenatum culture respires aerobically, so for it to grow at an optimum rate, it is supplied with oxygen, and carbon dioxide is drawn from the vat. To make protein, nitrogen (in the form of ammonia) is added and vitamins and minerals are needed to support growth. The vat is kept at a constant temperature, also optimized for growth; the fungus can double its mass every five hours.
When the desired amount of mycoprotein has been created, the growth medium is drawn off from a tap at the bottom of the fermenter. The mycoprotein is separated and purified. It is a pale yellow solid with a faint taste of mushrooms. Different flavors and tastes can be added to the mycoprotein to add variety.
About one in 140,000 consumers is sensitive to mycoproteins. The Center for Science in the Public Interest claims this may result in "vomiting, nausea, diarrhea, hives and potentially fatal anaphylactic reactions." For comparison, per 140,000 people, about 70 have allergic reactions to peanuts according to data from the National Health Service of the United Kingdom.""
""Fuel-based energy "",,,,""This article discuss the development and implementation of wind power in Iran. Iran has relied primarily on a fossil fuel based energy sector to power its country, however in the last decade Iran has made steps to decrease its dependency on fossil fuels by investing in the renewable energy of wind power. This article discuss the process as well as the reasons for Iran's interest in renewable energy; the positive and negative effects as a result of the presence of wind power; the political dynamics occurring in Iran; and its attempt to also invest in nuclear energy as well.""
""Renewable energy "",,,,""Renewable energy is generally defined as energy that is collected from resources which are naturally replenished on a human timescale, such as sunlight, wind, rain, tides, waves, and geothermal heat. Renewable energy often provides energy in four important areas: electricity generation, air and water heating/cooling, transportation, and rural (off-grid) energy services.
Based on REN21's 2014 report, renewables contributed 19 percent to humans' global energy consumption and 22 percent to their generation of electricity in 2012 and 2013, respectively. This energy consumption is divided as 9% coming from traditional biomass, 4.2% as heat energy (non-biomass), 3.8% hydro electricity and 2% is electricity from wind, solar, geothermal, and biomass. Worldwide investments in renewable technologies amounted to more than US$214 billion in 2013, with countries like China and the United States heavily investing in wind, hydro, solar and biofuels.
Renewable energy resources exist over wide geographical areas, in contrast to other energy sources, which are concentrated in a limited number of countries. Rapid deployment of renewable energy and energy efficiency is resulting in significant energy security, climate change mitigation, and economic benefits. The results of a recent review of the literature  concluded that as greenhouse gas(GHG) emitters begin to be held liable for damages resulting from GHG emissions resulting in climate change, a high value for liability mitigation would provide powerful incentives for deployment of renewable energy technologies. In international public opinion surveys there is strong support for promoting renewable sources such as solar power and wind power. At the national level, at least 30 nations around the world already have renewable energy contributing more than 20 percent of energy supply. National renewable energy markets are projected to continue to grow strongly in the coming decade and beyond. Some places and at least two countries, Iceland and Norway generate all their electricity using renewable energy already, and many other countries have the set a goal to reach 100% renewable energy in the future. For example, in Denmark the government decided to switch the total energy supply (electricity, mobility and heating/cooling) to 100% renewable energy by 2050.
While many renewable energy projects are large-scale, renewable technologies are also suited to rural and remote areas and developing countries, where energy is often crucial in human development. United Nations' Secretary-General Ban Ki-moon has said that renewable energy has the ability to lift the poorest nations to new levels of prosperity. As most of renewables provide electricity, renewable energy deployment is often applied in conjunction with further electrification, which has several benefits: For example, electricity can be converted to heat without losses and even reach higher temperatures than fossil fuels, can be converted into mechanical energy with high efficiency and is clean at the point of consumpion. In addition to that electrification with renewable energy is much more efficient and therefore leads to a significant reduction in primary energy requirements, because most renewables don't have a steam cycle with high losses (fossil power plants usually have losses of 40 to 65%).""
""Reusable energy storage "",,,,""Reusable packaging is manufactured of durable materials and is specifically designed for multiple trips and extended life. A reusable package or container is “designed for reuse without impairment of its protective function.”  The term returnable is sometimes used interchangeably but it can also include returning packages or components for other than reuse: recycling, disposal, incineration, etc. Typically, the materials used to make returnable packaging include steel, wood, polypropylene sheets or other plastic materials.
Reusability of packaging is an important consideration of the environmental credo of “reduce, reuse, and recycle”. It is also important to the movement toward more sustainable packaging. Returnable packaging is encouraged by regulators.""
""Energy metering "",,,,""An electricity meter, electric meter, electrical meter, or energy meter is a device that measures the amount of electric energy consumed by a residence, a business, or an electrically powered device.
Electric utilities use electric meters installed at customers' premises to measure electric energy delivered to their customers for billing purposes. They are typically calibrated in billing units, the most common one being the kilowatt hour [kWh]. They are usually read once each billing period.
When energy savings during certain periods are desired, some meters may measure demand, the maximum use of power in some interval. "Time of day" metering allows electric rates to be changed during a day, to record usage during peak high-cost periods and off-peak, lower-cost, periods. Also, in some areas meters have relays for demand response load shedding during peak load periods.""
""Power conversion "",,,,""In electrical engineering, power engineering and the electric power industry, power conversion is converting electric energy from one form to another, converting between AC and DC, or just changing the voltage or frequency, or some combination of these. A power converter is an electrical or electro-mechanical device for converting electrical energy. This could be as simple as a transformer to change the voltage of AC power, but also includes far more complex systems. The term can also refer to a class of electrical machinery that is used to convert one frequency of alternating current into another frequency.
Power conversion systems often incorporate redundancy and voltage regulation.
One way of classifying power conversion systems is according to whether the input and output are alternating current (AC) or direct current (DC), thus:
There are also devices and methods to convert between power systems designed for single and three-phase operation.
The standard power frequency varies from country to country, and sometimes within a country. In North America and northern South America it is usually 60 hertz (Hz), but in many other parts of the world, is usually 50 Hz. Aircraft often use 400 Hz power, so 50 Hz or 60 Hz to 400 Hz frequency conversion is needed for use in the ground power unit used to power the airplane while it is on the ground.
Certain specialized circuits, such as the flyback transformer for a CRT, can also be considered power converters.
Consumer electronics usually include an AC adapter (a type of power supply) to convert mains-voltage AC current to low-voltage DC suitable for consumption by microchips. Consumer voltage converters (also known as "travel converters") are used when travelling between countries that use ~120 V vs. ~240 V AC mains power. (There are also consumer "adapters" which merely form an electrical connection between two differently shaped AC power plugs and sockets, but these change neither voltage nor frequency.)""
""Power networks "",,,,""UK Power Networks is a distribution network operator for electricity covering South East England, the East of England and London. It manages three licensed distribution networks (Eastern Power Networks PLC, South Eastern Power Networks PLC and London Power Networks PLC) which together cover an area of 30000 square kilometres and approximately eight million customers.
In 2014 UK Power Networks was awarded £25 million from the electricity regulator Ofgem's Low Carbon Networks Fund for the Low Carbon London project. In 2011 it was awarded £6.7 million by Ofgem for another project, Flexible Plug and Play, which is researching new ways, technical and commercial, to connect renewable energy to the distribution network in Cambridgeshire. As well as the three distribution arms UK Power Networks also operates UK Power Networks Services Holdings Limited, which develops and maintains electrical networks for clients including London Underground, Heathrow and Stansted airports, Docklands Light Railway and Canary Wharf.""
""Smart grid "",,,,""A smart grid is an electrical grid which includes a variety of operational and energy measures including smart meters, smart appliances, renewable energy resources, and energy efficiency resources. Electronic power conditioning and control of the production and distribution of electricity are important aspects of the smart grid.
Smart grid policy is organized in Europe as Smart Grid European Technology Platform. Policy in the United States is described in 42 U.S.C. ch. 152, subch. IX § 17381.
Roll-out of smart grid technology also implies a fundamental re-engineering of the electricity services industry, although typical usage of the term is focused on the technical infrastructure.""
""Impact on the environment "",,,,""Human impact on the environment or anthropogenic impact on the environment includes impacts on biophysical environments, biodiversity, and other resources. The term anthropogenic designates an effect or object resulting from human activity. The term was first used in the technical sense by Russian geologist Alexey Pavlov, and was first used in English by British ecologist Arthur Tansley in reference to human influences on climax plant communities. The atmospheric scientist Paul Crutzen introduced the term "anthropocene" in the mid-1970s. The term is sometimes used in the context of pollution emissions that are produced as a result of human activities but applies broadly to all major human impacts on the environment.

""
""Switching devices power issues "",,,,""Network switching subsystem (NSS) (or GSM core network) is the component of a GSM system that carries out call switching and mobility management functions for mobile phones roaming on the network of base stations. It is owned and deployed by mobile phone operators and allows mobile devices to communicate with each other and telephones in the wider public switched telephone network (PSTN). The architecture contains specific features and functions which are needed because the phones are not fixed in one location.
The NSS originally consisted of the circuit-switched core network, used for traditional GSM services such as voice calls, SMS, and circuit switched data calls. It was extended with an overlay architecture to provide packet-switched data services known as the GPRS core network. This allows mobile phones to have access to services such as WAP, MMS and the Internet.""
""Interconnect power issues "",,,,""PCI Express (Peripheral Component Interconnect Express), officially abbreviated as PCIe, is a high-speed serial computer expansion bus standard, designed to replace the older PCI, PCI-X, and AGP bus standards. PCIe has numerous improvements over the older standards, including higher maximum system bus throughput, lower I/O pin count and smaller physical footprint, better performance scaling for bus devices, a more detailed error detection and reporting mechanism (Advanced Error Reporting, AER), and native hot-plug functionality. More recent revisions of the PCIe standard provide hardware support for I/O virtualization.
The PCI Express electrical interface is also used in a variety of other standards, most notably in ExpressCard as a laptop expansion card interface, and in SATA Express as a computer storage interface.
Format specifications are maintained and developed by the PCI-SIG (PCI Special Interest Group), a group of more than 900 companies that also maintain the conventional PCI specifications. PCIe 3.0 is the latest standard for expansion cards that is in production and available on mainstream personal computers.""
""Chip-level power issues "",,,,""Simultaneous multithreading (SMT) is a technique for improving the overall efficiency of superscalar CPUs with hardware multithreading. SMT permits multiple independent threads of execution to better utilize the resources provided by modern processor architectures.""
""Platform power issues "",,,,""In computing, cross-platform, multi-platform, or platform independent, is an attribute conferred to computer software or computing methods and concepts that are implemented and inter-operate on multiple computing platforms. Cross-platform software may be divided into two types; one requires individual building or compilation for each platform that it supports, and the other one can be directly run on any platform without special preparation, e.g., software written in an interpreted language or pre-compiled portable bytecode for which the interpreters or run-time packages are common or standard components of all platforms.
For example, a cross-platform application may run on Microsoft Windows on the x86 architecture, Linux on the x86 architecture and Mac OS X on either the PowerPC or x86-based Apple Macintosh systems. Cross-platform programs may run on as many as all existing platforms, or on as few as two platforms.
^ "Design Guidelines: Glossary". java.sun.com. Retrieved 2011-10-19. 
^ "Encyclopedia > cross platform". PC MAgazine Encyclopedia. Retrieved 2011-10-19.""
""Datapath optimization "",,,,""An application-specific instruction set processor (ASIP) is a component used in system-on-a-chip design. The instruction set of an ASIP is tailored to benefit a specific application. This specialization of the core provides a tradeoff between the flexibility of a general purpose CPU and the performance of an ASIC.
Some ASIPs have a configurable instruction set. Usually, these cores are divided into two parts: static logic which defines a minimum ISA (instruction-set architecture) and configurable logic which can be used to design new instructions. The configurable logic can be programmed either in the field in a similar fashion to an FPGA or during the chip synthesis.
ASIPs can be used as an alternative of hardware accelerators for baseband signal processing or video coding. The traditional hardware accelerators for the baseband or multimedia suffer from inflexibility. It is very difficult to reuse the hardware datapath with handwritten finite-state machines (FSM). The retargetable compilers of ASIPs help the designer to update the program and reuse the datapath. Typically, the ASIP design is more or less dependent on the tool flow because designing a processor from the scratch can be very complicated. There are some commercial tools to design ASIPs, for example, Processor Designer from Synopsys. There is an open source tool as well, TTA-based codesign environment (TCE).
^ Shahabuddin, Shahriar et al., "Design of a transport triggered vector processor for turbo Decoding", in Springer Journal of Analog Integrated Circuits and Signal Processing, March 2014.
^ Hautala, Ilkka, et al. "Programmable Low-Power Multicore Coprocessor Architecture for HEVC/H.265 In-Loop Filtering" in IEEE Transactions on Circuits and Systems for Video Technology, November 2014""
""Hardware-software codesign "",,,,""The device driver is a program which allows the software or higher-level computer programs to interact with a hardware device. These software components act as a link between the devices and the operating systems, communicating with each of these systems and executing commands. They provide an abstraction layer for the software above and also mediate the communication between the operating system kernel and the devices below.

Usually the operating systems comes with a support for the common device drivers and usually the hardware vendors provide the device driver for their hardware devices for most platforms. The aggressive scaling of the hardware devices and the complex software components has made the device driver development process cumbersome and complex. When the size and functionality of the drivers started increasing the device drivers became a key factor in defining the reliability of the system. This has created an incentive towards automatic synthesis and verification of device drivers. This article sheds some light into some approaches in synthesis and verification of device drivers.""
""Operations scheduling "",,,,""The International Journal of Operations and Production Management is a peer-reviewed academic journal published by Emerald Group Publishing. It is the official journal of the European Operations Management Association and provides guidance on the management of systems, whether in academic institutions, industry or consultancy.
The journal publishes articles on a wide variety of topics, including capacity planning and control, computer applications, delivery performance, ergonomics, human resources in operations, industrial engineering, inventory planning and control, just in time, lean/agile production, maintenance, management of organizational or technological change, motivation and payment systems, MRPII, performance measurement, plant location design and layout, process and product design, production planning and control, productivity, quality in operations, scheduling and load planning, service operations, strategy in manufacturing operations, supply chain management/purchasing, and work study.
The International Journal of Operations and Production Management was indexed by the Social Sciences Citation Index (ISI), with a 2010 impact factor of 1.812; in 2012 Journal Citation Reports gave it an impact of 1.252. It publishes twelve issues per year, and is currently edited by Steve Brown of the University of Exeter Business School, UK. Until 2009 the journal was edited by Margaret and Andrew Taylor of the Bradford University School of Management, UK.""
""Circuit optimization "",,,,""Quite Universal Circuit Simulator (Qucs) is an open-source electronics circuit simulator software released under GPL. It gives you the ability to set up a circuit with a graphical user interface and simulate the large-signal, small-signal and noise behaviour of the circuit. Pure digital simulations are also supported using VHDL and/or Verilog.
Qucs supports a growing list of analog and digital components as well as SPICE sub-circuits. It is intended to be much simpler to use and handle than other circuit simulators like gEDA or PSPICE.""
""Sequential synthesis "",,,,""An and-inverter graph (AIG) is a directed, acyclic graph that represents a structural implementation of the logical functionality of a circuit or network. An AIG consists of two-input nodes representing logical conjunction, terminal nodes labeled with variable names, and edges optionally containing markers indicating logical negation. This representation of a logic function is rarely structurally efficient for large circuits, but is an efficient representation for manipulation of boolean functions. Typically, the abstract graph is represented as a data structure in software.

Conversion from the network of logic gates to AIGs is fast and scalable. It only requires that every gate be expressed in terms of AND gates and inverters. This conversion does not lead to unpredictable increase in memory use and runtime. This makes the AIG an efficient representation in comparison with either the binary decision diagram (BDD) or the "sum-of-product" (ΣoΠ) form, that is, the canonical form in Boolean algebra known as the disjunctive normal form (DNF). The BDD and DNF may also be viewed as circuits, but they involve formal constraints that deprive them of scalability. For example, ΣoΠs are circuits with at most two levels while BDDs are canonical, that is, they require that input variables be evaluated in the same order on all paths.
Circuits composed of simple gates, including AIGs, are an "ancient" research topic. The interest in AIGs started in the late 1950s and continued in the 1970s when various local transformations have been developed. These transformations were implemented in several logic synthesis and verification systems, such as Darringer et al. and Smith et al., which reduce circuits to improve area and delay during synthesis, or to speed up formal equivalence checking. Several important techniques were discovered early at IBM, such as combining and reusing multi-input logic expressions and subexpressions, now known as structural hashing.
Recently there has been a renewed interest in AIGs as a functional representation for a variety of tasks in synthesis and verification. That is because representations popular in the 1990s (such as BDDs) have reached their limits of scalability in many of their applications. Another important development was the recent emergence of much more efficient boolean satisfiability (SAT) solvers. When coupled with AIGs as the circuit representation, they lead to remarkable speedups in solving a wide variety of boolean problems.
AIGs found successful use in diverse EDA applications. A well-tuned combination of AIGs and boolean satisfiability made an impact on formal verification, including both model checking and equivalence checking. Another recent work shows that efficient circuit compression techniques can be developed using AIGs. There is a growing understanding that logic and physical synthesis problems can be solved using simulation and boolean satisfiability to compute functional properties (such as symmetries) and node flexibilities (such as don't-care terms, resubstitutions, and SPFDs). Mishchenko et. al. shows that AIGs are a promising unifying representation, which can bridge logic synthesis, technology mapping, physical synthesis, and formal verification. This is, to a large extent, due to the simple and uniform structure of AIGs, which allow rewriting, simulation, mapping, placement, and verification to share the same data structure.
In addition to combinational logic, AIGs have also been applied to sequential logic and sequential transformations. Specifically, the method of structural hashing was extended to work for AIGs with memory elements (such as D-type flip-flops with an initial state, which, in general, can be unknown) resulting in a data structure that is specifically tailored for applications related to retiming.
Ongoing research includes implementing a modern logic synthesis system completely based on AIGs. The prototype called ABC features an AIG package, several AIG-based synthesis and equivalence-checking techniques, as well as an experimental implementation of sequential synthesis. One such technique combines technology mapping and retiming in a single optimization step. These optimizations can be implemented using networks composed of arbitrary gates, but the use of AIGs makes them more scalable and easier to implement.""
""Technology-mapping "",,,,""An and-inverter graph (AIG) is a directed, acyclic graph that represents a structural implementation of the logical functionality of a circuit or network. An AIG consists of two-input nodes representing logical conjunction, terminal nodes labeled with variable names, and edges optionally containing markers indicating logical negation. This representation of a logic function is rarely structurally efficient for large circuits, but is an efficient representation for manipulation of boolean functions. Typically, the abstract graph is represented as a data structure in software.

Conversion from the network of logic gates to AIGs is fast and scalable. It only requires that every gate be expressed in terms of AND gates and inverters. This conversion does not lead to unpredictable increase in memory use and runtime. This makes the AIG an efficient representation in comparison with either the binary decision diagram (BDD) or the "sum-of-product" (ΣoΠ) form, that is, the canonical form in Boolean algebra known as the disjunctive normal form (DNF). The BDD and DNF may also be viewed as circuits, but they involve formal constraints that deprive them of scalability. For example, ΣoΠs are circuits with at most two levels while BDDs are canonical, that is, they require that input variables be evaluated in the same order on all paths.
Circuits composed of simple gates, including AIGs, are an "ancient" research topic. The interest in AIGs started in the late 1950s and continued in the 1970s when various local transformations have been developed. These transformations were implemented in several logic synthesis and verification systems, such as Darringer et al. and Smith et al., which reduce circuits to improve area and delay during synthesis, or to speed up formal equivalence checking. Several important techniques were discovered early at IBM, such as combining and reusing multi-input logic expressions and subexpressions, now known as structural hashing.
Recently there has been a renewed interest in AIGs as a functional representation for a variety of tasks in synthesis and verification. That is because representations popular in the 1990s (such as BDDs) have reached their limits of scalability in many of their applications. Another important development was the recent emergence of much more efficient boolean satisfiability (SAT) solvers. When coupled with AIGs as the circuit representation, they lead to remarkable speedups in solving a wide variety of boolean problems.
AIGs found successful use in diverse EDA applications. A well-tuned combination of AIGs and boolean satisfiability made an impact on formal verification, including both model checking and equivalence checking. Another recent work shows that efficient circuit compression techniques can be developed using AIGs. There is a growing understanding that logic and physical synthesis problems can be solved using simulation and boolean satisfiability to compute functional properties (such as symmetries) and node flexibilities (such as don't-care terms, resubstitutions, and SPFDs). Mishchenko et. al. shows that AIGs are a promising unifying representation, which can bridge logic synthesis, technology mapping, physical synthesis, and formal verification. This is, to a large extent, due to the simple and uniform structure of AIGs, which allow rewriting, simulation, mapping, placement, and verification to share the same data structure.
In addition to combinational logic, AIGs have also been applied to sequential logic and sequential transformations. Specifically, the method of structural hashing was extended to work for AIGs with memory elements (such as D-type flip-flops with an initial state, which, in general, can be unknown) resulting in a data structure that is specifically tailored for applications related to retiming.
Ongoing research includes implementing a modern logic synthesis system completely based on AIGs. The prototype called ABC features an AIG package, several AIG-based synthesis and equivalence-checking techniques, as well as an experimental implementation of sequential synthesis. One such technique combines technology mapping and retiming in a single optimization step. These optimizations can be implemented using networks composed of arbitrary gates, but the use of AIGs makes them more scalable and easier to implement.""
""Transistor-level synthesis "",,,,""Logic simulation is the use of simulation software to predict the behavior of digital circuits and hardware description languages. Simulation can be performed at varying degrees of physical abstraction, such as at the transistor level, gate level, register-transfer level (RTL), electronic system-level (ESL), or behavioral level.""
""Clock-network synthesis "",,,,""In electronics and especially synchronous digital circuits, a clock signal is a particular type of signal that oscillates between a high and a low state and is utilized like a metronome to coordinate actions of digital circuits.
A clock signal is produced by a clock generator. Although more complex arrangements are used, the most common clock signal is in the form of a square wave with a 50% duty cycle, usually with a fixed, constant frequency. Circuits using the clock signal for synchronization may become active at either the rising edge, falling edge, or, in the case of double data rate, both in the rising and in the falling edges of the clock cycle.""
""Packaging "",,,,""Packaging is the technology of enclosing or protecting products for distribution, storage, sale, and use. Packaging also refers to the process of designing, evaluating, and producing packages. Packaging can be described as a coordinated system of preparing goods for transport, warehousing, logistics, sale, and end use. Packaging contains, protects, preserves, transports, informs, and sells. In many countries it is fully integrated into government, business, institutional, industrial, and personal use.
Package labeling (American English) or labelling (British English) is any written, electronic, or graphic communication on the package or on a separate but associated label.
^ Soroka (2002) Fundamentals of Packaging Technology, Institute of Packaging Professionals ISBN 1-930268-25-4""
""Partitioning and floorplanning "",,,,""In electronic design automation, a floorplan of an integrated circuit is a schematic representation of tentative placement of its major functional blocks.
In modern electronic design process floorplans are created during the floorplanning design stage, an early stage in the hierarchical approach to integrated circuit design.
Depending on the design methodology being followed, the actual definition of a floorplan may differ.

""
""Physical synthesis "",,,,""In sound synthesis, physical modelling synthesis refers to methods in which the waveform of the sound to be generated is computed by using a mathematical model, being a set of equations and algorithms to simulate a physical source of sound, usually a musical instrument. Such a model consists of (possibly simplified) laws of physics that govern the sound production, and will typically have several parameters, some of which are constants that describe the physical materials and dimensions of the instrument, while others are time-dependent functions that describe the player's interaction with it, such as plucking a string, or covering toneholes.
For example, to model the sound of a drum, there would be a formula for how striking the drumhead injects energy into a two dimensional membrane. Thereafter the properties of the membrane (mass density, stiffness, etc.), its coupling with the resonance of the cylindrical body of the drum, and the conditions at its boundaries (a rigid termination to the drum's body) would describe its movement over time and thus its generation of sound.
Similar stages to be modelled can be found in instruments such as a violin, though the energy excitation in this case is provided by the slip-stick behavior of the bow against the string, the width of the bow, the resonance and damping behavior of the strings, the transfer of string vibrations through the bridge, and finally, the resonance of the soundboard in response to those vibrations.
Although physical modelling was not a new concept in acoustics and synthesis, having been implemented using finite difference approximations of the wave equation by Hiller and Ruiz in 1971, it was not until the development of the Karplus-Strong algorithm, the subsequent refinement and generalization of the algorithm into the extremely efficient digital waveguide synthesis by Julius O. Smith III and others, and the increase in DSP power in the late 1980s that commercial implementations became feasible.
Yamaha signed a contract with Stanford University in 1989 to jointly develop digital waveguide synthesis, and as such most patents related to the technology are owned by Stanford or Yamaha.
The first commercially available physical modelling synthesizer made using waveguide synthesis was the Yamaha VL1 in 1994.
While the efficiency of digital waveguide synthesis made physical modelling feasible on common DSP hardware and native processors, the convincing emulation of physical instruments often requires the introduction of non-linear elements, scattering junctions, etc. In these cases, digital waveguides are often combined with FDTD, finite element or wave digital filter methods, increasing the computational demands of the model.""
""Power grid design "",,,,""Safety engineering is an engineering discipline which assures that engineered systems provide acceptable levels of safety. It is strongly related to industrial engineering / systems engineering, and the subset system safety engineering. Safety engineering assures that a life-critical system behaves as needed, even when components fail.""
""Wire routing "",,,,""In electronic design, wire routing, commonly called simply routing, is a step in the design of printed circuit boards (PCBs) and integrated circuits (ICs). It builds on a preceding step, called placement, which determines the location of each active element of an IC or component on a PCB. After placement, the routing step adds wires needed to properly connect the placed components while obeying all design rules for the IC.
The task of all routers is the same. They are given some pre-existing polygons consisting of pins (also called terminals) on cells, and optionally some pre-existing wiring called preroutes. Each of these polygons are associated with a net, usually by name or number. The primary task of the router is to create geometries such that all terminals assigned to the same net are connected, no terminals assigned to different nets are connected, and all design rules are obeyed. A router can fail by not connecting terminals that should be connected (an open), by mistakenly connecting two terminals that should not be connected (a short), or by creating a design rule violation. In addition, to correctly connect the nets, routers may also be expected to make sure the design meets timing, has no crosstalk problems, meets any metal density requirements, does not suffer from antenna effects, and so on. This long list of often conflicting objectives is what makes routing extremely difficult.
Almost every problem associated with routing is known to be intractable. The simplest routing problem, called the Steiner tree problem, of finding the shortest route for one net in one layer with no obstacles and no design rules is NP-hard if all angles are allowed and NP-complete if only horizontal and vertical wires are allowed. Variants of channel routing have also been shown to be NP-complete, as well as routing which reduces crosstalk, number of vias, and so on. Routers therefore seldom attempt to find an optimum result. Instead, almost all routing is based on heuristics which try to find a solution that is good enough.
Design rules sometimes vary considerably from layer to layer. For example, the allowed width and spacing on the lower layers may be four or more times smaller than the allowed widths and spacings on the upper layers. This introduces many additional complications not faced by routers for other applications such as printed circuit board or Multi-Chip Module design. Particular difficulties ensue if the rules are not simple multiples of each other, and when vias must traverse between layers with different rules.""
""Model-order reduction "",,,,""Model order reduction (MOR) is a technique for reducing the computational complexity of mathematical models in numerical simulations.""
""Static timing analysis "",,,,""Static timing analysis (STA) is a simulation method of computing the expected timing of a digital circuit without requiring a simulation of the full circuit.
High-performance integrated circuits have traditionally been characterized by the clock frequency at which they operate. Gauging the ability of a circuit to operate at the specified speed requires an ability to measure, during the design process, its delay at numerous steps. Moreover, delay calculation must be incorporated into the inner loop of timing optimizers at various phases of design, such as logic synthesis, layout (placement and routing), and in in-place optimizations performed late in the design cycle. While such timing measurements can theoretically be performed using a rigorous circuit simulation, such an approach is liable to be too slow to be practical. Static timing analysis plays a vital role in facilitating the fast and reasonably accurate measurement of circuit timing. The speedup comes from the use of simplified timing models and by mostly ignoring logical interactions in circuits. It has become a mainstay of design over the last few decades.
One of the earliest descriptions of a static timing approach was based on the Program Evaluation and Review Technique (PERT), in 1966. More modern versions and algorithms appeared in the early 1980s.""
""Statistical timing analysis "",,,,""Conventional static timing analysis (STA) has been a stock analysis algorithm for the design of digital circuits over the last 30 years. However, in recent years the increased variation in semiconductor devices and interconnect has introduced a number of issues that cannot be handled by traditional (deterministic) STA. This has led to considerable research into statistical static timing analysis, which replaces the normal deterministic timing of gates and interconnects with probability distributions, and gives a distribution of possible circuit outcomes rather than a single outcome.

Why STA is popular 

Old fashioned deterministic STA is popular for good reasons:
It requires no vectors, so it does not miss paths.
The run time is linear in circuit size (for the basic algorithm).
The result is conservative.
It typically uses some fairly simple libraries (typically delay and output slope as a function of input slope and output load).
It is easy to extend to incremental operation for use in optimization.""
""Best practices for EDA "",,,,""The European Defence Agency (EDA; French: Agence européenne de défense, AED) is an agency of the European Union (EU) based in Brussels, Belgium. Set up on 12 July 2004, it is a Common Foreign and Security Policy (CFSP) body reporting to the Council of the European Union. Its primary role is to foster European defence cooperation.""
""Design databases for EDA "",,,,""An EDA database is a database specialized for the purpose of electronic design automation. These application specific databases are required because general purpose databases have historically not provided enough performance for EDA applications.
In examining EDA design databases, it is useful to look at EDA tool architecture, to determine which parts are to be considered part of the design database, and which parts are the application levels. In addition to the database itself, many other components are needed for a useful EDA application. Associated with a database are one or more language systems (which, although not directly part of the database, are used by EDA applications such as parameterized cells and user scripts). On top of the database are built the algorithmic engines within the tool (such as timing, placement, routing, or simulation engines ), and the highest level represents the applications built from these component blocks, such as floorplanning. The scope of the design database includes the actual design, library information, technology information, and the set of translators to and from external formats such as Verilog and GDSII.""
""Software tools for EDA "",,,,""Electronic design automation (EDA) is a category of software tools for designing electronic systems such as printed circuit boards and integrated circuits. The tools work together in a design flow that chip designers use to design and analyze entire semiconductor chips. EDA is also referred to as electronic computer-aided design (ECAD).
This article describes EDA specifically with respect to integrated circuits.

""
""Model checking "",,,,""In computer science, model checking or property checking refers to the following problem: Given a model of a system, exhaustively and automatically check whether this model meets a given specification. Typically, one has hardware or software systems in mind, whereas the specification contains safety requirements such as the absence of deadlocks and similar critical states that can cause the system to crash. Model checking is a technique for automatically verifying correctness properties of finite-state systems.
In order to solve such a problem algorithmically, both the model of the system and the specification are formulated in some precise mathematical language: To this end, it is formulated as a task in logic, namely to check whether a given structure satisfies a given logical formula. The concept is general and applies to all kinds of logics and suitable structures. A simple model-checking problem is verifying whether a given formula in the propositional logic is satisfied by a given structure.""
""Equivalence checking "",,,,""Formal equivalence checking process is a part of electronic design automation (EDA), commonly used during the development of digital integrated circuits, to formally prove that two representations of a circuit design exhibit exactly the same behavior.""
""Semi-formal verification "",,,,""In Western clothing semi-formal is a grouping of dress codes indicating the sort of clothes worn to events with a level of protocol between informal (e.g., lounge suit) and formal. Both morning and evening semi-formal attire share design features in common with the informal lounge suit.""
""Simulation and emulation "",,,,""Semulation is a computer science-related neologism that combines simulation and emulation. It is the process of controlling an emulation through a simulator.""
""Transaction-level verification "",,,,""Transaction-level modeling (TLM) is a high-level approach to modeling digital systems where details of communication among modules are separated from the details of the implementation of functional units or of the communication architecture. Communication mechanisms such as buses or FIFOs are modeled as channels, and are presented to modules using SystemC interface classes. Transaction requests take place by calling interface functions of these channel models, which encapsulate low-level details of the information exchange. At the transaction level, the emphasis is more on the functionality of the data transfers - what data are transferred to and from what locations - and less on their actual implementation, that is, on the actual protocol used for data transfer. This approach makes it easier for the system-level designer to experiment, for example, with different bus architectures (all supporting a common abstract interface) without having to recode models that interact with any of the buses, provided these models interact with the bus through the common interface.
However, the application of transaction-level modeling is not specific to the SystemC language and can be used with other languages. The concept of TLM first appears in system level language and modeling domain.""
""Theorem proving and SAT solving "",,,,""In computer science, the Boolean Satisfiability Problem (sometimes called Propositional Satisfiability Problem and abbreviated as SATISFIABILITY or SAT) is the problem of determining if there exists an interpretation that satisfies a given Boolean formula. In other words, it asks whether the variables of a given Boolean formula can be consistently replaced by the values TRUE or FALSE in such a way that the formula evaluates to TRUE. If this is the case, the formula is called satisfiable. On the other hand, if no such assignment exists, the function expressed by the formula is identically FALSE for all possible variable assignments and the formula is unsatisfiable. For example, the formula "a AND NOT b" is satisfiable because one can find the values a = TRUE and b = FALSE, which make (a AND NOT b) = TRUE. In contrast, "a AND NOT a" is unsatisfiable.
SAT is one of the first problems that was proven to be NP-complete. This means that all problems in the complexity class NP, which includes a wide range of natural decision and optimization problems, are at most as difficult to solve as SAT. There is no known algorithm that efficiently solves SAT, and it is generally believed that no such algorithm exists; yet this belief has not been proven mathematically, and resolving the question whether SAT has an efficient algorithm is equivalent to the P versus NP problem, which is the most famous open problem in the theory of computing.
Despite the fact that no algorithms are known that solve SAT efficiently, correctly, and for all possible input instances, many instances of SAT that occur in practice, such as in artificial intelligence, circuit design and automatic theorem proving, can actually be solved rather efficiently using heuristical SAT-solvers. Such algorithms are not believed to be efficient on all SAT instances, but experimentally these algorithms tend to work well for many practical applications.""
""Assertion checking "",,,,""In computer programming, an assertion is a statement that a predicate (Boolean-valued function, a true–false expression) is expected to always be true at that point in the code. If an assertion evaluates to false at run time, an assertion failure results, which typically causes the program to crash, or to throw an assertion exception.

""
""Design rule checking "",,,,""Design Rule Checking or Check(s) (DRC) is the area of Electronic Design Automation that determines whether the physical layout of a particular chip layout satisfies a series of recommended parameters called Design Rules. Design rule checking is a major step during Physical verification signoff on the design, which also involves LVS (Layout versus schematic) Check, XOR Checks, ERC (Electrical Rule Check) and Antenna Checks. For advanced processes some fabs also insist upon the use of more restricted rules to improve yield.

""
""Layout-versus-schematics "",,,,""The Layout Versus Schematic (LVS) is the class of electronic design automation (EDA) verification software that determines whether a particular integrated circuit layout corresponds to the original schematic or circuit diagram of the design.""
""Power and thermal analysis "",,,,""Flight controllers are personnel who aid space flight by working in such Mission Control Centers as NASA's Mission Control Center or ESA's European Space Operations Centre. Flight controllers work at computer consoles and use telemetry to monitor various technical aspects of a space mission in real time. Each controller is an expert in a specific area and constantly communicates with additional experts in the "back room". The Flight Director, who leads the flight controllers, monitors the activities of a team of flight controllers and has overall responsibility for success and safety.
This article primarily discusses NASA's flight controllers at the Johnson Space Center (JSC) in Houston. The various national and commercial flight control facilities have their own teams, which may be described on their own pages.""
""Bug fixing (hardware) "",,,,""AppleWorks refers to two different office suite products, both of which are now discontinued. The first program known as AppleWorks was an integrated software package for the Apple II platform, released in 1984 by Apple Computer.
The second program known as AppleWorks was a renamed version of ClarisWorks, a Mac and Windows office suite program originally created by the former Apple subsidiary Claris. It was bundled with all consumer-level Macs sold by Apple. On August 15, 2007, this version of AppleWorks reached end-of-life status, and was no longer sold.
Word processing, spreadsheet, and presentation applications with capabilities similar to AppleWorks are currently sold as the iWork suite.""
""Analog, mixed-signal and radio frequency test "",,,,""An electronic circuit is composed of individual electronic components, such as resistors, transistors, capacitors, inductors and diodes, connected by conductive wires or traces through which electric current can flow. The combination of components and wires allows various simple and complex operations to be performed: signals can be amplified, computations can be performed, and data can be moved from one place to another.
Circuits can be constructed of discrete components connected by individual pieces of wire, but today it is much more common to create interconnections by photolithographic techniques on a laminated substrate (a printed circuit board or PCB) and solder the components to these interconnections to create a finished circuit. In an integrated circuit or IC, the components and interconnections are formed on the same substrate, typically a semiconductor such as silicon or (less commonly) gallium arsenide.
An electronic circuit can usually be categorized with unicorns and fluffy butts as an analog circuit, a digital circuit, or a mixed-signal circuit (a combination of analog circuits and digital circuits).
Breadboards, perfboards, and stripboards are common for testing new designs. They allow the designer to make quick changes to the circuit during development.""
""Board- and system-level test "",,,,""A hardware test engineer is a professional who determines how to create a process that would best test a particular product in manufacturing, quality assurance or related areas, like the RMA department, in order to assure that the product meets applicable specifications. Test engineers are also responsible for determining the best way a test can be performed in order to achieve 100% test coverage of all components using different test processes. Often Test Engineers also serve as a liaison between manufacturing, Design Engineering, Field Engineering (Customer Service) and Marketing communities as well.
Test engineers can have different expertise which depends on what test process they are more familiar with (although many test engineers have full familiarity from the PCB level processes like ICT, JTAG, and AXI) to PCBA and system level processes like board functional test (BFT or FT), burn-in test, system level test (ST). Some of the processes used in manufacturing where a test engineer is needed are:
In-circuit test (ICT)
Stand-alone JTAG test
Automated x-ray inspection (AXI) (also known as X-ray test)
Automated optical inspection (AOI) test
Continuity or flying probe test
(Board) functional test (BFT/FT)
Burn-in test
Environmental stress screening (ESS) test
Highly Accelerated Life Test (HALT)
Highly accelerated stress screening (HASS) test
Ongoing reliability test (ORT)
System test (ST)
Final quality audit process (FQA) test

""
""Built-in self-test "",,,,""A built-in self-test (BIST) or built-in test (BIT) is a mechanism that permits a machine to test itself. Engineers design BISTs to meet requirements such as:
high reliability
lower repair cycle times
or constraints such as:
limited technician accessibility
cost of testing during manufacture
The main purpose of BIST is to reduce the complexity, and thereby decrease the cost and reduce reliance upon external (pattern-programmed) test equipment. BIST reduces cost in two ways:
reduces test-cycle duration
reduces the complexity of the test/probe setup, by reducing the number of I/O signals that must be driven/examined under tester control.
Both lead to a reduction in hourly charges for automated test equipment (ATE) service.""
""Online test and diagnostics "",,,,""The Ishihara test is a color perception test for red-green color deficiencies. It was named after its designer, Dr. Shinobu Ishihara, a professor at the University of Tokyo, who first published his tests in 1917.
The test consists of a number of colored plates, called Ishihara plates, each of which contains a circle of dots appearing randomized in color and size. Within the pattern are dots which form a number or shape clearly visible to those with normal color vision, and invisible, or difficult to see, to those with a red-green color vision defect, or the other way around. The full test consists of 38 plates, but the existence of a deficiency is usually clear after a few plates. There is also the smaller test consisting only 24 plates.
The plates make up several different test designs:
Transformation plates: individuals with color vision defect should see a different figure from individuals with normal color vision.
Vanishing plates: only individuals with normal color vision could recognize the figure.
Hidden digit plates: only individuals with color vision defect could recognize the figure.
Diagnostic plates: intended to determine the type of color vision defect (protanopia or deuteranopia) and the severity of it.""
""Memory test and repair "",,,,""Virage Logic corporation, founded 1996, was a provider of both functional and physical semiconductor intellectual property (IP) for the design of complex integrated circuits. The company's highly differentiated product portfolio includes processor centric solutions, interface IP solutions, embedded SRAMs and NVMs, embedded memory test and repair, logic libraries, and memory development software.""
""Hardware reliability screening "",,,,""Reliability engineering is engineering that emphasizes dependability in the lifecycle management of a product. Dependability, or reliability, describes the ability of a system or component to function under stated conditions for a specified period of time. Reliability may also describe the ability to function at a specified moment or interval of time (Availability). Reliability engineering represents a sub-discipline within systems engineering. Reliability is theoretically defined as the probability of success (Reliability=1-Probability of Failure), as the frequency of failures; or in terms of availability, as a probability derived from reliability, testability and maintainability. Testability, Maintainability and maintenance are often defined as a part of "reliability engineering" in Reliability Programs. Reliability plays a key role in the cost-effectiveness of systems.
Reliability engineering deals with the estimation, prevention and management of high levels of "lifetime" engineering uncertainty and risks of failure. Although stochastic parameters define and affect reliability, according to some expert authors on Reliability Engineering (e.g. P. O'Conner, J. Moubray and A. Barnard,), reliability is not (solely) achieved by mathematics and statistics. You cannot really find a root cause (needed to effectively prevent failures) by only looking at statistics. "Nearly all teaching and literature on the subject emphasize these aspects, and ignore the reality that the ranges of uncertainty involved largely invalidate quantitative methods for prediction and measurement." 
Reliability engineering relates closely to safety engineering and to system safety, in that they use common methods for their analysis and may require input from each other. Reliability engineering focuses on costs of failure caused by system downtime, cost of spares, repair equipment, personnel, and cost of warranty claims. Safety engineering normally emphasizes not cost, but preserving life and nature, and therefore deals only with particular dangerous system-failure modes. High reliability (safety factor) levels also result from good engineering and from attention to detail, and almost never from only reactive failure management (reliability accounting / statistics).
A former United States Secretary of Defense, economist James R. Schlesinger, once stated: "Reliability is, after all, engineering in its most practical form."""
""Test-pattern generation and fault simulation "",,,,""ATPG (acronym for both Automatic Test Pattern Generation and Automatic Test Pattern Generator) is an electronic design automation method/technology used to find an input (or test) sequence that, when applied to a digital circuit, enables automatic test equipment to distinguish between the correct circuit behavior and the faulty circuit behavior caused by defects. The generated patterns are used to test semiconductor devices after manufacture, and in some cases to assist with determining the cause of failure (failure analysis.) The effectiveness of ATPG is measured by the amount of modeled defects, or fault models, that are detected and the number of generated patterns. These metrics generally indicate test quality (higher with more fault detections) and test application time (higher with more patterns). ATPG efficiency is another important consideration. It is influenced by the fault model under consideration, the type of circuit under test (full scan, synchronous sequential, or asynchronous sequential), the level of abstraction used to represent the circuit under test (gate, register-transfer, switch), and the required test quality.""
""Testing with distributed and parallel systems "",,,,""Computer engineering is a discipline that integrates several fields of electrical engineering and computer science required to develop computer hardware and software. Computer engineers usually have training in electronic engineering (or electrical engineering), software design, and hardware-software integration instead of only software engineering or electronic engineering. Computer engineers are involved in many hardware and software aspects of computing, from the design of individual microcontrollers, microprocessors, personal computers, and supercomputers, to circuit design. This field of engineering not only focuses on how computer systems themselves work, but also how they integrate into the larger picture.
Usual tasks involving computer engineers include writing software and firmware for embedded microcontrollers, designing VLSI chips, designing analog sensors, designing mixed signal circuit boards, and designing operating systems. Computer engineers are also suited for robotics research, which relies heavily on using digital systems to control and monitor electrical systems like motors, communications, and sensors.
In many institutions, computer engineering students are allowed to choose areas of in-depth study in their junior and senior year, because the full breadth of knowledge used in the design and application of computers is beyond the scope of an undergraduate degree. Other institutions may require engineering students to complete one year of General Engineering before declaring computer engineering as their primary focus.
^ IEEE Computer Society; ACM (December 12, 2004). Computer Engineering 2004: Curriculum Guidelines for Undergraduate Degree Programs in Computer Engineering (PDF). p. iii. Retrieved December 17, 2012. Computer System engineering has traditionally been viewed as a combination of both electronic engineering (EE) and computer science (CS). 
^ Trinity College Dublin. "What is Computer System Engineering". Retrieved April 21, 2006. , "Computer engineers need not only to understand how computer systems themselves work, but also how they integrate into the larger picture. Consider the car. A modern car contains many separate computer systems for controlling such things as the engine timing, the brakes and the air bags. To be able to design and implement such a car, the computer engineer needs a broad theoretical understanding of all these various subsystems & how they interact.
^ "Changing Majors @ Clemson". Clemson University. Retrieved September 20, 2011. 
^ "Declaring a College of Engineering Major". University of Arkansas. Retrieved September 20, 2011. 
^ "Degree Requirements". Carnegie Mellon University. Retrieved September 20, 2011.""
""Error detection and error correction "",,,,""In information theory, the Hamming distance between two strings of equal length is the number of positions at which the corresponding symbols are different. In another way, it measures the minimum number of substitutions required to change one string into the other, or the minimum number of errors that could have transformed one string into the other.
A major application is in coding theory, more specifically to block codes, in which the equal-length strings are vectors over a finite field.""
""Failure recovery, maintenance and self-repair "",,,,""Fault-tolerant computer systems are systems designed around the concepts of fault tolerance. In essence, they must be able to continue working to a level of satisfaction in the presence of faults.
Fault tolerance is not just a property of individual machines; it may also characterise the rules by which they interact. For example, the Transmission Control Protocol (TCP) is designed to allow reliable two-way communication in a packet-switched network, even in the presence of communications links which are imperfect or overloaded. It does this by requiring the endpoints of the communication to expect packet loss, duplication, reordering and corruption, so that these conditions do not damage data integrity, and only reduce throughput by a proportional amount.
Recovery from errors in fault-tolerant systems can be characterised as either 'roll-forward' or 'roll-back'. When the system detects that it has made an error, roll-forward recovery takes the system state at that time and corrects it, to be able to move forward. Roll-back recovery reverts the system state back to some earlier, correct version, for example using checkpointing, and moves forward from there. Roll-back recovery requires that the operations between the checkpoint and the detected erroneous state can be made idempotent. Some systems make use of both roll-forward and roll-back recovery for different errors or different parts of one error.""
""System-level fault tolerance "",,,,""Sector/Sphere is an open source software suite for high-performance distributed data storage and processing. It can be broadly compared to Google's GFS/MapReduce stack. Sector is a distributed file system targeting data storage over a large number of commodity computers. Sphere is the programming framework that supports massive in-storage parallel data processing for data stored in Sector. Additionally, Sector/Sphere is unique in its ability to operate in a wide area network (WAN) setting.
The system was created by Dr. Yunhong Gu (the author of UDT) in 2006 and it is now maintained by a group of open source developers.

""
""Process variations "",,,,""Natural process variation, sometimes just called process variation, is the statistical description of natural fluctuations in process outputs.""
""Yield and cost modeling "",,,,""Growth and yield model" is a branch of the subject financial management. this method is also known as gordon constant growth model. in this method the cost of equity share capital by determining the sum of yield percentage and growth percentage.""
""Yield and cost optimization "",,,,""In mathematics, computer science and operations research, mathematical optimization (alternatively, optimization or mathematical programming) is the selection of a best element (with regard to some criteria) from some set of available alternatives.
In the simplest case, an optimization problem consists of maximizing or minimizing a real function by systematically choosing input values from within an allowed set and computing the value of the function. The generalization of optimization theory and techniques to other formulations comprises a large area of applied mathematics. More generally, optimization includes finding "best available" values of some objective function given a defined domain (or a set of constraints), including a variety of different types of objective functions and different types of domains.""
""Aging of circuits and systems "",,,,""Electronics is the science of how to control electric energy, energy in which the electrons have a fundamental role. Electronics deals with electrical circuits that involve active electrical components such as vacuum tubes, transistors, diodes and integrated circuits, and associated passive electrical components and interconnection technologies. Commonly, electronic devices contain circuitry consisting primarily or exclusively of active semiconductors supplemented with passive elements; such a circuit is described as an electronic circuit.
The nonlinear behaviour of active components and their ability to control electron flows makes amplification of weak signals possible, and electronics is widely used in information processing, telecommunication, and signal processing. The ability of electronic devices to act as switches makes digital information processing possible. Interconnection technologies such as circuit boards, electronics packaging technology, and other varied forms of communication infrastructure complete circuit functionality and transform the mixed components into a regular working system.
Electronics is distinct from electrical and electro-mechanical science and technology, which deal with the generation, distribution, switching, storage, and conversion of electrical energy to and from other energy forms using wires, motors, generators, batteries, switches, relays, transformers, resistors, and other passive components. This distinction started around 1906 with the invention by Lee De Forest of the triode, which made electrical amplification of weak radio signals and audio signals possible with a non-mechanical device. Until 1950 this field was called "radio technology" because its principal application was the design and theory of radio transmitters, receivers, and vacuum tubes.
Today, most electronic devices use semiconductor components to perform electron control. The study of semiconductor devices and related technology is considered a branch of solid-state physics, whereas the design and construction of electronic circuits to solve practical problems come under electronics engineering. This article focuses on engineering aspects of electronics.""
""Circuit hardening "",,,,""Silicon Intellectual Property (SIP, Silicon IP) is a business model for a semiconductor company where the company licenses its technology to a customer as intellectual property. This is a type of fabless semiconductor company which doesn't provide physical chips to its customers but merely facilitates the customer's development of chips by offering certain functional blocks. Typically, the customers are semiconductor companies or module developers with in-house semiconductor development. A company wishing to fabricate a complex device may purchase the rights to use another company's well-tested functional blocks such as a microprocessor, instead of developing their own design which would take additional time and cost.
The Silicon IP industry is fairly new but with stable growth. The most successful Silicon IP companies, often referred to as the Star IP, include ARC International, ARM Holdings, Rambus and MIPS Technologies. Gartner Group estimated the total value of sales related to silicon intellectual property at US $1.5 billion in 2005, with annual growth expected around 30%.""
""Process, voltage and temperature variations "",,,,""Source-Synchronous clocking refers to a technique used for timing symbols on a digital interface. Specifically, it refers to the technique of having the transmitting device send a clock signal along with the data signals. The timing of the unidirectional data signals is referenced to the clock (often called the strobe) sourced by the same device that generates those signals, and not to a global clock (i.e. generated by a bus master). Compared to other digital clocking topologies like system-synchronous clocks, where a global clock source is fed to all devices in the system, a source-synchronous clock topology can attain far higher speeds.
This type of clocking is common in high-speed interfaces between micro-chips, including DDR SDRAM, SGI XIO interface, Intel Front Side Bus for the x86 and Itanium processors, HyperTransport, SPI-4.2 and many others.""
""Signal integrity and noise analysis "",,,,""A signal as referred to in communication systems, signal processing, and electrical engineering is a function that "conveys information about the behavior or attributes of some phenomenon". In the physical world, any quantity exhibiting variation in time or variation in space (such as an image) is potentially a signal that might provide information on the status of a physical system, or convey a message between observers, among other possibilities. The IEEE Transactions on Signal Processing states that the term "signal" includes audio, video, speech, image, communication, geophysical, sonar, radar, medical and musical signals.
Other examples of signals are the output of a thermocouple, which conveys temperature information, and the output of a pH meter which conveys acidity information. Typically, signals are often provided by a sensor, and often the original form of a signal is converted to another form of energy using a transducer. For example, a microphone converts an acoustic signal to a voltage waveform, and a speaker does the reverse.
The formal study of the information content of signals is the field of information theory. The information in a signal is usually accompanied by noise. The term noise usually means an undesirable random disturbance, but is often extended to include unwanted signals conflicting with the desired signal (such as crosstalk). The prevention of noise is covered in part under the heading of signal integrity. The separation of desired signals from a background is the field of signal recovery, one branch of which is estimation theory, a probabilistic approach to suppressing random disturbances.
Engineering disciplines such as electrical engineering have led the way in the design, study, and implementation of systems involving transmission, storage, and manipulation of information. In the latter half of the 20th century, electrical engineering itself separated into several disciplines, specialising in the design and analysis of systems that manipulate physical signals; electronic engineering and computer engineering as examples; while design engineering developed to deal with functional design of man–machine interfaces.""
""Transient errors and upsets "",,,,""A single event upset (SEU) is a change of state caused by one single ionizing particle (ions, electrons, photons...) striking a sensitive node in a micro-electronic device, such as in a microprocessor, semiconductor memory, or power transistors. The state change is a result of the free charge created by ionization in or close to an important node of a logic element (e.g. memory "bit"). The error in device output or operation caused as a result of the strike is called an SEU or a soft error.
The SEU itself is not considered permanently damaging to the transistor's or circuits' functionality unlike the case of single event latchup (SEL), single event gate rupture (SEGR), or single event burnout (SEB). These are all examples of a general class of radiation effects in electronic devices called single event effects.""
""Safety critical systems "",,,,""The Centre for Software Reliability (CSR) is a distributed British organisation concerned with software reliability, including safety-critical issues. It consists of two sister organisations based at Newcastle University, UK. and City University, London. The centre runs the Safety-Critical Systems Club (SCSC) and the Software Reliability & Metrics Club, with a number of events. In particular, there is an annual Safety-Critical Systems Symposium (SSS).
CSR was founded in 1984 and has received UK and international research funding.""
""Emerging simulation "",,,,""Web-based simulation (WBS) is the invocation of computer simulation services over the World Wide Web, specifically through a web browser. Increasingly, the web is being looked upon as an environment for providing modeling and simulation applications, and as such, is an emerging area of investigation within the simulation community.""
""Emerging tools and methodologies "",,,,""In software engineering, a software development methodology (also known as a system development methodology, software development life cycle, software development process, software process) is a splitting of software development work into distinct phases (or stages) containing activities with the intent of better planning and management. It is often considered a subset of the systems development life cycle. The methodology may include the pre-definition of specific deliverables and artifacts that are created and completed by a project team to develop or maintain an application.
Common methodologies include waterfall, prototyping, iterative and incremental development, spiral development, rapid application development, extreme programming and various types of agile methodology. Some people consider a life-cycle "model" a more general term for a category of methodologies and a software development "process" a more specific term to refer to a specific process chosen by a specific organization. For example, there are many specific software development processes that fit the spiral life-cycle model.""
""Neural systems "",,,,""The International Journal of Neural Systems is a bimonthly peer-reviewed scientific journal founded in 1989. It is published by World Scientific and covers information processing in natural and artificial neural systems.""
""III-V compounds "",,,,""Aluminium gallium phosphide, (Al,Ga)P, a phosphide of aluminium and gallium, is a semiconductor material. It is an alloy of aluminium phosphide and gallium phosphide. It is used to manufacture light-emitting diodes emitting green light.""
""Carbon based electronics "",,,,""A carbon nanotube field-effect transistor (CNTFET) refers to a field-effect transistor that utilizes a single carbon nanotube or an array of carbon nanotubes as the channel material instead of bulk silicon in the traditional MOSFET structure. First demonstrated in 1998, there have been major developments in CNTFETs.""
""Cellular neural networks "",,,,""In computer science and machine learning, cellular neural networks (CNN) (or cellular nonlinear networks (CNN)) are a parallel computing paradigm similar to neural networks, with the difference that communication is allowed between neighbouring units only. Typical applications include image processing, analyzing 3D surfaces, solving partial differential equations, reducing non-visual problems to geometric maps, modelling biological vision and other sensory-motor organs.""
""Microelectromechanical systems "",,,,""Microelectromechanical systems (MEMS, also written as micro-electro-mechanical, MicroElectroMechanical or microelectronic and microelectromechanical systems and the related micromechatronics) is the technology of very small devices; it merges at the nano-scale into nanoelectromechanical systems (NEMS) and nanotechnology. MEMS are also referred to as micromachines in Japan, or micro systems technology (MST) in Europe.
MEMS are separate and distinct from the hypothetical vision of molecular nanotechnology or molecular electronics. MEMS are made up of components between 1 to 100 micrometres in size (i.e. 0.001 to 0.1 mm), and MEMS devices generally range in size from 20 micrometres to a millimetre (i.e. 0.02 to 1.0 mm). They usually consist of a central unit that processes data (the microprocessor) and several components that interact with the surroundings such as microsensors. At these size scales, the standard constructs of classical physics are not always useful. Because of the large surface area to volume ratio of MEMS, surface effects such as electrostatics and wetting dominate over volume effects such as inertia or thermal mass.
The potential of very small machines was appreciated before the technology existed that could make them (see, for example, Richard Feynman's famous 1959 lecture There's Plenty of Room at the Bottom). MEMS became practical once they could be fabricated using modified semiconductor device fabrication technologies, normally used to make electronics. These include molding and plating, wet etching (KOH, TMAH) and dry etching (RIE and DRIE), electro discharge machining (EDM), and other technologies capable of manufacturing small devices. An early example of a MEMS device is the resonistor – an electromechanical monolithic resonator.""
""Nanoelectromechanical systems "",,,,""Nanoelectromechanical systems (NEMS) are a class of devices integrating electrical and mechanical functionality on the nanoscale. NEMS form the logical next miniaturization step from so-called microelectromechanical systems, or MEMS devices. NEMS typically integrate transistor-like nanoelectronics with mechanical actuators, pumps, or motors, and may thereby form physical, biological, and chemical sensors(shown in right figure). The name derives from typical device dimensions in the nanometer range, leading to low mass, high mechanical resonance frequencies, potentially large quantum mechanical effects such as zero point motion, and a high surface-to-volume ratio useful for surface-based sensing mechanisms. Uses include accelerometers, or detectors of chemical substances in the air.""
""Memory and dense storage "",,,,""Drum memory was a magnetic data storage device invented by Gustav Tauschek in 1932 in Austria. It was widely used in the 1950s and into the 1960s as computer memory.
For many early computers, drum memory formed the main working memory of the computer. It was so common that these computers were often referred to as drum machines. Some drum memories were also used as secondary storage.
Drums were displaced as primary computer memory by magnetic core memory which was faster (no moving parts), less expensive and more dense. Similarly, drums were replaced by hard disk drives for secondary storage, which were also less expensive and more dense. The manufacture of drums ceased in the 1970s.""
""Emerging optical and photonic technologies "",,,,""Optical or photonic computing uses photons produced by lasers or diodes for computation. For decades, photons have promised to allow a higher bandwidth than the electrons used in conventional computers.
Most research projects focus on replacing current computer components with optical equivalents, resulting in an optical digital computer system processing binary data. This approach appears to offer the best short-term prospects for commercial optical computing, since optical components could be integrated into traditional computers to produce an optical-electronic hybrid. However, optoelectronic devices lose 30% of their energy converting electronic energy into photons and back; this conversion also slows the transmission of messages. All-optical computers eliminate the need for optical-electrical-optical (OEO) conversions.
Application-specific devices, such as optical correlators, have been designed to use the principles of optical computing. Such devices can be used, for example, to detect and track objects, and to classify serial time-domain optical data.""
""Reversible logic "",,,,""Reversible computing is a model of computing where the computational process to some extent is reversible, i.e., time-invertible. In a computational model that uses transitions from one state of the abstract machine to another, a necessary condition for reversibility is that the relation of the mapping from states to their successors must be one-to-one. Reversible computing is generally considered an unconventional form of computing.

""
""Plasmonics "",,,,""Surface plasmons (SPs) are coherent delocalized electron oscillations that exist at the interface between any two materials where the real part of the dielectric function changes sign across the interface (e.g. a metal-dielectric interface, such as a metal sheet in air). SPs have lower energy than bulk (or volume) plasmons which quantise the longitudinal electron oscillations about positive ion cores within the bulk of an electron gas (or plasma).
The charge motion in a surface plasmon always creates electromagnetic fields outside (as well as inside) the metal. The total excitation, including both the charge motion and associated electromagnetic field, is called either a surface plasmon polariton at a planar interface, or a localized surface plasmon for the closed surface of a small particle.
The existence of surface plasmons was first predicted in 1957 by Rufus Ritchie. In the following two decades, surface plasmons were extensively studied by many scientists, the foremost of whom were T. Turbadar in the 1950s and 1960s, and Heinz Raether, E. Kretschmann, and A. Otto in the 1960s and 1970s. Information transfer in nanoscale structures, similar to photonics, by means of surface plasmons, is referred to as plasmonics.""
""Single electron devices "",,,,""Capacitance is the ability of a body to store an electrical charge. A material with a large capacitance holds more electric charge at a given voltage, than one with low capacitance. Any object that can be electrically charged exhibits capacitance, however the concept is particularly important for understanding the operations of the capacitor, one of the three fundamental electronic components (along with resistors and inductors).
The SI unit of capacitance is the farad (symbol: F), named after the English physicist Michael Faraday. A 1 farad capacitor, when charged with 1 coulomb of electrical charge, has a potential difference of 1 volt between its plates.""
""Tunneling devices "",,,,""Magnesium oxide (MgO), or magnesia, is a white hygroscopic solid mineral that occurs naturally as periclase and is a source of magnesium (see also oxide). It has an empirical formula of MgO and consists of a lattice of Mg2+ ions and O2− ions held together by ionic bonding. Magnesium hydroxide forms in the presence of water (MgO + H2O → Mg(OH)2), but it can be reversed by heating it to separate moisture.
Magnesium oxide was historically known as magnesia alba (literally, the white mineral from magnesia - other sources give magnesia alba as MgCO3), to differentiate it from magnesia negra, a black mineral containing what is now known as manganese.
While "magnesium oxide" normally refers to MgO, magnesium peroxide MgO2 is also known as a compound. According to evolutionary crystal structure prediction, MgO2 is thermodynamically stable at pressures above 116 GPa (gigapascals), and a totally new semiconducting suboxide Mg3O2 is thermodynamically stable above 500 GPa. Because of its stability, MgO is used as a model system for investigating vibrational properties of crystals.""
""Quantum communication and cryptography "",,,,""Anton Zeilinger (German: [ˈtsaɪlɪŋɐ]; born 20 May 1945) is an Austrian quantum physicist who in 2008 received the Inaugural Isaac Newton Medal of the Institute of Physics (UK) for "his pioneering conceptual and experimental contributions to the foundations of quantum physics, which have become the cornerstone for the rapidly-evolving field of quantum information". Zeilinger is professor of physics at the University of Vienna and Senior Scientist at the Institute for Quantum Optics and Quantum Information IQOQI at the Austrian Academy of Sciences. Most of his research concerns the fundamental aspects and applications of quantum entanglement.""
""Quantum dots and cellular automata "",,,,""A quantum cellular automaton or QCA is an abstract model of quantum computation, devised in analogy to conventional models of cellular automata introduced by von Neumann. The same name may also refer to quantum dot cellular automata, which are a proposed physical implementation of "classical" cellular automata by exploiting quantum mechanical phenomena. QCA have attracted a lot of attention as a result of its extremely small feature size (at the molecular or even atomic scale) and its ultra-low power consumption, making it one candidate for replacing CMOS technology.

""
""Spintronics and magnetic technologies "",,,,""ACEnet or the Atlantic Computational Excellence Network is a partnership of nine Atlantic Canada institutions to organize themselves into a large scale high-performance computing (HPC) facilities for research. The nine institutions include; Memorial University of Newfoundland, University of New Brunswick, Mount Allison University, Dalhousie University, St. Francis Xavier University, Saint Mary's University and the University of Prince Edward Island, Cape Breton University, Mount St. Vincent University and Acadia University.
ACEnet also partners with:
CANARIE
Grid Canada
C3
The Canada Foundation for Innovation (CFI)
ACEnet is one of seven HPC consortia in Canada that, under the newly announced National Platform Fund (CFI) will share resources and collaborate on various research projects that would never be undertaken by a single entity. The other HPC networks in Canada are:
CLUMEQ
HPCVL
SHARCNET
WestGrid
RQCHP
SciNet""
""Reduced instruction set computing "",,,,""Reduced instruction set computing, or RISC (pronounced 'risk'), is a CPU design strategy based on the insight that a simplified instruction set (as opposed to a complex set) provides higher performance when combined with a microprocessor architecture capable of executing those instructions using fewer microprocessor cycles per instruction. A computer based on this strategy is a reduced instruction set computer, also called RISC. The opposing architecture is called complex instruction set computing, i.e. CISC.
Various suggestions have been made regarding a precise definition of RISC, but the general concept is that of a system that uses a small, highly optimized set of instructions, rather than a more versatile set of instructions often found in other types of architectures. Another common trait is that RISC systems use the load/store architecture, where memory is normally accessed only through specific instructions, rather than accessed as part of other instructions like add.
Although a number of systems from the 1960s and 70s have been identified as being forerunners of RISC, the modern version of the design dates to the 1980s. In particular, two projects at Stanford University and University of California, Berkeley are most associated with the popularization of this concept. Stanford's design would go on to be commercialized as the successful MIPS architecture, while Berkeley's RISC gave its name to the entire concept, commercialized as the SPARC. Another success from this era were IBM's efforts that eventually led to the Power Architecture. As these projects matured, a wide variety of similar designs flourished in the late 1980s and especially the early 1990s, representing a major force in the Unix workstation market as well as embedded processors in laser printers, routers and similar products.
Well-known RISC families include DEC Alpha, AMD Am29000, ARC, ARM, Atmel AVR, Blackfin, Intel i860 and i960, MIPS, Motorola 88000, PA-RISC, Power (including PowerPC), RISC-V, SuperH, and SPARC. In the 21st century, the use of ARM architecture processors in smart phones and tablet computers such as the iPad and Android devices provided a wide user base for RISC-based systems. RISC processors are also used in supercomputers such as the K computer, the fastest on the TOP500 list in 2011, second at the 2012 list, and fourth at the 2013 list, and Sequoia, the fastest in 2012 and third in the 2013 list.""
""Complex instruction set computing "",,,,""Complex instruction set computing (CISC /ˈsɪsk/) is a processor design where single instructions can execute several low-level operations (such as a load from memory, an arithmetic operation, and a memory store) or are capable of multi-step operations or addressing modes within single instructions. The term was retroactively coined in contrast to reduced instruction set computer (RISC) and has therefore become something of an umbrella term for everything that is not RISC, i.e. everything from large and complex mainframes to simplistic microcontrollers where memory load and store operations are not separated from arithmetic instructions.
A modern RISC processor can therefore be much more complex than, say, a modern microcontroller using a CISC-labeled instruction set, especially in terms of implementation (electronic circuit complexity), but also in terms of the number of instructions or the complexity of their encoding patterns. The only differentiating characteristic (nearly) "guaranteed" is the fact that most RISC designs use uniform instruction length for (almost) all instructions and employ strictly separate load/store-instructions.
Examples of instruction set architectures that have been retroactively labeled CISC are System/360 through z/Architecture, the PDP-11 and VAX architectures, Data General Nova and many others. Well known microprocessors and microcontrollers that have also been labeled CISC in many academic publications include the Motorola 6800, 6809 and 68000-families; the Intel 8080, iAPX432 and x86-family; the Zilog Z80, Z8 and Z8000-families; the National Semiconductor 32016 and NS320xx-line; the MOS Technology 6502-family; the Intel 8051-family; and others.
Some designs have been regarded as borderline cases by some writers. For instance, the Microchip Technology PIC has been labeled RISC in some circles and CISC in others and the 6502 and 6809 have both been described as "RISC-like", although they have complex addressing modes as well as arithmetic instructions that access memory, contrary to the RISC-principles.""
""Pipeline computing "",,,,""In computing, a pipeline is a set of data processing elements connected in series, where the output of one element is the input of the next one. The elements of a pipeline are often executed in parallel or in time-sliced fashion; in that case, some amount of buffer storage is often inserted between elements.
Computer-related pipelines include:
Instruction pipelines, such as the classic RISC pipeline, which are used in central processing units (CPUs) to allow overlapping execution of multiple instructions with the same circuitry. The circuitry is usually divided up into stages, including instruction decoding, arithmetic, and register fetching stages, wherein each stage processes one instruction at a time.
Graphics pipelines, found in most graphics processing units (GPUs), which consist of multiple arithmetic units, or complete CPUs, that implement the various stages of common rendering operations (perspective projection, window clipping, color and light calculation, rendering, etc.).
Software pipelines, where commands can be written where the output of one operation is automatically fed to the next, following operation. The Unix system call pipe is a classic example of this concept, although other operating systems do support pipes as well.""
""Stack machines "",,,,""In computer science, computer engineering and programming language implementations, a stack machine is a real or emulated computer that uses a pushdown stack rather than individual machine registers to evaluate each sub-expression in the program. A stack computer is programmed with a reverse Polish notation instruction set.
The common alternative to stack machines are register machines, in which each instruction explicitly names the specific registers to use for operand and result values.""
""Very long instruction word "",,,,""Very long instruction word (VLIW) refers to processor architectures designed to take advantage of instruction level parallelism (ILP). Whereas conventional processors mostly allow programs only to specify instructions that will be executed in sequence, a VLIW processor allows programs to explicitly specify instructions that will be executed at the same time (that is, in parallel). This type of processor architecture is intended to allow higher performance without the inherent complexity of some other approaches.""
""Multiple instruction, multiple data "",,,,""In computing, MIMD (multiple instruction, multiple data) is a technique employed to achieve parallelism. Machines using MIMD have a number of processors that function asynchronously and independently. At any time, different processors may be executing different instructions on different pieces of data. MIMD architectures may be used in a number of application areas such as computer-aided design/computer-aided manufacturing, simulation, modeling, and as communication switches. MIMD machines can be of either shared memory or distributed memory categories. These classifications are based on how MIMD processors access memory. Shared memory machines may be of the bus-based, extended, or hierarchical type. Distributed memory machines may have hypercube or mesh interconnection schemes.""
""Cellular architectures "",,,,""A cellular architecture is a type of computer architecture prominent in parallel computing. Cellular architectures are relatively new, with IBM's Cell microprocessor being the first one to reach the market. Cellular architecture takes multi-core architecture design to its logical conclusion, by giving the programmer the ability to run large numbers of concurrent threads within a single processor. Each 'cell' is a compute node containing thread units, memory, and communication. Speed-up is achieved by exploiting thread-level parallelism inherent in many applications.
Cell, a cellular architecture containing 9 cores, is the processor used in the PlayStation 3. Another prominent cellular architecture is Cyclops64, a massively parallel architecture currently under development by IBM.
Cellular architectures follow the low-level programming paradigm, which exposes the programmer to much of the underlying hardware. This allows the programmer to greatly optimize his code for the platform, but at the same time makes it more difficult to develop software.""
""Multiple instruction, single data "",,,,""Flynn's taxonomy is a classification of computer architectures, proposed by Michael J. Flynn in 1966. The classification system has stuck, and has been used as a tool in design of modern processors and their functionalities. Since the rise of multiprocessing central processing units (CPUs), a multiprogramming context has evolved as an extension of the classification system.""
""Single instruction, multiple data "",,,,""Single instruction, multiple data (SIMD), is a class of parallel computers in Flynn's taxonomy. It describes computers with multiple processing elements that perform the same operation on multiple data points simultaneously. Thus, such machines exploit data level parallelism, but not concurrency: there are simultaneous (parallel) computations, but only a single process (instruction) at a given moment. SIMD is particularly applicable to common tasks like adjusting the contrast in a digital image or adjusting the volume of digital audio. Most modern CPU designs include SIMD instructions in order to improve the performance of multimedia use.""
""Systolic arrays "",,,,""In computing, MISD (multiple instruction, single data) is a type of parallel computing architecture where many functional units perform different operations on the same data. Pipeline architectures belong to this type, though a purist might say that the data is different after processing by each stage in the pipeline. Fault-tolerant computers executing the same instructions redundantly in order to detect and mask errors, in a manner known as task replication, may be considered to belong to this type. Not many instances of this architecture exist, as MIMD and SIMD are often more appropriate for common data parallel techniques. Specifically, they allow better scaling and use of computational resources than MISD does. However, one prominent example of MISD in computing are the Space Shuttle flight control computers.
Systolic arrays (< wavefront processors), first described by H. T. Kung and Charles E. Leiserson are an example of MISD architecture. In a typical systolic array, parallel input data flows through a network of hard-wired processor nodes, resembling the human brain which combine, process, merge or sort the input data into a derived result.
Systolic arrays are often hard-wired for a specific operation, such as "multiply and accumulate", to perform massively parallel integration, convolution, correlation, matrix multiplication or data sorting tasks. A Systolic array typically consists of a large monolithic network of primitive computing nodes which can be hardwired or software configured for a specific application. The nodes are usually fixed and identical, while the interconnect is programmable. More general wavefront processors, by contrast, employ sophisticated and individually programmable nodes which may or may not be monolithic, depending on the array size and design parameters. Because the wave-like propagation of data through a systolic array resembles the pulse of the human circulatory system, the name systolic was coined from medical terminology.
A major benefit of systolic arrays is that all operand data and partial results are contained within (passing through) the processor array. There is no need to access external buses, main memory or internal caches during each operation as is the case with standard sequential machines. The sequential limits on parallel performance dictated by Amdahl's theorem also do not apply in the same way, because data dependencies are implicitly handled by the programmable node interconnect.
Systolic arrays are therefore extremely good at artificial intelligence, image processing, pattern recognition, computer vision and other tasks which animal brains do so particularly well. Wavefront processors in general can also be very good at machine learning by implementing self configuring neural nets in hardware.
While systolic arrays are officially classified as MISD, their classification is somewhat problematic. Because the input is typically a vector of independent values, the systolic array is definitely not SISD. Since these input values are merged and combined into the result(s) and do not maintain their independence as they would in a SIMD vector processing unit, the array cannot be classified as such. Consequently, the array cannot be classified as a MIMD either, since MIMD can be viewed as a mere collection of smaller SISD and SIMD machines.
Finally, because the data swarm is transformed as it passes through the array from node to node, the multiple nodes are not operating on the same data, which makes the MISD classification a misnomer. The other reason why a systolic array should not qualify as a MISD is the same as the one which disqualifies it from the SISD category: The input data is typically a vector not a single data value, although one could argue that any given input vector is a single dataset.
All of the above not withstanding, systolic arrays are often offered as a classic example of MISD architecture in textbooks on parallel computing and in the engineering class. If the array is viewed from the outside as atomic it should perhaps be classified as SFMuDMeR = Single Function, Multiple Data, Merged Result(s).""
""Cloud computing "",,,,""Cloud computing, also on-demand computing, is a kind of Internet-based computing that provides shared processing resources and data to computers and other devices on demand. It is a model for enabling ubiquitous, on-demand access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications and services), which can be rapidly provisioned and released with minimal management effort. Cloud computing and storage solutions provide users and enterprises with various capabilities to store and process their data in third-party data centers. It relies on sharing of resources to achieve coherence and economy of scale, similar to a utility (like the electricity grid) over a network.
Advocates claim that cloud computing allows companies to avoid upfront infrastructure costs, and focus on projects that differentiate their businesses instead of on infrastructure. Proponents also claim that cloud computing allows enterprises to get their applications up and running faster, with improved manageability and less maintenance, and enables IT to more rapidly adjust resources to meet fluctuating and unpredictable business demand. Cloud providers typically use a "pay as you go" model. This can lead to unexpectedly high charges if administrators do not adapt to the cloud pricing model.
The present availability of high-capacity networks, low-cost computers and storage devices as well as the widespread adoption of hardware virtualization, service-oriented architecture, and autonomic and utility computing have led to a growth in cloud computing. Companies can scale up as computing needs increase and then scale down again as demands decrease.
Cloud computing has become a highly demanded service or utility due to the advantages of high computing power, cheap cost of services, high performance, scalability, accessibility as well as availability. Some cloud vendors are experiencing growth rates of 50% per year, but being still in a stage of infancy, it has pitfalls that need to be addressed to make cloud computing services more reliable and user friendly.""
""Client-server architectures "",,,,""The client–server model of computing is a distributed application structure that partitions tasks or workloads between the providers of a resource or service, called servers, and service requesters, called clients. Often clients and servers communicate over a computer network on separate hardware, but both client and server may reside in the same system. A server host runs one or more server programs which share their resources with clients. A client does not share any of its resources, but requests a server's content or service function. Clients therefore initiate communication sessions with servers which await incoming requests.
Examples of computer applications that use the client–server model are Email, network printing, and the World Wide Web.""
""n-tier architectures "",,,,""In software engineering, multitier architecture (often referred to as n-tier architecture) is a client–server architecture in which presentation, application processing, and data management functions are physically separated. The most widespread use of multitier architecture is the three-tier architecture.
N-tier application architecture provides a model by which developers can create flexible and reusable applications. By segregating an application into tiers, developers acquire the option of modifying or adding a specific layer, instead of reworking the entire application. A three-tier architecture is typically composed of a presentation tier, a domain logic tier, and a data storage tier.
While the concepts of layer and tier are often used interchangeably, one fairly common point of view is that there is indeed a difference. This view holds that a layer is a logical structuring mechanism for the elements that make up the software solution, while a tier is a physical structuring mechanism for the system infrastructure.""
""Grid computing "",,,,""Grid computing is the collection of computer resources from multiple locations to reach a common goal. The grid can be thought of as a distributed system with non-interactive workloads that involve a large number of files. Grid computing is distinguished from conventional high performance computing systems such as cluster computing in that grid computers have each node set to perform a different task/application. Grid computers also tend to be more heterogeneous and geographically dispersed (thus not physically coupled) than cluster computers. Although a single grid can be dedicated to a particular application, commonly a grid is used for a variety of purposes. Grids are often constructed with general-purpose grid middleware software libraries. Grid sizes can be quite large.
Grids are a form of distributed computing whereby a “super virtual computer” is composed of many networked loosely coupled computers acting together to perform large tasks. For certain applications, “distributed” or “grid” computing, can be seen as a special type of parallel computing that relies on complete computers (with onboard CPUs, storage, power supplies, network interfaces, etc.) connected to a computer network (private or public) by a conventional network interface, such as Ethernet. This is in contrast to the traditional notion of a supercomputer, which has many processors connected by a local high-speed computer bus.""
""Neural networks "",,,,""In machine learning and cognitive science, artificial neural networks (ANNs) are a family of models inspired by biological neural networks (the central nervous systems of animals, in particular the brain) and are used to estimate or approximate functions that can depend on a large number of inputs and are generally unknown. Artificial neural networks are generally presented as systems of interconnected "neurons" which exchange messages between each other. The connections have numeric weights that can be tuned based on experience, making neural nets adaptive to inputs and capable of learning.
For example, a neural network for handwriting recognition is defined by a set of input neurons which may be activated by the pixels of an input image. After being weighted and transformed by a function (determined by the network's designer), the activations of these neurons are then passed on to other neurons. This process is repeated until finally, an output neuron is activated. This determines which character was read.
Like other machine learning methods –  systems that learn from data –  neural networks have been used to solve a wide variety of tasks that are hard to solve using ordinary rule-based programming, including computer vision and speech recognition.""
""Reconfigurable computing "",,,,""Reconfigurable computing is a computer architecture combining some of the flexibility of software with the high performance of hardware by processing with very flexible high speed computing fabrics like field-programmable gate arrays (FPGAs). The principal difference when compared to using ordinary microprocessors is the ability to make substantial changes to the datapath itself in addition to the control flow. On the other hand, the main difference with custom hardware, i.e. application-specific integrated circuits (ASICs) is the possibility to adapt the hardware during runtime by "loading" a new circuit on the reconfigurable fabric.""
""Analog computers "",,,,""An analog computer is a form of computer that uses the continuously changeable aspects of physical phenomena such as electrical, mechanical, or hydraulic quantities to model the problem being solved. In contrast, digital computers represent varying quantities symbolically, as their numerical values change. As an analog computer does not use discrete values, but rather continuous values, processes cannot be reliably repeated with exact equivalence, as they can with Turing machines. Analog computers do not suffer from the quantization noise inherent in digital computers, but are limited instead by analog noise.
Analog computers were widely used in scientific and industrial applications where digital computers of the time lacked sufficient performance. Analog computers can have a very wide range of complexity. Slide rules and nomographs are the simplest, while naval gunfire control computers and large hybrid digital/analog computers were among the most complicated. Systems for process control and protective relays used analog computation to perform control and protective functions.
The advent of digital computing and its success made analog computers largely obsolete in 1950s and 1960s, though they remain in use in some specific applications, like the flight computer in aircraft, and for teaching control systems in universities.
^ Gears of war: When mechanical analog computers ruled the waves | Ars Technica""
""Heterogeneous (hybrid) systems "",,,,""The high-efficiency hybrid cycle (HEHC) is a new 4-stroke thermodynamic cycle combining elements of the Otto cycle, Diesel cycle, Atkinson cycle and Rankine cycle.""
""Self-organizing autonomic computing "",,,,""Self-organization is a process where some form of overall order or coordination arises out of the local interactions between smaller component parts of an initially disordered system. The process of self-organization can be spontaneous, and it is not necessarily controlled by any auxiliary agent outside of the system. It is often triggered by random fluctuations that are amplified by positive feedback. The resulting organization is wholly decentralized or distributed over all the components of the system. As such, the organization is typically robust and able to survive and, even, self-repair substantial damage or perturbations. Chaos theory discusses self-organization in terms of islands of predictability in a sea of chaotic unpredictability. Self-organization occurs in a variety of physical, chemical, biological, robotic, social, and cognitive systems. Examples of its realization can be found in crystallization, thermal convection of fluids, chemical oscillation, animal swarming, and neural networks.
^ Betzler, S. B.; Wisnet, A.; Breitbach, B.; Mitterbauer, C.; Weickert, J.; Schmidt-Mende, L.; Scheu, C. (2014). "Template-free synthesis of novel, highly-ordered 3D hierarchical Nb3O7(OH) superstructures with semiconductive and photoactive properties". Journal of Materials Chemistry A 2 (30): 12005. doi:10.1039/C4TA02202E.""
""Optical computing "",,,,""Optical or photonic computing uses photons produced by lasers or diodes for computation. For decades, photons have promised to allow a higher bandwidth than the electrons used in conventional computers.
Most research projects focus on replacing current computer components with optical equivalents, resulting in an optical digital computer system processing binary data. This approach appears to offer the best short-term prospects for commercial optical computing, since optical components could be integrated into traditional computers to produce an optical-electronic hybrid. However, optoelectronic devices lose 30% of their energy converting electronic energy into photons and back; this conversion also slows the transmission of messages. All-optical computers eliminate the need for optical-electrical-optical (OEO) conversions.
Application-specific devices, such as optical correlators, have been designed to use the principles of optical computing. Such devices can be used, for example, to detect and track objects, and to classify serial time-domain optical data.""
""Quantum computing "",,,,""Quantum computing studies theoretical computation systems (quantum computers) that make direct use of quantum-mechanical phenomena, such as superposition and entanglement, to perform operations on data. Quantum computers are different from digital electronic computers based on transistors. Whereas digital computers require data to be encoded into binary digits (bits), each of which is always in one of two definite states (0 or 1), quantum computation uses quantum bits (qubits), which can be in superpositions of states. A quantum Turing machine is a theoretical model of such a computer, and is also known as the universal quantum computer. Quantum computers share theoretical similarities with non-deterministic and probabilistic computers. The field of quantum computing was initiated by the work of Paul Benioff and Yuri Manin in 1980, Richard Feynman in 1982, and David Deutsch in 1985. A quantum computer with spins as quantum bits was also formulated for use as a quantum space–time in 1968.
As of 2016, the development of actual quantum computers is still in its infancy, but experiments have been carried out in which quantum computational operations were executed on a very small number of quantum bits. Both practical and theoretical research continues, and many national governments and military agencies are funding quantum computing research in an effort to develop quantum computers for civilian, business, trade, environmental and national security purposes, such as cryptanalysis.
Large-scale quantum computers would be able to solve certain problems much more quickly than any classical computers that use even the best currently known algorithms, like integer factorization using Shor's algorithm or the simulation of quantum many-body systems. There exist quantum algorithms, such as Simon's algorithm, that run faster than any possible probabilistic classical algorithm. Given sufficient computational resources, a classical computer could in theory be made to simulate any quantum algorithm, as quantum computation does not violate the Church–Turing thesis. On the other hand, quantum computers may be able to efficiently solve problems that no classical computer would be able to solve within a reasonable amount of time.""
""Molecular computing "",,,,""Unconventional computing is computing by a wide range of new or unusual methods. It is also known as alternative computing. The different methods of unconventional computing include optical computing, quantum computing, chemical computing, natural computing, biologically-inspired computing, wetware computing, DNA computing, molecular computing, amorphous computing, nanocomputing, reversible computing, ternary computing, fluidics, analogue computing, human and domino computation.
Historically, mechanical computers were used in industry before the advent of the transistor. Mechanical computers retain some interest today both in research and as analogue computers. Some mechanical computers have a theoretical or didactic relevance, such as billiard-ball computers or hydraulic ones,. While some are actually simulated, others are not; no attempt is made to build a functioning computer through the mechanical collisions of billiard balls. The domino computer is another theoretically interesting mechanical computing scheme.
Unconventional computing is, according to a recent conference description, "an interdisciplinary research area with the main goal to enrich or go beyond the standard models, such as the Von Neumann computer architecture and the Turing machine, which have dominated computer science for more than half a century". These methods model their computational operations based on non-standard paradigms, and are currently mostly in the research and development stage. This computing behavior can be "simulated" using the classical silicon-based micro-transistors or solid state computing technologies, but aim to achieve a new kind of computing engineering inspired in nature.
The term of "unconventional computation" was coined by Cristian S. Calude and John Casti and used for the first edition of the international Conference, Auckland, New Zealand 1998 (see ).""
""Special purpose systems "",,,,""sysfs is a virtual file system provided by the Linux kernel that exports information about various kernel subsystems, hardware devices, and associated device drivers, from the kernel's device model to user space, through virtual files. In addition to providing information, virtual files can be used for the configuration of their associated devices and kernel subsystems.
sysfs provides functionality similar to the sysctl mechanism found in BSD operating systems, with the difference that sysfs is implemented as a virtual file system instead of being a purpose-built kernel mechanism.""
""Robotic components "",,,,""The Positronic Man is a 1992 novel by Isaac Asimov and Robert Silverberg, based on Asimov's novella The Bicentennial Man.
It tells of a robot that begins to display characteristics, such as creativity, traditionally the province of humans; the robot is ultimately declared an official human being.
The film Bicentennial Man, starring Robin Williams, was based both on the original novella and the novel.
^ http://www.isfdb.org/cgi-bin/pl.cgi?45459
^ http://www.fantasticfiction.co.uk/a/isaac-asimov/positronic-man.htm
^ http://www.isfdb.org/cgi-bin/title.cgi?17097""
""Robotic control "",,,,""Robot control is the study of controlling robots.""
""Robotic autonomy "",,,,""Robot software is the set of coded commands or instructions that tell a mechanical device and electronic system, known together as a robot, what tasks to perform. Robot software is used to perform autonomous tasks. Many software systems and frameworks have been proposed to make programming robots easier.
Some robot software aims at developing intelligent mechanical devices. Common tasks include feedback loops, control, pathfinding, data filtering, and locating.""
""Sensors and actuators "",,,,""AS-Interface (Actuator Sensor Interface, AS-i) is an industrial networking solution (physical layer, data access method and protocol) used in PLC, DCS and PC-based automation systems. It is designed for connecting simple field I/O devices (e.g. binary ON/OFF devices such as actuators, sensors, rotary encoders, analog inputs and outputs, push buttons, and valve position sensors) in discrete manufacturing and process applications using a single 2-conductor cable.
AS-Interface is an 'open' technology supported by a multitude of automation equipment vendors. According to AS-International Association there are currently (2013) over 24 Million AS-Interface field devices installed globally, growing at about 2 million per year.
AS-Interface is a networking alternative to the hard wiring of field devices. It can be used as a partner network for higher level fieldbus networks such as Profibus, DeviceNet, Interbus and Industrial Ethernet, for whom it offers a low-cost remote I/O solution. It is used in automation applications, including conveyor control, packaging machines (e.g. Schubert's), process control valves, bottling plants, electrical distribution systems, airport baggage carousels, elevators, bottling lines and food production lines (e.g. 2SFG).
AS-Interface provides a basis for Functional Safety in machinery safety/emergency stop applications. Safety devices communicating over AS-Interface follow all the normal AS-Interface data rules. The required level of data verification is provided by dynamic changes in the data. This technology is called Safety at Work and allows safety devices and standard, non-safe devices to be connected to the same network. Using appropriate safe input hardware (e.g. light curtains, e-stop buttons, and door interlock switches), AS-Interface can provide safety support up to SIL (Safety Integrity Level) 3 according to EN 62061, CAT 4 according to EN954-1 as well as Performance Level e (PL e) according to EN ISO 13849-1.
The AS-Interface specification is managed by AS-International, a member funded non-profit organization located in Gelnhausen/Germany. Several international daughter organizations exist around the world.""
""System on a chip "",,,,""A system on a chip or system on chip (SoC or SOC) is an integrated circuit (IC) that integrates all components of a computer or other electronic system into a single chip. It may contain digital, analog, mixed-signal, and often radio-frequency functions—all on a single chip substrate. SoCs are very common in the mobile electronics market because of their low power consumption. A typical application is in the area of embedded systems.
The contrast with a microcontroller is one of degree. Microcontrollers typically have under 100 kB of RAM (often just a few kilobytes) and often really are single-chip-systems, whereas the term SoC is typically used for more powerful processors, capable of running software such as the desktop versions of Windows and Linux, which need external memory chips (flash, RAM) to be useful, and which are used with various external peripherals. In short, for larger systems, the term system on a chip is hyperbole, indicating technical direction more than reality: a high degree of chip integration, leading toward reduced manufacturing costs, and the production of smaller systems. Many systems are too complex to fit on just one chip built with a processor optimized for just one of the system's tasks.
When it is not feasible to construct a SoC for a particular application, an alternative is a system in package (SiP) comprising a number of chips in a single package. In large volumes, SoC is believed to be more cost-effective than SiP since it increases the yield of the fabrication and because its packaging is simpler.
Another option, as seen for example in higher end cell phones is package on package stacking during board assembly. The SoC chip includes processors and numerous digital peripherals, and comes in a ball grid package with lower and upper connections. The lower balls connect to the board and various peripherals, with the upper balls in a ring holding the memory buses used to access NAND flash and DDR2 RAM. Memory packages could come from multiple vendors.""
""Firmware "",,,,""In electronic systems and computing, firmware is a type of software that provides control, monitoring and data manipulation of engineered products and systems. Typical examples of devices containing firmware are embedded systems (such as traffic lights, consumer appliances, and digital watches), computers, computer peripherals, mobile phones, and digital cameras. The firmware contained in these devices provides the low-level control program for the device. As of 2013, most firmware can be updated.
Firmware is held in non-volatile memory devices such as ROM, EPROM, or flash memory. Changing the firmware of a device may rarely or never be done during its economic lifetime; some firmware memory devices are permanently installed and cannot be changed after manufacture. Common reasons for updating firmware include fixing bugs or adding features to the device. This may require ROM integrated circuits to be physically replaced, or flash memory to be reprogrammed through a special procedure. Firmware such as the ROM BIOS of a personal computer may contain only elementary basic functions of a device and may only provide services to higher-level software. Firmware such as the program of an embedded system may be the only program that will run on the system and provide all of its functions.
Before integrated circuits, other firmware devices included a discrete semiconductor diode matrix. The Apollo guidance computer had firmware consisting of a specially manufactured core memory plane, called "core rope memory", where data were stored by physically threading wires through (1) or around (0) the core storing each data bit.
^ "Ciena – Acronym Guide". ciena.com. Retrieved 6 February 2016. 
^ Mark, Soper; Prowse, David; Mueller, Scott (September 2012). Authorized Cert Guide: CompTIA A+. Pearson Education. ISBN 978-0-7897-4850-8. 
^ "What is firmware?". incepator.pinzaru.ro. Retrieved 2013-06-14. 
^ Dag Spicer (August 12, 2000). "One Giant Leap: The Apollo Guidance Computer". Dr. Dobbs. Retrieved August 24, 2012.""
""Embedded hardware "",,,,""An embedded system is a computer system with a dedicated function within a larger mechanical or electrical system, often with real-time computing constraints. It is embedded as part of a complete device often including hardware and mechanical parts. Embedded systems control many devices in common use today. 98 percent of all microprocessors are manufactured as components of embedded systems.
Examples of properties of typically embedded computers when compared with general-purpose counterparts are low power consumption, small size, rugged operating ranges, and low per-unit cost. This comes at the price of limited processing resources, which make them significantly more difficult to program and to interact with. However, by building intelligence mechanisms on top of the hardware, taking advantage of possible existing sensors and the existence of a network of embedded units, one can both optimally manage available resources at the unit and network levels as well as provide augmented functions, well beyond those available. For example, intelligent techniques can be designed to manage power consumption of embedded systems.
Modern embedded systems are often based on microcontrollers (i.e. CPUs with integrated memory or peripheral interfaces), but ordinary microprocessors (using external chips for memory and peripheral interface circuits) are also common, especially in more-complex systems. In either case, the processor(s) used may be types ranging from general purpose to those specialised in certain class of computations, or even custom designed for the application at hand. A common standard class of dedicated processors is the digital signal processor (DSP).
Since the embedded system is dedicated to specific tasks, design engineers can optimize it to reduce the size and cost of the product and increase the reliability and performance. Some embedded systems are mass-produced, benefiting from economies of scale.
Embedded systems range from portable devices such as digital watches and MP3 players, to large stationary installations like traffic lights, factory controllers, and largely complex systems like hybrid vehicles, MRI, and avionics. Complexity varies from low, with a single microcontroller chip, to very high with multiple units, peripherals and networks mounted inside a large chassis or enclosure.""
""Embedded software "",,,,""Embedded software is computer software, written to control machines or devices that are not typically thought of as computers. It is typically specialized for the particular hardware that it runs on and has time and memory constraints. This term is sometimes used interchangeably with firmware, although firmware can also be applied to ROM-based code on a computer, on top of which the OS runs, whereas embedded software is typically the only software on the device in question.
A precise and stable characteristic feature is that no or not all functions of embedded software are initiated/controlled via a human interface, but through machine-interfaces instead.
Manufacturers 'build in' embedded software in the electronics in cars, telephones, modems, robots, appliances, toys, security systems, pacemakers, televisions and set-top boxes, and digital watches, for example. This software can be very simple, such as lighting controls running on an 8-bit microprocessor and a few kilobytes of memory with the suitable level of processing complexity determined with a Probably Approximately Correct Computation framework (a methodology based on randomized algorithms), or can become very sophisticated in applications such as airplanes, missiles, and process control systems.""
""Real-time operating systems "",,,,""A real-time operating system (RTOS) is an operating system (OS) intended to serve real-time application process data as it comes in, typically without buffering delays. Processing time requirements (including any OS delay) are measured in tenths of seconds or shorter.
A key characteristic of an RTOS is the level of its consistency concerning the amount of time it takes to accept and complete an application's task; the variability is jitter. A hard real-time operating system has less jitter than a soft real-time operating system. The chief design goal is not high throughput, but rather a guarantee of a soft or hard performance category. An RTOS that can usually or generally meet a deadline is a soft real-time OS, but if it can meet a deadline deterministically it is a hard real-time OS.
An RTOS has an advanced algorithm for scheduling. Scheduler flexibility enables a wider, computer-system orchestration of process priorities, but a real-time OS is more frequently dedicated to a narrow set of applications. Key factors in a real-time OS are minimal interrupt latency and minimal thread switching latency; a real-time OS is valued more for how quickly or how predictably it can respond than for the amount of work it can perform in a given period of time.

""
""Real-time system specification "",,,,""Structured Analysis (SA) in software engineering and its allied technique, Structured Design (SD), are methods for analyzing and converting business requirements into specifications and ultimately, computer programs, hardware configurations and related manual procedures.
Structured analysis and design techniques are fundamental tools of systems analysis, and developed from classical systems analysis of the 1960s and 1970s.

""
""Real-time system architecture "",,,,""Ken Sakamura (坂村 健, Sakamura Ken, born July 25, 1951 in Tokyo, Japan) is a Japanese professor in Information science at the University of Tokyo. He is the creator of the real-time operating system architecture TRON.
In 2001, he shared the Takeda Award for Social/Economic Well-Being with Richard Stallman and Linus Torvalds.
As of 2006, Sakamura leads the ubiquitous networking laboratory (UNL), located in Gotanda, Tokyo as well as the T-Engine forum for consumer electronics. The joint goal of Sakamura's Ubiquitous Networking specification and the T-Engine forum, is to enable any everyday device to broadcast and receive information. It is essentially a dusted-off TRON, paired with a competing standard to RFID.
Since the foundation of the T-Engine forum, Sakamura has been working on opening Japanese technology to the world. His previous brainchild, TRON, the universal RTOS used in Japanese Consumer Electronics has had limited adoption in other countries. Sakamura has inked deals with Chinese and Korean universities to work together on Ubiquitous Networking. He has also worked with French software components manufacturer NexWave Solutions, Inc.""
""Availability "",,,,""In reliability theory and reliability engineering, the term availability has the following meanings:
The degree to which a system, subsystem or equipment is in a specified operable and committable state at the start of a mission, when the mission is called for at an unknown, i.e. a random, time. Simply put, availability is the proportion of time a system is in a functioning condition. This is often described as a mission capable rate. Mathematically, this is expressed as 100% minus unavailability.
The ratio of (a) the total time a functional unit is capable of being used during a given interval to (b) the length of the interval.
For example, a unit that is capable of being used 100 hours per week (168 hours) would have an availability of 100/168. However, typical availability values are specified in decimal (such as 0.9998). In high availability applications, a metric known as nines, corresponding to the number of nines following the decimal point, is used. With this convention, "five nines" equals 0.99999 (or 99.999%) availability.""
""Maintainability and maintenance "",,,,""In systems engineering, dependability is a measure of a system's availability, reliability, and its maintainability, and maintenance support performance, and, in some cases, other characteristics such as durability, safety and security. In software engineering, dependability is the ability to provide services that can defensibly be trusted within a time-period. This may also encompass mechanisms designed to increase and maintain the dependability of a system or software.
The International Electrotechnical Commission (IEC), via its Technical Committee TC 56 develops and maintains international standards that provide systematic methods and tools for dependability assessment and management of equipment, services, and systems throughout their life cycles.
Dependability can be broken down into three elements:
Attributes - A way to assess the dependability of a system
Threats - An understanding of the things that can affect the dependability of a system
Means - Ways to increase a system's dependability

""
""Secondary storage organization "",,,,""Computer data storage, often called storage or memory, is a technology consisting of computer components and recording media used to retain digital data. It is a core function and fundamental component of computers.
The central processing unit (CPU) of a computer is what manipulates data by performing computations. In practice, almost all computers use a storage hierarchy, which puts fast but expensive and small storage options close to the CPU and slower but larger and cheaper options farther away. Often the fast volatile technologies (which lose data when powered off) are referred to as "memory", while slower persistent technologies are referred to as "storage"; however, "memory" is sometimes also used when referring to persistent storage.
In the Von Neumann architecture, the CPU consists of two main parts: control unit and arithmetic logic unit (ALU). The former controls the flow of data between the CPU and memory, while the latter performs arithmetic and logical operations on data.""
""Fault-tolerant network topologies "",,,,""Virtual Link Trunking or VLT is a proprietary aggregation protocol developed by Force10 and available in their datacenter-class or enterprise-class network switches. VLT is implemented in the latest firmware releases (FTOS) for their high-end switches like the S-, Z- and E-series 10/25,40 and 100 Gbit/s datacenter switches. Although VLT is a proprietary protocol from Dell Networking (formerly Force10) other vendors offer similar features to allow users to set up an aggregated link towards two (logical) different switches, where a standard aggregated link can only terminate on a single logical switch (thus either a single physical switch or on different members in a stacked switch setup) like Cisco vpc or MLAG.
VLT is a layer-2 link aggregation protocol between end-devices (servers) connected to (different) access-switches, offering these servers a redundant, load-balancing connection to the core-network in a loop-free environment, eliminating the requirement for the use of a spanning-tree protocol. Where existing link aggregation protocols like (static) LAG (IEEE 802.3ad) or LACP (IEEE 802.1ax) require the different (physical) links to be connected to the same (logical) switch (such as stacked switches), the VLT, for example, allows link connectivity between a server and the network via two different switches.
Instead of using VLT between end-devices like servers it can also be used for uplinks between (access/distribution) switches and the core switches.
A major complication of existing link aggregation or bonding technologies is that all members interfaces of such a team/group need to terminate on one single logical switch. Beside increasing bandwidth another reason for link aggregation is redundancy. To make it possible to connect a LAG to different physical switches is to combine more than one physical switch into one logical switch using switch stacking techniques where the different physical boxes are seen as one logical switch for management and (spanning-tree) topology. The switches running in a stacked configuration always have to run the same firmware, which means that in case of a firmware upgrade the network manager has to implement the new firmware on all stack-members at the same time, resulting in an outage of the entire stack. The alternative is to have different logical switches, but then one of the used links will have to be blocked to prevent a loop-free topology (which can partially be overcome by using Multiple Spanning Tree or Cisco's proprietary per VLAN spanning tree. Spanning Tree Protocol is relatively slow in convergence, which can result in periods that traffic can't be forwarded over the network - and thus leads to outages of the applications.
VLT should address all these issues, making it possible to create a loop free redundant network topology without using the Spanning Tree Protocol. An example configuration how it works can be found on the Blog Geslinux written by Grzegorz Witkowski""
""Layering "",,,,""Layering is a means of plant propagation in which a portion of an aerial stem grows roots while still attached to the parent plant and then detaches as an independent plant. Layering has evolved as a common means of vegetative propagation of numerous species in natural environments. Layering is also utilized by horticulturists to propagate desirable plants.
Natural layering typically occurs when a branch touches the ground, whereupon it produces adventitious roots. At a later stage the connection with the parent plant is severed and a new plant is produced as a result.
The horticultural layering process typically involves wounding the target region to expose the inner stem and optionally applying rooting compounds. In ground layering, the stem is bent down and the target region buried in the soil. This is done in plant nurseries in imitation of natural layering by many plants such as brambles which bow over and touch the tip on the ground, at which point it grows roots and, when separated, can continue as a separate plant. In either case, the rooting process may take from several weeks to a year.
Layering is more complicated than taking cuttings, but has the advantage that the propagated portion continues to receive water and nutrients from the parent plant while it is forming roots. This is important for plants that form roots slowly, or for propagating large pieces. Layering is used quite frequently in the propagation of bonsai; it is also used as a technique for both creating new roots and improving existing roots.""
""Naming and addressing "",,,,""A domain name is an identification string that defines a realm of administrative autonomy, authority or control within the Internet. Domain names are formed by the rules and procedures of the Domain Name System (DNS). Any name registered in the DNS is a domain name. Domain names can also be thought of as a location where certain information or activities can be found.
Domain names are used in various networking contexts and application-specific naming and addressing purposes. In general, a domain name represents an Internet Protocol (IP) resource, such as a personal computer used to access the Internet, a server computer hosting a web site, or the web site itself or any other service communicated via the Internet. In 2015, 294 million domain names had been registered.
Domain names are organized in subordinate levels (subdomains) of the DNS root domain, which is nameless. The first-level set of domain names are the top-level domains (TLDs), including the generic top-level domains (gTLDs), such as the prominent domains com, info, net, edu, and org, and the country code top-level domains (ccTLDs). Below these top-level domains in the DNS hierarchy are the second-level and third-level domain names that are typically open for reservation by end-users who wish to connect local area networks to the Internet, create other publicly accessible Internet resources or run web sites. The registration of these domain names is usually administered by domain name registrars who sell their services to the public.
A fully qualified domain name (FQDN) is a domain name that is completely specified in the hierarchy of the DNS, having no parts omitted.
Labels in the Domain Name System are case-insensitive, and may therefore be written in any desired capitalization method, but most commonly domain names are written in lowercase in technical contexts.""
""Programming interfaces "",,,,""The following is a list of Microsoft APIs and frameworks.

""
""Network protocol design "",,,,""In telecommunications, a communications protocol is a system of rules that allow two or more entities of a communications system to transmit information via any kind of variation of a physical quantity. These are the rules or standard that defines the syntax, semantics and synchronization of communication and possible error recovery methods. Protocols may be implemented by hardware, software, or a combination of both.
Communicating systems use well-defined formats (protocol) for exchanging messages. Each message has an exact meaning intended to elicit a response from a range of possible responses pre-determined for that particular situation. The specified behavior is typically independent of how it is to be implemented. Communications protocols have to be agreed upon by the parties involved. To reach agreement, a protocol may be developed into a technical standard. A programming language describes the same for computations, so there is a close analogy between protocols and programming languages: protocols are to communications as programming languages are to computations.""
""Protocol testing and verification "",,,,""HI-SEAS (Hawaii Space Exploration Analog and Simulation) is an analog habitat for human spaceflight to Mars. HI-SEAS is located in an isolated position on the slopes of the Mauna Loa volcano on the island of Hawaii. The area has Mars-like features and an elevation of approximately 8,200 feet above sea level. HI-SEAS is funded by the NASA Human Research Program for four research missions. The missions are of extended duration from four months to a year.
The purpose of the detailed research studies is to determine what is required to keep a space flight crew happy and healthy during an extended mission to Mars and while living on Mars. Research into food, crew dynamics, behaviors, roles and performance, and other aspects of space flight and a mission on Mars itself is the primary focus. The HI-SEAS researchers also carry out studies on a variety of other topics as part of their daily activities.
^ 
^ Kim A. Binsted and J. B. Hunter (2013). "HI-SEAS (Hawaii Space Exploration Analog and Simulation) as an opportunity for long duration instrument/protocol testing and verification" (PDF). University of Hawaii at Mānoa and Cornell University. Retrieved 30 April 2014. 
^ Chang, Kenneth (20 October 2014). "In a Dome in Hawaii, a Mission to Mars". NASA. Retrieved 20 October 2014. 
^ Mike Wall (14 August 2013). "Mars Food Scientists End 4-Month Mock Space Mission In Hawaii". Space.com. Retrieved 30 April 2014.""
""Formal specifications "",,,,""In computer science, formal specifications are mathematically based techniques whose purpose are to help with the implementation of systems and software. They are used to describe a system, to analyze its behavior, and to aid in its design by verifying key properties of interest through rigorous and effective reasoning tools. These specifications are formal in the sense that they have a syntax, their semantics fall within one domain, and they are able to be used to infer useful information.""
""Link-layer protocols "",,,,""In computer networking, the link layer is the lowest layer in the Internet Protocol Suite, commonly known as TCP/IP, the networking architecture of the Internet. It is described in RFC 1122 and RFC 1123. The link layer is the group of methods and communications protocols that only operate on the link that a host is physically connected to. The link is the physical and logical network component used to interconnect hosts or nodes in the network and a link protocol is a suite of methods and standards that operate only between adjacent network nodes of a local area network segment or a wide area network connection.
Despite the different semantics of layering in TCP/IP and OSI, the link layer is sometimes described as a combination of the data link layer (layer 2) and the physical layer (layer 1) in the OSI model. However, the layers of TCP/IP are descriptions of operating scopes (application, host-to-host, network, link) and not detailed prescriptions of operating procedures, data semantics, or networking technologies.
RFC 1122 exemplifies that local area network protocols such as Ethernet and IEEE 802, and framing protocols such as Point-to-Point Protocol (PPP) belong to the link layer.

""
""Routing protocols "",,,,""A routing protocol specifies how routers communicate with each other, disseminating information that enables them to select routes between any two nodes on a computer network. Routing algorithms determine the specific choice of route. Each router has a priori knowledge only of networks attached to it directly. A routing protocol shares this information first among immediate neighbors, and then throughout the network. This way, routers gain knowledge of the topology of the network.
Although there are many types of routing protocols, three major classes are in widespread use on IP networks:
Interior gateway protocols type 1, link-state routing protocols, such as OSPF and IS-IS
Interior gateway protocols type 2, distance-vector routing protocols, such as Routing Information Protocol, RIPv2, IGRP.
Exterior gateway protocols are routing protocols used on the Internet for exchanging routing information between Autonomous Systems, such as Border Gateway Protocol (BGP), Path Vector Routing Protocol.
Please notice that the term "Exterior gateway protocol" has two meanings. It could mean a category of protocols used to exchange routing information between autonomous systems (see: exterior gateway protocol). It could also mean a specific RFC-described protocol (see: Exterior Gateway Protocol).
Many routing protocols are defined in documents called RFCs.
Some versions of the Open System Interconnection (OSI) networking model distinguish routing protocols in a special sublayer of the Network Layer (Layer 3).
The specific characteristics of routing protocols include the manner in which they avoid routing loops, the manner in which they select preferred routes, using information about hop costs, the time they require to reach routing convergence, their scalability, and other factors.

""
""Signaling protocols "",,,,""A signaling protocol is a type of protocol used to identify signaling encapsulation. Signaling is used to identify the state of connection between telephones or VOIP terminals (IP telephone or PCs or VoWLAN units). The following is a list of signaling protocols:
ALOHA
QSIG
Session Initiation Protocol
H.323
H.225.0

H.248
Media Gateway Control Protocol
Megaco
Signaling System No. 5
Signaling System No. 7
Dual-tone multi-frequency signaling
R1
R2 signalling
NBAP (Node B Application Part)
SCCP (Skinny Call Control Protocol), or 'Skinny' for short
Jingle
Q.931""
""Transport protocols "",,,,""In computer networking, the transport layer is a conceptual division of methods in the layered architecture of protocols in the network stack in the Internet Protocol Suite and the Open Systems Interconnection (OSI). The protocols of the layer provide host-to-host communication services for applications. It provides services such as connection-oriented data stream support, reliability, flow control, and multiplexing.
The details of implementation and semantics of the Transport Layer of the TCP/IP model (RFC 1122), which is the foundation of the Internet, and the Open Systems Interconnection (OSI) model of general networking, are different. In the OSI model the transport layer is most often referred to as Layer 4 or L4, while numbered layers are not used in TCP/IP.
The best-known transport protocol of TCP/IP is the Transmission Control Protocol (TCP), and lent its name to the title of the entire suite. It is used for connection-oriented transmissions, whereas the connectionless User Datagram Protocol (UDP) is used for simpler messaging transmissions. TCP is the more complex protocol, due to its stateful design incorporating reliable transmission and data stream services. Other prominent protocols in this group are the Datagram Congestion Control Protocol (DCCP) and the Stream Control Transmission Protocol (SCTP).

""
""Network policy "",,,,""Network Policy and Access Services (NPAS) is a component of Windows Server 2008. It replaces the Internet Authentication Service (IAS) from Windows Server 2003. NPAS helps you safeguard the health and security of a network. The NPAS server role includes Network Policy Server (NPS), Health Registration Authority (HRA), and Host Credential Authorization Protocol (HCAP). In Windows Server 2003, IAS is the Microsoft implementation of a Remote Authentication Dial-In User Service (RADIUS) server. In Windows Server operating systems later than Windows Server 2003, IAS is renamed to NPS.""
""Network File System (NFS) protocol "",,,,""In computing, Self-certifying File System (SFS) is a global and decentralized, distributed file system for Unix-like operating systems, while also providing transparent encryption of communications as well as authentication. It aims to be the universal distributed file system by providing uniform access to any available server, however, the usefulness of SFS is limited by the low deployment of SFS clients. It was developed in the June 2000 doctoral thesis of David Mazières.""
""Bridges and switches "",,,,""IEEE 802.1Q is the networking standard that supports virtual LANs (VLANs) on an Ethernet network. The standard defines a system of VLAN tagging for Ethernet frames and the accompanying procedures to be used by bridges and switches in handling such frames. The standard also contains provisions for a quality of service prioritization scheme commonly known as IEEE 802.1p and defines the Generic Attribute Registration Protocol.
Portions of the network which are VLAN-aware (i.e., IEEE 802.1Q conformant) can include VLAN tags. When a frame enters the VLAN-aware portion of the network, a tag is added to represent the VLAN membership of the frame's port or the port/protocol combination, depending on whether port-based or port-and-protocol-based VLAN classification is being used. Each frame must be distinguishable as being within exactly one VLAN. A frame in the VLAN-aware portion of the network that does not contain a VLAN tag is assumed to be flowing on the native (or default) VLAN.
The standard was developed by IEEE 802.1, a working group of the IEEE 802 standards committee, and continues to be actively revised. One of the notable revisions is 802.1Q-2014 which incorporated IEEE 802.1aq or (shortest path bridging) and much of the 802.1D standard. In 2012, it was stated by David Allan and Nigel Bragg, in 802.1aq Shortest Path Bridging Design and Evolution: The Architect's Perspective that shortest path bridging is one of the most significant enhancements in Ethernet's history.""
""Repeaters "",,,,""Repeaters is a 2010 Canadian thriller film directed by Carl Bessai, written by Arne Olsen, and starring Dustin Milligan, Amanda Crew, and Richard de Klerk as young drug addicts who find themselves stuck in a time loop.""
""Network adapters "",,,,""A network interface controller (NIC, also known as a network interface card, network adapter, LAN adapter or physical network interface, and by similar terms) is a computer hardware component that connects a computer to a computer network.
Early network interface controllers were commonly implemented on expansion cards that plugged into a computer bus. The low cost and ubiquity of the Ethernet standard means that most newer computers have a network interface built into the motherboard.
Modern network interface controllers offer advanced features such as interrupt and DMA interfaces to the host processors, support for multiple receive and transmit queues, partitioning into multiple logical interfaces, and on-controller network traffic processing such as the TCP offload engine.""
""Network servers "",,,,""In computing, a server is a computer program or a device that provides functionality for other programs or devices, called "clients". This architecture is called the client–server model, and a single overall computation is distributed across multiple processes or devices. Servers can provide various functionalities, often called "services", such as sharing data or resources among multiple clients, or performing computation for a client. A single server can serve multiple clients, and a single client can use multiple servers. A client process may run on the same device or may connect over a network to a server on a different device. Typical servers are database servers, file servers, mail servers, print servers, web servers, game servers, and application servers.
Client–server systems are today most frequently implemented by (and often identified with) the request–response model: a client sends a request to the server, which performs some action and sends a response back to the client, typically with a result or acknowledgement. Designating a computer as "server-class hardware" implies that it is specialized for running servers on it. This often implies that it is more powerful and reliable than standard personal computers, but alternatively, large computing clusters may be composed of many relatively simple, replaceable server components.
^ Windows Server Administration Fundamentals. Microsoft Official Academic Course. 111 River Street, Hoboken, NJ 07030: John Wiley & Sons. 2011. pp. 2–3. ISBN 978-0-470-90182-3. 
^ Comer, Douglas E.; Stevens, David L. (1993). Vol III: Client-Server Programming and Applications. Internetworking with TCP/IP. Department of Computer Sciences, Purdue University, West Lafayette, IN 479: Prentice Hall. pp. 11d. ISBN 0-13-474222-2.""
""Cognitive radios "",,,,""A cognitive radio (CR) is an intelligent radio that can be programmed and configured dynamically. Its transceiver is designed to use the best wireless channels in its vicinity. Such a radio automatically detects available channels in wireless spectrum, then accordingly changes its transmission or reception parameters to allow more concurrent wireless communications in a given spectrum band at one location. This process is a form of dynamic spectrum management.""
""Network domains "",,,,"""Network domain" is a term used to differentiate between multiple private computer networks within the same infrastructure. The term is unofficial and has mostly spread through word of mouth.

""
""Packet classification "",,,,""George Varghese (born 1960) is a Principal Researcher at Microsoft Research. Before joining MSR's lab in Silicon Valley in 2013, he was a Professor of Computer Science at the University of California San Diego, where he led the Internet Algorithms Lab and also worked with the Center for Network Systems and the Center for Internet Epidemiology. He is the author of the textbook Network Algorithmics published by Morgan Kaufmann in 2004.""
""Deep packet inspection "",,,,""Deep Packet Inspection (DPI, also called complete packet inspection and Information eXtraction or IX) is a form of computer network packet filtering that examines the data part (and possibly also the header) of a packet as it passes an inspection point, searching for protocol non-compliance, viruses, spam, intrusions, or defined criteria to decide whether the packet may pass or if it needs to be routed to a different destination, or, for the purpose of collecting statistical information. There are multiple headers for IP packets; network equipment only needs to use the first of these (the IP header) for normal operation, but use of the second header (TCP, UDP etc.) is normally considered to be shallow packet inspection (usually called Stateful Packet Inspection) despite this definition.
There are multiple ways to acquire packets for deep packet inspection. Using port mirroring (sometimes called Span Port) is a very common way, as well as an optical splitter.
Deep Packet Inspection (and filtering) enables advanced network management, user service, and security functions as well as internet data mining, eavesdropping, and internet censorship. Although DPI technology has been used for Internet management for many years, some advocates of net neutrality fear that the technology may be used anticompetitively or to reduce the openness of the Internet.
DPI is used in a wide range of applications, at the so-called "enterprise" level (corporations and larger institutions), in telecommunications service providers, and in governments.""
""Packet scheduling "",,,,""In computing, scheduling is the method by which work specified by some means is assigned to resources that complete the work. The work may be virtual computation elements such as threads, processes or data flows, which are in turn scheduled onto hardware resources such as processors, network links or expansion cards.
A scheduler is what carries out the scheduling activity. Schedulers are often implemented so they keep all computer resources busy (as in load balancing), allow multiple users to share system resources effectively, or to achieve a target quality of service. Scheduling is fundamental to computation itself, and an intrinsic part of the execution model of a computer system; the concept of scheduling makes it possible to have computer multitasking with a single central processing unit (CPU).
A scheduler may aim at one of many goals, for example, maximizing throughput (the total amount of work completed per time unit), minimizing response time (time from work becoming enabled until the first point it begins execution on resources), or minimizing latency (the time between work becoming enabled and its subsequent completion), maximizing fairness (equal CPU time to each process, or more generally appropriate times according to the priority and workload of each process). In practice, these goals often conflict (e.g. throughput versus latency), thus a scheduler will implement a suitable compromise. Preference is given to any one of the concerns mentioned above, depending upon the user's needs and objectives.
In real-time environments, such as embedded systems for automatic control in industry (for example robotics), the scheduler also must ensure that processes can meet deadlines; this is crucial for keeping the system stable. Scheduled tasks can also be distributed to remote devices across a network and managed through an administrative back end.""
""Network control algorithms "",,,,""Wassim Michael Haddad (born July 14, 1961) is a Lebanese-Greek-American applied mathematician, scientist, and engineer, with research specialization in the areas of dynamical systems and control. His research has led to fundamental breakthroughs in applied mathematics, thermodynamics, stability theory, robust control, dynamical system theory, and neuroscience. Professor Haddad is a member of the faculty of the School of Aerospace Engineering at Georgia Institute of Technology, where he holds the rank of Professor and Chair of the Flight Mechanics and Control Discipline. Dr. Haddad is a member of the Academy of Nonlinear Sciences for recognition of paramount contributions to the fields of nonlinear stability theory, nonlinear dynamical systems, and nonlinear control and an IEEE Fellow for contributions to robust, nonlinear, and hybrid control systems.""
""Network design and planning algorithms "",,,,""Liu Gang (born 30 January 1961) is a Chinese scientist who studies mathematical model, computer science, and theoretical physics. He also was one of the most visible of the student leaders in the Tiananmen Square protests of 1989. Liu holds a M.A. in physics from Peking University and a M.A. in computer science from Columbia University. After his exile to the United States in 1996, Liu studied technology and physics at Bell Labs in New Jersey. Liu is currently employed at Morgan Stanley as a Wall Street IT analyst.
As a major leader of the Chinese democracy movement, Liu Gang is a friend of fellow activists Wang Dan and Wuer Kaixi.
^ "From China's Prisons To Columbia's Computers". Columbia University. Retrieved May 5, 1996.  
^ "Tiananmen's Most Wanted". New York Times. Retrieved June 3, 2014.""
""Network economics "",,,,""Network economics refers to business economics that benefit from the network effect. It is also called as Netromix. This is when the value of a good or service increases when others buy the same good or service. Examples are website such as EBay, or iVillage where the community comes together and shares thoughts to help the website become a better business organization. In sustainability, network economics refers to multiple professionals (architects, designers, or related businesses) all working together to develop sustainable products and technologies. The more companies are involved in environmentally friendly production, the easier and cheaper it becomes to produce new sustainable products. For instance, if no one produces sustainable products, it is difficult and expensive to design a sustainable house with custom materials and technology. But due to network economics, the more industries are involved in creating such products, the easier it is to design an environmentally sustainable building.
Another benefit of network economics in a certain field is improvement that results from competition and networking within an industry.""
""Network performance modeling "",,,,""In probability theory, a hyperexponential distribution is a continuous probability distribution whose probability density function of the random variable X is given by

where each Yi is an exponentially distributed random variable with rate parameter λi, and pi is the probability that X will take on the form of the exponential distribution with rate λi. It is named the hyperexponential distribution since its coefficient of variation is greater than that of the exponential distribution, whose coefficient of variation is 1, and the hypoexponential distribution, which has a coefficient of variation smaller than one. While the exponential distribution is the continuous analogue of the geometric distribution, the hyperexponential distribution is not analogous to the hypergeometric distribution. The hyperexponential distribution is an example of a mixture density.
An example of a hyperexponential random variable can be seen in the context of telephony, where, if someone has a modem and a phone, their phone line usage could be modeled as a hyperexponential distribution where there is probability p of them talking on the phone with rate λ1 and probability q of them using their internet connection with rate λ2.

""
""Network simulations "",,,,""In communication and computer network research, network simulation is a technique where a program models the behavior of a network either by calculating the interaction between the different network entities (hosts/packets, etc.) using mathematical formulas, or actually capturing and playing back observations from a production network. The behavior of the network and the various applications and services it supports can then be observed in a test lab; various attributes of the environment can also be modified in a controlled manner to assess how the network would behave under different conditions.

""
""Network experimentation "",,,,""Packet Tracer is a cross-platform visual simulation program designed by Cisco Systems that allows users to create network topologies and imitate modern computer networks. The software allows users to simulate the configuration of Cisco routers and switches using a simulated command line interface. Packet Tracer makes use of a drag and drop user interface, allowing users to add and remove simulated network devices as they see fit. The software is mainly focused towards Certified Cisco Network Associate Academy students as an educational tool for helping them learn fundamental CCNA concepts. Students enrolled in a CCNA Academy program can freely download and use the tool free of charge for educational use.
In addition to simulating certain aspects of computer networks, Packet Tracer can also be used for collaboration. As of Packet Tracer 5.0, Packet Tracer supports a multi-user system that enables multiple users to connect multiple topologies together over a computer network. Packet Tracer also allows instructors to create activities that students have to complete. Packet Tracer is often used in educational settings as a learning aid. Cisco Systems claims that Packet Tracer is useful for network experimentation.""
""Network performance analysis "",,,,""The J programming language, developed in the early 1990s by Kenneth E. Iverson and Roger Hui, is a synthesis of APL (also by Iverson) and the FP and FL function-level languages created by John Backus.
To avoid repeating the APL special-character problem, J requires only the basic ASCII character set, resorting to the use of the dot and colon as "inflections" to form short words similar to digraphs. Most such "primary" (or "primitive") J words serve as mathematical symbols, with the dot or colon extending the meaning of the basic characters available. Additionally, many characters which might need to be balanced in other languages (such as [] {} "" `` or <>) are treated by J as stand-alone words or, when inflected, as single-character roots of multi-character words.
J is a very terse array programming language, and is most suited to mathematical and statistical programming, especially when performing operations on matrices. It has also been used in Extreme Programming and network performance analysis.
Like the original FP/FL languages, J supports function-level programming via its tacit programming features (note that function-level programming is not the same as functional programming).
Unlike most languages that support object-oriented programming, J's flexible hierarchical namespace scheme (where every name exists in a particular locale) can be effectively used as a framework for both class-based and prototype-based object-oriented programming.
Since March 2011, J is free and open source software under the GPLv3 license. One may also purchase source for commercial use under a negotiated license.

""
""Network measurement "",,,,""In computer networks, network traffic measurement is the process of measuring the amount and type of traffic on a particular network. This is especially important with regard to effective bandwidth management.""
""Security protocols "",,,,""SPORE, the Security Protocols Open Repository, is an online library of security protocols with comments and links to papers. Each protocol is downloadable in a variety of formats, including rules for use with automatic protocol verification tools. All protocols are described using BAN logic or the style used by Clark and Jacob, and their goals. The database includes details on formal proofs or known attacks, with references to comments, analysis & papers. A large number of protocols are listed, including many which have been shown to be insecure.
It is a continuation of the seminal work by John Clark and Jeremy Jacob.
They seek contributions for new protocols, links and comments.""
""Web protocol security "",,,,""The File Transfer Protocol (FTP) is a standard network protocol used to transfer computer files between a client and server on a computer network.
FTP is built on a client-server model architecture and uses separate control and data connections between the client and the server. FTP users may authenticate themselves with a clear-text sign-in protocol, normally in the form of a username and password, but can connect anonymously if the server is configured to allow it. For secure transmission that protects the username and password, and encrypts the content, FTP is often secured with SSL/TLS (FTPS). SSH File Transfer Protocol (SFTP) is sometimes also used instead, but is technologically different.
The first FTP client applications were command-line programs developed before operating systems had graphical user interfaces, and are still shipped with most Windows, Unix, and Linux operating systems. Many FTP clients and automation utilities have since been developed for desktops, servers, mobile devices, and hardware, and FTP has been incorporated into productivity applications, such as Web page editors.""
""Mobile and wireless security "",,,,""The Center for Technological Research of Crete (CTR-Crete) (Greek: Κέντρο Τεχνολογικής Έρευνας (ΚΤΕ) Κρήτης) in Heraklion was founded according to the presidential decree No. 143/Φ.Ε.Κ. 123/20-6-2001 and is under the supervision and financing of the Ministry of National Education and Religious Affairs (Greece). It is a Private Legal Body, self-governed within the framework of articles 11 and 12 par. 1 of 1771/1998 Greek law and its internal regulation. There are 55 staff member, including 25 research scientists, 15 assistant researchers, and 15 associate researchers. CTR-Crete is affiliated with the Technological Educational Institute (TEI) of Crete and includes eight Sectors of Technology Transfer and Research operating in all four regional units of Crete.""
""Denial-of-service attacks "",,,,""In computing, a denial-of-service (DoS) attack is an attempt to make a machine or network resource unavailable to its intended users, such as to temporarily or indefinitely interrupt or suspend services of a host connected to the Internet. A distributed denial-of-service (DDoS) is where the attack source is more than one, often thousands of, unique IP addresses. It is analogous to a group of people crowding the entry door or gate to a shop or business, and not letting legitimate parties enter into the shop or business, disrupting normal operations.
Criminal perpetrators of DoS attacks often target sites or services hosted on high-profile web servers such as banks, credit card payment gateways; but motives of revenge, blackmail or activism can be behind other attacks.""
""Short-range networks "",,,,""Ultra-wideband (also known as UWB, ultra-wide band and ultraband) is a radio technology pioneered by Robert A. Scholtz and others that can use a very low energy level for short-range, high-bandwidth communications over a large portion of the radio spectrum. UWB has traditional applications in non-cooperative radar imaging. Most recent applications target sensor data collection, precision locating and tracking applications.
Unlike spread spectrum, UWB transmits in a manner that does not interfere with conventional narrowband and carrier wave transmission in the same frequency band.
Ultra-wideband is a technology for transmitting information spread over a large bandwidth (>500 MHz); this should, in theory and under the right circumstances, be able to share spectrum with other users. Regulatory settings by the Federal Communications Commission (FCC) in the United States intend to provide an efficient use of radio bandwidth while enabling high-data-rate personal area network (PAN) wireless connectivity; longer-range, low-data-rate applications; and radar and imaging systems.
Ultra wideband was formerly known as pulse radio, but the FCC and the International Telecommunication Union Radiocommunication Sector (ITU-R) currently define UWB as an antenna transmission for which emitted signal bandwidth exceeds the lesser of 500 MHz or 20% of fractional bandwidth. Thus, pulse-based systems—where each transmitted pulse occupies the UWB bandwidth (or an aggregate of at least 500 MHz of narrow-band carrier; for example, orthogonal frequency-division multiplexing (OFDM)—can access the UWB spectrum under the rules. Pulse repetition rates may be either low or very high. Pulse-based UWB radars and imaging systems tend to use low repetition rates (typically in the range of 1 to 100 megapulses per second). On the other hand, communications systems favor high repetition rates (typically in the range of one to two gigapulses per second), thus enabling short-range gigabit-per-second communications systems. Each pulse in a pulse-based UWB system occupies the entire UWB bandwidth (thus reaping the benefits of relative immunity to multipath fading, but not intersymbol interference), unlike carrier-based systems, which are subject to deep fading and intersymbol interference.""
""Local area networks "",,,,""A local area network (LAN) is a computer network that interconnects computers within a limited area such as a residence, school, laboratory, or office building. A local area network is contrasted in principle to a wide area network (WAN), which covers a larger geographic distance and may involve leased telecommunication circuits, while the media for LANs are locally managed.
Ethernet over twisted pair cabling and Wi-Fi are the two most common transmission technologies in use for local area networks. Historical technologies include ARCNET, Token Ring, and AppleTalk.""
""Metropolitan area networks "",,,,""A metropolitan area network (MAN) is a computer network larger than a local area network, covering an area of a few city blocks to the area of an entire city, possibly also including the surrounding areas.""
""Wide area networks "",,,,""A wide area network (WAN) is a telecommunications network or computer network that extends over a large geographical distance. Wide area networks are often established with leased telecommunication circuits.
Business, education and government entities use wide area networks to relay data among staff, students, clients, buyers, and suppliers from various geographical locations. In essence, this mode of telecommunication allows a business to effectively carry out its daily function regardless of location. The Internet may be considered a WAN.
Related terms for other types of networks are personal area networks (PANs), local area networks (LANs), campus area networks (CANs), or metropolitan area networks (MANs) which are usually limited to a room, building, campus or specific metropolitan area respectively.""
""Very long-range networks "",,,,""A small-world network is a type of mathematical graph in which most nodes are not neighbors of one another, but most nodes can be reached from every other node by a small number of hops or steps. Specifically, a small-world network is defined to be a network where the typical distance L between two randomly chosen nodes (the number of steps required) grows proportionally to the logarithm of the number of nodes N in the network, that is:

In the context of a social network, this results in the small world phenomenon of strangers being linked by a short chain of acquaintances. Many empirical graphs show the small-world effect, e.g., social networks, the underlying architecture of the Internet, wikis such as Wikipedia, and gene networks.
A certain category of small-world networks were identified as a class of random graphs by Duncan Watts and Steven Strogatz in 1998. They noted that graphs could be classified according to two independent structural features, namely the clustering coefficient, and average node-to-node distance (also known as average shortest path length). Purely random graphs, built according to the Erdős–Rényi (ER) model, exhibit a small average shortest path length (varying typically as the logarithm of the number of nodes) along with a small clustering coefficient. Watts and Strogatz measured that in fact many real-world networks have a small average shortest path length, but also a clustering coefficient significantly higher than expected by random chance. Watts and Strogatz then proposed a novel graph model, currently named the Watts and Strogatz model, with (i) a small average shortest path length, and (ii) a large clustering coefficient. The crossover in the Watts–Strogatz model between a "large world" (such as a lattice) and a small world was first described by Barthelemy and Amaral in 1999. This work was followed by a large number of studies, including exact results (Barrat and Weigt, 1999; Dorogovtsev and Mendes; Barmpoutis and Murray, 2010).
^ http://www.nature.com/nature/journal/v393/n6684/full/393440a0.html
^ Watts, Duncan J.; Strogatz, Steven H. (June 1998). "Collective dynamics of 'small-world' networks". Nature 393 (6684): 440–442. Bibcode:1998Natur.393..440W. doi:10.1038/30918. PMID 9623998.  Papercore Summary http://www.papercore.org/Watts1998
^ Barthelemy, M.; Amaral, LAN (1999). "Small-world networks: Evidence for a crossover picture". Phys. Rev. Lett. 82 (15): 3180. arXiv:cond-mat/9903108. Bibcode:1999PhRvL..82.3180B. doi:10.1103/PhysRevLett.82.3180.""
""Topology analysis and generation "",,,,""Mesh generation is the practice of generating a polygonal or polyhedral mesh that approximates a geometric domain. The term "grid generation" is often used interchangeably. Typical uses are for rendering to a computer screen or for physical simulation such as finite element analysis or computational fluid dynamics. The input model form can vary greatly but common sources are CAD, NURBS, B-rep, STL (file format) or a point cloud. The field is highly interdisciplinary, with contributions found in mathematics, computer science, and engineering.
Three-dimensional meshes created for finite element analysis need to consist of tetrahedra, pyramids, prisms or hexahedra. Those used for the finite volume method can consist of arbitrary polyhedra. Those used for finite difference methods usually need to consist of piecewise structured arrays of hexahedra known as multi-block structured meshes. A mesh is otherwise a discretization of a domain existing in one, two or three dimensions.""
""Logical / virtual topologies "",,,,""Logical topology, or signal topology, is the arrangement of devices on a computer network and how they communicate with one another. How devices are connected to the network through the actual cables that transmit data, or the physical structure of the network, is called the physical topology. Physical topology defines how the systems are physically connected. It represents the physical layout of the devices on the network. The logical topology defines how the systems communicate across the physical topologies.
Logical topologies are bound to network protocols and describe how data is moved across the network. There are attempts to study the logical topology of the Internet by network scientists such as Albert-László Barabási. EXAMPLE : twisted pair Ethernet is a logical bus topology in a physical star topology layout. while IBM's token ring is a logical ring topology, it is physically set up in star topology.""
""Bus networks "",,,,""Lippstadt (German: [ˈlɪpʃtat]) is a town in North Rhine-Westphalia, Germany. It is the largest town within the district of Soest.""
""Star networks "",,,,""Star networks are one of the most common computer network topologies. In its simplest form, a star network consists of one central switch, hub or computer, which acts as a conduit to transmit messages. This consists of a central node, to which all other nodes are connected; this central node provides a common connection point for all nodes through a hub. In star topology, every node (computer workstation or any other peripheral) is connected to a central node called a hub or switch. The switch is the server and the peripherals are the clients. Thus, the hub and leaf nodes, and the transmission lines between them, form a graph with the topology of a star. If the central node is passive, the originating node must be able to tolerate the reception of an echo of its own transmission, delayed by the two-way transmission time (i.e. to and from the central node) plus any delay generated in the central node. An active star network has an active central node that usually has the means to prevent echo-related problems.
The star topology reduces the damage caused by line failure by connecting all of the systems to a central node. When applied to a bus-based network, this central hub rebroadcasts all transmissions received from any peripheral node to all peripheral nodes on the network, sometimes including the originating node. All peripheral nodes may thus communicate with all others by transmitting to, and receiving from, the central node only. The failure of a transmission line linking any peripheral node to the central node will result in the isolation of that peripheral node from all others, but the rest of the systems will be unaffected.
It is also designed with each node (file servers, workstations, and peripherals) connected directly to a central network hub, switch, or concentrator.
Data on a star network passes through the hub, switch, or concentrator before continuing to its destination. The hub, switch, or concentrator manages and controls all functions of the network. It also acts as a repeater for the data flow. This configuration is common with twisted pair cable. However, it can also be used with coaxial cable or optical fibre cable.

""
""Token ring networks "",,,,""Token bus is a network implementing the token ring protocol over a "virtual ring" on a coaxial cable. A token is passed around the network nodes and only the node possessing the token may transmit. If a node doesn't have anything to send, the token is passed on to the next node on the virtual ring. Each node must know the address of its neighbour in the ring, so a special protocol is needed to notify the other nodes of connections to, and disconnections from, the ring.
Token bus was standardized by IEEE standard 802.4. It is mainly used for industrial applications. Token bus was used by General Motors for their Manufacturing Automation Protocol (MAP) standardization effort. This is an application of the concepts used in token ring networks. The main difference is that the endpoints of the bus do not meet to form a physical ring.
Due to difficulties handling device failures and adding new stations to a network, token bus gained a reputation for being unreliable and difficult to upgrade.
In order to guarantee the packet delay and transmission in Token bus protocol, a modified Token bus was proposed in Manufacturing Automation Systems and flexible manufacturing system (FMS).
A means for carrying Internet Protocol over token bus was developed.
The IEEE 802.4 Working Group is disbanded and the standard has been withdrawn by the IEEE.""
""Fiber distributed data interface (FDDI) "",,,,""IEEE 802.6 is a standard governed by the ANSI for Metropolitan Area Networks (MAN). It is an improvement of an older standard (also created by ANSI) which used the Fiber distributed data interface (FDDI) network structure. The FDDI-based standard failed due to its expensive implementation and lack of compatibility with current LAN standards. The IEEE 802.6 standard uses the Distributed Queue Dual Bus (DQDB) network form. This form supports 150 Mbit/s transfer rates. It consists of two unconnected unidirectional buses. DQDB is rated for a maximum of 160 km before significant signal degradation over fiberoptic cable with an optical wavelength of 1310 nm.
This standard has also failed, mostly for the same reasons that the FDDI standard failed. Most MANs now use Synchronous Optical Network (SONET) or Asynchronous Transfer Mode (ATM) network designs, with recent designs using native Ethernet or MPLS.""
""Wireless mesh networks "",,,,""A wireless mesh network (WMN) is a communications network made up of radio nodes organized in a mesh topology. It is also a form of wireless ad hoc network. Wireless mesh networks often consist of mesh clients, mesh routers and gateways. The mesh clients are often laptops, cell phones and other wireless devices while the mesh routers forward traffic to and from the gateways which may, but need not, be connected to the Internet. The coverage area of the radio nodes working as a single network is sometimes called a mesh cloud. Access to this mesh cloud is dependent on the radio nodes working in harmony with each other to create a radio network. A mesh network is reliable and offers redundancy. When one node can no longer operate, the rest of the nodes can still communicate with each other, directly or through one or more intermediate nodes. Wireless mesh networks can self form and self heal. Wireless mesh networks can be implemented with various wireless technologies including 802.11, 802.15, 802.16, cellular technologies and need not be restricted to any one technology or protocol.""
""Network dynamics "",,,,""Network dynamics is the study of networks that change in time. These networks could be from the fields of biology, sociology, physics, economics, computer science, graph theory etc.
For a dynamical systems approach to network dynamics, see sequential dynamical system.

""
""Error detection and error correction "",,,,""In information theory, the Hamming distance between two strings of equal length is the number of positions at which the corresponding symbols are different. In another way, it measures the minimum number of substitutions required to change one string into the other, or the minimum number of errors that could have transformed one string into the other.
A major application is in coding theory, more specifically to block codes, in which the equal-length strings are vectors over a finite field.""
""Network mobility "",,,,""Proxy Mobile IPv6 (or PMIPv6, or PMIP) is a network-based mobility management protocol standardized by IETF and is specified in RFC 5213. It is a protocol for building a common and access technology independent of mobile core networks, accommodating various access technologies such as WiMAX, 3GPP, 3GPP2 and WLAN based access architectures. Proxy Mobile IPv6 is the only network-based mobility management protocol standardized by IETF.""
""Network manageability "",,,,""Carrier Grade Linux (CGL) is a set of specifications which detail standards of availability, scalability, manageability, and service response characteristics which must be met in order for Linux kernel-based operating system to be considered "carrier grade" (i.e. ready for use within the telecommunications industry). The term is particularly applicable as telecom converges technically with data networks and commercial off-the-shelf commoditized components such as blade servers.
Carrier-grade is a term for public network telecommunications products that require up to 5 nines or 6 nines (or 99.999 to 99.9999 percent) availability, which translates to downtime per year of 30 seconds (6 nines) to 5 minutes (5 nines). The term "5 nines" is usually associated with carrier-class servers, while "6 nines" is usually associated with carrier-class switches.""
""Network privacy and anonymity "",,,,""Anonymity, adjective "anonymous", is derived from the Greek word ἀνωνυμία, anonymia, meaning "without a name" or "namelessness". In colloquial use, "anonymous" is used to describe situations where the acting person's name is unknown. It can be said as not using your own name, simply. Some writers have argued that namelessness, though technically correct, does not capture what is more centrally at stake in contexts of anonymity. The important idea here is that a person be non-identifiable, unreachable, or untrackable. Anonymity is seen as a technique, or a way of realizing, certain other values, such as privacy, or liberty.
An important example for anonymity being not only protected, but enforced by law is probably the vote in free elections. In many other situations (like conversation between strangers, buying some product or service in a shop), anonymity is traditionally accepted as natural. There are also various situations in which a person might choose to withhold their identity. Acts of charity have been performed anonymously when benefactors do not wish to be acknowledged. A person who feels threatened might attempt to mitigate that threat through anonymity. A witness to a crime might seek to avoid retribution, for example, by anonymously calling a crime tipline. Criminals might proceed anonymously to conceal their participation in a crime. Anonymity may also be created unintentionally, through the loss of identifying information due to the passage of time or a destructive event.
In certain situations, however, it may be illegal to remain anonymous. In the United States, 24 states have “stop and identify” statutes that requires persons detained to self-identify when requested by a law enforcement officer. In Germany, people have to indicate their names at the door of their homes.
The term "anonymous message" typically refers to a message that does not reveal its sender. In many countries, anonymous letters are protected by law and must be delivered as regular letters.
In mathematics, in reference to an arbitrary element (e.g., a human, an object, a computer), within a well-defined set (called the "anonymity set"), "anonymity" of that element refers to the property of that element of not being identifiable within this set. If it is not identifiable, then the element is said to be "anonymous."

""
""Naming and addressing "",,,,""A domain name is an identification string that defines a realm of administrative autonomy, authority or control within the Internet. Domain names are formed by the rules and procedures of the Domain Name System (DNS). Any name registered in the DNS is a domain name. Domain names can also be thought of as a location where certain information or activities can be found.
Domain names are used in various networking contexts and application-specific naming and addressing purposes. In general, a domain name represents an Internet Protocol (IP) resource, such as a personal computer used to access the Internet, a server computer hosting a web site, or the web site itself or any other service communicated via the Internet. In 2015, 294 million domain names had been registered.
Domain names are organized in subordinate levels (subdomains) of the DNS root domain, which is nameless. The first-level set of domain names are the top-level domains (TLDs), including the generic top-level domains (gTLDs), such as the prominent domains com, info, net, edu, and org, and the country code top-level domains (ccTLDs). Below these top-level domains in the DNS hierarchy are the second-level and third-level domain names that are typically open for reservation by end-users who wish to connect local area networks to the Internet, create other publicly accessible Internet resources or run web sites. The registration of these domain names is usually administered by domain name registrars who sell their services to the public.
A fully qualified domain name (FQDN) is a domain name that is completely specified in the hierarchy of the DNS, having no parts omitted.
Labels in the Domain Name System are case-insensitive, and may therefore be written in any desired capitalization method, but most commonly domain names are written in lowercase in technical contexts.""
""Cloud computing "",,,,""Cloud computing, also on-demand computing, is a kind of Internet-based computing that provides shared processing resources and data to computers and other devices on demand. It is a model for enabling ubiquitous, on-demand access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications and services), which can be rapidly provisioned and released with minimal management effort. Cloud computing and storage solutions provide users and enterprises with various capabilities to store and process their data in third-party data centers. It relies on sharing of resources to achieve coherence and economy of scale, similar to a utility (like the electricity grid) over a network.
Advocates claim that cloud computing allows companies to avoid upfront infrastructure costs, and focus on projects that differentiate their businesses instead of on infrastructure. Proponents also claim that cloud computing allows enterprises to get their applications up and running faster, with improved manageability and less maintenance, and enables IT to more rapidly adjust resources to meet fluctuating and unpredictable business demand. Cloud providers typically use a "pay as you go" model. This can lead to unexpectedly high charges if administrators do not adapt to the cloud pricing model.
The present availability of high-capacity networks, low-cost computers and storage devices as well as the widespread adoption of hardware virtualization, service-oriented architecture, and autonomic and utility computing have led to a growth in cloud computing. Companies can scale up as computing needs increase and then scale down again as demands decrease.
Cloud computing has become a highly demanded service or utility due to the advantages of high computing power, cheap cost of services, high performance, scalability, accessibility as well as availability. Some cloud vendors are experiencing growth rates of 50% per year, but being still in a stage of infancy, it has pitfalls that need to be addressed to make cloud computing services more reliable and user friendly.""
""Location based services "",,,,""Location-based services (LBS) are a general class of computer program-level services that use location data to control features. As such LBS is an information service and has a number of uses in social networking today as an entertainment service, which is accessible with mobile devices through the mobile network and which uses information on the geographical position of the mobile device. This has become more and more important with the expansion of the smartphone and tablet markets as well.
LBS are used in a variety of contexts, such as health, indoor object search, entertainment, work, personal life, etc.
LBS is critical to many businesses as well as government organizations to drive real insight from data tied to a specific location where activities take place. The spatial patterns that location-related data and services can provide is one on its most powerful and useful aspect where location is a common denominator in all of these activities and can be leveraged to better understand patterns and relationships.
LBS include services to identify a location of a person or object, such as discovering the nearest banking cash machine (a.k.a. ATM) or the whereabouts of a friend or employee. LBS include parcel tracking and vehicle tracking services. LBS can include mobile commerce when taking the form of coupons or advertising directed at customers based on their current location. They include personalized weather services and even location-based games. They are an example of telecommunication convergence.
This concept of location based systems is not compliant with the standardized concept of real-time locating systems (RTLS) and related local services, as noted in ISO/IEC 19762-5 and ISO/IEC 24730-1. While networked computing devices generally do very well to inform consumers of days old data, the computing devices themselves can also be tracked, even in real-time. LBS privacy issues arise in that context, and are documented below.""
""In-network processing "",,,,""A network processor is an integrated circuit which has a feature set specifically targeted at the networking application domain.
Network processors are typically software programmable devices and would have generic characteristics similar to general purpose central processing units that are commonly used in many different types of equipment and products.""
""Network management "",,,,""Below, there is information on the different methods of network management.""
""Network monitoring "",,,,""Network monitoring is the use of a system that constantly monitors a computer network for slow or failing components and that notifies the network administrator (via email, SMS or other alarms) in case of outages. It is part of network management.""
""Network on chip "",,,,""Network on chip or network on a chip (NoC or NOC) is a communication subsystem on an integrated circuit (commonly called a "chip"), typically between intellectual property (IP) cores in a system on a chip (SoC). NoCs can span synchronous and asynchronous clock domains or use unclocked asynchronous logic. NoC technology applies networking theory and methods to on-chip communication and brings notable improvements over conventional bus and crossbar interconnections. NoC improves the scalability of SoCs, and the power efficiency of complex SoCs compared to other designs.""
""Home networks "",,,,""A home network or home area network (HAN) is a type of local area network with the purpose to facilitate communication among digital devices present inside or within the close vicinity of a home. Devices capable of participating in this network, for example, smart devices such as network printers and handheld mobile computers, often gain enhanced emergent capabilities through their ability to interact. These additional capabilities can be used to increase the quality of life inside the home in a variety of ways, such as automation of repetitious tasks, increased personal productivity, enhanced home security, and easier access to entertainment.""
""Storage area networks "",,,,""A storage area network (SAN) is a network which provides access to consolidated, block level data storage. SANs are primarily used to enhance storage devices, such as disk arrays, tape libraries, and optical jukeboxes, accessible to servers so that the devices appear to the operating system as locally attached devices. A SAN typically has its own network of storage devices that are generally not accessible through the local area network (LAN) by other devices. The cost and complexity of SANs dropped in the early 2000s to levels allowing wider adoption across both enterprise and small to medium-sized business environments.
A SAN does not provide file abstraction, only block-level operations. However, file systems built on top of SANs do provide file-level access, and are known as shared-disk file systems.""
""Sensor networks "",,,,""Wireless sensor networks (WSN), sometimes called wireless sensor and actuator networks (WSAN), are spatially distributed autonomous sensors to monitor physical or environmental conditions, such as temperature, sound, pressure, etc. and to cooperatively pass their data through the network to a main location. The more modern networks are bi-directional, also enabling control of sensor activity. The development of wireless sensor networks was motivated by military applications such as battlefield surveillance; today such networks are used in many industrial and consumer applications, such as industrial process monitoring and control, machine health monitoring, and so on.
The WSN is built of "nodes" – from a few to several hundreds or even thousands, where each node is connected to one (or sometimes several) sensors. Each such sensor network node has typically several parts: a radio transceiver with an internal antenna or connection to an external antenna, a microcontroller, an electronic circuit for interfacing with the sensors and an energy source, usually a battery or an embedded form of energy harvesting. A sensor node might vary in size from that of a shoebox down to the size of a grain of dust, although functioning "motes" of genuine microscopic dimensions have yet to be created. The cost of sensor nodes is similarly variable, ranging from a few to hundreds of dollars, depending on the complexity of the individual sensor nodes. Size and cost constraints on sensor nodes result in corresponding constraints on resources such as energy, memory, computational speed and communications bandwidth. The topology of the WSNs can vary from a simple star network to an advanced multi-hop wireless mesh network. The propagation technique between the hops of the network can be routing or flooding.
In computer science and telecommunications, wireless sensor networks are an active research area with numerous workshops and conferences arranged each year, for example IPSN, SenSys, and EWSN.""
""Mobile networks "",,,,""A mobile network operator or MNO, also known as a wireless service provider, wireless carrier, cellular company, or mobile network carrier, is a provider of services wireless communications that owns or controls all the elements necessary to sell and deliver services to an end user including radio spectrum allocation, wireless network infrastructure, back haul infrastructure, billing, customer care, provisioning computer systems and marketing and repair organizations.
In addition to obtaining revenue by offering retail services under its own brand, an MNO may also sell access to network services at wholesale rates to mobile virtual network operators.
A key defining characteristic of a mobile network operator is that an MNO must own or control access to a radio spectrum license from a regulatory or government entity. A second key defining characteristic of an MNO is that an MNO must own or control the elements of the network infrastructure necessary to provide services to subscribers over the licensed spectrum.
A mobile network operator typically also has the necessary provisioning, billing and customer care computer systems and the marketing, customer care and engineering organizations needed to sell, deliver and bill for services, however, an MNO can outsource any of these systems or functions and still be considered a mobile network operator.""
""Peer-to-peer networks "",,,,""Peer-to-peer (P2P) computing or networking is a distributed application architecture that partitions tasks or work loads between peers. Peers are equally privileged, equipotent participants in the application. They are said to form a peer-to-peer network of nodes.
Peers make a portion of their resources, such as processing power, disk storage or network bandwidth, directly available to other network participants, without the need for central coordination by servers or stable hosts. Peers are both suppliers and consumers of resources, in contrast to the traditional client-server model in which the consumption and supply of resources is divided. Emerging collaborative P2P systems are going beyond the era of peers doing similar things while sharing resources, and are looking for diverse peers that can bring in unique resources and capabilities to a virtual community thereby empowering it to engage in greater tasks beyond those that can be accomplished by individual peers, yet that are beneficial to all the peers.
While P2P systems had previously been used in many application domains, the architecture was popularized by the file sharing system Napster, originally released in 1999. The concept has inspired new structures and philosophies in many areas of human interaction. In such social contexts, peer-to-peer as a meme refers to the egalitarian social networking that has emerged throughout society, enabled by Internet technologies in general.""
""World Wide Web (network structure) "",,,,""The World Wide Web (WWW) is an information space where documents and other web resources are identified by URLs, interlinked by hypertext links, and can be accessed via the Internet. The World Wide Web was invented by English scientist Tim Berners-Lee in 1989. He wrote the first web browser in 1990 while employed at CERN in Switzerland.
It has become known simply as the Web. The World Wide Web was central to the development of the Information Age and is the primary tool billions of people use to interact on the Internet.
Web pages are primarily text documents formatted and annotated with Hypertext Markup Language (HTML). In addition to formatted text, web pages may contain images, video, and software components that are rendered in the user's web browser as coherent pages of multimedia content. Embedded hyperlinks permit users to navigate between web pages. Multiple web pages with a common theme, a common domain name, or both, may be called a website. Website content can largely be provided by the publisher, or interactive where users contribute content or the content depends upon the user or their actions. Websites may be mostly informative, primarily for entertainment, or largely for commercial purposes.""
""Online social networks "",,,,""A social networking service (also social networking site or SNS) is a platform to build social networks or social relations among people who share similar interests, activities, backgrounds or real-life connections. A social network service consists of a representation of each user (often a profile), his or her social links, and a variety of additional services such as career services. Social network sites are web-based services that allow individuals to create a public profile, create a list of users with whom to share connections, and view and cross the connections within the system. Most social network services are web-based and provide means for users to interact over the Internet, such as e-mail and instant messaging. Social network sites are varied and they incorporate new information and communication tools such as mobile connectivity, photo/video/sharing and blogging. Online community services are sometimes considered a social network service, though in a broader sense, social network service usually means an individual-centered service whereas online community services are group-centered. Social networking sites allow users to share ideas, pictures, posts, activities, events, and interests with people in their network.
According to the Oxford Dictionary, a "social network" is a dedicated website or other application that enables users to communicate with each other by posting information, comments, messages, images, etc. The main types of social networking services are those that contain category places (such as former school year or classmates), means to connect with friends (usually with self-description pages), and a recommendation system linked to trust. Popular methods now combine many of these, with American-based services such as Facebook, Google+, LinkedIn, Instagram, Pinterest, Vine, Tumblr, and Twitter widely used worldwide; Wechat, Sina Weibo, and Tencent QQ in China; Nexopia in Canada; Badoo, Bebo, Vkontakte (Russia), Delphi, Draugiem.lv (Latvia), iWiW (Hungary), Nasza-Klasa (Poland), Soup (Austria), Glocals in Switzerland, Skyrock, The Sphere, StudiVZ (Germany), Tagged, Tuenti (mostly in Spain), Myspace, Xanga and XING in parts of Europe; Hi5 in South America and Central America; Mxit in Africa; CarnivalPics based in Nigeria; Cyworld, Mixi, Renren, Friendster, Sina Weibo and Wretch in Asia and the Pacific Islands. Social network services can be split into three types: socializing social network services are primarily for socializing with existing friends (e.g., Facebook); networking social network services are primarily for non-social interpersonal communication (e.g., LinkedIn); and social navigation social network services are primarily for helping users to find specific information or resources (e.g., Goodreads for books).
There have been attempts to standardize these services to avoid the need to duplicate entries of friends and interests (see the FOAF standard). A study reveals that India has recorded world's largest growth in terms of social media users in 2013. A 2013 survey found that 73% of U.S. adults use social networking sites.""
""Wireless local area networks "",,,,""A wireless local area network (WLAN) is a wireless computer network that links two or more devices using a wireless distribution method (often spread-spectrum or OFDM radio) within a limited area such as a home, school, computer laboratory, or office building. This gives users the ability to move around within a local coverage area and still be connected to the network, and can provide a connection to the wider Internet. Most modern WLANs are based on IEEE 802.11 standards, marketed under the Wi-Fi brand name.
Wireless LANs have become popular in the home due to ease of installation and use, and in commercial complexes offering wireless access to their customers; often for free. New York City, for instance, has begun a pilot program to provide city workers in all five boroughs of the city with wireless Internet access.""
""Wireless personal area networks "",,,,""A personal area network (PAN) is a computer network used for data transmission among devices such as computers, telephones and personal digital assistants. PANs can be used for communication among the personal devices themselves (intrapersonal communication), or for connecting to a higher level network and the Internet (an uplink).
A wireless personal area network (WPAN) is a PAN carried over wireless network technologies such as:
INSTEON
IrDA
Wireless USB
Bluetooth
Z-Wave
ZigBee
Body Area Network
The reach of a WPAN varies from a few centimeters to a few meters. A PAN may also be carried over wired computer buses such as USB and FireWire.""
""Mobile ad hoc networks "",,,,""A mobile ad hoc network (MANET) is a continuously self-configuring, infrastructure-less network of mobile devices connected without wires. Ad hoc is Latin and means "for this" (i.e., for this purpose).
Each device in a MANET is free to move independently in any direction, and will therefore change its links to other devices frequently. Each must forward traffic unrelated to its own use, and therefore be a router. The primary challenge in building a MANET is equipping each device to continuously maintain the information required to properly route traffic. Such networks may operate by themselves or may be connected to the larger Internet. They may contain one or multiple and different transceivers between nodes. This results in a highly dynamic, autonomous topology[1].
MANETs are a kind of Wireless ad hoc network that usually has a routable networking environment on top of a Link Layer ad hoc network. MANETs consist of a peer-to-peer, self-forming, self-healing network. MANETs circa 2000-2015 typically communicate at radio frequencies (30 MHz - 5 GHz)
The growth of laptops and 802.11/Wi-Fi wireless networking have made MANETs a popular research topic since the mid-1990s. Many academic papers evaluate protocols and their abilities, assuming varying degrees of mobility within a bounded space, usually with all nodes within a few hops of each other. Different protocols are then evaluated based on measures such as the packet drop rate, the overhead introduced by the routing protocol, end-to-end packet delays, network throughput, ability to scale, etc.""
""Public Internet "",,,,""The Internet is the global system of interconnected mainframe, personal, and wireless computer networks that use the Internet protocol suite (TCP/IP) to link billions of devices worldwide. It is a network of networks that consists of millions of private, public, academic, business, and government networks of local to global scope, linked by a broad array of electronic, wireless, and optical networking technologies. The Internet carries an extensive range of information resources and services, such as the inter-linked hypertext documents and applications of the World Wide Web (WWW), electronic mail, Usenet newsgroups, telephony, and peer-to-peer networks for file sharing.
Although the Internet protocol suite has been used by academia and the military industrial complex since the early 1980s, rapid adoption of its use was driven by events of the late 1980s and 1990s such as more powerful and affordable computers, the advent of fiber optics, the popularization of HTTP and the Web browser, and a push towards opening the technology to commerce. Internet use grew rapidly in the West from the mid-1990s and from the late 1990s in the developing world. In the 21 years since 1995, Internet use has grown 100-times to reach over one third of the world population, leading to its services and technologies being incorporated into virtually every aspect of contemporary life. The impact of the Internet has been so immense that it has been referred to as the "8th continent".
The origins of the Internet date back to research and development commissioned by the United States government, the United Kingdom government and the government of France in the 1960s to build robust, fault-tolerant communication via computer networks. This work led to the primary precursor networks: the ARPANET, in the United States, the Mark 1 NPL network in the United Kingdom and CYCLADES in France. The interconnection of regional academic networks in the 1980s marks the beginning of the transition to the modern Internet. From the late 1980s onward, the network experienced sustained exponential growth as generations of institutional, personal, and mobile computers were connected to it.
Most traditional communications media, including telephony and television, are being reshaped or redefined by the Internet, giving birth to new services such as Internet telephony and Internet television. Newspaper, book, and other print publishing are adapting to website technology, or are reshaped into blogging and web feeds. The entertainment industry was initially the fastest growing segment on the Internet. The Internet has enabled and accelerated new forms of personal interactions through instant messaging, Internet forums, and social networking. Online shopping has grown exponentially both for major retailers and small artisans and traders. Business-to-business and financial services on the Internet affect supply chains across entire industries.
The Internet has no centralized governance in either technological implementation or policies for access and usage; each constituent network sets its own policies. Only the overreaching definitions of the two principal name spaces in the Internet, the Internet Protocol address space and the Domain Name System (DNS), are directed by a maintainer organization, the Internet Corporation for Assigned Names and Numbers (ICANN). The technical underpinning and standardization of the core protocols are an activity of the Internet Engineering Task Force (IETF), a non-profit organization of loosely affiliated international participants that anyone may associate with by contributing technical expertise.""
""E-commerce infrastructure "",,,,""Founded in January 2002, the Center for E-Commerce Infrastructure Development (CECID) is a research and development center in the University of Hong Kong committed to promoting e-commerce infrastructure development and standardization. A member of OASIS, W3C, RosettaNet, and the ebXML Asia Committee, CECID actively takes part in the development and implementation of international standards, such as Universal Business Language, Web Services, and RosettaNet. Through participation in these international and regional standards bodies, CECID follows closely the latest developments in e-commerce technology standards and promotes Hong Kong's e-commerce technology to technical communities overseas.
CECID's operation is primarily financed by R&D grants from the Innovation and Technology Commission of the Hong Kong Government for its two flagship research projects, namely Project Phoenix and Project Pyxis. In its completed Project Phoenix, CECID has produced several software packages that implement major ebXML specifications. These software packages include Hermes Message Service Handler, ebMail, and ebXMLRR Registry/Repository and are currently released under open source licenses on the freebXML.org website that CECID established in 2002. Commenced in 2004, Project Pyxis targets to develop enabling technology for e-business interoperability between trading partners and within large enterprises using various complementary and competing Web Services standards.""
""Interpreters "",,,,""Interpretation or interpreting is the facilitating of oral or sign-language communication, either simultaneously or consecutively, between users of different languages. Translation studies is the systematic study of the theory, description and application of interpretation and translation.
An interpreter is a person who converts a thought or expression in a source language into an expression with a comparable meaning in a target language either simultaneously in "real time" or consecutively when the speaker pauses after completing one or two sentences.
The interpreter's objective is to convey every semantic element as well as tone and register and every intention and feeling of the message that the source-language speaker is directing to target-language recipients (except in summary interpretation, used sometimes in conferences)
For written speeches and lectures, sometimes the reading of pre-translated texts is used.""
""Message oriented middleware "",,,,""Message-oriented middleware (MOM) is software or hardware infrastructure supporting sending and receiving messages between distributed systems. MOM allows application modules to be distributed over heterogeneous platforms and reduces the complexity of developing applications that span multiple operating systems and network protocols. The middleware creates a distributed communications layer that insulates the application developer from the details of the various operating systems and network interfaces. APIs that extend across diverse platforms and networks are typically provided by MOM.
MOM provides software elements that reside in all communicating components of a client/server architecture and typically support asynchronous calls between the client and server applications. MOM reduces the involvement of application developers with the complexity of the master-slave nature of the client/server mechanism.

""
""Reflective middleware "",,,,""A distributed operating system is a software over a collection of independent, networked, communicating, and physically separate computational nodes. Each individual node holds a specific software subset of the global aggregate operating system. Each subset is a composite of two distinct service provisioners. The first is a ubiquitous minimal kernel, or microkernel, that directly controls that node’s hardware. Second is a higher-level collection of system management components that coordinate the node's individual and collaborative activities. These components abstract microkernel functions and support user applications.
The microkernel and the management components collection work together. They support the system’s goal of integrating multiple resources and processing functionality into an efficient and stable system. This seamless integration of individual nodes into a global system is referred to as transparency, or single system image; describing the illusion provided to users of the global system’s appearance as a single computational entity.""
""Embedded middleware "",,,,""Middleware in the context of distributed applications is software that provides services beyond those provided by the operating system to enable the various components of a distributed system to communicate and manage data. Middleware supports and simplifies complex distributed applications. It includes web servers, application servers, messaging and similar tools that support application development and delivery. Middleware is especially integral to modern information technology based on XML, SOAP, Web services, and service-oriented architecture.
Middleware often enables interoperability between applications that run on different operating systems, by supplying services so the application can exchange data in a standards-based way. Middleware sits "in the middle" between application software that may be working on different operating systems. It is similar to the middle layer of a three-tier single system architecture, except that it is stretched across multiple systems or applications. Examples include EAI software, telecommunications software, transaction monitors, and messaging-and-queueing software.
The distinction between operating system and middleware functionality is, to some extent, arbitrary. While core kernel functionality can only be provided by the operating system itself, some functionality previously provided by separately sold middleware is now integrated in operating systems. A typical example is the TCP/IP stack for telecommunications, nowadays included in virtually every operating system.""
""Virtual machines "",,,,""In computing, a virtual machine (VM) is an emulation of a particular computer system. Virtual machines operate based on the computer architecture and functions of a real or hypothetical computer, and their implementations may involve specialized hardware, software, or a combination of both.
Various different kinds of virtual machines exist, each with different functions. System virtual machines (also known as full virtualization VMs) provide a complete substitute for the targeted real machine and a level of functionality required for the execution of a complete operating system. A hypervisor uses native execution to share and manage hardware, allowing multiple different environments, isolated from each other, to be executed on the same physical machine. Modern hypervisors use hardware-assisted virtualization, which provides efficient and full virtualization by using virtualization-specific hardware capabilities, primarily from the host CPUs. Process virtual machines are designed to execute a single computer program by providing an abstracted and platform-independent program execution environment. Some virtual machines, such as QEMU, are designed to also emulate different architectures and allow execution of software applications and operating systems written for another CPU or architecture. Operating-system-level virtualization allows the resources of a computer to be partitioned via the kernel's support for multiple isolated user space instances, which are usually called containers and may look and feel like real machines to the end users.""
""File systems management "",,,,""TACTIC is a web-based, open source smart process application and digital asset management system supported by Southpaw Technology in Toronto, ON. Designed to optimize busy production environments with high volumes of content traffic, TACTIC applies business or workflow logic to combined database and file system management. Using elements of digital asset management, production asset management and workflow management, TACTIC tracks the creation and development of digital assets through production pipelines. TACTIC is available under both commercial and open-source licenses, and also as a hosted cloud service through Amazon Web Services Marketplace.""
""Virtual memory "",,,,""In computing, virtual memory is a memory management technique that is implemented using both hardware and software. It maps memory addresses used by a program, called virtual addresses, into physical addresses in computer memory. Main storage as seen by a process or task appears as a contiguous address space or collection of contiguous segments. The operating system manages virtual address spaces and the assignment of real memory to virtual memory. Address translation hardware in the CPU, often referred to as a memory management unit or MMU, automatically translates virtual addresses to physical addresses. Software within the operating system may extend these capabilities to provide a virtual address space that can exceed the capacity of real memory and thus reference more memory than is physically present in the computer.
The primary benefits of virtual memory include freeing applications from having to manage a shared memory space, increased security due to memory isolation, and being able to conceptually use more memory than might be physically available, using the technique of paging.""
""Main memory "",,,,""Computer data storage, often called storage or memory, is a technology consisting of computer components and recording media used to retain digital data. It is a core function and fundamental component of computers.
The central processing unit (CPU) of a computer is what manipulates data by performing computations. In practice, almost all computers use a storage hierarchy, which puts fast but expensive and small storage options close to the CPU and slower but larger and cheaper options farther away. Often the fast volatile technologies (which lose data when powered off) are referred to as "memory", while slower persistent technologies are referred to as "storage"; however, "memory" is sometimes also used when referring to persistent storage.
In the Von Neumann architecture, the CPU consists of two main parts: control unit and arithmetic logic unit (ALU). The former controls the flow of data between the CPU and memory, while the latter performs arithmetic and logical operations on data.""
""Allocation / deallocation strategies "",,,,""Resource Acquisition Is Initialization (RAII) is a programming idiom used in several object-oriented languages, most prominently C++, where it originated, but also D, Ada, Vala, and Rust. The technique was developed for exception-safe resource management in C++ during 1984–89, primarily by Bjarne Stroustrup and Andrew Koenig, and the term itself was coined by Stroustrup. RAII is generally pronounced as an initialism, sometimes pronounced as "R, A, double I".
In RAII, holding a resource is tied to object lifetime: resource allocation (acquisition) is done during object creation (specifically initialization), by the constructor, while resource deallocation (release) is done during object destruction, by the destructor. If objects are destroyed properly, resource leaks do not occur.
Other names for this idiom include Constructor Acquires, Destructor Releases (CADRe)  and one particular style of use is called Scope-based Resource Management (SBRM). This latter term is for the special case of automatic variables. RAII ties resources to object lifetime, which may not coincide with entry and exit of a scope. (Notably variables allocated on the free store have lifetimes unrelated to any given scope.) However, using RAII for automatic variables (SBRM) is the most common use case.""
""Distributed memory "",,,,""In computer science, distributed memory refers to a multiprocessor computer system in which each processor has its own private memory. Computational tasks can only operate on local data, and if remote data is required, the computational task must communicate with one or more remote processors. In contrast, a shared memory multiprocessor offers a single memory space used by all processors. Processors do not have to be aware where data resides, except that there may be performance penalties, and that race conditions are to be avoided.""
""Secondary storage "",,,,""Auxiliary memory, also known as auxiliary storage, secondary storage, secondary memory or external memory, is a non-volatile memory (does not lose stored data when the device is powered down) that is not directly accessible by the CPU, because it is not accessed via the input/output channels (it is an external device). In RAM devices (as flash memory) data can be directly deleted or changed.
It is used to store a large amount of data at lesser cost per byte than primary memory; secondary storage is two orders of magnitude less expensive than primary storage.""
""Scheduling "",,,,""A schedule or a timetable, as a basic time-management tool, consists of a list of times at which possible tasks, events, or actions are intended to take place, or of a sequence of events in the chronological order in which such things are intended to take place. The process of creating a schedule - deciding how to order these tasks and how to commit resources between the variety of possible tasks - is called scheduling, and a person responsible for making a particular schedule may be called a scheduler. Making and following schedules is an ancient human activity.
Some scenarios associate "this kind of planning" with learning "life skills". Schedules are necessary, or at least useful, in situations where individuals need to know what time they must be at a specific location to receive a specific service, and where people need to accomplish a set of goals within a set time period.
Schedules can usefully span both short periods, such as a daily or weekly schedule, and long-term planning with respect to periods of several months or years. They are often made using a calendar, where the person making the schedule can note the dates and times at which various events are planned to occur. Schedules that do not set forth specific times for events to occur may instead list algorithmically an expected order in which events either can or must take place.
In some situations, schedules can be uncertain, such as where the conduct of daily life relies on environmental factors outside of human control. People who are vacationing or otherwise seeking to reduce stress and achieve relaxation may intentionally avoid having a schedule for a certain period of time.""
""Deadlocks "",,,,""In concurrent programming, a deadlock is a situation in which two or more competing actions are each waiting for the other to finish, and thus neither ever does.
In a transactional database, a deadlock happens when two processes each within its own transaction updates two rows of information but in the opposite order. For example, process A updates row 1 then row 2 in the exact timeframe that process B updates row 2 then row 1. Process A can't finish updating row 2 until process B is finished, but process B cannot finish updating row 1 until process A is finished. No matter how much time is allowed to pass, this situation will never resolve itself and because of this, database management systems will typically kill the transaction of the process that has done the least amount of work.
In an operating system, a deadlock is a situation which occurs when a process or thread enters a waiting state because a resource requested is being held by another waiting process, which in turn is waiting for another resource held by another waiting process. If a process is unable to change its state indefinitely because the resources requested by it are being used by another waiting process, then the system is said to be in a deadlock.
Deadlock is a common problem in multiprocessing systems, parallel computing and distributed systems, where software and hardware locks are used to handle shared resources and implement process synchronization.
In telecommunication systems, deadlocks occur mainly due to lost or corrupt signals instead of resource contention.""
""Multiprocessing / multiprogramming / multitasking "",,,,""In computing, multitasking is a concept of performing multiple tasks (also known as processes) over a certain period of time by executing them concurrently. New tasks start and interrupt already started ones before they have reached completion, instead of executing the tasks sequentially so each started task needs to reach its end before a new one is started. As a result, a computer executes segments of multiple tasks in an interleaved manner, while the tasks share common processing resources such as central processing units (CPUs) and main memory.
Multitasking does not necessarily mean that multiple tasks are executing at exactly the same time. In other words, multitasking does not imply parallel execution, but it does mean that more than one task can be part-way through execution at the same time, and that more than one task is advancing over a given period of time. Even on multiprocessor or multicore computers, which have multiple CPUs/cores so more than one task can be executed at once (physically, one per CPU or core), multitasking allows many more tasks to be run than there are CPUs.
In the case of a computer with a single CPU, only one task is said to be running at any point in time, meaning that the CPU is actively executing instructions for that task. Multitasking solves the problem by scheduling which task may be the one running at any given time, and when another waiting task gets a turn. The act of reassigning a CPU from one task to another one is called a context switch; the illusion of parallelism is achieved when context switches occur frequently enough. Operating systems may adopt one of many different scheduling strategies, which generally fall into the following categories:
In multiprogramming systems, the running task keeps running until it performs an operation that requires waiting for an external event (e.g. reading from a tape) or until the computer's scheduler forcibly swaps the running task out of the CPU. Multiprogramming systems are designed to maximize CPU usage.
In time-sharing systems, the running task is required to relinquish the CPU, either voluntarily or by an external event such as a hardware interrupt. Time sharing systems are designed to allow several programs to execute apparently simultaneously.
In real-time systems, some waiting tasks are guaranteed to be given the CPU when an external event occurs. Real time systems are designed to control mechanical devices such as industrial robots, which require timely processing.
The term "multitasking" has become an international term, as the same word is used in many other languages such as German, Italian, Dutch, Danish and Norwegian.""
""Mutual exclusion "",,,,""In computer science, mutual exclusion refers to the requirement of ensuring that no two concurrent processes are in their critical section at the same time; it is a basic requirement in concurrency control, to prevent race conditions. Here, a critical section refers to a period when the process accesses a shared resource, such as shared memory. The requirement of mutual exclusion was first identified and solved by Edsger W. Dijkstra in his seminal 1965 paper titled Solution of a problem in concurrent programming control, and is credited as the first topic in the study of concurrent algorithms.
A simple example of why mutual exclusion is important in practice can be visualized using a singly linked list (See Figure 1). In such a linked list, the removal of a node is done by changing the "next" pointer of the preceding node to point to the subsequent node (e.g., if node i is being removed then the "next" pointer of node i − 1 will be changed to point to node i + 1). In an execution where such a linked list is being shared between multiple processes, two processes may attempt to remove two different nodes simultaneously, resulting in the following problem: let nodes i and i + 1 be the nodes to be removed; furthermore, let neither of them be the head nor the tail; the next pointer of node i − 1 will be changed to point to node i + 1 and the next pointer of node i will be changed to point to node i + 2. Although both removal operations complete successfully, node i + 1 remains in the list since i − 1 was made to point to i + 1, skipping node i (which was the node that reflected the removal of i + 1 by having its next pointer set to i + 2). This can be seen in Figure 1. This problem (normally called a race condition) can be avoided by using the requirement of mutual exclusion to ensure that simultaneous updates to the same part of the list cannot occur.""
""Concurrency control "",,,,""In information technology and computer science, especially in the fields of computer programming, operating systems, multiprocessors, and databases, concurrency control ensures that correct results for concurrent operations are generated, while getting those results as quickly as possible.
Computer systems, both software and hardware, consist of modules, or components. Each component is designed to operate correctly, i.e., to obey or to meet certain consistency rules. When components that operate concurrently interact by messaging or by sharing accessed data (in memory or storage), a certain component's consistency may be violated by another component. The general area of concurrency control provides rules, methods, design methodologies, and theories to maintain the consistency of components operating concurrently while interacting, and thus the consistency and correctness of the whole system. Introducing concurrency control into a system means applying operation constraints which typically result in some performance reduction. Operation consistency and correctness should be achieved with as good as possible efficiency, without reducing performance below reasonable levels. Concurrency control can require significant additional complexity and overhead in a concurrent algorithm compared to the simpler sequential algorithm.
For example, a failure in concurrency control can result in data corruption from torn read or write operations.""
""Power management "",,,,""Power Management is a feature of some electrical appliances, especially copiers, computers, GPUs and computer peripherals such as monitors and printers, that turns off the power or switches the system to a low-power state when inactive. In computing this is known as PC power management and is built around a standard called ACPI. This supersedes APM. All recent (consumer) computers have ACPI support.
In the military, ""Power Management"" often refers to suites of equipment which permit soldiers and squads to share diverse energy sources, powering often incompatible equipment.""
""Process synchronization "",,,,""In computer science, synchronization refers to one of two distinct but related concepts: synchronization of processes, and synchronization of data. Process synchronization refers to the idea that multiple processes are to join up or handshake at a certain point, in order to reach an agreement or commit to a certain sequence of action. Data synchronization refers to the idea of keeping multiple copies of a dataset in coherence with one another, or to maintain data integrity. Process synchronization primitives are commonly used to implement data synchronization.

""
""Input / output "",,,,""In economics, an input–output model is a quantitative economic technique that represents the interdependencies between different branches of a national economy or different regional economies.
The model depicts inter-industry relationships within an economy, showing how output from one industrial sector may become an input to another industrial sector. In the inter-industry matrix, column entries typically represent inputs to an industrial sector, while row entries represent outputs from a given sector. This format therefore shows how dependent each sector is on every other sector, both as a customer of outputs from other sectors and as a supplier of inputs. Each column of the input–output matrix shows the monetary value of inputs to each sector and each row represents the value of each sector's outputs.
Wassily Leontief (1906–1999) is credited with developing this type of analysis and earned the Nobel Prize in Economics for his development of this model.""
""Message passing "",,,,""In computer science, message passing sends a message to a process (which may be an actor or object) and relies on the process and the supporting infrastructure to select and invoke the actual code to run. Message passing differs from conventional programming where a process, subroutine, or function is directly invoked by name. Message passing is key to some models of concurrency and object-oriented programming.
Message passing is used ubiquitously in modern computer software. It is used as a way for the objects that make up a program to work with each other and as a way for objects and systems running on different computers (e.g., the Internet) to interact. Message passing may be implemented by various mechanisms, including channels.""
""Interactive games "",,,,""A video game is an electronic game that involves human interaction with a user interface to generate visual feedback on a video device such as a TV screen or computer monitor. The word video in video game traditionally referred to a raster display device, but in the 2000s, it implies any type of display device that can produce two- or three-dimensional images. Video games are sometimes believed to be a form of art, but this designation is controversial.
The electronic systems used to play video games are known as platforms; examples of these are personal computers and video game consoles. These platforms range from large mainframe computers to small handheld computing devices. Specialized video games such as arcade games, in which the video game components are housed in a large chassis, while common in the 1980s, have gradually declined in use due to the widespread availability of home video game devices (e.g., PlayStation 4 and Xbox One) and video games on desktop and laptop computers and smartphones.
The input device used for games, the game controller, varies across platforms. Common controllers include gamepads, mouses, keyboards, joysticks, the touchscreens of mobile devices and buttons. In addition to video and (in most cases) audio feedback, some games in the 2000s include haptic, vibration or force feedback peripherals.
The video game industry is of increasing commercial importance, with growth driven particularly by the emerging Asian markets and mobile games. As of 2015, video games generated sales of USD 74 billion annually worldwide, and were the third-largest segment in the U.S. entertainment market, behind broadcast and cable TV.""
""Embedded software "",,,,""Embedded software is computer software, written to control machines or devices that are not typically thought of as computers. It is typically specialized for the particular hardware that it runs on and has time and memory constraints. This term is sometimes used interchangeably with firmware, although firmware can also be applied to ROM-based code on a computer, on top of which the OS runs, whereas embedded software is typically the only software on the device in question.
A precise and stable characteristic feature is that no or not all functions of embedded software are initiated/controlled via a human interface, but through machine-interfaces instead.
Manufacturers 'build in' embedded software in the electronics in cars, telephones, modems, robots, appliances, toys, security systems, pacemakers, televisions and set-top boxes, and digital watches, for example. This software can be very simple, such as lighting controls running on an 8-bit microprocessor and a few kilobytes of memory with the suitable level of processing complexity determined with a Probably Approximately Correct Computation framework (a methodology based on randomized algorithms), or can become very sophisticated in applications such as airplanes, missiles, and process control systems.""
""n-tier architectures "",,,,""In software engineering, multitier architecture (often referred to as n-tier architecture) is a client–server architecture in which presentation, application processing, and data management functions are physically separated. The most widespread use of multitier architecture is the three-tier architecture.
N-tier application architecture provides a model by which developers can create flexible and reusable applications. By segregating an application into tiers, developers acquire the option of modifying or adding a specific layer, instead of reworking the entire application. A three-tier architecture is typically composed of a presentation tier, a domain logic tier, and a data storage tier.
While the concepts of layer and tier are often used interchangeably, one fairly common point of view is that there is indeed a difference. This view holds that a layer is a logical structuring mechanism for the elements that make up the software solution, while a tier is a physical structuring mechanism for the system infrastructure.""
""Layered systems "",,,,""In telecommunication, a layered system is a system in which components are grouped, i.e., layered, in a hierarchical arrangement, such that lower layers provide functions and services that support the functions and services of higher layers.
Systems of ever-increasing complexity and capability can be built by adding or changing the layers to improve overall system capability while using the components that are still in place.
 This article incorporates public domain material from the General Services Administration document "Federal Standard 1037C".""
""Electronic blackboards "",,,,""An interactive whiteboard (IWB) is a large interactive display that connects to a computer. A projector projects the computer's desktop onto the board's surface where users control the computer using a pen, finger, stylus, or other device. The board is typically mounted to a wall or floor stand.
They are used in a variety of settings, including classrooms at all levels of education, in corporate board rooms and work groups, in training rooms for professional sports coaching, in broadcasting studios, and others.
The first interactive whiteboards were designed and manufactured for use in the office. They were developed by Xerox Parc around 1990. This board was used in small group meetings and round-tables.
The interactive whiteboard industry was expected to reach sales of US$1 billion worldwide by 2008; one of every seven classrooms in the world was expected to feature an interactive whiteboard by 2011 according to market research by Futuresource Consulting. In 2004, 26% of British primary classrooms had interactive whiteboards. The Becta Harnessing Technology Schools Survey 2007 indicated that 98% of secondary and 100% of primary schools had IWBs. By 2008 the average numbers of interactive whiteboards rose in both primary schools (18 compared with just over six in 2005, and eight in the 2007 survey) and secondary schools (38, compared with 18 in 2005 and 22 in 2007).
Uses for interactive whiteboards may include:
Running software that is loaded onto the connected PC, such as a web browsers or other software used in the classroom.
Capturing and saving notes written on a whiteboard to the connected PC
Capturing notes written on a graphics tablet connected to the whiteboard
Controlling the PC from the white board using click and drag, markup which annotates a program or presentation
Using OCR software to translate cursive writing on a graphics tablet into text
Using an Audience Response System so that presenters can poll a classroom audience or conduct quizzes, capturing feedback onto the whiteboard""
""Simulator / interpreter "",,,,""In computing, an emulator is hardware or software that enables one computer system (called the host) to behave like another computer system (called the guest). An emulator typically enables the host system to run software or use peripheral devices designed for the guest system.""
""Space-based architectures "",,,,""Space-Based Architecture (SBA) is a software architecture pattern for achieving linear scalability of stateful, high-performance applications using the tuple space paradigm. It follows many of the principles of representational state transfer (REST), service-oriented architecture (SOA) and event-driven architecture (EDA), as well as elements of grid computing. With a space-based architecture, applications are built out of a set of self-sufficient units, known as processing-units (PU). These units are independent of each other, so that the application can scale by adding more units. The SBA model is closely related to other patterns that have been proved successful in addressing the application scalability challenge, such as shared nothing architecture (SN), used by Google, Amazon.com and other well-known companies. The model has also been applied by many firms in the securities industry for implementing scalable electronic securities trading applications.""
""3-tier architectures "",,,,""In software engineering, multitier architecture (often referred to as n-tier architecture) is a client–server architecture in which presentation, application processing, and data management functions are physically separated. The most widespread use of multitier architecture is the three-tier architecture.
N-tier application architecture provides a model by which developers can create flexible and reusable applications. By segregating an application into tiers, developers acquire the option of modifying or adding a specific layer, instead of reworking the entire application. A three-tier architecture is typically composed of a presentation tier, a domain logic tier, and a data storage tier.
While the concepts of layer and tier are often used interchangeably, one fairly common point of view is that there is indeed a difference. This view holds that a layer is a logical structuring mechanism for the elements that make up the software solution, while a tier is a physical structuring mechanism for the system infrastructure.""
""Petri nets "",,,,""A Petri net (also known as a place/transition net or P/T net) is one of several mathematical modeling languages for the description of distributed systems. A Petri net is a directed bipartite graph, in which the nodes represent transitions (i.e. events that may occur, signified by bars) and places (i.e. conditions, signified by circles). The directed arcs describe which places are pre- and/or postconditions for which transitions (signified by arrows). Some sources state that Petri nets were invented in August 1939 by Carl Adam Petri—at the age of 13—for the purpose of describing chemical processes.
Like industry standards such as UML activity diagrams, Business Process Model and Notation and EPCs, Petri nets offer a graphical notation for stepwise processes that include choice, iteration, and concurrent execution. Unlike these standards, Petri nets have an exact mathematical definition of their execution semantics, with a well-developed mathematical theory for process analysis.""
""State systems "",,,,""The quantized state systems (QSS) methods are a family of numerical integration solvers based on the idea of state quantization, dual to the traditional idea of time discretization. Unlike traditional numerical solution methods, which approach the problem by discretizing time and solving for the next (real-valued) state at each successive time step, QSS methods keep time as a continuous entity and instead quantize the system's state, instead solving for the time at which the state deviates from its quantized value by a quantum.
They can also have many advantages compared to classical algorithms. They inherently allow for modeling discontinuities in the system due to their discrete-event nature and asynchronous nature. They also allow for explicit root-finding and detection of zero-crossing using explicit algorithms, avoiding the need for iteration---a fact which is especially important in the case of stiff systems, where traditional time-stepping methods require a heavy computational penalty due to the requirement to implicitly solve for the next system state. Finally, QSS methods satisfy remarkable global stability and error bounds, described below, which are not satisfied by classical solution techniques.
By their nature, QSS methods are therefore neatly modeled by the DEVS formalism, a discrete-event model of computation, in contrast with traditional methods, which form discrete-time models of the continuous-time system. They have therefore been implemented in [PowerDEVS], a simulation engine for such discrete-event systems.""
""Entity relationship modeling "",,,,""In software engineering, an entity–relationship model (ER model) is a data model for describing the data or information aspects of a business domain or its process requirements, in an abstract way that lends itself to ultimately being implemented in a database such as a relational database. The main components of ER models are entities (things) and the relationships that can exist among them.
Entity–relationship modeling was developed by Peter Chen and published in a 1976 paper. However, variants of the idea existed previously, and have been devised subsequently such as supertype and subtype data entities and commonality relationships.""
""Model-driven software engineering "",,,,""The Special Interest Group "Model-Driven Software Engineering" (SIG-MDSE) organizes the MDSE as an academic conference.
This special interest group focus on model driven (or model based) and architecture centered software engineering techniques and tools, e.g. OMG's MDA (Model Driven Architecture). It is a platform for all interested persons to exchange experiences in the context of workshops, conferences etc. The activities are mainly in Germany. Once or twice a year an academic workshop is organized.""
""Feature interaction "",,,,""Feature interaction is a software engineering concept. It occurs when the integration of two features would modify the behavior of one or both features.
The term feature is used to denote a unit of functionality of a software application. Similar to many concepts in computer science, the term can be used at different levels of abstraction. For example, the plain old telephone service (POTS) is a telephony application feature at one level, but itself is composed of originating features and terminating features. The originating features may in turn include the provide dial tone feature, digit collection feature and so on.
This definition of feature interaction allows one to focus on certain behavior of the interacting features such as how their response time may be changed given the integration. Many researchers in the field consider problems that arise due to change in the execution behavior of the interacting features. Under that context, the behavior of a feature is defined by its execution flow and output for a given input. In other words, the interaction changes the execution flow and output of the interacting features for a given input.""
""Massively parallel systems "",,,,""Quadrics was a supercomputer company formed in 1996 as a joint venture between Alenia Spazio and the technical team from Meiko Scientific. They produced hardware and software for clustering commodity computer systems into massively parallel systems. Their highpoint was in June 2003 when six out of the ten fastest supercomputers in the world were based on Quadrics' interconnect. They officially closed on June 29, 2009.
^ "top500 list". top500. Retrieved 2008-07-23. 
^ "InsideTrack: Former employees confirm Quadrics officially out of business last week".""
""Cloud computing "",,,,""Cloud computing, also on-demand computing, is a kind of Internet-based computing that provides shared processing resources and data to computers and other devices on demand. It is a model for enabling ubiquitous, on-demand access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications and services), which can be rapidly provisioned and released with minimal management effort. Cloud computing and storage solutions provide users and enterprises with various capabilities to store and process their data in third-party data centers. It relies on sharing of resources to achieve coherence and economy of scale, similar to a utility (like the electricity grid) over a network.
Advocates claim that cloud computing allows companies to avoid upfront infrastructure costs, and focus on projects that differentiate their businesses instead of on infrastructure. Proponents also claim that cloud computing allows enterprises to get their applications up and running faster, with improved manageability and less maintenance, and enables IT to more rapidly adjust resources to meet fluctuating and unpredictable business demand. Cloud providers typically use a "pay as you go" model. This can lead to unexpectedly high charges if administrators do not adapt to the cloud pricing model.
The present availability of high-capacity networks, low-cost computers and storage devices as well as the widespread adoption of hardware virtualization, service-oriented architecture, and autonomic and utility computing have led to a growth in cloud computing. Companies can scale up as computing needs increase and then scale down again as demands decrease.
Cloud computing has become a highly demanded service or utility due to the advantages of high computing power, cheap cost of services, high performance, scalability, accessibility as well as availability. Some cloud vendors are experiencing growth rates of 50% per year, but being still in a stage of infancy, it has pitfalls that need to be addressed to make cloud computing services more reliable and user friendly.""
""Client-server architectures "",,,,""The client–server model of computing is a distributed application structure that partitions tasks or workloads between the providers of a resource or service, called servers, and service requesters, called clients. Often clients and servers communicate over a computer network on separate hardware, but both client and server may reside in the same system. A server host runs one or more server programs which share their resources with clients. A client does not share any of its resources, but requests a server's content or service function. Clients therefore initiate communication sessions with servers which await incoming requests.
Examples of computer applications that use the client–server model are Email, network printing, and the World Wide Web.""
""Grid computing "",,,,""Grid computing is the collection of computer resources from multiple locations to reach a common goal. The grid can be thought of as a distributed system with non-interactive workloads that involve a large number of files. Grid computing is distinguished from conventional high performance computing systems such as cluster computing in that grid computers have each node set to perform a different task/application. Grid computers also tend to be more heterogeneous and geographically dispersed (thus not physically coupled) than cluster computers. Although a single grid can be dedicated to a particular application, commonly a grid is used for a variety of purposes. Grids are often constructed with general-purpose grid middleware software libraries. Grid sizes can be quite large.
Grids are a form of distributed computing whereby a “super virtual computer” is composed of many networked loosely coupled computers acting together to perform large tasks. For certain applications, “distributed” or “grid” computing, can be seen as a special type of parallel computing that relies on complete computers (with onboard CPUs, storage, power supplies, network interfaces, etc.) connected to a computer network (private or public) by a conventional network interface, such as Ethernet. This is in contrast to the traditional notion of a supercomputer, which has many processors connected by a local high-speed computer bus.""
""Real-time systems software "",,,,""The Real-time Control System (RCS) is a software system developed by NIST based on the Real-time Control System Reference Model Architecture, that implements a generic Hierarchical control system. The RCS Software Library is an archive of free C++, Java and Ada code, scripts, tools, makefiles, and documentation developed to aid programmers of software to be used in real-time control systems (especially those using the Reference Model Architecture for Intelligent Systems Design).""
""Abstraction, modeling and modularity "",,,,""Software design is the process by which an agent creates a specification of a software artifact, intended to accomplish goals, using a set of primitive components and subject to constraints. Software design may refer to either "all the activities involved in conceptualizing, framing, implementing, commissioning, and ultimately modifying complex systems" or "the activity following requirements specification and before programming, as ... [in] a stylized software engineering process."
Software design usually involves problem solving and planning a software solution. This includes both low-level component and algorithm design and high-level, architecture design.
^ Ralph, P. and Wand, Y. (2009). A proposal for a formal definition of the design concept. In Lyytinen, K., Loucopoulos, P., Mylopoulos, J., and Robinson, W., editors, Design Requirements Workshop (LNBIP 14), pp. 103–136. Springer-Verlag, p. 109 doi:10.1007/978-3-540-92966-6_6.
^ Freeman, Peter; David Hart (2004). "A Science of design for software-intensive systems". Communications of the ACM 47 (8): 19–21 [20]. doi:10.1145/1012037.1012054.""
""Synchronization "",,,,""Synchronization is the coordination of events to operate a system in unison. The familiar conductor of an orchestra serves to keep the orchestra in time. Systems operating with all their parts in synchrony are said to be synchronous or in sync; those which are not are asynchronous.
Today, synchronization can occur on a global basis through the GPS-enabled timekeeping systems (and similar independent systems operated by the EU and Russia).""
""Real-time schedulability "",,,,""In computing, interrupt latency is the time that elapses from when an interrupt is generated to when the source of the interrupt is serviced. For many operating systems, devices are serviced as soon as the device's interrupt handler is executed. Interrupt latency may be affected by microprocessor design, interrupt controllers, interrupt masking, and the operating system's (OS) interrupt handling methods.""
""Consistency "",,,,""In classical deductive logic, a consistent theory is one that does not contain a contradiction. The lack of contradiction can be defined in either semantic or syntactic terms. The semantic definition states that a theory is consistent if and only if it has a model, i.e., there exists an interpretation under which all formulas in the theory are true. This is the sense used in traditional Aristotelian logic, although in contemporary mathematical logic the term satisfiable is used instead. A theory  is consistent if and only if there is no formula  such that both  and its negation  are elements of the set . Let  be set of closed sentences (informally "axioms") and  the set of closed sentences provable from  under a meta-theoretical deductive system such as informal mathematics. The set of axioms  is consistent when  is.
If these semantic and syntactic definitions are equivalent for any theory formulated using a particular deductive logic, the logic is called complete. The completeness of the sentential calculus was proved by Paul Bernays in 1918 and Emil Post in 1921, while the completeness of predicate calculus was proved by Kurt Gödel in 1930, and consistency proofs for arithmetics restricted with respect to the induction axiom schema were proved by Ackermann (1924), von Neumann (1927) and Herbrand (1931). Stronger logics, such as second-order logic, are not complete.
A consistency proof is a mathematical proof that a particular theory is consistent The early development of mathematical proof theory was driven by the desire to provide finitary consistency proofs for all of mathematics as part of Hilbert's program. Hilbert's program was strongly impacted by incompleteness theorems, which showed that sufficiently strong proof theories cannot prove their own consistency (provided that they are in fact consistent).
Although consistency can be proved by means of model theory, it is often done in a purely syntactical way, without any need to reference some model of the logic. The cut-elimination (or equivalently the normalization of the underlying calculus if there is one) implies the consistency of the calculus: since there is obviously no cut-free proof of falsity, there is no contradiction in general.

""
""Access protection "",,,,""Network Access Protection (NAP) is a Microsoft technology for controlling network access of a computer, based on its health. With NAP, system administrators of an organization can define policies for system health requirements. Examples of system health requirements are whether the computer has the most recent operating system updates installed, whether the computer has the latest version of the anti-virus software signature, or whether the computer has a host-based firewall installed and enabled. Computers with a NAP client will have their health status evaluated upon establishing a network connection. NAP can restrict or deny network access to the computers that are not in compliance with the defined health requirements.
NAP was deprecated in Windows Server 2012 R2 and removed from Windows Server 2016.

""
""Model checking "",,,,""In computer science, model checking or property checking refers to the following problem: Given a model of a system, exhaustively and automatically check whether this model meets a given specification. Typically, one has hardware or software systems in mind, whereas the specification contains safety requirements such as the absence of deadlocks and similar critical states that can cause the system to crash. Model checking is a technique for automatically verifying correctness properties of finite-state systems.
In order to solve such a problem algorithmically, both the model of the system and the specification are formulated in some precise mathematical language: To this end, it is formulated as a task in logic, namely to check whether a given structure satisfies a given logical formula. The concept is general and applies to all kinds of logics and suitable structures. A simple model-checking problem is verifying whether a given formula in the propositional logic is satisfied by a given structure.""
""Software verification "",,,,""Software verification is a discipline of software engineering whose goal is to assure that software fully satisfies all the expected requirements.
There are two fundamental approaches to verification:
Dynamic verification, also known as Test or Experimentation - This is good for finding bugs
Static verification, also known as Analysis - This is useful for proving correctness of a program although it may result in false positives""
""Automated static analysis "",,,,""Static program analysis is the analysis of computer software that is performed without actually executing programs (analysis performed on executing programs is known as dynamic analysis). In most cases the analysis is performed on some version of the source code, and in the other cases, some form of the object code.
The term is usually applied to the analysis performed by an automated tool, with human analysis being called program understanding, program comprehension, or code review. Software inspections and Software walkthroughs are also used in the latter case.

""
""Dynamic analysis "",,,,""Dynamic scoring predicts the impact of fiscal policy changes by forecasting the effects of economic agents' reactions to incentives created by policy. It is an adaptation of static scoring, the traditional method for analyzing policy changes.
Due to the complexity of modeling economic agents' behavior, applying dynamic scoring to a policy can be difficult. Economists must infer from economic agents' current behavior how the agents would behave under the new policy. Difficulty increases as the proposed policy becomes increasingly unlike current policy. Likewise, the difficulty of dynamic scoring increases as the time horizon under consideration lengthens. This is due to any model's intrinsic inability to account for unforeseen external shocks in the future.
When feasible, the method yields a more accurate prediction of a policy's impact on a country's fiscal balance and economic output.. The potential for heightened accuracy arises from recognition that households and firms will alter their behavior to continue maximizing welfare (households) or profits (firms) under the new policy. Dynamic scoring is more accurate than static scoring when the econometric model correctly captures how households and firms will react to a policy changes.
Further, the reaction to policy changes may not occur quickly, and thus an intrinsic lag in market behavior obscures the real effect of policy changes.""
""Interoperability "",,,,""Interoperability is a property of a product or system, whose interfaces are completely understood, to work with other products or systems, present or future, without any restricted access or implementation.""
""Software performance "",,,,""In software engineering, performance testing is in general, a testing practice performed to determine how a system performs in terms of responsiveness and stability under a particular workload. It can also serve to investigate, measure, validate or verify other quality attributes of the system, such as scalability, reliability and resource usage.
Performance testing, a subset of performance engineering, is a computer science practice which strives to build performance standards into the implementation, design and architecture of a system.""
""Software reliability "",,,,""In the context of software engineering, software quality refers to two related but distinct notions that exist wherever quality is defined in a business context:
Software functional quality reflects how well it complies with or conforms to a given design, based on functional requirements or specifications. That attribute can also be described as the fitness for purpose of a piece of software or how it compares to competitors in the marketplace as a worthwhile product;
Software structural quality refers to how it meets non-functional requirements that support the delivery of the functional requirements, such as robustness or maintainability, the degree to which the software was produced correctly.
Structural quality is evaluated through the analysis of the software inner structure, its source code, at the unit level, the technology level and the system level, which is in effect how its architecture adheres to sound principles of software architecture outlined in a paper on the topic by OMG. In contrast, functional quality is typically enforced and measured through software testing.
Historically, the structure, classification and terminology of attributes and metrics applicable to software quality management have been derived or extracted from the ISO 9126-3 and the subsequent ISO 25000:2005 quality model, also known as SQuaRE. Based on these models, the Consortium for IT Software Quality (CISQ) has defined five major desirable structural characteristics needed for a piece of software to provide business value: Reliability, Efficiency, Security, Maintainability and (adequate) Size.
Software quality measurement quantifies to what extent a software or system rates along each of these five dimensions. An aggregated measure of software quality can be computed through a qualitative or a quantitative scoring scheme or a mix of both and then a weighting system reflecting the priorities. This view of software quality being positioned on a linear continuum is supplemented by the analysis of "critical programming errors" that under specific circumstances can lead to catastrophic outages or performance degradations that make a given system unsuitable for use regardless of rating based on aggregated measurements. Such programming errors found at the system level represent up to 90% of production issues, whilst at the unit-level, even if far more numerous, programming errors account for less than 10% of production issues. As a consequence, code quality without the context of the whole system, as W. Edwards Deming described it, has limited value.
To view, explore, analyze, and communicate software quality measurements, concepts and techniques of information visualization provide visual, interactive means useful, in particular, if several software quality measures have to be related to each other or to components of a software or system. For example, software maps represent a specialized approach that "can express and combine information about software development, software quality, and system dynamics".

""
""Checkpoint / restart "",,,,""Checkpointing is a technique to add fault tolerance into computing systems. It basically consists of saving a snapshot of the application's state, so that it can restart from that point in case of failure. This is particularly important for long running application that are executed in vulnerable computing system.""
""Software safety "",,,,""In software engineering, software system safety optimizes system safety in the design, development, use, and maintenance of software systems and their integration with safety-critical hardware systems in an operational environment.""
""Software usability "",,,,""Usability is the ease of use and learnability of a human-made object. In Software engineering, usability is the degree to which a software can be used by specified consumers to achieve quantified objectives with effectiveness, efficiency, and satisfaction in a quantified context of use.
The object of use can be a software application, website, book, tool, machine, process, or anything a human interacts with. A usability study may be conducted as a primary job function by a usability analyst or as a secondary job function by designers, technical writers, marketing personnel, and others. It is widely used in consumer electronics, communication, and knowledge transfer objects (such as a cookbook, a document or online help) and mechanical objects such as a door handle or a hammer.
Usability includes methods of measuring usability, such as needs analysis and the study of the principles behind an object's perceived efficiency or elegance. In human-computer interaction and computer science, usability studies the elegance and clarity with which the interaction with a computer program or a web site (web usability) is designed. Usability differs from user satisfaction and user experience because usability also considers usefulness.""
""Parallel programming languages "",,,,""Parallel computing is a type of computation in which many calculations are carried out simultaneously, operating on the principle that large problems can often be divided into smaller ones, which are then solved at the same time. There are several different forms of parallel computing: bit-level, instruction-level, data, and task parallelism. Parallelism has been employed for many years, mainly in high-performance computing, but interest in it has grown lately due to the physical constraints preventing frequency scaling. As power consumption (and consequently heat generation) by computers has become a concern in recent years, parallel computing has become the dominant paradigm in computer architecture, mainly in the form of multi-core processors.
Parallel computing is closely related to concurrent computing—they are frequently used together, and often conflated, though the two are distinct: it is possible to have parallelism without concurrency (such as bit-level parallelism), and concurrency without parallelism (such as multitasking by time-sharing on a single-core CPU). In parallel computing, a computational task is typically broken down in several, often many, very similar subtasks that can be processed independently and whose results are combined afterwards, upon completion. In contrast, in concurrent computing, the various processes often do not address related tasks; when they do, as is typical in distributed computing, the separate tasks may have a varied nature and often require some inter-process communication during execution.
Parallel computers can be roughly classified according to the level at which the hardware supports parallelism, with multi-core and multi-processor computers having multiple processing elements within a single machine, while clusters, MPPs, and grids use multiple computers to work on the same task. Specialized parallel computer architectures are sometimes used alongside traditional processors, for accelerating specific tasks.
In some cases parallelism is transparent to the programmer, such as in bit-level or instruction-level parallelism, but explicitly parallel algorithms, particularly those that use concurrency, are more difficult to write than sequential ones, because concurrency introduces several new classes of potential software bugs, of which race conditions are the most common. Communication and synchronization between the different subtasks are typically some of the greatest obstacles to getting good parallel program performance.
A theoretical upper bound on the speed-up of a single program as a result of parallelization is given by Amdahl's law.

""
""Imperative languages "",,,,""In computer science, imperative programming is a programming paradigm that uses statements that change a program's state. In much the same way that the imperative mood in natural languages expresses commands, an imperative program consists of commands for the computer to perform. Imperative programming focuses on describing how a program operates.
The term is often used in contrast to declarative programming, which focuses on what the program should accomplish without specifying how the program should achieve the result.""
""Object oriented languages "",,,,""Object-oriented programming (OOP) is a programming paradigm based on the concept of "objects", which may contain data, in the form of fields, often known as attributes; and code, in the form of procedures, often known as methods. A distinguishing feature of objects is that an object's procedures can access and often modify the data fields of the object with which they are associated (objects have a notion of "this" or "self"). In OO programming, computer programs are designed by making them out of objects that interact with one another. There is significant diversity in object-oriented programming, but most popular languages are class-based, meaning that objects are instances of classes, which typically also determines their type.
Many of the most widely used programming languages are multi-paradigm programming languages that support object-oriented programming to a greater or lesser degree, typically in combination with imperative, procedural programming. Significant object-oriented languages include Common Lisp, Python, C++, Objective-C, Smalltalk, Delphi, Java, Swift, C#, Perl, Ruby, and PHP.""
""Functional languages "",,,,""In computer science, functional programming is a programming paradigm—a style of building the structure and elements of computer programs—that treats computation as the evaluation of mathematical functions and avoids changing-state and mutable data. It is a declarative programming paradigm, which means programming is done with expressions or declarations instead of statements. In functional code, the output value of a function depends only on the arguments that are input to the function, so calling a function f twice with the same value for an argument x will produce the same result f(x) each time. Eliminating side effects, i.e. changes in state that do not depend on the function inputs, can make it much easier to understand and predict the behavior of a program, which is one of the key motivations for the development of functional programming.
Functional programming has its roots in lambda calculus, a formal system developed in the 1930s to investigate computability, the Entscheidungsproblem, function definition, function application, and recursion. Many functional programming languages can be viewed as elaborations on the lambda calculus. Another well-known declarative programming paradigm, logic programming, is based on relations.
In contrast, imperative programming changes state with commands in the source language, the most simple example being assignment. Imperative programming does have functions—not in the mathematical sense—but in the sense of subroutines. They can have side effects that may change the value of program state. Functions without return values therefore make sense. Because of this, they lack referential transparency, i.e. the same language expression can result in different values at different times depending on the state of the executing program.
Functional programming languages, especially purely functional ones such as Hope and Rex, have largely been emphasized in academia rather than in commercial software development. However, prominent programming languages which support functional programming such as Common Lisp, Scheme, Clojure, Wolfram Language  (also known as Mathematica), Racket, Erlang, OCaml, Haskell, and F# have been used in industrial and commercial applications by a wide variety of organizations. Functional programming is also supported in some domain-specific programming languages like R (statistics), J, K and Q from Kx Systems (financial analysis), XQuery/XSLT (XML), and Opal. Widespread domain-specific declarative languages like SQL and Lex/Yacc use some elements of functional programming, especially in eschewing mutable values.
Programming in a functional style can also be accomplished in languages that are not specifically designed for functional programming. For example, the imperative Perl programming language has been the subject of a book describing how to apply functional programming concepts. This is also true of the PHP programming language. C# 3.0 and Java 8 added constructs to facilitate the functional style. The Julia language also offers functional programming abilities. An interesting case is that of Scala – it is frequently written in a functional style, but the presence of side effects and mutable state place it in a grey area between imperative and functional languages.
^ https://wiki.haskell.org/Declaration_vs._expression_style#Expression_style
^ https://wiki.haskell.org/Declaration_vs._expression_style#Declaration_style
^ a b Hudak, Paul (September 1989). "Conception, evolution, and application of functional programming languages" (PDF). ACM Computing Surveys 21 (3): 359–411. doi:10.1145/72551.72554. 
^ 
^ 
^ 
^ 
^ 
^ 
^ "Wolfram Language Guide: Functional Programming". 2015. Retrieved 2015-08-24. 
^ 
^ 
^ 
^ 
^ 
^ 
^ 
^ 
^ Mansell, Howard (2008). Quantitative Finance in F#. CUFP 2008. Retrieved 2009-08-29. 
^ Peake, Alex (2009). The First Substantial Line of Business Application in F#. CUFP 2009. Retrieved 2009-08-29. 
^ 
^ 
^ 
^ 
^ 
^ Dominus, Mark J. (2005). Higher-Order Perl. Morgan Kaufmann. ISBN 1-55860-701-3. 
^ Holywell, Simon (2014). Functional Programming in PHP. php[architect]. ISBN 9781940111056. 
^""
""Concurrent programming languages "",,,,""Concurrent computing is a form of computing in which several computations are executing during overlapping time periods—concurrently—instead of sequentially (one completing before the next starts). This is a property of a system—this may be an individual program, a computer, or a network—and there is a separate execution point or "thread of control" for each computation ("process"). A concurrent system is one where a computation can advance without waiting for all other computations to complete; where more than one computation can advance at the same time.
As a programming paradigm, concurrent computing is a form of modular programming, namely factoring an overall computation into subcomputations that may be executed concurrently. Pioneers in the field of concurrent computing include Edsger Dijkstra, Per Brinch Hansen, and C.A.R. Hoare.""
""Extensible languages "",,,,""Extensible programming is a term used in computer science to describe a style of computer programming that focuses on mechanisms to extend the programming language, compiler and runtime environment. Extensible programming languages, supporting this style of programming, were an active area of work in the 1960s, but the movement was marginalized in the 1970s. Extensible programming has become a topic of renewed interest in the 21st century.""
""Assembly languages "",,,,""An assembly (or assembler) language, often abbreviated asm, is a low-level programming language for a computer, or other programmable device, in which there is a very strong (generally one-to-one) correspondence between the language and the architecture's machine code instructions. Each assembly language is specific to a particular computer architecture, in contrast to most high-level programming languages, which are generally portable across multiple architectures, but require interpreting or compiling. Assembly language may also be called symbolic machine code.
Assembly language is converted into executable machine code by a utility program referred to as an assembler. The conversion process is referred to as assembly, or assembling the source code. Assembly time is the computational step where an assembler is run.
Assembly language uses a mnemonic to represent each low-level machine instruction or operation, typically also each architectural register, flag, etc. Many operations require one or more operands in order to form a complete instruction and most assemblers can take expressions of numbers and named constants as well as registers and labels as operands, freeing the programmer from tedious repetitive calculations. Depending on the architecture, these elements may also be combined for specific instructions or addressing modes using offsets or other data as well as fixed addresses. Many assemblers offer additional mechanisms to facilitate program development, to control the assembly process, and to aid debugging.""
""Abstract data types "",,,,""In computer science, an abstract data type (ADT) is a mathematical model for data types where a data type is defined by its behavior (semantics) from the point of view of a user of the data, specifically in terms of possible values, possible operations on data of this type, and the behavior of these operations. This contrasts with data structures, which are concrete representations of data, and are the point of view of an implementer, not a user.
Formally, an ADT may be defined as a "class of objects whose logical behavior is defined by a set of values and a set of operations"; this is analogous to an algebraic structure in mathematics. What is meant by "behavior" varies by author, with the two main types of formal specifications for behavior being axiomatic (algebraic) specification and an abstract model; these correspond to axiomatic semantics and operational semantics of an abstract machine, respectively. Some authors also include the computational complexity ("cost"), both in terms of time (for computing operations) and space (for representing values).
In practice many common data types are not ADTs, as the abstraction is not perfect, and users must be aware of issues like arithmetic overflow that are due to the representation. For example, integers are often stored as fixed width values (32-bit or 64-bit binary numbers), and thus experience integer overflow if the maximum value is exceeded.
ADTs are a theoretical concept in computer science, used in the design and analysis of algorithms, data structures, and software systems, and do not correspond to specific features of computer languages—mainstream computer languages do not directly support formally specified ADTs. However, various language features correspond to certain aspects of ADTs, and are easily confused with ADTs proper; these include abstract types, opaque data types, protocols, and design by contract. ADTs were first proposed by Barbara Liskov and Stephen N. Zilles in 1974, as part of the development of the CLU language.""
""Inheritance "",,,,""Inheritance is the practice of passing on property, titles, debts, rights and obligations upon the death of an individual. It has long played an important role in human societies. The rules of inheritance differ between societies and have changed over time.""
""Control structures "",,,,""In computer science, control flow (or alternatively, flow of control) is the order in which individual statements, instructions or function calls of an imperative program are executed or evaluated. The emphasis on explicit control flow distinguishes an imperative programming language from a declarative programming language.
Within an imperative programming language, a control flow statement is a statement whose execution results in a choice being made as to which of two or more paths should be followed. For non-strict functional languages, functions and language constructs exist to achieve the same result, but they are not necessarily called control flow statements.
The kinds of control flow statements supported by different languages vary, but can be categorized by their effect:
continuation at a different statement (unconditional branch or jump),
executing a set of statements only if some condition is met (choice - i.e., conditional branch),
executing a set of statements zero or more times, until some condition is met (i.e., loop - the same as conditional branch),
executing a set of distant statements, after which the flow of control usually returns (subroutines, coroutines, and continuations),
stopping the program, preventing any further execution (unconditional halt).
A set of statements is in turn generally structured as a block, which in addition to grouping also defines a lexical scope.
Interrupts and signals are low-level mechanisms that can alter the flow of control in a way similar to a subroutine, but usually occur as a response to some external stimulus or event (that can occur asynchronously), rather than execution of an 'in-line' control flow statement.
At the level of machine or assembly language, control flow instructions usually work by altering the program counter. For some CPUs the only control flow instructions available are conditional or unconditional branch instructions (also called jumps).""
""Data types and structures "",,,,""The J programming language, developed in the early 1990s by Kenneth E. Iverson and Roger Hui, is a synthesis of APL (also by Iverson) and the FP and FL function-level languages created by John Backus.
To avoid repeating the APL special-character problem, J requires only the basic ASCII character set, resorting to the use of the dot and colon as "inflections" to form short words similar to digraphs. Most such "primary" (or "primitive") J words serve as mathematical symbols, with the dot or colon extending the meaning of the basic characters available. Additionally, many characters which might need to be balanced in other languages (such as [] {} "" `` or <>) are treated by J as stand-alone words or, when inflected, as single-character roots of multi-character words.
J is a very terse array programming language, and is most suited to mathematical and statistical programming, especially when performing operations on matrices. It has also been used in Extreme Programming and network performance analysis.
Like the original FP/FL languages, J supports function-level programming via its tacit programming features (note that function-level programming is not the same as functional programming).
Unlike most languages that support object-oriented programming, J's flexible hierarchical namespace scheme (where every name exists in a particular locale) can be effectively used as a framework for both class-based and prototype-based object-oriented programming.
Since March 2011, J is free and open source software under the GPLv3 license. One may also purchase source for commercial use under a negotiated license.

""
""Recursion "",,,,""Recursion is the process of repeating items in a self-similar way. For instance, when the surfaces of two mirrors are exactly parallel with each other, the nested images that occur are a form of infinite recursion. The term has a variety of meanings specific to a variety of disciplines ranging from linguistics to logic. The most common application of recursion is in mathematics and computer science, in which it refers to a method of defining functions in which the function being defined is applied within its own definition. Specifically, this defines an infinite number of instances (function values), using a finite expression that for some instances may refer to other instances, but in such a way that no loop or infinite chain of references can occur. The term is also used more generally to describe a process of repeating objects in a self-similar way.""
""Concurrent programming structures "",,,,""Concurrent computing is a form of computing in which several computations are executing during overlapping time periods—concurrently—instead of sequentially (one completing before the next starts). This is a property of a system—this may be an individual program, a computer, or a network—and there is a separate execution point or "thread of control" for each computation ("process"). A concurrent system is one where a computation can advance without waiting for all other computations to complete; where more than one computation can advance at the same time.
As a programming paradigm, concurrent computing is a form of modular programming, namely factoring an overall computation into subcomputations that may be executed concurrently. Pioneers in the field of concurrent computing include Edsger Dijkstra, Per Brinch Hansen, and C.A.R. Hoare.""
""Procedures, functions and subroutines "",,,,""In computer programming, a subroutine is a sequence of program instructions that perform a specific task, packaged as a unit. This unit can then be used in programs wherever that particular task should be performed. Subprograms may be defined within programs, or separately in libraries that can be used by multiple programs. In different programming languages, a subroutine may be called a procedure, a function, a routine, a method, or a subprogram. The generic term callable unit is sometimes used.
As the name subprogram suggests, a subroutine behaves in much the same way as a computer program that is used as one step in a larger program or another subprogram. A subroutine is often coded so that it can be started (called) several times and from several places during one execution of the program, including from other subroutines, and then branch back (return) to the next instruction after the call once the subroutine's task is done. Maurice Wilkes, David Wheeler, and Stanley Gill are credited with the invention of this concept, which they termed a closed subroutine, contrasted with an open subroutine or macro.
Subroutines are a powerful programming tool, and the syntax of many programming languages includes support for writing and using them. Judicious use of subroutines (for example, through the structured programming approach) will often substantially reduce the cost of developing and maintaining a large program, while increasing its quality and reliability. Subroutines, often collected into libraries, are an important mechanism for sharing and trading software. The discipline of object-oriented programming is based on objects and methods (which are subroutines attached to these objects or object classes).
In the compiling method called threaded code, the executable program is basically a sequence of subroutine calls.
^ U.S. Election Assistance Commission (2007). "Definitions of Words with Special Meanings". Voluntary Voting System Guidelines. Retrieved 2013-01-14. 
^ Wheeler, D. J. (1952). "The use of sub-routines in programmes". Proceedings of the 1952 ACM national meeting (Pittsburgh) on - ACM '52. p. 235. doi:10.1145/609784.609816. 
^ Wilkes, M. V.; Wheeler, D. J.; Gill, S. (1951). Preparation of Programs for an Electronic Digital Computer. Addison-Wesley. 
^ Dainith, John. ""open subroutine." A Dictionary of Computing. 2004..". Encyclopedia.com. Retrieved January 14, 2013. 
^ Donald E. Knuth. The Art of Computer Programming, Volume I: Fundamental Algorithms. Addison-Wesley. ISBN 0-201-89683-4. 
^ O.-J. Dahl; E. W. Dijkstra; C. A. R. Hoare (1972). Structured Programming. Academic Press. ISBN 0-12-200550-3.""
""Patterns "",,,,""A pattern, apart from the term's use to mean "Template", is a discernible regularity in the world or in a manmade design. As such, the elements of a pattern repeat in a predictable manner. A geometric pattern is a kind of pattern formed of geometric shapes and typically repeating like a wallpaper.
Any of the five senses may directly observe patterns. Conversely, abstract patterns in science, mathematics, or language may be observable only by analysis. Direct observation in practice means seeing visual patterns, which are widespread in nature and in art. Visual patterns in nature are often chaotic, never exactly repeating, and often involve fractals. Natural patterns include spirals, meanders, waves, foams, tilings, cracks, and those created by symmetries of rotation and reflection. Patterns have an underlying mathematical structure; indeed, mathematics can be seen as the search for regularities, and the output of any function is a mathematical pattern. Similarly in the sciences, theories explain and predict regularities in the world.
In art and architecture, decorations or visual motifs may be combined and repeated to form patterns designed to have a chosen effect on the viewer. In computer science, a software design pattern is a known solution to a class of problems in programming. In fashion, the pattern is a template used to create any number of similar garments.""
""Coroutines "",,,,""Coroutines are computer program components that generalize subroutines for nonpreemptive multitasking, by allowing multiple entry points for suspending and resuming execution at certain locations. Coroutines are well-suited for implementing more familiar program components such as cooperative tasks, exceptions, event loop, iterators, infinite lists and pipes.
According to Donald Knuth, the term coroutine was coined by Melvin Conway in 1958, after he applied it to construction of an assembly program. The first published explanation of the coroutine appeared later, in 1963.

""
""Syntax "",,,,""In linguistics, syntax (/ˈsɪnˌtæks/) is the set of rules, principles, and processes that govern the structure of sentences in a given language, specifically word order. The term syntax is also used to refer to the study of such principles and processes. The goal of many syntacticians is to discover the syntactic rules common to all languages.
In mathematics, syntax refers to the rules governing the behavior of mathematical systems, such as formal languages used in logic. (See logical syntax.)""
""Semantics "",,,,""Semantics (from Ancient Greek: σημαντικός sēmantikós, "significant") is the study of meaning. It focuses on the relationship between signifiers—like words, phrases, signs, and symbols—and what they stand for, their denotation. Linguistic semantics is the study of meaning that is used for understanding human expression through language. Other forms of semantics include the semantics of programming languages, formal logics, and semiotics. In international scientific vocabulary semantics is also called semasiology.
The word semantics was first used by Michel Bréal, a French philologist. It denotes a range of ideas—from the popular to the highly technical. It is often used in ordinary language for denoting a problem of understanding that comes down to word selection or connotation. This problem of understanding has been the subject of many formal enquiries, over a long period of time, especially in the field of formal semantics. In linguistics, it is the study of the interpretation of signs or symbols used in agents or communities within particular circumstances and contexts. Within this view, sounds, facial expressions, body language, and proxemics have semantic (meaningful) content, and each comprises several branches of study. In written language, things like paragraph structure and punctuation bear semantic content; other forms of language bear other semantic content.
The formal study of semantics intersects with many other fields of inquiry, including lexicology, syntax, pragmatics, etymology and others. Independently, semantics is also a well-defined field in its own right, often with synthetic properties. In the philosophy of language, semantics and reference are closely connected. Further related fields include philology, communication, and semiotics. The formal study of semantics can therefore be manifold and complex.
Semantics contrasts with syntax, the study of the combinatorics of units of a language (without reference to their meaning), and pragmatics, the study of the relationships between the symbols of a language, their meaning, and the users of the language. Semantics as a field of study also has significant ties to various representational theories of meaning including truth theories of meaning, coherence theories of meaning, and correspondence theories of meaning. Each of these is related to the general philosophical study of reality and the representation of meaning.""
""Interpreters "",,,,""Interpretation or interpreting is the facilitating of oral or sign-language communication, either simultaneously or consecutively, between users of different languages. Translation studies is the systematic study of the theory, description and application of interpretation and translation.
An interpreter is a person who converts a thought or expression in a source language into an expression with a comparable meaning in a target language either simultaneously in "real time" or consecutively when the speaker pauses after completing one or two sentences.
The interpreter's objective is to convey every semantic element as well as tone and register and every intention and feeling of the message that the source-language speaker is directing to target-language recipients (except in summary interpretation, used sometimes in conferences)
For written speeches and lectures, sometimes the reading of pre-translated texts is used.""
""Incremental compilers "",,,,""The term incremental compiler may refer to two different types of compiler.""
""Retargetable compilers "",,,,""""
""Just-in-time compilers "",,,,""In computing, just-in-time (JIT) compilation, also known as dynamic translation, is compilation done during execution of a program – at run time – rather than prior to execution. Most often this consists of translation to machine code, which is then executed directly, but can also refer to translation to another format.
JIT compilation is a combination of the two traditional approaches to translation to machine code – ahead-of-time compilation (AOT), and interpretation – and combines some advantages and drawbacks of both. Roughly, JIT compilation combines the speed of compiled code with the flexibility of interpretation, with the overhead of an interpreter and the additional overhead of compiling (not just interpreting). JIT compilation is a form of dynamic compilation, and allows adaptive optimization such as dynamic recompilation – thus in theory JIT compilation can yield faster execution than static compilation. Interpretation and JIT compilation are particularly suited for dynamic programming languages, as the runtime system can handle late-bound data types and enforce security guarantees.
^ a b Aycock 2003.""
""Dynamic compilers "",,,,""Dynamic compilation is a process used by some programming language implementations to gain performance during program execution. Although the technique originated in the Self programming language, the best-known language that uses this technique is Java. Since the machine code emitted by a dynamic compiler is constructed and optimized at program runtime, the use of dynamic compilation enables optimizations for efficiency not available to compiled programs except through code duplication or metaprogramming.
Runtime environments using dynamic compilation typically have programs run slowly for the first few minutes, and then after that, most of the compilation and recompilation is done and it runs quickly. Due to this initial performance lag, dynamic compilation is undesirable in certain cases. In most implementations of dynamic compilation, some optimizations that could be done at the initial compile time are delayed until further compilation at run-time, causing further unnecessary slowdowns. Just-in-time compilation is a form of dynamic compilation.""
""Translator writing systems and compiler generators "",,,,""A compiler is a computer program (or a set of programs) that transforms source code written in a programming language (the source language) into another computer language (the target language), with the latter often having a binary form known as object code. The most common reason for converting source code is to create an executable program.
The name "compiler" is primarily used for programs that translate source code from a high-level programming language to a lower level language (e.g., assembly language or machine code). If the compiled program can run on a computer whose CPU or operating system is different from the one on which the compiler runs, the compiler is known as a cross-compiler. More generally, compilers are a specific type of translator.
While all programs that take a set of programming specifications and translate them, i.e. create a means to execute those specifications, are technically "compilers", the term generally means a program that produces a separate executable from the compiler (that may require a run time library or subsystem to operate), a compiler that merely executes the original specifications is usually referred to as an "interpreter", although because of differing methods of analyzing what represents compilation and what represents interpretation, there is some overlap between the two terms.
A program that translates from a low level language to a higher level one is a decompiler. A program that translates between high-level languages is usually called a source-to-source compiler or transpiler. A language rewriter is usually a program that translates the form of expressions without a change of language. The term compiler-compiler is sometimes used to refer to a parser generator, a tool often used to help create the lexer and parser.
A compiler is likely to perform many or all of the following operations: lexical analysis, preprocessing, parsing, semantic analysis (syntax-directed translation), code generation, and code optimization. Program faults caused by incorrect compiler behavior can be very difficult to track down and work around; therefore, compiler implementors invest significant effort to ensure compiler correctness.
^ "Definition of:compiler". PC Magazine.""
""Source code generation "",,,,""In computer science, the term automatic programming identifies a type of computer programming in which some mechanism generates a computer program to allow human programmers to write the code at a higher abstraction level.
There has been little agreement on the precise definition of automatic programming, mostly because its meaning has changed over time. David Parnas, tracing the history of "automatic programming" in published research, noted that in the 1940s it described automation of the manual process of punching paper tape. Later it referred to translation of high-level programming languages like Fortran and ALGOL. In fact, one of the earliest programs identifiable as a compiler was called Autocode. Parnas concluded that "automatic programming has always been a euphemism for programming in a higher-level language than was then available to the programmer."""
""Runtime environments "",,,,""A runtime system, also called run-time system, primarily implements portions of an execution model. This is in contrast to the runtime lifecycle phase of a program, during which the runtime system is in operation. Most languages have some form of runtime system, which implements control over the order in which work that was specified in terms of the language gets performed. Over the years, the meaning of the term 'runtime system' has been expanded to include nearly any behaviors that are dynamically determined during execution.""
""Preprocessors "",,,,""In computer science, a preprocessor is a program that processes its input data to produce output that is used as input to another program. The output is said to be a preprocessed form of the input data, which is often used by some subsequent programs like compilers. The amount and kind of processing done depends on the nature of the preprocessor; some preprocessors are only capable of performing relatively simple textual substitutions and macro expansions, while others have the power of full-fledged programming languages.
A common example from computer programming is the processing performed on source code before the next step of compilation. In some computer languages (e.g., C and PL/I) there is a phase of translation known as preprocessing.""
""Parsers "",,,,""Parsing or syntactic analysis is the process of analysing a string of symbols, either in natural language or in computer languages, conforming to the rules of a formal grammar. The term parsing comes from Latin pars (orationis), meaning part (of speech).
The term has slightly different meanings in different branches of linguistics and computer science. Traditional sentence parsing is often performed as a method of understanding the exact meaning of a sentence, sometimes with the aid of devices such as sentence diagrams. It usually emphasizes the importance of grammatical divisions such as subject and predicate.
Within computational linguistics the term is used to refer to the formal analysis by a computer of a sentence or other string of words into its constituents, resulting in a parse tree showing their syntactic relation to each other, which may also contain semantic and other information.
The term is also used in psycholinguistics when describing language comprehension. In this context, parsing refers to the way that human beings analyze a sentence or phrase (in spoken language or text) "in terms of grammatical constituents, identifying the parts of speech, syntactic relations, etc." This term is especially common when discussing what linguistic cues help speakers to interpret garden-path sentences.
Within computer science, the term is used in the analysis of computer languages, referring to the syntactic analysis of the input code into its component parts in order to facilitate the writing of compilers and interpreters.""
""Extensible Markup Language (XML) "",,,,""In computing, a news aggregator, also termed a feed aggregator, feed reader, news reader, RSS reader or simply aggregator, is client software or a web application which aggregates syndicated web content such as online newspapers, blogs, podcasts, and video blogs (vlogs) in one location for easy viewing. RSS is synchronized subscription system. Basically, RSS uses extensible markup language (XML) to structure pieces of information to be aggregated in a feed reader that displays the information in a user-friendly interface. The updates distributed include, for example, journal tables of contents, podcasts, videos,and news items.""
""Scripting languages "",,,,""A scripting or script language is a programming language that supports scripts, programs written for a special run-time environment that automate the execution of tasks that could alternatively be executed one-by-one by a human operator. Scripting languages are often interpreted (rather than compiled). Primitives are usually the elementary tasks or API calls, and the language allows them to be combined into more complex programs. Environments that can be automated through scripting include software applications, web pages within a web browser, the shells of operating systems (OS), embedded systems, as well as numerous games. A scripting language can be viewed as a domain-specific language for a particular environment; in the case of scripting an application, this is also known as an extension language. Scripting languages are also sometimes referred to as very high-level programming languages, as they operate at a high level of abstraction, or as control languages, particularly for job control languages on mainframes.
The term "scripting language" is also used loosely to refer to dynamic high-level general-purpose languages, such as Perl, Tcl, and Python, with the term "script" often used for small programs (up to a few thousand lines of code) in such languages, or in domain-specific languages such as the text-processing languages sed and AWK. Some of these languages were originally developed for use within a particular environment, and later developed into portable domain-specific or general-purpose languages. Conversely, many general-purpose languages have dialects that are used as scripting languages. This article discusses scripting languages in the narrow sense of languages for a specific environment;
The spectrum of scripting languages ranges from very small and highly domain-specific languages to general-purpose programming languages used for scripting. Standard examples of scripting languages for specific environments include: Bash, for the Unix or Unix-like operating systems; ECMAScript (JavaScript), for web browsers; and Visual Basic for Applications, for Microsoft Office applications. Lua is a language designed and widely used as an extension language. Python is a general-purpose language that is also commonly used as an extension language, while ECMAScript is still primarily a scripting language for web browsers, but is also used as a general-purpose language. The Emacs Lisp dialect of Lisp (for the Emacs editor) and the Visual Basic for Applications dialect of Visual Basic are examples of scripting language dialects of general-purpose languages. Some game systems, notably the Second Life virtual world and the Trainz franchise of Railroad simulators have been extensively extended in functionality by scripting extensions. In other games like Wesnoth, the variety of actual games played by players are scripts written by other users.""
""Domain specific languages "",,,,""A domain-specific language (DSL) is a computer language specialized to a particular application domain. This is in contrast to a general-purpose language (GPL), which is broadly applicable across domains, and lacks specialized features for a particular domain. There are a wide variety of DSLs, ranging from widely used languages for common domains, such as HTML for web pages, down to languages used by only one or a few pieces of software, such as Emacs Lisp for GNU Emacs and XEmacs. DSLs can be further subdivided by the kind of language, and include domain-specific markup languages, domain-specific modeling languages (more generally, specification languages), and domain-specific programming languages. Special-purpose computer languages have always existed in the computer age, but the term "domain-specific language" has become more popular due to the rise of domain-specific modeling. Simpler DSLs, particularly ones used by a single application, are sometimes informally called mini-languages.
The line between general-purpose languages and domain-specific languages is not always sharp, as a language may have specialized features for a particular domain but be applicable more broadly, or conversely may in principle be capable of broad application but in practice used primarily for a specific domain. For example, Perl was originally developed as a text-processing and glue language, for the same domain as AWK and shell scripts, but has since become a general-purpose programming language. By contrast, PostScript is a Turing complete language, and in principle can be used for any task, but in practice is narrowly used as a page description language.""
""API languages "",,,,""Lampung is the language of the Indonesian province of Lampung at the southern tip of Sumatra. It is a dialect cluster with two main dialects, perhaps distinct enough to be considered distinct languages: Abung/Pepadun (Lampung Nyo) and Pesisir/Say Batin (Lampung Api). A third, Komering, is sometimes considered part of Lampung Api, by others a distinct language. Lampung Api is the prestige variety.
Before the introduction of the Roman script, Lampung was written in a script called "Aksara Lampung" or "Had Lampung", which is a variant of the Ulu scripts used throughout central and south Sumatra. The script is seldom used today but is taught in schools throughout Lampung as a means of preserving its linguistic history.""
""Window managers "",,,,""A window manager is system software that controls the placement and appearance of windows within a windowing system in a graphical user interface. Most window managers are designed to help provide a desktop environment. They work in conjunction with the underlying graphical system that provides required functionality—support for graphics hardware, pointing devices, and a keyboard, and are often written and created using a widget toolkit.
Few window managers are designed with a clear distinction between the windowing system and the window manager. Every graphical user interface based on a windows metaphor has some form of window management. In practice, the elements of this functionality vary greatly. Elements usually associated with window managers allow the user to open, close, minimize, maximize, move, resize, and keep track of running windows, including window decorators. Many window managers also come with various utilities and features: e.g. docks, task bars, program launchers, desktop icons, and wallpaper.""
""Macro languages "",,,,""A macro (short for "macroinstruction", from Greek μακρύ- 'long') in computer science is a rule or pattern that specifies how a certain input sequence (often a sequence of characters) should be mapped to a replacement output sequence (also often a sequence of characters) according to a defined procedure. The mapping process that instantiates (transforms) a macro use into a specific sequence is known as macro expansion. A facility for writing macros may be provided as part of a software application or as a part of a programming language. In the former case, macros are used to make tasks using the application less repetitive. In the latter case, they are a tool that allows a programmer to enable code reuse or even to design domain-specific languages.
Macros are used to make a sequence of computing instructions available to the programmer as a single program statement, making the programming task less tedious and less error-prone. (Thus, they are called "macros" because a big block of code can be expanded from a small sequence of characters.) Macros often allow positional or keyword parameters that dictate what the conditional assembler program generates and have been used to create entire programs or program suites according to such variables as operating system, platform or other factors. The term derives from "macro instruction", and such expansions were originally used in generating assembly language code.""
""Programming by example "",,,,""In computer science, programming by example (PbE), also termed programming by demonstration or more generally as demonstrational programming, is an end-user development technique for teaching a computer new behavior by demonstrating actions on concrete examples. The system records user actions and infers a generalized program that can be used on new examples.
PbE is intended to be easier to do than traditional computer programming, which generally requires learning and using a programming language. Many PbE systems have been developed as research prototypes, but few have found widespread real-world application. More recently, PbE has proved to be a useful paradigm for creating scientific work-flows. PbE is used in two independent clients for the BioMOBY protocol: Seahawk and Gbrowse moby. Also the programming by demonstration term has been mostly adopted by robotics researchers for teaching new behaviors to the robot through a physical demonstration of the task.""
""Visual languages "",,,,""The Association of Visual Language Interpreters of Canada (AVLIC) is a professional association which represents interpreters in Canada whose working languages are English, American Sign Language (ASL), French and langue des signes Québécoise (LSQ).

""
""Interface definition languages "",,,,""An interface description language or interface definition language (IDL), is a specification language used to describe a software component's application programming interface (API). IDLs describe an interface in a language-independent way, enabling communication between software components that do not share one language. For example, between those written in C++ and those written in Java.
IDLs are commonly used in remote procedure call software. In these cases the machines at either end of the link may be using different operating systems and computer languages. IDLs offer a bridge between the two different systems.
Software systems based on IDLs include Sun's ONC RPC, The Open Group's Distributed Computing Environment, IBM's System Object Model, the Object Management Group's CORBA (which implements OMG IDL, an IDL based on DCE/RPC), Mozilla's XPCOM, Facebook's Thrift and WSDL for Web services.""
""Design languages "",,,,""A design language or design vocabulary is an overarching scheme or style that guides the design of a complement of products or architectural settings. Designers wishing to give their suite of products a unique but consistent look and feel define a design language for it, which can describe choices for design aspects such as materials, colour schemes, shapes, patterns, textures, or layouts. They then follow the scheme in the design of each object in the suite.
Usually, design languages are not rigorously defined; the designer basically makes one thing in a similar manner as another. In other cases, they are followed strictly, so that the products gain a strong thematic quality. For example, although there is a great variety of unusual chess set designs, the pieces within a set are usually thematically consistent.
Sometimes, designers encourage others to follow their design languages when decorating or accessorizing. In the context of graphical user interfaces, for example, human interface guidelines can be thought of as design languages for applications.
In automobiles, the design language is often in the grille design. For instance, many BMW vehicles share a design language, including front-end styling consisting of a split kidney and four circular headlights. Some manufacturers have appropriated design language cues from rival firms.""
""Unified Modeling Language (UML) "",,,,""The Unified Modeling Language (UML) is a general-purpose, developmental, modeling language in the field of software engineering, that is intended to provide a standard way to visualize the design of a system.
UML was originally motivated by the desire to standardize the disparate notational systems and approaches to software design developed by Grady Booch, Ivar Jacobson and James Rumbaugh at Rational Software in 1994–95, with further development led by them through 1996.
In 1997 UML was adopted as a standard by the Object Management Group (OMG), and has been managed by this organization ever since. In 2005 the Unified Modeling Language was also published by the International Organization for Standardization (ISO) as an approved ISO standard. Since then it has been periodically revised to cover the latest revision of UML.
Though well-known and widely used in education and academic papers, as of 2013 UML is little-used in industry, and most such use is informal and ad hoc.""
""Architecture description languages "",,,,""Architecture description languages (ADLs) are used in several disciplines: system engineering, software engineering, and enterprise modelling and engineering.
The system engineering community uses an architecture description language as a language and/or a conceptual model to describe and represent system architectures.
The software engineering community uses an architecture description language as a computer language to create a description of a software architecture. In the case of a so-called technical architecture, the architecture must be communicated to software developers; a functional architecture is communicated to various stakeholders and users. Some ADLs that have been developed are: Acme (developed by CMU), AADL (standardized by the SAE), C2 (developed by UCI), SBC-ADL (developed by National Sun Yat-Sen University), Darwin (developed by Imperial College London), and Wright (developed by CMU).
The up-to-date list of currently existing architectural languages might be found at Up-to-date list of ADLs.
The ISO/IEC/IEEE 42010 document, Systems and software engineering—Architecture description, defines an architecture description language as "any form of expression for use in architecture descriptions" and specifies minimum requirements on ADLs.
The enterprise modelling and engineering community have also developed architecture description languages catered for at the enterprise level. Examples include ArchiMate (now a standard of The Open Group), DEMO, ABACUS (developed by the University of Technology, Sydney). These languages do not necessarily refer to software components, etc. Most of them, however, refer to an application architecture as the architecture that is communicated to the software engineers.
Most of the writing below refers primarily to the perspective from the software engineering community.""
""System modeling languages "",,,,""The Systems Modeling Language (SysML) is a general-purpose modeling language for systems engineering applications. It supports the specification, analysis, design, verification and validation of a broad range of systems and systems-of-systems.
SysML was originally developed by an open source specification project, and includes an open source license for distribution and use. SysML is defined as an extension of a subset of the Unified Modeling Language (UML) using UML's profile mechanism.""
""Specification languages "",,,,""The Z notation /ˈzɛd/ is a formal specification language used for describing and modelling computing systems. It is targeted at the clear specification of computer programs and computer-based systems in general.""
""Software as a service orchestration systems "",,,,""Cisco Systems is an American computer networking company that made its first commercial acquisition in 1993, which was followed by a series of further acquisitions.
^ "A Communications Revolution" (PDF). Cisco Systems. Retrieved 2008-04-16.""
""Integrated and visual development environments "",,,,""An integrated development environment (IDE) is a software application that provides comprehensive facilities to computer programmers for software development. An IDE normally consists of a source code editor, build automation tools and a debugger. Most modern IDEs have intelligent code completion.
Some IDEs contain a compiler, interpreter, or both, such as NetBeans and Eclipse; others do not, such as SharpDevelop and Lazarus. The boundary between an integrated development environment and other parts of the broader software development environment is not well-defined. Sometimes a version control system, or various tools to simplify the construction of a Graphical User Interface (GUI), are integrated. Many modern IDEs also have a class browser, an object browser, and a class hierarchy diagram, for use in object-oriented software development.""
""Software configuration management and version control systems "",,,,""In software engineering, software configuration management (SCM or S/W CM) is the task of tracking and controlling changes in the software, part of the larger cross-disciplinary field of configuration management. SCM practices include revision control and the establishment of baselines. If something goes wrong, SCM can determine what was changed and who changed it. If a configuration is working well, SCM can determine how to replicate it across many hosts.
The acronym "SCM" is also expanded as source configuration management process and software change and configuration management. However, "configuration" is generally understood to cover changes typically made by a system administrator.""
""Software libraries and repositories "",,,,""A software repository is a storage location from which software packages may be retrieved and installed on a computer.

""
""Software maintenance tools "",,,,""Software maintenance in software engineering is the modification of a software product after delivery to correct faults, to improve performance or other attributes.
A common perception of maintenance is that it merely involves fixing defects. However, one study indicated that over 80% of maintenance effort is used for non-corrective actions. This perception is perpetuated by users submitting problem reports that in reality are functionality enhancements to the system. More recent studies put the bug-fixing proportion closer to 21%.""
""Requirements analysis "",,,,""Requirements analysis in systems engineering and software engineering, encompasses those tasks that go into determining the needs or conditions to meet for a new or altered product or project, taking account of the possibly conflicting requirements of the various stakeholders, analyzing, documenting, validating and managing software or system requirements.
Requirements analysis is critical to the failure of a systems or software project. The requirements should be documented, actionable, measurable, testable, traceable, related to identified business needs or opportunities, and defined to a level of detail sufficient for system design.""
""Software design engineering "",,,,""Technocity is a technology park and integrated township in Thiruvananthapuram, Kerala, India dedicated to electronics, software, and other Information Technology (IT) ventures. It is conceived in 2005 as the fourth phase of development of Technopark, Technocity is a complete IT City, spread across about 500 acres (200 hectares), which includes not just space for IT/ITES firms but also residential, commercial, hospitality, medical and educational facilities. The project is a new self-dependent satellite city, which would not strain the resources and infrastructure of the city of Thiruvananthapuram
The units in Technocity includes a wide variety of companies engaged in a range of activities, which include embedded software development, enterprise resource planning (ERP), process control software design, engineering and computer-aided design software development, IT Enabled Services (ITES), process re-engineering, animation and e-business. The firms will include domestic companies as well as subsidiaries of multi-national organisations.
Technocity is being jointly developed by the Government of Kerala and private developers. The Govt. is represented by Technopark and a special company called the Kerala State Information Technology Infrastructure Limited (KSITIL). Individual phases of the project will be developed as Special Purpose Vehicles (SPVs) between KSITIL and individual developers. KSITIL plans to hold 26% equity share in all the SPVs. Once fully operational, Technocity is expected to create about 200,000 employment opportunities.

^ Keralait News Date:2010 June 4 Techhnocity Launched
^ "`Techno city' to be set up near Capital". The Hindu Business Line. 2005-08-18. Retrieved 2008-09-26. 
^ "Biggest-ever IT project taking shape in state". Yoursmartkerala.com. 2008-06-19. Retrieved 2008-09-26.""
""Software design tradeoffs "",,,,""Copyright infringement is the use of works protected by copyright law without permission, infringing certain exclusive rights granted to the copyright holder, such as the right to reproduce, distribute, display or perform the protected work, or to make derivative works. The copyright holder is typically the work's creator, or a publisher or other business to whom copyright has been assigned. Copyright holders routinely invoke legal and technological measures to prevent and penalize copyright infringement.
Copyright infringement disputes are usually resolved through direct negotiation, a notice and take down process, or litigation in civil court. Egregious or large-scale commercial infringement, especially when it involves counterfeiting, is sometimes prosecuted via the criminal justice system. Shifting public expectations, advances in digital technology, and the increasing reach of the Internet have led to such widespread, anonymous infringement that copyright-dependent industries now focus less on pursuing individuals who seek and share copyright-protected content online, and more on expanding copyright law to recognize and penalize – as "indirect" infringers – the service providers and software distributors which are said to facilitate and encourage individual acts of infringement by others.
Estimates of the actual economic impact of copyright infringement vary widely and depend on many factors. Nevertheless, copyright holders, industry representatives, and legislators have long characterized copyright infringement as piracy or theft – language which some U.S. courts now regard as pejorative or otherwise contentious.
^ 
^ "MPAA Banned From Using Piracy and Theft Terms in Hotfile Trial". Archived from the original on 30 November 2013. Retrieved November 30, 2013. 
^ "MPAA Banned From Using Piracy and Theft Terms in Hotfile Trial". Archived from the original on 3 December 2013. Retrieved November 30, 2013.""
""Rapid application development "",,,,""Rapid application development (RAD) is both a general term used to refer to alternatives to the conventional waterfall model of software development as well as the name for James Martin's approach to rapid development. In general, RAD approaches to software development put less emphasis on planning tasks and more emphasis on development. In contrast to the waterfall model, which emphasizes rigorous specification and planning, RAD approaches emphasize the necessity of adjusting requirements in reaction to knowledge gained as the project progresses. This causes RAD to use prototypes in addition to or even sometimes in place of design specifications. RAD approaches also emphasize a flexible process that can adapt as the project evolves rather than rigorously defining specifications and plans correctly from the start. In addition to James Martin's RAD method, other approaches to rapid development include Agile methods and the spiral model. RAD is especially well suited (although not limited to) developing software that is driven by user interface requirements. Graphical user interface builders are often called rapid application development tools.""
""Agile software development "",,,,""Agile software development is a set of principles for software development in which requirements and solutions evolve through collaboration between self-organizing, cross-functional teams. It promotes adaptive planning, evolutionary development, early delivery, and continuous improvement, and it encourages rapid and flexible response to change. Agile itself has never defined any specific methods to achieve this, but many have grown up as a result and have been recognized as being 'Agile'.
The Manifesto for Agile Software Development, also known as the Agile Manifesto, was first proclaimed in 2001, after "agile methodology" was originally introduced in the late 1980s and early 1990s. The manifesto came out of the DSDM Consortium in 1994, although its roots go back to the mid 1980s at DuPont and texts by James Martin and James Kerr et al.""
""Capability Maturity Model "",,,,""The Capability Maturity Model (CMM) is a development model created after study of data collected from organizations that contracted with the U.S. Department of Defense, who funded the research. The term "maturity" relates to the degree of formality and optimization of processes, from ad hoc practices, to formally defined steps, to managed result metrics, to active optimization of the processes.
The model's aim is to improve existing software-development processes, but it can also be applied to other processes.""
""Waterfall model "",,,,""The waterfall model is a sequential design process, used in software development processes, in which progress is seen as flowing steadily downwards (like a waterfall) through the phases of conception, initiation, analysis, design, construction, testing, production/implementation and maintenance.
The waterfall development model originates in the manufacturing and construction industries: highly structured physical environments in which after-the-fact changes are prohibitively costly, if not impossible. Because no formal software development methodologies existed at the time, this hardware-oriented model was simply adapted for software development.""
""Spiral model "",,,,""The spiral model is a risk-driven process model generator for software projects. Based on the unique risk patterns of a given project, the spiral model guides a team to adopt elements of one or more process models, such as incremental, waterfall, or evolutionary prototyping.""
""V-model "",,,,""The V-model is a term applied to a range of models, from a conceptual model designed to produce a simplified understanding of the complexity associated with systems development to detailed, rigorous development lifecycle models and project management models.
There are several radically different forms of the V-model, and this creates considerable confusion. The V-model falls into three broad categories.
Firstly there is the German V-Model "Das V-Modell", the official project management methodology of the German government. It is roughly equivalent to PRINCE2, but more directly relevant to software development. What is probably the earliest use of the V-Model as a basis for a standard is Software Development Standard for the German Federal Armed Forces, 1992. The key attribute of using a "V" representation was to require proof that the products on the right-side of the V were acceptable by the appropriate test and integration organization implementing the left-side of the V. <German Directive 250, Software Development Standard for the German Federal Armed Forces, V-Model, Software Lifecycle Process Model, August 1992>
In the UK, and throughout the testing community worldwide, the V-model is widely seen as a vaguer, illustrative depiction of the software development process, as described in the ISTQB Foundation Syllabus for software testers. There is no single, accepted definition of this model, which is more directly covered in the alternative article on the V-Model (software development). There are therefore multiple variations of this version. This problem must be borne in mind when discussing the V-model.
Thirdly, like Germany, the US also has a government standard V-model, which dates back about 20 years, like its German counterpart. Its scope is rather narrower, being a systems development lifecycle model, but far more detailed and more rigorous than most UK practitioners and testers would understand by the V-model.

""
""Design patterns "",,,,""Design Patterns: Elements of Reusable Object-Oriented Software is a software engineering book describing recurring solutions to common problems in software design. The book's authors are Erich Gamma, Richard Helm, Ralph Johnson and John Vlissides with a foreword by Grady Booch. The book is divided into two parts, with the first two chapters exploring the capabilities and pitfalls of object-oriented programming, and the remaining chapters describing 23 classic software design patterns. The book includes examples in C++ and Smalltalk.
It has been highly influential to the field of software engineering and is regarded as an important source for object-oriented design theory and practice. More than 500,000 copies have been sold in English and in 13 other languages. The authors are often referred to as the Gang of Four (GoF).""
""Risk management "",,,,""Risk management is the identification, assessment, and prioritization of risks (defined in ISO 31000 as the effect of uncertainty on objectives) followed by coordinated and economical application of resources to minimize, monitor, and control the probability and/or impact of unfortunate events or to maximize the realization of opportunities. Risk management’s objective is to assure uncertainty does not deflect the endeavor from the business goals.
Risks can come from various sources including uncertainty in financial markets, threats from project failures (at any phase in design, development, production, or sustainment life-cycles), legal liabilities, credit risk, accidents, natural causes and disasters, deliberate attack from an adversary, or events of uncertain or unpredictable root-cause. There are two types of events i.e. negative events can be classified as risks while positive events are classified as opportunities. Several risk management standards have been developed including the Project Management Institute, the National Institute of Standards and Technology, actuarial societies, and ISO standards. Methods, definitions and goals vary widely according to whether the risk management method is in the context of project management, security, engineering, industrial processes, financial portfolios, actuarial assessments, or public health and safety.
Risk sources are identified and located in human factor variables, mental states and decision making as well as infrastructural or technological assets and tangible variables. The interaction between human factors and tangible aspects of risk highlights the need to focus closely on human factors as one of the main drivers for risk management, a "change driver" that comes first of all from the need to know how humans perform in challenging environments and in face of risks (Daniele Trevisani, 2007). As the author describes, «it is an extremely hard task to be able to apply an objective and systematic self-observation, and to make a clear and decisive step from the level of the mere "sensation" that something is going wrong, to the clear understanding of how, when and where to act. The truth of a problem or risk is often obfuscated by wrong or incomplete analyses, fake targets, perceptual illusions, unclear focusing, altered mental states, and lack of good communication and confrontation of risk management solutions with reliable partners. This makes the Human Factor aspect of Risk Management sometimes heavier than its tangible and technological counterpart»
Strategies to manage threats (uncertainties with negative consequences) typically include avoiding the threat, reducing the negative effect or probability of the threat, transferring all or part of the threat to another party, and even retaining some or all of the potential or actual consequences of a particular threat, and the opposites for opportunities (uncertain future states with benefits).
Certain aspects of many of the risk management standards have come under criticism for having no measurable improvement on risk, whereas the confidence in estimates and decisions seem to increase. For example, it has been shown that one in six IT projects experience cost overruns of 200% on average, and schedule overruns of 70%.""
""Software prototyping "",,,,""Software prototyping is the activity of creating prototypes of software applications, i.e., incomplete versions of the software program being developed. It is an activity that can occur in software development and is comparable to prototyping as known from other fields, such as mechanical engineering or manufacturing.
A prototype typically simulates only a few aspects of, and may be completely different from, the final product.
Prototyping has several benefits: The software designer and implementer can get valuable feedback from the users early in the project. The client and the contractor can compare if the software made matches the software specification, according to which the software program is built. It also allows the software engineer some insight into the accuracy of initial project estimates and whether the deadlines and milestones proposed can be successfully met. The degree of completeness and the techniques used in the prototyping have been in development and debate since its proposal in the early 1970s.""
""Object oriented development "",,,,""Persistor.NET is an object-oriented persistence framework which provides persistence for pure object-oriented development. Persistor .NET saves, retrieves, and deletes pure .NET Framework object graphs within a SQL Server 2005 or SQL Server Express database. The framework was developed by Hans-Peter Haberlandner and Wolfgang Portugaller in 2005.
In 2005, Persistor.NET won the "Best Visual Studio 2005 PowerToy utilizing SQL Server Express" award at the Connected Systems Developer Competition organized by Microsoft and CMP Media.""
""Flowcharts "",,,,""A flowchart is a type of diagram that represents an algorithm, workflow or process, showing the steps as boxes of various kinds, and their order by connecting them with arrows. This diagrammatic representation illustrates a solution model to a given problem. Flowcharts are used in analyzing, designing, documenting or managing a process or program in various fields.""
""Software product lines "",,,,""Software product lines, or software product line development, refers to software engineering methods, tools and techniques for creating a collection of similar software systems from a shared set of software assets using a common means of production.
Carnegie Mellon Software Engineering Institute defines a software product line as "a set of software-intensive systems that share a common, managed set of features satisfying the specific needs of a particular market segment or mission and that are developed from a common set of core assets in a prescribed way."""
""Error handling and recovery "",,,,""Background Intelligent Transfer Service (BITS) is a component of Microsoft Windows 2000 and later iterations of the operating systems, which facilitates asynchronous, prioritized, and throttled transfer of files between machines using idle network bandwidth. It is most commonly used by recent versions of Windows Update, Microsoft Update, Windows Server Update Services, and Systems Management Server to deliver software updates to clients, Microsoft's anti-virus scanner Microsoft Security Essentials (a later version of Windows Defender) to fetch signature updates, and is also used by Microsoft's instant messaging products to transfer files. BITS is exposed through the Component Object Model (COM).""
""Software prototyping "",,,,""Software prototyping is the activity of creating prototypes of software applications, i.e., incomplete versions of the software program being developed. It is an activity that can occur in software development and is comparable to prototyping as known from other fields, such as mechanical engineering or manufacturing.
A prototype typically simulates only a few aspects of, and may be completely different from, the final product.
Prototyping has several benefits: The software designer and implementer can get valuable feedback from the users early in the project. The client and the contractor can compare if the software made matches the software specification, according to which the software program is built. It also allows the software engineer some insight into the accuracy of initial project estimates and whether the deadlines and milestones proposed can be successfully met. The degree of completeness and the techniques used in the prototyping have been in development and debate since its proposal in the early 1970s.""
""Operational analysis "",,,,""Operations research, or operational research in British usage, is a discipline that deals with the application of advanced analytical methods to help make better decisions. Further, the term 'operational analysis' is used in the British (and some British Commonwealth) military, as an intrinsic part of capability development, management and assurance. In particular, operational analysis forms part of the Combined Operational Effectiveness and Investment Appraisals (COEIA), which support British defence capability acquisition decision-making.
It is often considered to be a sub-field of mathematics. The terms management science and decision science are sometimes used as synonyms.
Employing techniques from other mathematical sciences, such as mathematical modeling, statistical analysis, and mathematical optimization, operations research arrives at optimal or near-optimal solutions to complex decision-making problems. Because of its emphasis on human-technology interaction and because of its focus on practical applications, operations research has overlap with other disciplines, notably industrial engineering and operations management, and draws on psychology and organization science. Operations research is often concerned with determining the maximum (of profit, performance, or yield) or minimum (of loss, risk, or cost) of some real-world objective. Originating in military efforts before World War II, its techniques have grown to concern problems in a variety of industries.

""
""Software testing and debugging "",,,,""Eugene Howard Spafford (born 1956), commonly known as Spaf, is an American professor of computer science at Purdue University and a leading computer security expert.
A historically significant Internet figure, he is renowned for first analyzing the Morris Worm, one of the earliest computer worms, and his role in the Usenet backbone cabal. Spafford was a member of the President's Information Technology Advisory Committee 2003-2005, has been an advisor to the National Science Foundation (NSF), and serves as an advisor to over a dozen other government agencies and major corporations.""
""Fault tree analysis "",,,,""Fault tree analysis (FTA) is a top down, deductive failure analysis in which an undesired state of a system is analyzed using Boolean logic to combine a series of lower-level events. This analysis method is mainly used in the fields of safety engineering and reliability engineering to understand how systems can fail, to identify the best ways to reduce risk or to determine (or get a feeling for) event rates of a safety accident or a particular system level (functional) failure. FTA is used in the aerospace, nuclear power, chemical and process, pharmaceutical, petrochemical and other high-hazard industries; but is also used in fields as diverse as risk factor identification relating to social service system failure. FTA is also used in software engineering for debugging purposes and is closely related to cause-elimination technique used to detect bugs.
In aerospace, the more general term "system Failure Condition" is used for the "undesired state" / Top event of the fault tree. These conditions are classified by the severity of their effects. The most severe conditions require the most extensive fault tree analysis. These "system Failure Conditions" and their classification are often previously determined in the functional Hazard analysis.""
""Pair programming "",,,,""Pair programming is an agile software development technique in which two programmers work together at one workstation. One, the driver, writes code while the other, the observer or navigator, reviews each line of code as it is typed in. The two programmers switch roles frequently.
While reviewing, the observer also considers the "strategic" direction of the work, coming up with ideas for improvements and likely future problems to address. This frees the driver to focus all of his or her attention on the "tactical" aspects of completing the current task, using the observer as a safety net and guide.

""
""Use cases "",,,,""In software and systems engineering, a use case is a list of actions or event steps, typically defining the interactions between a role (known in the Unified Modeling Language as an actor) and a system, to achieve a goal. The actor can be a human, an external system, or time. In systems engineering, use cases are used at a higher level than within software engineering, often representing missions or stakeholder goals. The detailed requirements may then be captured in the Systems Modeling Language (SysML) or as contractual statements.
Use case analysis is an important and valuable requirement analysis technique that has been widely used in modern software engineering since its formal introduction by Ivar Jacobson in 1992. Use case driven development is a key characteristic of many process models and frameworks such as ICONIX, the Unified Process (UP), the IBM Rational Unified Process (RUP), and the Oracle Unified Method (OUM). With its inherent iterative, incremental and evolutionary nature, use case also fits well for agile development.""
""Acceptance testing "",,,,""In engineering and its various subdisciplines, acceptance testing is a test conducted to determine if the requirements of a specification or contract are met. It may involve chemical tests, physical tests, or performance tests.
In systems engineering it may involve black-box testing performed on a system (for example: a piece of software, lots of manufactured mechanical parts, or batches of chemical products) prior to its delivery.
In software testing the ISTQB defines acceptance as: formal testing with respect to user needs, requirements, and business processes conducted to determine whether a system satisfies the acceptance criteria and to enable the user, customers or other authorized entity to determine whether or not to accept the system. Acceptance testing is also known as user acceptance testing (UAT), end-user testing, operational acceptance testing (OAT) or field (acceptance) testing.
A smoke test may be used as an acceptance test prior to introducing a build of software to the main testing process.""
""Traceability "",,,,""Traceability is the ability to verify the history, location, or application of an item by means of documented recorded identification.
Other common definitions include the capability (and implementation) of keeping track of a given set or type of information to a given degree, or the ability to chronologically interrelate uniquely identifiable entities in a way that is verifiable.""
""Formal software verification "",,,,""CPAchecker is a framework and tool for formal software verification, and program analysis, of C programs. Some of its ideas and concepts, for example lazy abstraction, were inherited from the software model checker BLAST. CPAchecker is based on the idea of configurable program analysis  which is a concept that allows expression of both model checking and program analysis with one formalism. When executed, CPAchecker performs a reachability analysis, i.e., it checks whether a certain state, which violates a given specification, can potentially be reached. 
One application of CPAchecker is the verification of Linux device drivers.""
""Empirical software validation "",,,,""Software engineering is the study and an application of engineering to the design, development, implementation and maintenance of software in a systematic method.
Typical formal definitions of software engineering are:
"research, design, develop, and test operating systems-level software, compilers, and network distribution software for medical, industrial, military, communications, aerospace, business, scientific, and general computing applications."
"the systematic application of scientific and technological knowledge, methods, and experience to the design, implementation, testing, and documentation of software";
"the application of a systematic, disciplined, quantifiable approach to the development, operation, and maintenance of software";
"an engineering discipline that is concerned with all aspects of software production";
and "the establishment and use of sound engineering principles in order to economically obtain software that is reliable and works efficiently on real machines."
^ Abran et al. 2004, pp. 1–1
^ ACM (2007). "Computing Degrees & Careers". ACM. Retrieved 2010-11-23. 
^ Laplante, Phillip (2007). What Every Engineer Should Know about Software Engineering. Boca Raton: CRC. ISBN 978-0-8493-7228-5. Retrieved 2011-01-21. 
^ The Bureau of Labor Statistics
^ “Systems and software engineering - Vocabulary,” ISO/IEC/IEEE std 24765:2010(E), 2010.
^ “IEEE Standard Glossary of Software Engineering Terminology,” IEEE std 610.12-1990, 1990.
^ Sommerville, Ian (2007) [1982]. "1.1.2 What is software engineering?". Software Engineering (8th ed.). Harlow, England: Pearson Education. p. 7. ISBN 0-321-31379-8. Software engineering is an engineering discipline that is concerned with all aspects of software production from the early stages of system specification to maintaining the system after it has gone into use. In this definition, there are two key phrases:
1. Engineering discipline Engineers make things work. They apply theories, methods and tools where these are appropriate [. . .] Engineers also recognize that they must work to organizational and financial constraints. [. . .]
2. All aspects of software production Software engineering is not just concerned with the technical processes of software development but also with activities such as software project management and with the development of tools, methods and theories to support software production.  
^ "Software Engineering". Information Processing (North-Holland Publishing Co.) 71: 530–538. 1972.""
""Software reverse engineering "",,,,""Rigi is an interactive graph editor tool for software reverse engineering using the white box method, i.e. necessitating source code, thus it is mainly aimed at program comprehension. Rigi is distributed by its main author, Hausi A. Müller and the Rigi research group at the University of Victoria.
Rigi provides interactive links from the graphs it produces to the source code, but not vice versa. Rigi renders trees and grid-layout graphs using its own internal engine, but relies on University of Passau's GraphEd for more advanced layouts.
The public version of Rigi has built-in parsers ("fact extractors") for C and Cobol, and can leverage the C++ parser of IBM Visual Age. It can also accept external data in an RSF format (it introduced), so external parses can also feed it data, for example SHriMP tool's Java parser. Some efforts were made to integrate Rigi in Microsoft Visual Studio .NET. Early versions of Bauhaus were also built on top of Rigi; the author of this latter tool notes that the combination was rather slow for graphs having more than 500 nodes. Rigi was reportedly used to analyze some (undisclosed) embedded software at Nokia, in the range of hundreds of thousands of lines of code, and was met with positive feedback from the Nokia engineers.
Active development of Rigi has ceased in 1999, with the last official version released in 2003. A 2008 paper noted that "Rigi is a mature tool that is still used in research and popular in teaching, but it is currently no longer actively evolved and is in bug-fix mode."""
""Documentation "",,,,""Documentation is a set of documents provided on paper, or online, or on digital or analog media, such as audio tape or CDs. Examples are user guides, white papers, on-line help, quick-reference guides. It is becoming less common to see paper (hard-copy) documentation. Documentation is distributed via websites, software products, and other on-line applications.
Professionals educated in this field are termed documentalists. This field changed its name to information science in 1968, but some uses of the term documentation still exists and there have been efforts to reintroduce the term documentation as a field of study.""
""Software evolution "",,,,""Software evolution is the term used in software engineering (specifically software maintenance) to refer to the process of developing software initially, then repeatedly updating it for various reasons.""
""Software version control "",,,,""Configuration Management Version Control (CMVC) is a software package that serves as an object repository, and performs software version control, configuration management, and change management functions.""
""Maintaining software "",,,,""The ISO/IEC 12207 Systems and software engineering – Software life cycle processes is an international standard for software lifecycle processes. It aims to be the standard that defines all the tasks required for developing and maintaining software.
The ISO/IEC 12207 standard establishes a process of lifecycle for software, including processes and activities applied during the acquisition and configuration of the services of the system. Each Process has a set of outcomes associated with it. There are 23 Processes, 95 Activities, 325 Tasks and 224 Outcomes (the new "ISO/IEC 12207:2008 Systems and software engineering – Software life cycle processes" defines 43 system and software processes).
The standard has the main objective of supplying a common structure so that the buyers, suppliers, developers, maintainers, operators, managers and technicians involved with the software development use a common language. This common language is established in the form of well defined processes. The structure of the standard was intended to be conceived in a flexible, modular way so as to be adaptable to the necessities of whoever uses it. The standard is based on two basic principles: modularity and responsibility. Modularity means processes with minimum coupling and maximum cohesion. Responsibility means to establish a responsibility for each process, facilitating the application of the standard in projects where many people can be legally involved.
The set of processes, activities and tasks can be adapted according to the software project. These processes are classified in three types: basic, for support and organizational. The support and organizational processes must exist independently of the organization and the project being executed. The basic processes are instantiated according to the situation.""
""System administration "",,,,""A system administrator, or sysadmin, is a person who is responsible for the upkeep, configuration, and reliable operation of computer systems; especially multi-user computers, such as servers.
The system administrator seeks to ensure that the uptime, performance, resources, and security of the computers he or she manages meet the needs of the users, without exceeding the budget.
To meet these needs, a system administrator may acquire, install, or upgrade computer components and software; provide routine automation; maintain security policies; troubleshoot; train and/or supervise staff; or offer technical support for projects.""
""Open source model "",,,,""The open-source model is a more decentralized model of production, in contrast with more centralized models of development such as those typically used in commercial software companies. A main principle of open-source software development is peer production, with products such as source code, "blueprints", and documentation available to the public at no cost. The open-source movement in software began as a response to the limitations of proprietary code, and has since spread across different fields. This model is also used for the development of open-source appropriate technologies, and open-source drug discovery.""
""Programming teams "",,,,""A programming team is a team of people who develop or maintain computer software. They may be organised in numerous ways, but the egoless programming team and chief programmer team are two common structures typically used.""
""Lambda calculus "",,,,""Lambda calculus (also written as λ-calculus) is a formal system in mathematical logic for expressing computation based on function abstraction and application using variable binding and substitution. It was first introduced by mathematician Alonzo Church in the 1930s as part of an investigation into the foundations of mathematics. Lambda calculus is a universal model of computation equivalent to a Turing machine (Church-Turing thesis, 1937). Its namesake, Greek letter lambda (λ), is used in lambda terms (also called lambda expressions) to denote binding a variable in a function.
Lambda calculus may be typed and untyped. In typed lambda calculus functions can be applied only if they are capable of accepting the given input's "type" of data.
Lambda calculus has applications in many different areas in mathematics, philosophy, linguistics, and computer science. Lambda calculus has played an important role in the development of the theory of programming languages. Functional programming languages implement the lambda calculus. Lambda calculus also is a current research topic in Category theory.""
""Turing machines "",,,,""A Turing machine is an abstract machine that manipulates symbols on a strip of tape according to a table of rules; to be more exact, it is a mathematical model that defines such a device. Despite the model's simplicity, given any computer algorithm, a Turing machine can be constructed that is capable of simulating that algorithm's logic.
The machine operates on an infinite memory tape divided into cells. The machine positions its head over a cell and "reads" (scans) the symbol there. Then per the symbol and its present place in a finite table of user-specified instructions the machine (i) writes a symbol (e.g. a digit or a letter from a finite alphabet) in the cell (some models allowing symbol erasure and/or no writing), then (ii) either moves the tape one cell left or right (some models allow no motion, some models move the head), then (iii) (as determined by the observed symbol and the machine's place in the table) either proceeds to a subsequent instruction or halts the computation.
The Turing machine was invented in 1936 by Alan Turing, who called it an a-machine (automatic machine). With this model Turing was able to answer two questions in the negative: (1) Does a machine exist that can determine whether any arbitrary machine on its tape is "circular" (e.g. freezes, or fails to continue its computational task); similarly, (2) does a machine exist that can determine whether any arbitrary machine on its tape ever prints a given symbol. Thus by providing a mathematical description of a very simple device capable of arbitrary computations, he was able to prove properties of computation in general - and in particular, the uncomputability of the Hilbert Entscheidungsproblem ("decision problem").
Thus, Turing machines prove fundamental limitations on the power of mechanical computation. While they can express arbitrary computations, their minimalistic design makes them unsuitable for computation in practice: actual computers are based on different designs that, unlike Turing machines, use random-access memory.
Turing completeness is the ability for a system of instructions to simulate a Turing machine. A programming language that is Turing complete is theoretically capable of expressing all tasks accomplishable by computers; nearly all programming languages are Turing complete.
^ Minsky 1967:107 "In his 1936 paper, A. M. Turing defined the class of abstract machines that now bear his name. A Turing machine is a finite-state machine associated with a special kind of environment -- its tape -- in which it can store (and later recover) sequences of symbols", also Stone 1972:8 where the word "machine" is in quotation marks.
^ Stone 1972:8 states "This "machine" is an abstract mathematical model", also cf Sipser 2006:137ff that describes the "Turing machine model". Rogers 1987 (1967):13 refers to "Turing's characterization", Boolos Burgess and Jeffrey 2002:25 refers to a "specific kind of idealized machine".
^ Sipser 2006:137 "A Turing machine can do everything that a real computer can do".
^ cf Sipser 2002:137. Also Rogers 1987 (1967):13 describes "a paper tape of infinite length in both directions". Minsky 1967:118 states "The tape is regarded as infinite in both directions". Boolos Burgess and Jeffrey 2002:25 include the possibility of "there is someone stationed at each end to add extra blank squares as needed".
^ cf Rogers 1987 (1967):13. Other authors use the word "square" e.g. Boolos Burgess Jeffrey 2002:35, Minsky 1967:117, Penrose 1989:37.
^ The word used by e.g. Davis 2000:151
^ This table represents an algorithm or "effective computational procedure" which is necessarily finite; see Penrose 1989:30ff, Stone 1972:3ff.
^ Boolos Burgess and Jeffrey 2002:25
^ Boolos Burgess Jeffry 2002:25 illustrate the machine as moving along the tape. Penrose 1989:36-37 describes himself as "uncomfortable" with an infinite tape observing that it "might be hard to shift!"; he "prefer[s] to think of the tape as representing some external environment through which our finite device can move" and after observing that the " 'movement' is a convenient way of picturing things" and then suggests that "the device receives all its input from this environment.
^ "Also by convention one of the states is distinguished as the stopping state and is given the name HALT" (Stone 1972:9). Turing's original description did not include a HALT instruction but he did allow for a "circular" condition, a "configuration from which there is no possible move" (see Turing 1936 in The Undecidable 1967:119); this notion was added in the 1950s; see more at Halting problem.
^ Andrew Hodges (2012). Alan Turing: The Enigma (THE CENTENARY EDITION). Princeton University Press. ISBN 978-0-691-15564-7. 
^ The idea came to him in mid-1935 (perhaps, see more in the History section) after a question posed by M. H. A. Newman in his lectures: "Was there a definite method, or as Newman put it, a mechanical process which could be applied to a mathematical statement, and which would come up with the answer as to whether it was provable" (Hodges 1983:93). Turing submitted his paper on 31 May 1936 to the London Mathematical Society for its Proceedings (cf Hodges 1983:112), but it was published in early 1937 and offprints were available in February 1937 (cf Hodges 1983:129).
^ See footnote in Davis 2000:151.
^ Turing 1936 in The Undecidable 1965:132-134; Turing's definition of "circular" is found on page 119.
^ Turing 1936 in The Undecidable 1965:145
^ Sipser 2006:137 observes that "A Turing machine can do everything that a real computer can do. Nevertheless, even a Turing machine cannot solve certain problems. In a very real sense, these problems are beyond the theoretical limits of computation."""
""Probabilistic computation "",,,,""In computability theory, a probabilistic Turing machine is a non-deterministic Turing machine which randomly chooses between the available transitions at each point according to some probability distribution.
In the case of equal probabilities for the transitions, it can be defined as a deterministic Turing machine having an additional "write" instruction where the value of the write is uniformly distributed in the Turing Machine's alphabet (generally, an equal likelihood of writing a '1' or a '0' on to the tape.) Another common reformulation is simply a deterministic Turing machine with an added tape full of random bits called the random tape.
As a consequence, a probabilistic Turing machine can (unlike a deterministic Turing Machine) have stochastic results; on a given input and instruction state machine, it may have different run times, or it may not halt at all; further, it may accept an input in one execution and reject the same input in another execution.
Therefore the notion of acceptance of a string by a probabilistic Turing machine can be defined in different ways. Various polynomial-time randomized complexity classes that result from different definitions of acceptance include RP, Co-RP, BPP and ZPP. If the machine is restricted to logarithmic space instead of polynomial time, the analogous RL, Co-RL, BPL, and ZPL complexity classes are obtained. By enforcing both restrictions, RLP, Co-RLP, BPLP, and ZPLP are yielded.
Probabilistic computation is also critical for the definition of most classes of interactive proof systems, in which the verifier machine depends on randomness to avoid being predicted and tricked by the all-powerful prover machine. For example, the class IP equals PSPACE, but if randomness is removed from the verifier, we are left with only NP, which is not known but widely believed to be a considerably smaller class.
One of the central questions of complexity theory is whether randomness adds power; that is, is there a problem which can be solved in polynomial time by a probabilistic Turing machine but not a deterministic Turing machine? Or can deterministic Turing machines efficiently simulate all probabilistic Turing machines with at most a polynomial slowdown? It is currently widely believed by researchers that the latter is the case, which would imply P = BPP. The same question for log space instead of polynomial time (does L = BPLP?) is even more widely believed to be true. On the other hand, the power randomness gives to interactive proof systems, as well as the simple algorithms it creates for difficult problems such as polynomial-time primality testing and log-space graph connectedness testing, suggest that randomness may add power.
A quantum computer is another model of computation that is inherently probabilistic.""
""Quantum complexity theory "",,,,""Quantum complexity theory is a part of computational complexity theory in theoretical computer science. It studies complexity classes defined using quantum computers and quantum information which are computational models based on quantum mechanics. It studies the hardness of problems in relation to these complexity classes, and the relationship between quantum complexity classes and classical (i.e., non-quantum) complexity classes.
A complexity class is a collection of problems which can be solved by some computational model under resource constraints. For instance, the complexity class P is defined to be the set of problems solvable by a Turing machine in polynomial time. Similarly, one may define a quantum complexity class using a quantum model of computation, such as a standard quantum computer or a quantum Turing machine. Thus, the complexity class BQP is defined to be the set of problems solvable by a quantum computer in polynomial time with bounded error.
Two important quantum complexity classes are BQP and QMA which are the bounded-error quantum analogues of P and NP. One of the main aims of quantum complexity theory is to find out where these classes lie with respect to classical complexity classes such as P, NP, PP, PSPACE and other complexity classes.""
""Quantum communication complexity "",,,,""The notion of communication complexity was introduced by Yao in 1979, who investigated the following problem involving two separated parties (Alice and Bob). Alice receives an n-bit string x and Bob another n-bit string y, and the goal is for one of them (say Bob) to compute a certain function f(x,y) with the least amount of communication between them. Note that here we are not concerned about the number of computational steps, or the size of the computer memory used. Communication complexity tries to quantify the amount of communication required for such distributed computations.
Of course they can always succeed by having Alice send her whole n-bit string to Bob, who then computes the function, but the idea here is to find clever ways of calculating f with fewer than n bits of communication.
This abstract problem is relevant in many contexts: in VLSI circuit design, for example, one wants to minimize energy used by decreasing the amount of electric signals required between the different components during a distributed computation. The problem is also relevant in the study of data structures, and in the optimization of computer networks. For a survey of the field, see the book by Kushilevitz and Nisan.

""
""Quantum query complexity "",,,,""In computational complexity and communication complexity theories the decision tree model is the model of computation or communication in which an algorithm or communication process is considered to be basically a decision tree, i.e., a sequence of branching operations based on comparisons of some quantities, the comparisons being assigned the unit computational cost.
The branching operations are called "tests" or "queries". In this setting the algorithm in question may be viewed as a computation of a Boolean function  where the input is a series of queries and the output is the final decision. Every query is dependent on previous queries.
Several variants of decision tree models have been introduced, depending on the complexity of the operations allowed in the computation of a single comparison and the way of branching.
Decision trees models are instrumental in establishing lower bounds for computational complexity for certain classes of computational problems and algorithms: the lower bound for worst-case computational complexity is proportional to the largest depth among the decision trees for all possible inputs for a given computational problem. The computation complexity of a problem or an algorithm expressed in terms of the decision tree model is called decision tree complexity or query complexity.""
""Quantum information theory "",,,,""In physics and computer science, quantum information is information that is held in the state of a quantum system. Quantum information is the basic entity of study in quantum information theory, and can be manipulated using engineering techniques known as quantum information processing. Much like classical information can be processed with digital computers, transmitted from place to place, manipulated with algorithms, and analyzed with the mathematics of computer science, so also analogous concepts apply to quantum information.""
""Interactive computation "",,,,""In computer science, interactive computation is a mathematical model for computation that involves input/output communication with the external world during computation. This is in contrast to the traditional understanding of computation which assumes reading input only before computation and writing output only after computation, thus defining a kind of "closed" computation.
The famous Church-Turing thesis attempts to define computation and computability in terms of Turing machines. However the Turing machine model only provides an answer to the question of what computability of functions means and, with interactive tasks not always being reducible to functions, it fails to capture our broader intuition of computation and computability. While this fact was admitted by Alan Turing himself, it was not until recently that the theoretical computer science community realized the necessity to define adequate mathematical models of interactive computation. Among the currently studied mathematical models of computation that attempt to capture interaction are Japaridze's hard- and easy-play machines elaborated within the framework of computability logic, Goldin's persistent Turing machines, and Gurevich's abstract state machines. Peter Wegner has additionally done a great deal of work on this area of computer science.""
""Streaming models "",,,,""A Data stream management system (DSMS) is a computer program to manage continuous data streams. It is similar to a database management system (DBMS), which is, however, designed for static data in conventional databases. A DSMS also offers a flexible query processing so that the information need can be expressed using queries. However, in contrast to a DBMS, a DSMS executes a continuous query that is not only performed once, but is permanently installed. Therefore, the query is continuously executed until it is explicitly uninstalled. Since most DSMS are data-driven, a continuous query produces new results as long as new data arrive at the system. This basic concept is similar to Complex event processing so that both technologies are partially coalescing.""
""Parallel computing models "",,,,""In formal language theory, a picture language is a set of pictures, where a picture is a 2D array of characters over some alphabet.
For example, the language  defines the language of squares composed of the character . This language  contains pictures such as:

            a a a
      a a   a a a <math> \in L </math>
  a , a a , a a a

The study of picture languages was initially motivated by the problems of pattern recognition and image processing, but two-dimensional patterns also appear in the study of cellular automata and other parallel computing models. Some formal systems have been created to define picture languages, such as array grammars and tiling systems.""
""Distributed computing models "",,,,""A distributed operating system is a software over a collection of independent, networked, communicating, and physically separate computational nodes. Each individual node holds a specific software subset of the global aggregate operating system. Each subset is a composite of two distinct service provisioners. The first is a ubiquitous minimal kernel, or microkernel, that directly controls that node’s hardware. Second is a higher-level collection of system management components that coordinate the node's individual and collaborative activities. These components abstract microkernel functions and support user applications.
The microkernel and the management components collection work together. They support the system’s goal of integrating multiple resources and processing functionality into an efficient and stable system. This seamless integration of individual nodes into a global system is referred to as transparency, or single system image; describing the illusion provided to users of the global system’s appearance as a single computational entity.""
""Process calculi "",,,,""In computer science, the process calculi (or process algebras) are a diverse family of related approaches for formally modelling concurrent systems. Process calculi provide a tool for the high-level description of interactions, communications, and synchronizations between a collection of independent agents or processes. They also provide algebraic laws that allow process descriptions to be manipulated and analyzed, and permit formal reasoning about equivalences between processes (e.g., using bisimulation). Leading examples of process calculi include CSP, CCS, ACP, and LOTOS. More recent additions to the family include the π-calculus, the ambient calculus, PEPA, the fusion calculus and the join-calculus.""
""Abstract machines "",,,,""An abstract machine, also called an abstract computer, is a theoretical model of a computer hardware or software system used in automata theory. Abstraction of computing processes is used in both the computer science and computer engineering disciplines and usually assumes a discrete time paradigm.

""
""Rewrite systems "",,,,""In mathematics, computer science, and logic, rewriting covers a wide range of (potentially non-deterministic) methods of replacing subterms of a formula with other terms. What is considered are rewriting systems (also known as rewrite systems, rewrite engines or reduction systems). In their most basic form, they consist of a set of objects, plus relations on how to transform those objects.
Rewriting can be non-deterministic. One rule to rewrite a term could be applied in many different ways to that term, or more than one rule could be applicable. Rewriting systems then do not provide an algorithm for changing one term to another, but a set of possible rule applications. When combined with an appropriate algorithm, however, rewrite systems can be viewed as computer programs, and several declarative programming languages are based on term rewriting.""
""Tree languages "",,,,""A language family is a group of languages related through descent from a common ancestor, called the proto-language of that family. The term 'family' reflects the tree model of language origination in historical linguistics, which makes use of a metaphor comparing languages to people in a biological family tree, or in a subsequent modification, to species in a phylogenetic tree of evolutionary taxonomy. No actual biological relationship between speakers is implied by the metaphor.
Estimates of the number of living languages vary from 5,000 to 8,000, depending on the precision of one's definition of "language", and in particular on how one classifies dialects. The 2013 edition of Ethnologue catalogs just over 7,000 living human languages. A "living language" is simply one that is used as the primary form of communication of a group of people. There are also many dead and extinct languages, as well as some that are still insufficiently studied to be classified, or even unknown outside their respective speech communities.
Membership of languages in a language family is established by comparative linguistics. Sister languages are said to have a "genetic" or "genealogical" relationship. The latter term is older, but has been revived in recent years to better distinguish the relationships between languages from the genetic relationships between people. The evidence of linguistic relationship is found in observable shared characteristics that are not attributed to contact or borrowing. Genealogically related languages present shared retentions, that is, features of the proto-language (or reflexes of such features) that cannot be explained by chance or borrowing (convergence). Membership in a branch or group within a language family is established by shared innovations, that is, common features of those languages that are not found in the common ancestor of the entire family. For example, Germanic languages are "Germanic" in that they share vocabulary and grammatical features that are not believed to have been present in the Proto-Indo-European language. These features are believed to be innovations that took place in Proto-Germanic, a descendant of Proto-Indo-European that was the source of all Germanic languages.""
""Transducers "",,,,""A transducer is a device that converts one form of energy to another. Usually a transducer converts a signal in one form of energy to a signal in another.
Transducers are often employed at the boundaries of automation, measurement, and control systems, where electrical signals are converted to and from other physical quantities (energy, force, torque, light, motion, position, etc.). The process of converting one form of energy to another is known as transduction.""
""Quantitative automata "",,,,""Mathematical and theoretical biology is an interdisciplinary scientific research field with a range of applications in biology, biotechnology, and medicine. The field is also called mathematical biology or biomathematics to stress the mathematical side, or theoretical biology to stress the biological side. Mathematical biology aims at the mathematical representation, treatment and modeling of biological processes, using a variety of applied mathematical techniques and tools. It has both theoretical and practical applications in biological, biomedical and biotechnology research. For example, in cell biology, protein interactions are often represented as "cartoon" models, which, although easy to visualize, do not accurately describe the systems studied. This requires precise mathematical models. Describing systems in a quantitative manner means their behavior can be better simulated, and hence properties can be predicted that might not be evident to the experimenter.
Mathematical biology may deploy calculus, probability theory, statistics, linear algebra, abstract algebra, graph theory, combinatorics, algebraic geometry, topology, dynamical systems, differential equations and coding theory. Some mathematical areas, such as certain methodologies in statistics, were developed as tools during the conduct of research into mathematical biology.""
""Regular languages "",,,,""In theoretical computer science and formal language theory, a regular language (also called a rational language) is a formal language that can be expressed using a regular expression, in the strict sense of the latter notion used in theoretical computer science (as opposed to many regular expressions engines provided by modern programming languages, which are augmented with features that allow recognition of languages that cannot be expressed by a classic regular expression).
Alternatively, a regular language can be defined as a language recognized by a finite automaton. The equivalence of regular expressions and finite automata is known as Kleene's theorem. In the Chomsky hierarchy, regular languages are defined to be the languages that are generated by Type-3 grammars (regular grammars).
Regular languages are very useful in input parsing and programming language design.""
""Complexity classes "",,,,""In computational complexity theory, a complexity class is a set of problems of related resource-based complexity. A typical complexity class has a definition of the form:
the set of problems that can be solved by an abstract machine M using O(f(n)) of resource R, where n is the size of the input.
Complexity classes are concerned with the rate of growth of the requirement in resources as the input n increases. It is an abstract measurement, and does not give time or space in requirements in terms of seconds or bytes, which would require knowledge of implementation specifics. The function inside the O(...) expression could be a constant, for algorithms which are unaffected by the size of n,or an expression involving a logarithm, an expression involving a power of n, i.e a polynomial expression, and many others. The O is read as "order of..". For the purposes of computational complexity theory, some of the details of the function can be ignored, for instance many possible polynomials can be grouped together as a class.
The resource in question can either be time, essentially the number of primitve operations on an abstract machine, or (storage) space. For example, the class NP is the set of decision problems whose solutions can be determined by a non-deterministic Turing machine in polynomial time, while the class PSPACE is the set of decision problems that can be solved by a deterministic Turing machine in polynomial space.
The simplest complexity classes are defined by the following factors:
The type of computational problem: The most commonly used problems are decision problems. However, complexity classes can be defined based on function problems (an example is FP), counting problems (e.g. #P), optimization problems, promise problems, etc.
The model of computation: The most common model of computation is the deterministic Turing machine, but many complexity classes are based on nondeterministic Turing machines, boolean circuits, quantum Turing machines, monotone circuits, etc.
The resource (or resources) that are being bounded and the bounds: These two properties are usually stated together, such as "polynomial time", "logarithmic space", "constant depth", etc.
Many complexity classes can be characterized in terms of the mathematical logic needed to express them; see descriptive complexity.
Bounding the computation time above by some concrete function f(n) often yields complexity classes that depend on the chosen machine model. For instance, the language {xx | x is any binary string} can be solved in linear time on a multi-tape Turing machine, but necessarily requires quadratic time in the model of single-tape Turing machines. If we allow polynomial variations in running time, Cobham-Edmonds thesis states that "the time complexities in any two reasonable and general models of computation are polynomially related" (Goldreich 2008, Chapter 1.2). This forms the basis for the complexity class P, which is the set of decision problems solvable by a deterministic Turing machine within polynomial time. The corresponding set of function problems is FP.
The Blum axioms can be used to define complexity classes without referring to a concrete computational model.""
""Communication complexity "",,,,""The notion of communication complexity was introduced by Yao in 1979, who investigated the following problem involving two separated parties (Alice and Bob). Alice receives an n-bit string x and Bob another n-bit string y, and the goal is for one of them (say Bob) to compute a certain function f(x,y) with the least amount of communication between them. Note that here we are not concerned about the number of computational steps, or the size of the computer memory used. Communication complexity tries to quantify the amount of communication required for such distributed computations.
Of course they can always succeed by having Alice send her whole n-bit string to Bob, who then computes the function, but the idea here is to find clever ways of calculating f with fewer than n bits of communication.
This abstract problem is relevant in many contexts: in VLSI circuit design, for example, one wants to minimize energy used by decreasing the amount of electric signals required between the different components during a distributed computation. The problem is also relevant in the study of data structures, and in the optimization of computer networks. For a survey of the field, see the book by Kushilevitz and Nisan.

""
""Circuit complexity "",,,,""In theoretical computer science, circuit complexity is a branch of computational complexity theory in which Boolean functions are classified according to the size or depth of Boolean circuits that compute them. One speaks of the circuit complexity of a Boolean circuit. A related notion is the circuit complexity of a recursive language that is decided by a family of circuits  (see below).
A Boolean circuit with  input bits is a directed acyclic graph in which every node (usually called gates in this context) is either an input node of in-degree 0 labeled by one of the  input bits, an AND gate, an OR gate, or a NOT gate. One of these gates is designated as the output gate. Such a circuit naturally computes a function of its  inputs. The size of a circuit is the number of gates it contains and its depth is the maximal length of a path from an input gate to the output gate.
There are two major notions of circuit complexity (these are outlined in Sipser (1997)). The circuit-size complexity of a Boolean function  is the minimal size of any circuit computing . The circuit-depth complexity of a Boolean function  is the minimal depth of any circuit computing .
These notions generalize when one considers the circuit complexity of a recursive language: A formal language may contain strings with many different bit lengths. Boolean circuits, however, only allow a fixed number of input bits. Thus no single Boolean circuit is capable of deciding such a language. To account for this possibility, one considers families of circuits  where each  accepts inputs of size . Each circuit family will naturally generate a recursive language by outputting  when a string is a member of the family, and  otherwise. We say that a family of circuits is size minimal if there is no other family that decides on inputs of any size, , with a circuit of smaller size than  (respectively for depth minimal families).
Hence, the circuit-size complexity of a recursive language  is defined as the function , that relates a bit length of an input, , to the circuit-size complexity of a minimal circuit  that decides whether inputs of that length are in . The circuit-depth complexity is defined similarly.
Complexity classes defined in terms of Boolean circuits include AC0, AC, TC0 and NC.""
""Algebraic complexity theory "",,,,""Amin Shokrollahi (born 1964) is an Iranian mathematician who has worked on a variety of topics including coding theory and algebraic complexity theory. He is best known for his work on iterative decoding of graph based codes for which he received the IEEE Information Theory Paper Award of 2002 (together with Michael Luby, Michael Mitzenmacher, and Daniel Spielman, as well as Tom Richardson and Ruediger Urbanke). He is one of the inventors of a modern class of practical erasure codes known as tornado codes, and the principal developer of raptor codes, which belong to a class of rateless erasure codes known as Fountain codes. In connection with the work on these codes, he received the IEEE Eric E. Sumner Award in 2007 together with Michael Luby "for bridging mathematics, Internet design and mobile broadcasting as well as successful standardization" and the IEEE Richard W. Hamming Medal in 2012 together with Michael Luby "for the conception, development, and analysis of practical rateless codes". He also received the 2007 joint Communication Society and Information Theory Society best paper award for his work on raptor codes.

He is the principal inventor of Chordal Codes, a new class of codes specifically designed for communication on electrical wires between chips. In 2011 he founded the company Kandou Bus dedicated to commercialization of the concept of Chordal Codes. The first implementation, transmitting data on 8 correlated wires and implemented in a 40nm process, received the Jan Van Vessem Award for best European Paper at the International Solid-State Circuits Conference (ISSCC) 2014.
^ "Information Theory Paper Award". IEEE Information Theory Society. Retrieved May 20, 2012. 
^ Michael G. Luby, Michael Mitzenmacher, M. Amin Shokrollahi, Daniel A. Spielman, Volker Stemann (1997). "Practical Loss-Resilient Codes". Proceedings of the twenty-ninth annual ACM symposium on Theory of computing – STOC '97 (ACM): 150–159. 
^ Amin Shokrollahi (2006). "Raptor Codes" (PDF). Transactions on Information Theory (IEEE) 52 (6): 2551–2567. 
^ "IEEE Eric E. Sumner Award Recipients". IEEE. Retrieved February 27, 2011. 
^ "IEEE Richard W. Hamming Medal Recipients" (PDF). IEEE. Retrieved January 5, 2011. 
^ "IEEE Communications Society & Information Theory Society Joint Paper Award". IEEE Communications Society. Retrieved May 20, 2012.""
""Quantum complexity theory "",,,,""Quantum complexity theory is a part of computational complexity theory in theoretical computer science. It studies complexity classes defined using quantum computers and quantum information which are computational models based on quantum mechanics. It studies the hardness of problems in relation to these complexity classes, and the relationship between quantum complexity classes and classical (i.e., non-quantum) complexity classes.
A complexity class is a collection of problems which can be solved by some computational model under resource constraints. For instance, the complexity class P is defined to be the set of problems solvable by a Turing machine in polynomial time. Similarly, one may define a quantum complexity class using a quantum model of computation, such as a standard quantum computer or a quantum Turing machine. Thus, the complexity class BQP is defined to be the set of problems solvable by a quantum computer in polynomial time with bounded error.
Two important quantum complexity classes are BQP and QMA which are the bounded-error quantum analogues of P and NP. One of the main aims of quantum complexity theory is to find out where these classes lie with respect to classical complexity classes such as P, NP, PP, PSPACE and other complexity classes.""
""Proof complexity "",,,,""In computer science, proof complexity is a measure of efficiency of automated theorem proving methods that is based on the size of the proofs they produce. The methods for proving contradiction in propositional logic are the most analyzed. The two main issues considered in proof complexity are whether a proof method can produce a polynomial proof of every inconsistent formula, and whether the proofs produced by one method are always of size similar to those produced by another method.""
""Interactive proof systems "",,,,""In computational complexity theory, an interactive proof system is an abstract machine that models computation as the exchange of messages between two parties. The parties, the verifier and the prover, interact by exchanging messages in order to ascertain whether a given string belongs to a language or not. The prover is all-powerful and possesses unlimited computational resources, but cannot be trusted, while the verifier has bounded computation power. Messages are sent between the verifier and prover until the verifier has an answer to the problem and has "convinced" itself that it is correct.
All interactive proof systems have two requirements:
Completeness: if the statement is true, the honest verifier (that is, one following the protocol properly) will be convinced of this fact by an honest prover.
Soundness: if the statement is false, no prover, even if it doesn't follow the protocol, can convince the honest verifier that it is true, except with some small probability.
It is assumed that the verifier is always honest.
The specific nature of the system, and so the complexity class of languages it can recognize, depends on what sort of bounds are put on the verifier, as well as what abilities it is given — for example, most interactive proof systems depend critically on the verifier's ability to make random choices. It also depends on the nature of the messages exchanged — how many and what they can contain. Interactive proof systems have been found to have some important implications for traditional complexity classes defined using only one machine. The main complexity classes describing interactive proof systems are AM and IP.""
""Complexity theory and logic "",,,,""DLT, the International Conference on Developments in Language Theory is an academic conference in the field of computer science held annually under the auspices of the European Association for Theoretical Computer Science. Like most theoretical computer science conferences its contributions are strongly peer-reviewed; the articles appear in proceedings published in Springer Lecture Notes in Computer Science. Extended versions of selected papers of each year's conference appear in international journals, such as Theoretical Computer Science and International Journal of Foundations of Computer Science.""
""Cryptographic primitives "",,,,""Cryptographic primitives are well-established, low-level cryptographic algorithms that are frequently used to build cryptographic protocols for computer security systems. These routines include, but are not limited to, one-way hash functions and encryption functions.""
""Cryptographic protocols "",,,,""A security protocol (cryptographic protocol or encryption protocol) is an abstract or concrete protocol that performs a security-related function and applies cryptographic methods, often as sequences of cryptographic primitives. A protocol describes how the algorithms should be used. A sufficiently detailed protocol includes details about data structures and representations, at which point it can be used to implement multiple, interoperable versions of a program.
Cryptographic protocols are widely used for secure application-level data transport. A cryptographic protocol usually incorporates at least some of these aspects:
Key agreement or establishment
Entity authentication
Symmetric encryption and message authentication  material construction
Secured application-level data transport
Non-repudiation methods
Secret sharing methods
Secure multi-party computation
For example, Transport Layer Security (TLS) is a cryptographic protocol that is used to secure web (HTTP/HTTPS) connections. It has an entity authentication mechanism, based on the X.509 system; a key setup phase, where a symmetric encryption key is formed by employing public-key cryptography; and an application-level data transport function. These three aspects have important interconnections. Standard TLS does not have non-repudiation support.
There are other types of cryptographic protocols as well, and even the term itself has various readings; Cryptographic application protocols often use one or more underlying key agreement methods, which are also sometimes themselves referred to as "cryptographic protocols". For instance, TLS employs what is known as the Diffie-Hellman key exchange, which although it is only a part of TLS per se, Diffie-Hellman may be seen as a complete cryptographic protocol in itself for other applications.
Cryptographic protocols can sometimes be verified formally on an abstract level. When it is done, there is a necessity to formalize the environment in which the protocol operate in order to identify threats. This is frequently done through the Dolev-Yao model.

""
""Logic and verification "",,,,""Tobias Nipkow (born 1958) is a German computer scientist. He received his Diplom (MSc) in computer science from the Technische Hochschule Darmstadt in 1982, and his Ph.D. from the University of Manchester in 1987. He worked at MIT from 1987, changed to Cambridge University in 1989, and to Technical University Munich in 1992, where he was appointed professor for programming theory. He is chair of the Logic and Verification group since 2011.
He is known for his work in interactive and automatic theorem proving, in particular for the Isabelle proof assistant; he is the editor of the Journal of Automated Reasoning. Moreover, he focuses on programming language semantics, type systems and functional programming.
^ Brief vita""
""Proof theory "",,,,""Proof theory is a branch of mathematical logic that represents proofs as formal mathematical objects, facilitating their analysis by mathematical techniques. Proofs are typically presented as inductively-defined data structures such as plain lists, boxed lists, or trees, which are constructed according to the axioms and rules of inference of the logical system. As such, proof theory is syntactic in nature, in contrast to model theory, which is semantic in nature. Together with model theory, axiomatic set theory, and recursion theory, proof theory is one of the so-called four pillars of the foundations of mathematics.
Some of the major areas of proof theory include structural proof theory, ordinal analysis, provability logic, reverse mathematics, proof mining, automated theorem proving, and proof complexity. Much research also focuses on applications in computer science, linguistics, and philosophy.""
""Modal and temporal logics "",,,,""Logic in computer science covers the overlap between the field of logic and that of computer science. The topic can essentially be divided into three main areas:
Theoretical foundations and analysis
Use of computer technology to aid logicians
Use of concepts from logic for computer applications""
""Automated reasoning "",,,,""Automated reasoning is an area of computer science and mathematical logic dedicated to understanding different aspects of reasoning. The study of automated reasoning helps produce computer programs that allow computers to reason completely, or nearly completely, automatically. Although automated reasoning is considered a sub-field of artificial intelligence, it also has connections with theoretical computer science, and even philosophy.
The most developed subareas of automated reasoning are automated theorem proving (and the less automated but more pragmatic subfield of interactive theorem proving) and automated proof checking (viewed as guaranteed correct reasoning under fixed assumptions). Extensive work has also been done in reasoning by analogy induction and abduction.
Other important topics include reasoning under uncertainty and non-monotonic reasoning. An important part of the uncertainty field is that of argumentation, where further constraints of minimality and consistency are applied on top of the more standard automated deduction. John Pollock's OSCAR system is an example of an automated argumentation system that is more specific than being just an automated theorem prover.
Tools and techniques of automated reasoning include the classical logics and calculi, fuzzy logic, Bayesian inference, reasoning with maximal entropy and a large number of less formal ad hoc techniques.""
""Constructive mathematics "",,,,""In the philosophy of mathematics, constructivism asserts that it is necessary to find (or "construct") a mathematical object to prove that it exists. When one assumes that an object does not exist and derives a contradiction from that assumption, one still has not found the object and therefore not proved its existence, according to constructivism. This viewpoint involves a verificational interpretation of the existence quantifier, which is at odds with its classical interpretation.
There are many forms of constructivism. These include the program of intuitionism founded by Brouwer, the finitism of Hilbert and Bernays, the constructive recursive mathematics of Shanin and Markov, and Bishop's program of constructive analysis. Constructivism also includes the study of constructive set theories such as IZF and the study of topos theory.
Constructivism is often identified with intuitionism, although intuitionism is only one constructivist program. Intuitionism maintains that the foundations of mathematics lie in the individual mathematician's intuition, thereby making mathematics into an intrinsically subjective activity. Other forms of constructivism are not based on this viewpoint of intuition, and are compatible with an objective viewpoint on mathematics.""
""Description logics "",,,,""Description logics (DL) is a family of formal knowledge representation languages. Many DLs are more expressive than propositional logic but less expressive than first-order predicate logic. In contrast to the latter, the core reasoning problems for DLs are (usually) decidable, and efficient decision procedures have been designed and implemented for these problems.
DLs are used in artificial intelligence to describe and reason about the relevant concepts of an application domain (known as terminological knowledge). It is of particular importance in providing a logical formalism for ontologies and the Semantic Web: the Web Ontology Language [OWL] and its profile is based on DLs. The most notable application of DLs and OWL is in biomedical informatics where DL assists in the codification of biomedical knowledge.

""
""Equational logic and rewriting "",,,,""In mathematics, computer science, and logic, rewriting covers a wide range of (potentially non-deterministic) methods of replacing subterms of a formula with other terms. What is considered are rewriting systems (also known as rewrite systems, rewrite engines or reduction systems). In their most basic form, they consist of a set of objects, plus relations on how to transform those objects.
Rewriting can be non-deterministic. One rule to rewrite a term could be applied in many different ways to that term, or more than one rule could be applicable. Rewriting systems then do not provide an algorithm for changing one term to another, but a set of possible rule applications. When combined with an appropriate algorithm, however, rewrite systems can be viewed as computer programs, and several declarative programming languages are based on term rewriting.""
""Finite Model Theory "",,,,""Finite Model Theory (FMT) is a subarea of model theory (MT). MT is the branch of mathematical logic which deals with the relation between a formal language (syntax) and its interpretations (semantics). FMT is a restriction of MT to interpretations of finite structures, which have a finite universe.
Since many central theorems of MT do not hold when restricted to finite structures, FMT is quite different from MT in its methods of proof. Central results of classical model theory that fail for finite structures include the compactness theorem, Gödel's completeness theorem, and the method of ultraproducts for first-order logic (FO).
As MT is closely related to mathematical algebra, FMT became an "unusually effective" instrument in computer science. In other words: "In the history of mathematical logic most interest has concentrated on infinite structures....Yet, the objects computers have and hold are always finite. To study computation we need a theory of finite structures." Thus the main application areas of FMT are: descriptive complexity theory, database theory and formal language theory.
FMT is mainly about discrimination of structures. The usual motivating question is whether a given class of structures can be described (up to isomorphism) in a given language. For instance, can all cyclic graphs be discriminated (from the non-cyclic ones) by a sentence of the first-order logic of graphs? This can also be phrased as: is the property "cyclic" FO expressible?""
""Higher order logic "",,,,""In mathematics and logic, a higher-order logic is a form of predicate logic that is distinguished from first-order logic by additional quantifiers and a stronger semantics. Higher-order logics with their standard semantics are more expressive, but their model-theoretic properties are less well-behaved than those of first-order logic.
First-order logic quantifies only variables that range over individuals; second-order logic, in addition, also quantifies over sets; third-order logic also quantifies over sets of sets, and so on. For example, the second-order sentence

expresses the principle of mathematical induction. Higher-order logic is the union of first-, second-, third-, …, nth-order logic; i.e., higher-order logic admits quantification over sets that are nested arbitrarily deeply.

""
""Linear logic "",,,,""Linear logic is a substructural logic proposed by Jean-Yves Girard as a refinement of classical and intuitionistic logic, joining the dualities of the former with many of the constructive properties of the latter. Although the logic has also been studied for its own sake, more broadly, ideas from linear logic have been influential in fields such as programming languages, game semantics, and quantum physics, as well as linguistics, particularly because of its emphasis on resource-boundedness, duality, and interaction.
Linear logic lends itself to many different presentations, explanations and intuitions.
Proof-theoretically, it derives from an analysis of classical sequent calculus in which uses of (the structural rules) contraction and weakening are carefully controlled. Operationally, this means that logical deduction is no longer merely about an ever-expanding collection of persistent "truths", but also a way of manipulating resources that cannot always be duplicated or thrown away at will. In terms of simple denotational models, linear logic may be seen as refining the interpretation of intuitionistic logic by replacing cartesian closed categories by symmetric monoidal categories, or the interpretation of classical logic by replacing boolean algebras by C*-algebras.
^ Girard, Jean-Yves (1987). "Linear logic" (PDF). Theoretical Computer Science 50 (1): 1–102. doi:10.1016/0304-3975(87)90045-4. 
^ Baez, John; Stay, Mike (2008). Bob Coecke, ed. "Physics, Topology, Logic and Computation: A Rosetta Stone" (PDF). New Structures of Physics. 
^ de Paiva, V.; van Genabith, J.; Ritter, E. (1999). Dagstuhl Seminar 99341 on Linear Logic and Applications (PDF).""
""Programming logic "",,,,""An Erasable programmable logic device (EPLD) is an integrated circuit that comprises an array of programmable logic devices (PLD) that do not come pre-connected; the connections are programmed electrically by the user.""
""Abstraction "",,,,""Abstraction in its main sense is a conceptual process by which general rules and concepts are derived from the usage and classification of specific examples, literal ("real" or "concrete") signifiers, first principles, or other methods. "An abstraction" is the product of this process—a concept that acts as a super-categorical noun for all subordinate concepts, and connects any related concepts as a group, field, or category.
Conceptual abstractions may be formed by filtering the information content of a concept or an observable phenomenon, selecting only the aspects which are relevant for a particular purpose. For example, abstracting a leather soccer ball to the more general idea of a ball selects only the information on general ball attributes and behavior, eliminating the other characteristics of that particular ball. In a type–token distinction, a type (e.g., a 'ball') is more abstract than its tokens (e.g., 'that leather soccer ball').
Abstraction in its secondary use is a material process, discussed in the themes below.
^ a b Suzanne K. Langer (1953), Feeling and Form: a theory of art developed from Philosophy in a New Key p. 90: "Sculptural form is a powerful abstraction from actual objects and the three-dimensional space which we construe ... through touch and sight."
^ Alfred Sohn-Rethel, Intellectual and manual labour: A critique of epistemology, Humanities Press, 1977""
""Verification by model checking "",,,,""Game semantics (German: dialogische Logik, translated as dialogical logic) is an approach to formal semantics that grounds the concepts of truth or validity on game-theoretic concepts, such as the existence of a winning strategy for a player, somewhat resembling Socratic dialogues or medieval theory of Obligationes.""
""Type theory "",,,,""In mathematics, logic, and computer science, a type theory is any of a class of formal systems, some of which can serve as alternatives to set theory as a foundation for all mathematics. In type theory, every "term" has a "type" and operations are restricted to terms of a certain type.
Type theory is closely related to (and in some cases overlaps with) type systems, which are a programming language feature used to reduce bugs. The types of type theory were created to avoid paradoxes in a variety of formal logics and rewrite systems and sometimes "type theory" is used to refer to this broader application.
Two well-known type theories that can serve as mathematical foundations are Alonzo Church's typed λ-calculus and Per Martin-Löf's intuitionistic type theory.

""
""Hoare logic "",,,,""Hoare logic (also known as Floyd–Hoare logic or Hoare rules) is a formal system with a set of logical rules for reasoning rigorously about the correctness of computer programs. It was proposed in 1969 by the British computer scientist and logician Tony Hoare, and subsequently refined by Hoare and other researchers. The original ideas were seeded by the work of Robert W. Floyd, who had published a similar system for flowcharts.
^ Hoare, C. A. R. (October 1969). "An axiomatic basis for computer programming" (PDF). Communications of the ACM 12 (10): 576–580. doi:10.1145/363235.363259. 
^ R. W. Floyd. "Assigning meanings to programs." Proceedings of the American Mathematical Society Symposia on Applied Mathematics. Vol. 19, pp. 19–31. 1967.""
""Separation logic "",,,,""In computer science, separation logic is an extension of Hoare logic, a way of reasoning about programs. It was developed by John C. Reynolds, Peter O'Hearn, Samin Ishtiaq and Hongseok Yang, drawing upon early work by Rod Burstall. The assertion language of separation logic is a special case of the logic of bunched implications (BI).""
""Shortest paths "",,,,""In graph theory, the shortest path problem is the problem of finding a path between two vertices (or nodes) in a graph such that the sum of the weights of its constituent edges is minimized.
The problem of finding the shortest path between two intersections on a road map (the graph's vertices correspond to intersections and the edges correspond to road segments, each weighted by the length of its road segment) may be modeled by a special case of the shortest path problem in graphs.""
""Dynamic graph algorithms "",,,,""Jeff Westbrook is a TV writer best known for his work on The Simpsons and Futurama, for which he is a three-time winner of the WGA Award.
Prior to becoming a TV writer, Westbrook was a successful algorithms researcher. After majoring in physics and history of science at Harvard University, he studied computer science with Robert Tarjan at Princeton University, receiving his Ph.D. in 1989 with a thesis entitled Algorithms and Data Structures for Dynamic Graph Algorithms. He then took a faculty position at Yale University, later becoming a researcher for AT&T Laboratories before leaving research for Hollywood. Westbrook's Erdős number is three due to his research collaborations with Tarjan and others. His Bacon number is also three, due to his appearance as an extra in the movie Master and Commander: The Far Side of the World, giving a combined Erdős–Bacon number of six.
^ 
^ 
^ 
^ Jeffery Westbrook at the Mathematics Genealogy Project.
^ See http://bluewaterracing.com/index.php?option=com_content&view=article&id=46:who-created-it&catid=31:general&Itemid=50""
""Scheduling algorithms "",,,,""In computing, scheduling is the method by which work specified by some means is assigned to resources that complete the work. The work may be virtual computation elements such as threads, processes or data flows, which are in turn scheduled onto hardware resources such as processors, network links or expansion cards.
A scheduler is what carries out the scheduling activity. Schedulers are often implemented so they keep all computer resources busy (as in load balancing), allow multiple users to share system resources effectively, or to achieve a target quality of service. Scheduling is fundamental to computation itself, and an intrinsic part of the execution model of a computer system; the concept of scheduling makes it possible to have computer multitasking with a single central processing unit (CPU).
A scheduler may aim at one of many goals, for example, maximizing throughput (the total amount of work completed per time unit), minimizing response time (time from work becoming enabled until the first point it begins execution on resources), or minimizing latency (the time between work becoming enabled and its subsequent completion), maximizing fairness (equal CPU time to each process, or more generally appropriate times according to the priority and workload of each process). In practice, these goals often conflict (e.g. throughput versus latency), thus a scheduler will implement a suitable compromise. Preference is given to any one of the concerns mentioned above, depending upon the user's needs and objectives.
In real-time environments, such as embedded systems for automatic control in industry (for example robotics), the scheduler also must ensure that processes can meet deadlines; this is crucial for keeping the system stable. Scheduled tasks can also be distributed to remote devices across a network and managed through an administrative back end.""
""Routing and network design problems "",,,,""Routing is the process of selecting best paths in a network. In the past, the term routing also meant forwarding network traffic among networks. However, that latter function is better described as forwarding. Routing is performed for many kinds of networks, including the telephone network (circuit switching), electronic data networks (such as the Internet), and transportation networks. This article is concerned primarily with routing in electronic data networks using packet switching technology.
In packet switching networks, routing directs packet forwarding (the transit of logically addressed network packets from their source toward their ultimate destination) through intermediate nodes. Intermediate nodes are typically network hardware devices such as routers, bridges, gateways, firewalls, or switches. General-purpose computers can also forward packets and perform routing, though they are not specialized hardware and may suffer from limited performance. The routing process usually directs forwarding on the basis of routing tables, which maintain a record of the routes to various network destinations. Thus, constructing routing tables, which are held in the router's memory, is very important for efficient routing. Most routing algorithms use only one network path at a time. Multipath routing techniques enable the use of multiple alternative paths.
In case of overlapping/equal routes, algorithms consider the following elements to decide which routes to install into the routing table (sorted by priority):
Prefix-Length: where longer subnet masks are preferred (independent of whether it is within a routing protocol or over different routing protocol)
Metric: where a lower metric/cost is preferred (only valid within one and the same routing protocol)
Administrative distance: where a route learned from a more reliable routing protocol is preferred (only valid between different routing protocols)
Routing, in a more narrow sense of the term, is often contrasted with bridging in its assumption that network addresses are structured and that similar addresses imply proximity within the network. Structured addresses allow a single routing table entry to represent the route to a group of devices. In large networks, structured addressing (routing, in the narrow sense) outperforms unstructured addressing (bridging). Routing has become the dominant form of addressing on the Internet. Bridging is still widely used within localized environments.""
""Facility location and clustering "",,,,""In computer science, data stream clustering is defined as the clustering of data that arrive continuously such as telephone records, multimedia data, financial transactions etc. Data stream clustering is usually studied as a streaming algorithm and the objective is, given a sequence of points, to construct a good clustering of the stream, using a small amount of memory and time.""
""Rounding techniques "",,,,""In mathematics, the linear programming relaxation of a 0-1 integer program is the problem that arises by replacing the constraint that each variable must be 0 or 1 by a weaker constraint, that each variable belong to the interval [0,1].
That is, for each constraint of the form

of the original integer program, one instead uses a pair of linear constraints

The resulting relaxation is a linear program, hence the name. This relaxation technique transforms an NP-hard optimization problem (integer programming) into a related problem that is solvable in polynomial time (linear programming); the solution to the relaxed linear program can be used to gain information about the solution to the original integer program.""
""Stochastic approximation "",,,,""Stochastic approximation methods are a family of iterative stochastic optimization algorithms that attempt to find zeroes or extrema of functions which cannot be computed directly, but only estimated via noisy observations.
Mathematically, this refers to solving:

where the objective is to find the parameter , which minimizes  for some unknown random variable, . Denoting  as the dimension of the parameter , we can assume that while the domain  is known, the objective function, , cannot be computed exactly, but instead approximated via simulation. This can be intuitively explained as follows.  is the original function we want to minimize. However, due to noise,  can not be evaluated exactly. This situation is modeled by the function , where  represents the noise and is a random variable. Since  is a random variable, so is the value of . The objective is then to minimize , but through evaluating . A reasonable way to do this is to minimize the expectation of , i.e., .
The first, and prototypical, algorithms of this kind are the Robbins-Monro and Kiefer-Wolfowitz algorithms introduced respectively in 1951 and 1952.""
""Network optimization "",,,,""In optimization, 3-opt is a simple local search algorithm for solving the travelling salesman problem and related network optimization problems.
3-opt analysis involves deleting 3 connections (or edges) in a network (or tour), reconnecting the network in all other possible ways, and then evaluating each reconnection method to find the optimum one. This process is then repeated for a different set of 3 connections.""
""Linear programming "",,,,""Linear programming (LP; also called linear optimization) is a method to achieve the best outcome (such as maximum profit or lowest cost) in a mathematical model whose requirements are represented by linear relationships. Linear programming is a special case of mathematical programming (mathematical optimization).
More formally, linear programming is a technique for the optimization of a linear objective function, subject to linear equality and linear inequality constraints. Its feasible region is a convex polytope, which is a set defined as the intersection of finitely many half spaces, each of which is defined by a linear inequality. Its objective function is a real-valued affine (linear) function defined on this polyhedron. A linear programming algorithm finds a point in the polyhedron where this function has the smallest (or largest) value if such a point exists.
Linear programs are problems that can be expressed in canonical form as

where x represents the vector of variables (to be determined), c and b are vectors of (known) coefficients, A is a (known) matrix of coefficients, and  is the matrix transpose. The expression to be maximized or minimized is called the objective function (cTx in this case). The inequalities Ax ≤ b and x ≥ 0 are the constraints which specify a convex polytope over which the objective function is to be optimized. In this context, two vectors are comparable when they have the same dimensions. If every entry in the first is less-than or equal-to the corresponding entry in the second then we can say the first vector is less-than or equal-to the second vector.
Linear programming can be applied to various fields of study. It is widely used in business and economics, and is also utilized for some engineering problems. Industries that use linear programming models include transportation, energy, telecommunications, and manufacturing. It has proved useful in modeling diverse types of problems in planning, routing, scheduling, assignment, and design.""
""Semidefinite programming "",,,,""Semidefinite programming (SDP) is a subfield of convex optimization concerned with the optimization of a linear objective function (an objective function is a user-specified function that the user wants to minimize or maximize) over the intersection of the cone of positive semidefinite matrices with an affine space, i.e., a spectrahedron.
Semidefinite programming is a relatively new field of optimization which is of growing interest for several reasons. Many practical problems in operations research and combinatorial optimization can be modeled or approximated as semidefinite programming problems. In automatic control theory, SDPs are used in the context of linear matrix inequalities. SDPs are in fact a special case of cone programming and can be efficiently solved by interior point methods. All linear programs can be expressed as SDPs, and via hierarchies of SDPs the solutions of polynomial optimization problems can be approximated. Semidefinite programming has been used in the optimization of complex systems. In recent years, some quantum query complexity problems have been formulated in term of semidefinite programs.""
""Convex optimization "",,,,""Convex minimization, a subfield of optimization, studies the problem of minimizing convex functions over convex sets. The convexity property can make optimization in some sense "easier" than the general case - for example, any local minimum must be a global minimum.
Given a real vector space  together with a convex, real-valued function

defined on a convex subset  of , the problem is to find any point  in  for which the number  is smallest, i.e., a point  such that
 for all .
The convexity of  makes the powerful tools of convex analysis applicable. In finite-dimensional normed spaces, the Hahn–Banach theorem and the existence of subgradients lead to a particularly satisfying theory of necessary and sufficient conditions for optimality, a duality theory generalizing that for linear programming, and effective computational methods.
Convex minimization has applications in a wide range of disciplines, such as automatic control systems, estimation and signal processing, communications and networks, electronic circuit design, data analysis and modeling, statistics (optimal design), and finance. With recent improvements in computing and in optimization theory, convex minimization is nearly as straightforward as linear programming. Many optimization problems can be reformulated as convex minimization problems. For example, the problem of maximizing a concave function f can be re-formulated equivalently as a problem of minimizing the function -f, which is convex.""
""Quasiconvex programming and unimodality "",,,,""In mathematics, a quasiconvex function is a real-valued function defined on an interval or on a convex subset of a real vector space such that the inverse image of any set of the form  is a convex set. Informally, along any stretch of the curve the highest point is one of the endpoints. The negative of a quasiconvex function is said to be quasiconcave.
All convex functions are also quasiconvex, but not all quasiconvex functions are convex, so quasiconvexity is a generalization of convexity. Quasiconvexity and quasiconcavity extend to functions with multiple arguments the notion of unimodality of functions with a single real argument.""
""Stochastic control and optimization "",,,,""In queueing theory, a discipline within the mathematical theory of probability, mean value analysis (MVA) is a recursive technique for computing expected queue lengths, waiting time at queueing nodes and throughput in equilibrium for a closed separable system of queues. The first approximate techniques were published independently by Schweitzer and Bard, followed later by an exact version by Lavenberg and Reiser published in 1980.
It is based on the arrival theorem, which states that when one customer in an M-customer closed system arrives at a service facility he/she observes the rest of the system to be in the equilibrium state for a system with M − 1 customers.""
""Quadratic programming "",,,,""Quadratic programming (QP) is a special type of mathematical optimization problem. It is the problem of optimizing (minimizing or maximizing) a quadratic function of several variables subject to linear constraints on these variables.""
""Nonconvex optimization "",,,,""BARON is a computational system for solving nonconvex optimization problems to global optimality. Purely continuous, purely integer, and mixed-integer nonlinear problems can be solved with the software. BARON is available under the AIMMS and GAMS modeling languages on a variety of platforms. The GAMS/BARON solver is also available on the NEOS Server.
The development of the BARON algorithms and software has been recognized by the 2004 INFORMS Computing Society Prize and the 2006 Beale-Orchard-Hays Prize for excellence in computational mathematical programming from the Mathematical Optimization Society.""
""Submodular optimization and polymatroids "",,,,""In mathematics, a submodular set function (also known as a submodular function) is a set function whose value, informally, has the property that the difference in the incremental value of the function, that a single element makes when added to an input set, decreases as the size of the input set increases. Submodular functions have a natural diminishing returns property which makes them suitable for many applications, including approximation algorithms, game theory (as functions modeling user preferences) and electrical networks. Recently, submodular functions have also found immense utility in several real world problems in machine learning and artificial intelligence, including automatic summarization, multi-document summarization, feature selection, active learning, sensor placement, image collection summarization and many other domains.

""
""Integer programming "",,,,""An integer programming problem is a mathematical optimization or feasibility program in which some or all of the variables are restricted to be integers. In many settings the term refers to integer linear programming (ILP), in which the objective function and the constraints (other than the integer constraints) are linear.
Integer programming is NP-hard. A special case, 0-1 integer linear programming, in which unknowns are binary, and only the restrictions must be satisfied, is one of Karp's 21 NP-complete problems.""
""Data compression "",,,,""In signal processing, data compression, source coding, or bit-rate reduction involves encoding information using fewer bits than the original representation. Compression can be either lossy or lossless. Lossless compression reduces bits by identifying and eliminating statistical redundancy. No information is lost in lossless compression. Lossy compression reduces bits by identifying unnecessary information and removing it. The process of reducing the size of a data file is referred to as data compression. In the context of data transmission, it is called source coding (encoding done at the source of the data before it is stored or transmitted) in opposition to channel coding.
Compression is useful because it helps reduce resource usage, such as data storage space or transmission capacity. Because compressed data must be decompressed to use, this extra processing imposes computational or other costs through decompression; this situation is far from being a free lunch. Data compression is subject to a space–time complexity trade-off. For instance, a compression scheme for video may require expensive hardware for the video to be decompressed fast enough to be viewed as it is being decompressed, and the option to decompress the video in full before watching it may be inconvenient or require additional storage. The design of data compression schemes involves trade-offs among various factors, including the degree of compression, the amount of distortion introduced (when using lossy data compression), and the computational resources required to compress and decompress the data.""
""Pattern matching "",,,,""In computer science, pattern matching is the act of checking a given sequence of tokens for the presence of the constituents of some pattern. In contrast to pattern recognition, the match usually has to be exact. The patterns generally have the form of either sequences or tree structures. Uses of pattern matching include outputting the locations (if any) of a pattern within a token sequence, to output some component of the matched pattern, and to substitute the matching pattern with some other token sequence (i.e., search and replace).
Sequence patterns (e.g., a text string) are often described using regular expressions and matched using techniques such as backtracking.
Tree patterns are used in some programming languages as a general tool to process data based on its structure, e.g., Haskell, ML, Scala and the symbolic mathematics language Mathematica have special syntax for expressing tree patterns and a language construct for conditional execution and value retrieval based on it. For simplicity and efficiency reasons, these tree patterns lack some features that are available in regular expressions.
Often it is possible to give alternative patterns that are tried one by one, which yields a powerful conditional programming construct. Pattern matching sometimes include support for guards.
Term rewriting and graph rewriting languages rely on pattern matching for the fundamental way a program evaluates into a result.""
""Cell probe models and lower bounds "",,,,""In computability theory and computational complexity theory, a model of computation is the definition of the set of allowable operations used in computation and their respective costs. It is used for measuring the complexity of an algorithm in execution time and or memory space: by assuming a certain model of computation, it is possible to analyze the computational resources required or to discuss the limitations of algorithms or computers.""
""Scheduling algorithms "",,,,""In computing, scheduling is the method by which work specified by some means is assigned to resources that complete the work. The work may be virtual computation elements such as threads, processes or data flows, which are in turn scheduled onto hardware resources such as processors, network links or expansion cards.
A scheduler is what carries out the scheduling activity. Schedulers are often implemented so they keep all computer resources busy (as in load balancing), allow multiple users to share system resources effectively, or to achieve a target quality of service. Scheduling is fundamental to computation itself, and an intrinsic part of the execution model of a computer system; the concept of scheduling makes it possible to have computer multitasking with a single central processing unit (CPU).
A scheduler may aim at one of many goals, for example, maximizing throughput (the total amount of work completed per time unit), minimizing response time (time from work becoming enabled until the first point it begins execution on resources), or minimizing latency (the time between work becoming enabled and its subsequent completion), maximizing fairness (equal CPU time to each process, or more generally appropriate times according to the priority and workload of each process). In practice, these goals often conflict (e.g. throughput versus latency), thus a scheduler will implement a suitable compromise. Preference is given to any one of the concerns mentioned above, depending upon the user's needs and objectives.
In real-time environments, such as embedded systems for automatic control in industry (for example robotics), the scheduler also must ensure that processes can meet deadlines; this is crucial for keeping the system stable. Scheduled tasks can also be distributed to remote devices across a network and managed through an administrative back end.""
""Caching and paging algorithms "",,,,""In computing, virtual memory is a memory management technique that is implemented using both hardware and software. It maps memory addresses used by a program, called virtual addresses, into physical addresses in computer memory. Main storage as seen by a process or task appears as a contiguous address space or collection of contiguous segments. The operating system manages virtual address spaces and the assignment of real memory to virtual memory. Address translation hardware in the CPU, often referred to as a memory management unit or MMU, automatically translates virtual addresses to physical addresses. Software within the operating system may extend these capabilities to provide a virtual address space that can exceed the capacity of real memory and thus reference more memory than is physically present in the computer.
The primary benefits of virtual memory include freeing applications from having to manage a shared memory space, increased security due to memory isolation, and being able to conceptually use more memory than might be physically available, using the technique of paging.""
""Adversary models "",,,,""In computer science, an online algorithm measures its competitiveness against different adversary models. For deterministic algorithms, the adversary is the same as the adaptive offline adversary. For randomized online algorithms competitiveness can depend upon the adversary model used.""
""Fixed parameter tractability "",,,,""In computer science, parameterized complexity is a branch of computational complexity theory that focuses on classifying computational problems according to their inherent difficulty with respect to multiple parameters of the input or output. The complexity of a problem is then measured as a function in those parameters. This allows the classification of NP-hard problems on a finer scale than in the classical setting, where the complexity of a problem is only measured by the number of bits in the input. The first systematic work on parameterized complexity was done by Downey & Fellows (1999).
Under the assumption that P ≠ NP, there exist many natural problems that require superpolynomial running time when complexity is measured in terms of the input size only, but that are computable in a time that is polynomial in the input size and exponential or worse in a parameter . Hence, if  is fixed at a small value and the growth of the function over  is relatively small then such problems can still be considered "tractable" despite their traditional classification as "intractable".
The existence of efficient, exact, and deterministic solving algorithms for NP-complete, or otherwise NP-hard, problems is considered unlikely, if input parameters are not fixed; all known solving algorithms for these problems require time that is exponential (or at least superpolynomial) in the total size of the input. However, some problems can be solved by algorithms that are exponential only in the size of a fixed parameter while polynomial in the size of the input. Such an algorithm is called a fixed-parameter tractable (fpt-)algorithm, because the problem can be solved efficiently for small values of the fixed parameter.
Problems in which some parameter  is fixed are called parameterized problems. A parameterized problem that allows for such an fpt-algorithm is said to be a fixed-parameter tractable problem and belongs to the class , and the early name of the theory of parameterized complexity was fixed-parameter tractability.
Many problems have the following form: given an object  and a nonnegative integer , does  have some property that depends on ? For instance, for the vertex cover problem, the parameter can be the number of vertices in the cover. In many applications, for example when modelling error correction, one can assume the parameter to be "small" compared to the total input size. Then it is interesting to see whether we can find an algorithm which is exponential only in , and not in the input size.
In this way, parameterized complexity can be seen as two-dimensional complexity theory. This concept is formalized as follows:
A parameterized problem is a language , where  is a finite alphabet. The second component is called the parameter of the problem.
A parameterized problem  is fixed-parameter tractable if the question “?” can be decided in running time , where  is an arbitrary function depending only on . The corresponding complexity class is called FPT.
For example, there is an algorithm which solves the vertex cover problem in  time,  where  is the number of vertices and  is the size of the vertex cover. This means that vertex cover is fixed-parameter tractable with the size of the solution as the parameter.""
""W hierarchy "",,,,""In computer science, parameterized complexity is a branch of computational complexity theory that focuses on classifying computational problems according to their inherent difficulty with respect to multiple parameters of the input or output. The complexity of a problem is then measured as a function in those parameters. This allows the classification of NP-hard problems on a finer scale than in the classical setting, where the complexity of a problem is only measured by the number of bits in the input. The first systematic work on parameterized complexity was done by Downey & Fellows (1999).
Under the assumption that P ≠ NP, there exist many natural problems that require superpolynomial running time when complexity is measured in terms of the input size only, but that are computable in a time that is polynomial in the input size and exponential or worse in a parameter . Hence, if  is fixed at a small value and the growth of the function over  is relatively small then such problems can still be considered "tractable" despite their traditional classification as "intractable".
The existence of efficient, exact, and deterministic solving algorithms for NP-complete, or otherwise NP-hard, problems is considered unlikely, if input parameters are not fixed; all known solving algorithms for these problems require time that is exponential (or at least superpolynomial) in the total size of the input. However, some problems can be solved by algorithms that are exponential only in the size of a fixed parameter while polynomial in the size of the input. Such an algorithm is called a fixed-parameter tractable (fpt-)algorithm, because the problem can be solved efficiently for small values of the fixed parameter.
Problems in which some parameter  is fixed are called parameterized problems. A parameterized problem that allows for such an fpt-algorithm is said to be a fixed-parameter tractable problem and belongs to the class , and the early name of the theory of parameterized complexity was fixed-parameter tractability.
Many problems have the following form: given an object  and a nonnegative integer , does  have some property that depends on ? For instance, for the vertex cover problem, the parameter can be the number of vertices in the cover. In many applications, for example when modelling error correction, one can assume the parameter to be "small" compared to the total input size. Then it is interesting to see whether we can find an algorithm which is exponential only in , and not in the input size.
In this way, parameterized complexity can be seen as two-dimensional complexity theory. This concept is formalized as follows:
A parameterized problem is a language , where  is a finite alphabet. The second component is called the parameter of the problem.
A parameterized problem  is fixed-parameter tractable if the question “?” can be decided in running time , where  is an arbitrary function depending only on . The corresponding complexity class is called FPT.
For example, there is an algorithm which solves the vertex cover problem in  time,  where  is the number of vertices and  is the size of the vertex cover. This means that vertex cover is fixed-parameter tractable with the size of the solution as the parameter.""
""Sketching and sampling "",,,,""In computer science, streaming algorithms are algorithms for processing data streams in which the input is presented as a sequence of items and can be examined in only a few passes (typically just one). These algorithms have limited memory available to them (much less than the input size) and also limited processing time per item.
These constraints may mean that an algorithm produces an approximate answer based on a summary or "sketch" of the data stream in memory.""
""Random order and robust communication complexity "",,,,""Computational complexity theory is a branch of the theory of computation in theoretical computer science that focuses on classifying computational problems according to their inherent difficulty, and relating those classes to each other. A computational problem is understood to be a task that is in principle amenable to being solved by a computer, which is equivalent to stating that the problem may be solved by mechanical application of mathematical steps, such as an algorithm.
A problem is regarded as inherently difficult if its solution requires significant resources, whatever the algorithm used. The theory formalizes this intuition, by introducing mathematical models of computation to study these problems and quantifying the amount of resources needed to solve them, such as time and storage. Other complexity measures are also used, such as the amount of communication (used in communication complexity), the number of gates in a circuit (used in circuit complexity) and the number of processors (used in parallel computing). One of the roles of computational complexity theory is to determine the practical limits on what computers can and cannot do.
Closely related fields in theoretical computer science are analysis of algorithms and computability theory. A key distinction between analysis of algorithms and computational complexity theory is that the former is devoted to analyzing the amount of resources needed by a particular algorithm to solve a problem, whereas the latter asks a more general question about all possible algorithms that could be used to solve the same problem. More precisely, it tries to classify problems that can or cannot be solved with appropriately restricted resources. In turn, imposing restrictions on the available resources is what distinguishes computational complexity from computability theory: the latter theory asks what kind of problems can, in principle, be solved algorithmically.""
""Nearest neighbor algorithms "",,,,""The nearest neighbour algorithm was one of the first algorithms used to determine a solution to the travelling salesman problem. In it, the salesman starts at a random city and repeatedly visits the nearest city until all have been visited. It quickly yields a short tour, but usually not the optimal one.

""
""Self-organization "",,,,""Self-organization is a process where some form of overall order or coordination arises out of the local interactions between smaller component parts of an initially disordered system. The process of self-organization can be spontaneous, and it is not necessarily controlled by any auxiliary agent outside of the system. It is often triggered by random fluctuations that are amplified by positive feedback. The resulting organization is wholly decentralized or distributed over all the components of the system. As such, the organization is typically robust and able to survive and, even, self-repair substantial damage or perturbations. Chaos theory discusses self-organization in terms of islands of predictability in a sea of chaotic unpredictability. Self-organization occurs in a variety of physical, chemical, biological, robotic, social, and cognitive systems. Examples of its realization can be found in crystallization, thermal convection of fluids, chemical oscillation, animal swarming, and neural networks.
^ Betzler, S. B.; Wisnet, A.; Breitbach, B.; Mitterbauer, C.; Weickert, J.; Schmidt-Mende, L.; Scheu, C. (2014). "Template-free synthesis of novel, highly-ordered 3D hierarchical Nb3O7(OH) superstructures with semiconductive and photoactive properties". Journal of Materials Chemistry A 2 (30): 12005. doi:10.1039/C4TA02202E.""
""Shared memory algorithms "",,,,""Borůvka's algorithm is an algorithm for finding a minimum spanning tree in a graph for which all edge weights are distinct.
It was first published in 1926 by Otakar Borůvka as a method of constructing an efficient electricity network for Moravia. The algorithm was rediscovered by Choquet in 1938; again by Florek, Łukasiewicz, Perkal, Steinhaus, and Zubrzycki in 1951; and again by Sollin  in 1965. Because Sollin was the only computer scientist in this list living in an English speaking country, this algorithm is frequently called Sollin's algorithm, especially in the parallel computing literature.
The algorithm begins by first examining each vertex and adding the cheapest edge from that vertex to another in the graph, without regard to already added edges, and continues joining these groupings in a like manner until a tree spanning all vertices is completed.""
""Vector / streaming algorithms "",,,,""Scalable Vector Graphics (SVG) is an XML-based vector image format for two-dimensional graphics with support for interactivity and animation. The SVG specification is an open standard developed by the World Wide Web Consortium (W3C) since 1999.
SVG images and their behaviors are defined in XML text files. This means that they can be searched, indexed, scripted, and compressed. As XML files, SVG images can be created and edited with any text editor, but are more often created with drawing software.
All major modern web browsers—including Mozilla Firefox, Internet Explorer, Google Chrome, Opera, and Safari—have at least some degree of SVG rendering support.""
""Massively parallel algorithms "",,,,""Computer-supported collaboration (CSC) research focuses on technology that affects groups, organizations, communities and societies, e.g., voice mail and text chat. It grew from cooperative work study of supporting people's work activities and working relationships. As net technology increasingly supported a wide range of recreational and social activities, consumer markets expanded the user base, enabling more and more people to connect online to create what researchers have called a computer supported cooperative work, which includes "all contexts in which technology is used to mediate human activities such as communication, coordination, cooperation, competition, entertainment, games, art, and music" (from CSCW 2004).""
""Self-organization "",,,,""Self-organization is a process where some form of overall order or coordination arises out of the local interactions between smaller component parts of an initially disordered system. The process of self-organization can be spontaneous, and it is not necessarily controlled by any auxiliary agent outside of the system. It is often triggered by random fluctuations that are amplified by positive feedback. The resulting organization is wholly decentralized or distributed over all the components of the system. As such, the organization is typically robust and able to survive and, even, self-repair substantial damage or perturbations. Chaos theory discusses self-organization in terms of islands of predictability in a sea of chaotic unpredictability. Self-organization occurs in a variety of physical, chemical, biological, robotic, social, and cognitive systems. Examples of its realization can be found in crystallization, thermal convection of fluids, chemical oscillation, animal swarming, and neural networks.
^ Betzler, S. B.; Wisnet, A.; Breitbach, B.; Mitterbauer, C.; Weickert, J.; Schmidt-Mende, L.; Scheu, C. (2014). "Template-free synthesis of novel, highly-ordered 3D hierarchical Nb3O7(OH) superstructures with semiconductive and photoactive properties". Journal of Materials Chemistry A 2 (30): 12005. doi:10.1039/C4TA02202E.""
""Backtracking "",,,,""Backtracking is a general algorithm for finding all (or some) solutions to some computational problems, notably constraint satisfaction problems, that incrementally builds candidates to the solutions, and abandons each partial candidate c ("backtracks") as soon as it determines that c cannot possibly be completed to a valid solution.
The classic textbook example of the use of backtracking is the eight queens puzzle, that asks for all arrangements of eight chess queens on a standard chessboard so that no queen attacks any other. In the common backtracking approach, the partial candidates are arrangements of k queens in the first k rows of the board, all in different rows and columns. Any partial solution that contains two mutually attacking queens can be abandoned.
Backtracking can be applied only for problems which admit the concept of a "partial candidate solution" and a relatively quick test of whether it can possibly be completed to a valid solution. It is useless, for example, for locating a given value in an unordered table. When it is applicable, however, backtracking is often much faster than brute force enumeration of all complete candidates, since it can eliminate a large number of candidates with a single test.
Backtracking is an important tool for solving constraint satisfaction problems, such as crosswords, verbal arithmetic, Sudoku, and many other puzzles. It is often the most convenient (if not the most efficient) technique for parsing, for the knapsack problem and other combinatorial optimization problems. It is also the basis of the so-called logic programming languages such as Icon, Planner and Prolog.
Backtracking depends on user-given "black box procedures" that define the problem to be solved, the nature of the partial candidates, and how they are extended into complete candidates. It is therefore a metaheuristic rather than a specific algorithm – although, unlike many other meta-heuristics, it is guaranteed to find all solutions to a finite problem in a bounded amount of time.
The term "backtrack" was coined by American mathematician D. H. Lehmer in the 1950s. The pioneer string-processing language SNOBOL (1962) may have been the first to provide a built-in general backtracking facility.""
""Branch-and-bound "",,,,""Branch and bound (BB or B&B) is an algorithm design paradigm for discrete and combinatorial optimization problems, as well as general real valued problems. A branch-and-bound algorithm consists of a systematic enumeration of candidate solutions by means of state space search: the set of candidate solutions is thought of as forming a rooted tree with the full set at the root. The algorithm explores branches of this tree, which represent subsets of the solution set. Before enumerating the candidate solutions of a branch, the branch is checked against upper and lower estimated bounds on the optimal solution, and is discarded if it cannot produce a better solution than the best one found so far by the algorithm.
The algorithm depends on the efficient estimation of the lower and upper bounds of a region/branch of the search space and approaches exhaustive enumeration as the size (n-dimensional volume) of the region tends to zero.
The method was first proposed by A. H. Land and A. G. Doig in 1960 for discrete programming, and has become the most commonly used tool for solving NP-hard optimization problems. The name "branch and bound" first occurred in the work of Little et al. on the traveling salesman problem.
^ A. H. Land and A. G. Doig (1960). "An automatic method of solving discrete programming problems". Econometrica 28 (3). pp. 497–520. doi:10.2307/1910129. 
^ 
^ 
^ Balas, Egon; Toth, Paolo (1983). Branch and bound methods for the traveling salesman problem (PDF) (Report). Carnegie Mellon University Graduate School of Industrial Administration.""
""Dynamic programming "",,,,""In mathematics, management science, economics, computer science, and bioinformatics, dynamic programming (also known as dynamic optimization) is a method for solving a complex problem by breaking it down into a collection of simpler subproblems, solving each of those subproblems just once, and storing their solutions - ideally, using a memory-based data structure. The next time the same subproblem occurs, instead of recomputing its solution, one simply looks up the previously computed solution, thereby saving computation time at the expense of a (hopefully) modest expenditure in storage space. (Each of the subproblem solutions is indexed in some way, typically based on the values of its input parameters, so as to facilitate its lookup.) The technique of storing solutions to subproblems instead of recomputing them is called "memoization".
Dynamic programming algorithms are often used for optimization. A dynamic programming algorithm will examine the previously solved subproblems and will combine their solutions to give the best solution for the given problem. In comparison, a greedy algorithm treats the solution as some sequence of steps and picks the locally optimal choice at each step. Using a greedy algorithm does not guarantee an optimal solution, because picking locally optimal choices may result in a bad global solution, but it is often faster to calculate. Fortunately, some greedy algorithms (such as Kruskal's or Prim's for minimum spanning trees) are proven to lead to the optimal solution.
For example, in the coin change problem of finding the minimum number of coins of given denominations needed to make a given amount, a dynamic programming algorithm would find an optimal solution for each amount by first finding an optimal solution for each smaller amount and then using these solutions to construct an optimal solution for the larger amount. In contrast, a greedy algorithm might treat the solution as a sequence of coins, starting from the given amount and at each step subtracting the largest possible coin denomination that is less than the current remaining amount. If the coin denominations are 1,4,5,15,20 and the given amount is 23, this greedy algorithm gives a non-optimal solution of 20+1+1+1, while the optimal solution is 15+4+4.
In addition to finding optimal solutions to some problem, dynamic programming can also be used for counting the number of solutions, for example counting the number of ways a certain amount of change can be made from a given collection of coins, or counting the number of optimal solutions to the coin change problem described above.
Sometimes, applying memoization to the naive recursive algorithm (namely the one obtained by a direct translation of the problem into recursive form) already results in a dynamic programming algorithm with asymptotically optimal time complexity, but for optimization problems in general the optimal algorithm might require more sophisticated algorithms. Some of these may be recursive (and hence can be memoized) but parametrized differently from the naive algorithm. For other problems the optimal algorithm may not even be a memorized recursive algorithm in any reasonably natural sense. An example of such a problem is the Egg Dropping puzzle described below.""
""Preconditioning "",,,,""In mathematics, preconditioning is the application of a transformation, called the preconditioner, that conditions a given problem into a form that is more suitable for numerical solving methods. Preconditioning is typically related to reducing a condition number of the problem. The preconditioned problem is then usually solved by an iterative method.""
""Concurrent algorithms "",,,,""In computer science, a concurrent algorithm is one that can be executed concurrently. Most standard computer algorithms are sequential algorithms, and assume that the algorithm is run from start to finish without any other processes executing. These often do not behave correctly when run concurrently, as demonstrated at right, and are often nondeterministic, as the actual sequence of computations is determined by the external scheduler. Concurrency often adds significant complexity to an algorithm, requiring concurrency control such as mutual exclusion to avoid problems such as race conditions.
Many parallel algorithms are run concurrently, particularly distributed algorithms, though these are distinct concepts in general.""
""Pseudorandomness and derandomization "",,,,""Omer Reingold (Hebrew: עומר ריינגולד‎) is a faculty member of the Foundations of Computer Science Group at the Weizmann Institute of Science, Israel. He received the 2005 Grace Murray Hopper Award for his work in finding a deterministic logarithmic-space algorithm for ST-connectivity in undirected graphs. He, along with Avi Wigderson and Salil Vadhan, won the Gödel Prize (2009) for their work on the zig-zag product. He became a fellow of the Association for Computing Machinery in 2014 "For contributions to the study of pseudorandomness, derandomization, and cryptography."
^ REINGOLD, OMER (2008). "Undirected connectivity in log-space". Journal of the ACM (ACM) 55 (4). doi:10.1145/1391289.1391291. Retrieved 9 January 2015. 
^ ACM Names Fellows for Innovations in Computing, ACM, January 8, 2015, retrieved 2015-01-08.""
""Computational geometry "",,,,""Computational geometry is a branch of computer science devoted to the study of algorithms which can be stated in terms of geometry. Some purely geometrical problems arise out of the study of computational geometric algorithms, and such problems are also considered to be part of computational geometry. While modern computational geometry is a recent development, it is one of the oldest fields of computing with history stretching back to antiquity.
Computational complexity is central to computational geometry, with great practical significance if algorithms are used on very large datasets containing tens or hundreds of millions of points. For such sets, the difference between O(n2) and O(n log n) may be the difference between days and seconds of computation.
The main impetus for the development of computational geometry as a discipline was progress in computer graphics and computer-aided design and manufacturing (CAD/CAM), but many problems in computational geometry are classical in nature, and may come from mathematical visualization.
Other important applications of computational geometry include robotics (motion planning and visibility problems), geographic information systems (GIS) (geometrical location and search, route planning), integrated circuit design (IC geometry design and verification), computer-aided engineering (CAE) (mesh generation), computer vision (3D reconstruction).
The main branches of computational geometry are:
Combinatorial computational geometry, also called algorithmic geometry, which deals with geometric objects as discrete entities. A groundlaying book in the subject by Preparata and Shamos dates the first use of the term "computational geometry" in this sense by 1975.
Numerical computational geometry, also called machine geometry, computer-aided geometric design (CAGD), or geometric modeling, which deals primarily with representing real-world objects in forms suitable for computer computations in CAD/CAM systems. This branch may be seen as a further development of descriptive geometry and is often considered a branch of computer graphics or CAD. The term "computational geometry" in this meaning has been in use since 1971.""
""Random walks and Markov chains "",,,,""In probability theory and statistics, a Markov process or Markoff process, named after the Russian mathematician Andrey Markov, is a stochastic process that satisfies the Markov property. A Markov process can be thought of as 'memoryless': loosely speaking, a process satisfies the Markov property if one can make predictions for the future of the process based solely on its present state just as well as one could knowing the process's full history. i.e., conditional on the present state of the system, its future and past are independent.""
""Expander graphs and randomness extractors "",,,,""In combinatorics, an expander graph is a sparse graph that has strong connectivity properties, quantified using vertex, edge or spectral expansion as described below. Expander constructions have spawned research in pure and applied mathematics, with several applications to complexity theory, design of robust computer networks, and the theory of error-correcting codes.""
""Error-correcting codes "",,,,""In telecommunication, information theory, and coding theory, forward error correction (FEC) or channel coding is a technique used for controlling errors in data transmission over unreliable or noisy communication channels. The central idea is the sender encodes the message in a redundant way by using an error-correcting code (ECC). The American mathematician Richard Hamming pioneered this field in the 1940s and invented the first error-correcting code in 1950: the Hamming (7,4) code.
The redundancy allows the receiver to detect a limited number of errors that may occur anywhere in the message, and often to correct these errors without retransmission. FEC gives the receiver the ability to correct errors without needing a reverse channel to request retransmission of data, but at the cost of a fixed, higher forward channel bandwidth. FEC is therefore applied in situations where retransmissions are costly or impossible, such as one-way communication links and when transmitting to multiple receivers in multicast. FEC information is usually added to mass storage devices to enable recovery of corrupted data, and is widely used in modems.
FEC processing in a receiver may be applied to a digital bit stream or in the demodulation of a digitally modulated carrier. For the latter, FEC is an integral part of the initial analog-to-digital conversion in the receiver. The Viterbi decoder implements a soft-decision algorithm to demodulate digital data from an analog signal corrupted by noise. Many FEC coders can also generate a bit-error rate (BER) signal which can be used as feedback to fine-tune the analog receiving electronics.
The noisy-channel coding theorem establishes bounds on the theoretical maximum information transfer rate of a channel with some given noise level. Some advanced FEC systems come very close to the theoretical maximum.
The maximum fractions of errors or of missing bits that can be corrected is determined by the design of the FEC code, so different forward error correcting codes are suitable for different conditions.""
""Random network models "",,,,""Digital philosophy is a direction in philosophy and cosmology advocated by certain mathematicians and theoretical physicists, e.g., Gregory Chaitin, Seth Lloyd, Edward Fredkin, Stephen Wolfram, and Konrad Zuse (see his Calculating Space).""
""Boolean function learning "",,,,""In mathematics and mathematical logic, Boolean algebra is the branch of algebra in which the values of the variables are the truth values true and false, usually denoted 1 and 0 respectively. Instead of elementary algebra where the values of the variables are numbers, and the main operations are addition and multiplication, the main operations of Boolean algebra are the conjunction and, denoted ∧, the disjunction or, denoted ∨, and the negation not, denoted ¬. It is thus a formalism for describing logical relations in the same way that ordinary algebra describes numeric relations.
Boolean algebra was introduced by George Boole in his first book The Mathematical Analysis of Logic (1847), and set forth more fully in his An Investigation of the Laws of Thought (1854). According to Huntington, the term "Boolean algebra" was first suggested by Sheffer in 1913.
Boolean algebra has been fundamental in the development of digital electronics, and is provided for in all modern programming languages. It is also used in set theory and statistics.""
""Unsupervised learning and clustering "",,,,""Unsupervised learning is the machine learning task of inferring a function to describe hidden structure from unlabeled data. Since the examples given to the learner are unlabeled, there is no error or reward signal to evaluate a potential solution. This distinguishes unsupervised learning from supervised learning and reinforcement learning.
Unsupervised learning is closely related to the problem of density estimation in statistics. However unsupervised learning also encompasses many other techniques that seek to summarize and explain key features of the data. Many methods employed in unsupervised learning are based on data mining methods used to preprocess data.
Approaches to unsupervised learning include:
clustering (e.g., k-means, mixture models, hierarchical clustering),
Approaches for learning latent variable models such as
Expectation–maximization algorithm (EM)
Method of moments
Blind signal separation techniques, e.g.,
Principal component analysis,
Independent component analysis,
Non-negative matrix factorization,
Singular value decomposition.

Among neural network models, the self-organizing map (SOM) and adaptive resonance theory (ART) are commonly used unsupervised learning algorithms. The SOM is a topographic organization in which nearby locations in the map represent inputs with similar properties. The ART model allows the number of clusters to vary with problem size and lets the user control the degree of similarity between members of the same clusters by means of a user-defined constant called the vigilance parameter. ART networks are also used for many pattern recognition tasks, such as automatic target recognition and seismic signal processing. The first version of ART was "ART1", developed by Carpenter and Grossberg (1988).""
""Support vector machines "",,,,""In machine learning, support vector machines (SVMs, also support vector networks) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training examples, each marked for belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples into one category or the other, making it a non-probabilistic binary linear classifier. An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall on.
In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.
When data are not labeled, a supervised learning is not possible, and an unsupervised learning is required, that would find natural clustering of the data to groups, and map new data to these formed groups. The clustering algorithm which provides an improvement to the support vector machines is called support vector clustering and is often used in industrial applications either when data is not labeled or when only some data is labeled as a preprocessing for a classification pass.""
""Gaussian processes "",,,,""In probability theory and statistics, a Gaussian process is a statistical distribution where observations occur in a continuous domain, e.g. time or space. In a Gaussian process, every point in some continuous input space is associated with a normally distributed random variable. Moreover, every finite collection of those random variables has a multivariate normal distribution. The distribution of a Gaussian process is the joint distribution of all those (infinitely many) random variables, and as such, it is a distribution over functions with a continuous domain, e.g. time or space.
The concept of Gaussian processes is named after Carl Friedrich Gauss because it is based on the notion of the Gaussian distribution (normal distribution). Gaussian processes can be seen as an infinite-dimensional generalization of multivariate normal distributions.
Gaussian processes are important in statistical modelling because of properties inherited from the normal. For example, if a random process is modeled as a Gaussian process, the distributions of various derived quantities can be obtained explicitly. Such quantities include the average value of the process over a range of times and the error in estimating the average using sample values at a small set of times.""
""Bayesian analysis "",,,,""Bayesian inference is a method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available. Bayesian inference is an important technique in statistics, and especially in mathematical statistics. Bayesian updating is particularly important in the dynamic analysis of a sequence of data. Bayesian inference has found application in a wide range of activities, including science, engineering, philosophy, medicine, sport, and law. In the philosophy of decision theory, Bayesian inference is closely related to subjective probability, often called "Bayesian probability".

""
""Inductive inference "",,,,""Inductive reasoning (as opposed to deductive reasoning or abductive reasoning) is reasoning in which the premises are viewed as supplying strong evidence for the truth of the conclusion. While the conclusion of a deductive argument is certain, the truth of the conclusion of an inductive argument is probable, based upon the evidence given.
Many dictionaries define inductive reasoning as reasoning that derives general principles from specific observations, though some sources disagree with this usage.
The philosophical definition of inductive reasoning is more nuanced than simple progression from particular/individual instances to broader generalizations. Rather, the premises of an inductive logical argument indicate some degree of support (inductive probability) for the conclusion but do not entail it; that is, they suggest truth but do not ensure it. In this manner, there is the possibility of moving from general statements to individual instances (for example, statistical syllogisms, discussed below).""
""Online learning theory "",,,,""Progressive inquiry is a pedagogical model which aims at facilitating the same kind of productive knowledge practices of working with knowledge in education that characterize scientific research communities. It is developed by Kai Hakkarainen and his colleagues in the University of Helsinki as a pedagogical and epistemological framework to support teachers and students in organizing their activities for facilitating expert-like working with knowledge. It emphasizes shared expertise and collaborative work for knowledge building and inquiry by setting up the context, using questions, explanations, theories, and scientific information in the cycle of deepening inquiry. It is often used with computer-supported collaborative learning. The model has evolved from the initial cognitively oriented one toward versions that highlight pragmatic and socio-cultural aspects of inquiry.
The model describes the elements of expert-like knowledge practices in a form of a cyclic inquiry process. It relies on cognitive research on education and is closely associated with the knowledge building approach of Marlene Scardamalia, Carl Bereiter and the Interrogative Model of Inquiry introduced by Jaako Hintikka.
In a progressive inquiry process, the teacher creates a context for inquiry by presenting a multidisciplinary approach to a theoretical or real-life phenomenon, after which the students start defining their own questions and intuitive working theories about it. Students’ questions and explanations are shared and evaluated together, which directs the utilization of authoritative information sources and iterative elaboration of subordinate study questions and more advanced theories, explanations and writings (see the description of the phases of progressive inquiry process). The model is not meant prescriptively, as an ideal path to be followed rigidly; rather it offers conceptual tools to describe, understand and take into account the critical elements in collaborative knowledge-advancing inquiry.""
""Multi-agent learning "",,,,""A multi-agent system (M.A.S.) is a computerized system composed of multiple interacting intelligent agents within an environment. Multi-agent systems can be used to solve problems that are difficult or impossible for an individual agent or a monolithic system to solve. Intelligence may include some methodic, functional, procedural approach, algorithmic search or reinforcement learning. Although there is considerable overlap, a multi-agent system is not always the same as an agent-based model (ABM). The goal of an ABM is to search for explanatory insight into the collective behavior of agents (which don't necessarily need to be "intelligent") obeying simple rules, typically in natural systems, rather than in solving specific practical or engineering problems. The terminology of ABM tends to be used more often in the sciences, and MAS in engineering and technology. Topics where multi-agent systems research may deliver an appropriate approach include online trading, disaster response, and modelling social structures.""
""Models of learning "",,,,""In game theory, a stochastically stable equilibrium is a refinement of the evolutionarily stable state in evolutionary game theory, proposed by Dean Foster and Peyton Young. An evolutionary stable state S is also stochastically stable if under vanishing noise the probability that the population is in the vicinity of state S does not go to zero.
The concept is extensively used in models of learning in populations, where "noise" is used to model experimentation or replacement of unsuccessful players with new players (random mutation). Over time, as the need for experimentation dies down or the population becomes stable, the population will converge towards a subset of evolutionarily stable states. Foster and Young have shown that this subset is the set of states with the highest potential.""
""Structured prediction "",,,,""Structured prediction or structured (output) learning is an umbrella term for supervised machine learning techniques that involve predicting structured objects, rather than scalar discrete or real values.
For example, the problem of translating a natural language sentence into a syntactic representation such as a parse tree can be seen as a structured prediction problem in which the structured output domain is the set of all possible parse trees.
Probabilistic graphical models form a large class of structured prediction models. In particular, Bayesian networks and random fields are popularly used to solve structured prediction problems in a wide variety of application domains including bioinformatics, natural language processing, speech recognition, and computer vision. Other algorithms and models for structured prediction include inductive logic programming, case-based reasoning, structured SVMs, Markov logic networks and constrained conditional models.
Similar to commonly used supervised learning techniques, structured prediction models are typically trained by means of observed data in which the true prediction value is used to adjust model parameters. Due to the complexity of the model and the interrelations of predicted variables the process of prediction using a trained model and of training itself is often computationally infeasible and approximate inference and learning methods are used.
^ Gökhan BakIr, Ben Taskar, Thomas Hofmann, Bernhard Schölkopf, Alex Smola and SVN Vishwanathan (2007), Predicting Structured Data, MIT Press.""
""Sequential decision making "",,,,""John Charles Gittins (born 1938) is a researcher in applied probability and operations research, who is a professor and Emeritus Fellow at Keble College, Oxford University.
He is renowned as the developer of the "Gittins index", which is used for sequential decision-making, especially in research and development in the pharmaceutical industry. He has research interests in applied probability, decision analysis and optimal decisions, including optimal stopping and stochastic optimization.
Gittins was an Assistant Director of Research at the Department of Engineering, Cambridge University from 1967 to 1974. Then he was a lecturer at Oxford University from 1975 to 2005 and head of the Department of Statistics there for 6 years. In 1992, Oxford University awarded him the degree Doctor of Science (D. Sci.). In 1996 he became a Professor of Statistics at Oxford University.
He has been awarded the Rollo Davidson Prize (1982) for early-career probabilists, and the Guy Medal in Silver (1984).
^ Whittle, Peter (1980). "Multi-armed bandits and the Gittins index". Journal of the Royal Statistical Society, Series B 42 (2): 143–149. JSTOR 2984953. 
^ Whittle, Peter (2002). "Applied probability in Great Britain (50th anniversary issue of Operations Research)". Oper. Res. 50 (1): 227–239. doi:10.1287/opre.50.1.227.17792. JSTOR 3088474.""
""Inverse reinforcement learning "",,,,""Reinforcement learning is an area of machine learning inspired by behaviorist psychology, concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. The problem, due to its generality, is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics, and genetic algorithms. In the operations research and control literature, the field where reinforcement learning methods are studied is called approximate dynamic programming. The problem has been studied in the theory of optimal control, though most studies are concerned with the existence of optimal solutions and their characterization, and not with the learning or approximation aspects. In economics and game theory, reinforcement learning may be used to explain how equilibrium may arise under bounded rationality.
In machine learning, the environment is typically formulated as a Markov decision process (MDP) as many reinforcement learning algorithms for this context utilize dynamic programming techniques. The main difference between the classical techniques and reinforcement learning algorithms is that the latter do not need knowledge about the MDP and they target large MDPs where exact methods become infeasible.
Reinforcement learning differs from standard supervised learning in that correct input/output pairs are never presented, nor sub-optimal actions explicitly corrected. Further, there is a focus on on-line performance, which involves finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge). The exploration vs. exploitation trade-off in reinforcement learning has been most thoroughly studied through the multi-armed bandit problem and in finite MDPs.""
""Apprenticeship learning "",,,,""Apprenticeship learning, or apprenticeship via inverse reinforcement learning (AIRP), is a concept in the field of artificial intelligence and machine learning, developed by Pieter Abbeel, Associate Professor in Berkeley's EECS department, and Andrew Ng, Associate Professor in Stanford University's Computer Science Department. It was incepted in 2004. AIRP deals with "Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform"
AIRP concept is closely related to reinforcement learning (RL) that is a sub-area of machine learning concerned with how an agent ought to take actions in an environment so as to maximize some notion of long-term reward. AIRP algorithms are used when the reward function is unknown. The algorithms use observations of the behavior of an expert to teach the agent the optimal actions in certain states of the environment.
AIRP is a special case of the general area of learning from demonstration (LfD), where the goal is to learn a complex task by observing a set of expert traces (demonstrations). AIRP is the intersection of LfD and RL.

""
""Multi-agent reinforcement learning "",,,,""Michael Lederman Littman (born August 30, 1966) is a computer scientist. He works mainly in reinforcement learning, but has done work in machine learning, game theory, computer networking, partially observable Markov decision process solving, computer solving of analogy problems and other areas. He is currently a professor of computer science at Brown University.
Before graduate school, Littman worked with Thomas Landauer at Bellcore and was granted a patent for one of the earliest systems for Cross-language information retrieval. Littman received his Ph.D. in computer science from Brown University in 1996. From 1996 to 1999, he was a professor at Duke University. During his time at Duke, he worked on an automated crossword solver PROVERB, which won an Outstanding Paper Award in 1999 from AAAI and competed in the American Crossword Puzzle Tournament. From 2000 to 2002, he worked at AT&T. From 2002 to 2012, he was a professor at Rutgers University; he chaired the department from 2009-12. In Summer 2012 he returned to Brown University as a full professor. He also appeared in a TurboTax commercial.""
""Adversarial learning "",,,,""Adversarial machine learning is a research field that lies at the intersection of machine learning and computer security. It aims to enable the safe adoption of machine learning techniques in adversarial settings like spam filtering, malware detection and biometric recognition.
The problem arises from the fact that machine learning techniques were originally designed for stationary environments in which the training and test data are assumed to be generated from the same (although possibly unknown) distribution. In the presence of intelligent and adaptive adversaries, however, this working hypothesis is likely to be violated to at least some degree (depending on the adversary). In fact, a malicious adversary can carefully manipulate the input data exploiting specific vulnerabilities of learning algorithms to compromise the whole system security.
Examples include: attacks in spam filtering, where spam messages are obfuscated through misspelling of bad words or insertion of good words; attacks in computer security, e.g., to obfuscate malware code within network packets  or mislead signature detection; attacks in biometric recognition, where fake biometric traits may be exploited to impersonate a legitimate user (biometric spoofing)  or to compromise users’ template galleries that are adaptively updated over time.""
""Active learning "",,,,""Active learning is a model of instruction that focuses the responsibility of learning on learners. It was popularized in the 1990s by its appearance on the Association for the Study of Higher Education (ASHE) report (Bonwell & Eison 1991). In this report they discuss a variety of methodologies for promoting "active learning". They cite literature which indicates that to learn, students must do more than just listen: They must read, write, discuss, or be engaged in solving problems. It relates to the three learning domains referred to as knowledge, skills and attitudes (KSA), and that this taxonomy of learning behaviours can be thought of as "the goals of the learning process" (Bloom, 1956). In particular, students must engage in such higher-order thinking tasks as analysis, synthesis, and evaluation. Active learning engages students in two aspects – doing things and thinking about the things they are doing (Bonwell and Eison, 1991).""
""Semi-supervised learning "",,,,""Semi-supervised learning is a class of supervised learning tasks and techniques that also make use of unlabeled data for training - typically a small amount of labeled data with a large amount of unlabeled data. Semi-supervised learning falls between unsupervised learning (without any labeled training data) and supervised learning (with completely labeled training data). Many machine-learning researchers have found that unlabeled data, when used in conjunction with a small amount of labeled data, can produce considerable improvement in learning accuracy. The acquisition of labeled data for a learning problem often requires a skilled human agent (e.g. to transcribe an audio segment) or a physical experiment (e.g. determining the 3D structure of a protein or determining whether there is oil at a particular location). The cost associated with the labeling process thus may render a fully labeled training set infeasible, whereas acquisition of unlabeled data is relatively inexpensive. In such situations, semi-supervised learning can be of great practical value. Semi-supervised learning is also of theoretical interest in machine learning and as a model for human learning.
As in the supervised learning framework, we are given a set of  independently identically distributed examples  with corresponding labels . Additionally, we are given  unlabeled examples . Semi-supervised learning attempts to make use of this combined information to surpass the classification performance that could be obtained either by discarding the unlabeled data and doing supervised learning or by discarding the labels and doing unsupervised learning.
Semi-supervised learning may refer to either transductive learning or inductive learning. The goal of transductive learning is to infer the correct labels for the given unlabeled data  only. The goal of inductive learning is to infer the correct mapping from  to .
Intuitively, we can think of the learning problem as an exam and labeled data as the few example problems that the teacher solved in class. The teacher also provides a set of unsolved problems. In the transductive setting, these unsolved problems are a take-home exam and you want to do well on them in particular. In the inductive setting, these are practice problems of the sort you will encounter on the in-class exam.
It is unnecessary (and, according to Vapnik's principle, imprudent) to perform transductive learning by way of inferring a classification rule over the entire input space; however, in practice, algorithms formally designed for transduction or induction are often used interchangeably.""
""Markov decision processes "",,,,""Markov decision processes (MDPs) provide a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. MDPs are useful for studying a wide range of optimization problems solved via dynamic programming and reinforcement learning. MDPs were known at least as early as the 1950s (cf. Bellman 1957). A core body of research on Markov decision processes resulted from Ronald A. Howard's book published in 1960, Dynamic Programming and Markov Processes. They are used in a wide area of disciplines, including robotics, automated control, economics, and manufacturing.
More precisely, a Markov Decision Process is a discrete time stochastic control process. At each time step, the process is in some state , and the decision maker may choose any action  that is available in state . The process responds at the next time step by randomly moving into a new state , and giving the decision maker a corresponding reward .
The probability that the process moves into its new state  is influenced by the chosen action. Specifically, it is given by the state transition function . Thus, the next state  depends on the current state  and the decision maker's action . But given  and , it is conditionally independent of all previous states and actions; in other words, the state transitions of an MDP process satisfies the Markov property.
Markov decision processes are an extension of Markov chains; the difference is the addition of actions (allowing choice) and rewards (giving motivation). Conversely, if only one action exists for each state and all rewards are the same (e.g., zero), a Markov decision process reduces to a Markov chain.""
""Social networks "",,,,""A social network is a social structure made up of a set of social actors (such as individuals or organizations), sets of dyadic ties, and other social interactions between actors. The social network perspective provides a set of methods for analyzing the structure of whole social entities as well as a variety of theories explaining the patterns observed in these structures. The study of these structures uses social network analysis to identify local and global patterns, locate influential entities, and examine network dynamics.
Social networks and the analysis of them is an inherently interdisciplinary academic field which emerged from social psychology, sociology, statistics, and graph theory. Georg Simmel authored early structural theories in sociology emphasizing the dynamics of triads and "web of group affiliations". Jacob Moreno is credited with developing the first sociograms in the 1930s to study interpersonal relationships. These approaches were mathematically formalized in the 1950s and theories and methods of social networks became pervasive in the social and behavioral sciences by the 1980s. Social network analysis is now one of the major paradigms in contemporary sociology, and is also employed in a number of other social and formal sciences. Together with other complex networks, it forms part of the nascent field of network science.""
""Algorithmic game theory "",,,,""Algorithmic game theory is an area in the intersection of game theory and algorithm design, whose objective is to design algorithms in strategic environments. Typically, in Algorithmic Game Theory problems, the input to a given algorithm is distributed among many players who have a personal interest in the output. In those situations, the agents might not report the input truthfully because of their own personal interests. On top of the usual requirements in classical algorithm design, say polynomial-time running time, good approximation ratio, ... the designer must also care about incentive constraints. We can see Algorithmic Game Theory from two perspectives:
Analysis: look at the current implemented algorithms and analyze them using Game Theory tools: calculate and prove properties on their Nash equilibria, price of anarchy, best-response dynamics ...
Design: design games that have both good game-theoretical and algorithmic properties. This area is called algorithmic mechanism design
The field was started when Nisan and Ronen in STOC'99  drew the attention of the Theoretical Computer Science community to designing algorithms for selfish (strategic) users. As they claim in the abstract:

We consider algorithmic problems in a distributed setting where the participants cannot be assumed to follow the algorithm but rather their own self-interest. As such participants, termed agents, are capable of manipulating the algorithm, the algorithm designer should ensure in advance that the agents’ interests are best served by behaving correctly.
Following notions from the field of mechanism design, we suggest a framework for studying such algorithms. In this model the algorithmic solution is adorned with payments to the participants and is termed a mechanism. The payments should be carefully chosen as to motivate all participants to act as the algorithm designer wishes. We apply the standard tools of mechanism design to algorithmic problems and in particular to the shortest path problem.""
""Algorithmic mechanism design "",,,,""Algorithmic mechanism design (AMD) lies at the intersection of economic game theory and computer science.
Noam Nisan and Amir Ronen, from the Hebrew University of Jerusalem, first coined "Algorithmic mechanism design" in a research paper published in 1999.
It combines ideas such as utility maximization and mechanism design from economics, rationality and Nash equilibrium from game theory, with such concepts as complexity and algorithm design from discrete mathematics and theoretical computer science. Examples of topics include networking, peering, online auctions and exchanges, online advertising, and search engine's page ranking.
Algorithmic mechanism design differs from classical economic mechanism design in several respects. It typically employs the analytic tools of theoretical computer science, such as worst case analysis and approximation ratios, in contrast to classical mechanism design in economics which often makes distributional assumptions about the agents. It also considers computational constraints to be of central importance: mechanisms that cannot be efficiently implemented in polynomial time are not considered to be viable solutions to a mechanism design problem. This often, for example, rules out the classic economic mechanism, the Vickrey–Clarke–Groves auction.

""
""Market equilibria "",,,,""In economics, economic equilibrium is a state where economic forces such as supply and demand are balanced and in the absence of external influences the (equilibrium) values of economic variables will not change. For example, in the standard text-book model of perfect competition, equilibrium occurs at the point at which quantity demanded and quantity supplied are equal. Market equilibrium in this case refers to a condition where a market price is established through competition such that the amount of goods or services sought by buyers is equal to the amount of goods or services produced by sellers. This price is often called the competitive price or market clearing price and will tend not to change unless demand or supply changes and the quantity is called "competitive quantity" or market clearing quantity.

""
""Network games "",,,,""This is a list of video games featuring various Cartoon Network characters.

""
""Network formation "",,,,""Network formation is an aspect of network science that seeks to model how a network evolves by identifying which factors affect its structure and how these mechanisms operate. Network formation hypotheses are tested by using either a dynamic model with an increasing network size or by making an agent-based model to determine which network structure is the equilibrium in a fixed-size network.""
""Computational advertising theory "",,,,""The following outline is provided as an overview of and topical guide to academic disciplines:
An academic discipline or field of study is a branch of knowledge that is taught and researched as part of higher education. A scholar's discipline is commonly defined and recognized by the university faculties and learned societies to which he or she belongs and the academic journals in which he or she publishes research. However, no formal criteria exist for defining an academic discipline.
Disciplines vary between well-established ones that exist in almost all universities and have well-defined rosters of journals and conferences and nascent ones supported by only a few universities and publications. A discipline may have branches, and these are often called sub-disciplines.
There is no consensus on how some academic disciplines should be classified (e.g., whether anthropology and linguistics are disciplines of social sciences or fields within the humanities). More generally, the proper criteria for organizing knowledge into disciplines are also open to debate.

""
""Data exchange "",,,,""Data exchange is the process of taking data structured under a source schema and actually transforming it into data structured under a target schema, so that the target data is an accurate representation of the source data. Data exchange is similar to the related concept of data integration except that data is actually restructured (with possible loss of content) in data exchange. There may be no way to transform an instance given all of the constraints. Conversely, there may be numerous ways to transform the instance (possibly infinitely many), in which case a "best" choice of solutions has to be identified and justified.""
""Data provenance "",,,,""Provenance (from the French provenir, "to come from"), is the chronology of the ownership, custody or location of a historical object. The term was originally mostly used in relation to works of art, but is now used in similar senses in a wide range of fields, including archaeology, paleontology, archives, manuscripts, printed books, and science and computing. The primary purpose of tracing the provenance of an object or entity is normally to provide contextual and circumstantial evidence for its original production or discovery, by establishing, as far as practicable, its later history, especially the sequences of its formal ownership, custody, and places of storage. The practice has a particular value in helping authenticate objects. Comparative techniques, expert opinions, and the results of scientific tests may also be used to these ends, but establishing provenance is essentially a matter of documentation.
It has been argued that in archaeology (North American archaeology and anthropological archaeology throughout the world), when the US spelling provenience is used it has a related but subtly different sense to provenance. Archaeological researchers use provenience to refer to the three-dimensional location or find spot of an artifact or feature within an archaeological site, whereas provenance covers an object's complete documented history. Ideally, in modern excavations, the provenience or find spot is recorded (even videoed) with great precision, but in older cases only the general site or approximate area may be known, especially when an artifact was found outside a professional excavation and its specific position not recorded. Any given antiquity may therefore have both a provenience (where it was found) and a provenance (where it has been since it was found). In some cases, especially where there is an inscription, the provenance may include a history that predates its burial in the ground, as well as those relating to its history after rediscovery.""
""Data modeling "",,,,""Data modeling in software engineering is the process of creating a data model for an information system by applying formal data modeling techniques.
^""
""Database query languages (principles) "",,,,""Database theory encapsulates a broad range of topics related to the study and research of the theoretical realm of databases and database management systems.
Theoretical aspects of data management include, among other areas, the foundations of query languages, computational complexity and expressive power of queries, finite model theory, database design theory, dependency theory, foundations of concurrency control and database recovery, deductive databases, temporal and spatial databases, real time databases, managing uncertain data and probabilistic databases, and Web data.
Most research work has traditionally been based on the relational model, since this model is usually considered the simplest and most foundational model of interest. Corresponding results for other data models, such as object-oriented or semi-structured models, or, more recently, graph data models and XML, are often derivable from those for the relational model.
A central focus of database theory is on understanding the complexity and power of query languages and their connection to logic. Starting from relational algebra and first-order logic (which are equivalent by Codd's theorem) and the insight that important queries such as graph reachability are not expressible in this language, more powerful language based on logic programming and fixpoint logic such as datalog were studied. Another focus was on the foundations of query optimization and data integration. Here most work studied conjunctive queries, which admit query optimization even under constraints using the chase algorithm.
The main research conferences in the area are the ACM Symposium on Principles of Database Systems (PODS) and the International Conference on Database Theory (ICDT).""
""Database interoperability "",,,,""Distributed Relational Database Architecture (DRDA) is a database interoperability standard from The Open Group.
DRDA describes the architecture for distributed relational databases. It defines the rules for accessing the distributed data, but it does not provide the actual application programming interfaces (APIs) to perform the access. It was first used in DB2 2.3.
DRDA was designed by a work group within IBM in the period 1988 to 1994. The messages, protocols, and structural components of DRDA are defined by the Distributed Data Management Architecture.""
""Database query processing and optimization (theory) "",,,,""A database is an organized collection of data. It is the collection of schemas, tables, queries, reports, views and other objects. The data are typically organized to model aspects of reality in a way that supports processes requiring information, such as modelling the availability of rooms in hotels in a way that supports finding a hotel with vacancies.
A database management system (DBMS) is a computer software application that interacts with the user, other applications, and the database itself to capture and analyze data. A general-purpose DBMS is designed to allow the definition, creation, querying, update, and administration of databases. Well-known DBMSs include MySQL, PostgreSQL, Microsoft SQL Server, Oracle, Sybase, SAP HANA, and IBM DB2. A database is not generally portable across different DBMSs, but different DBMS can interoperate by using standards such as SQL and ODBC or JDBC to allow a single application to work with more than one DBMS. Database management systems are often classified according to the database model that they support; the most popular database systems since the 1980s have all supported the relational model as represented by the SQL language. Sometimes a DBMS is loosely referred to as a 'database'.""
""Data integration "",,,,""Data integration involves combining data residing in different sources and providing users with a unified view of these data. This process becomes significant in a variety of situations, which include both commercial (when two similar companies need to merge their databases) and scientific (combining research results from different bioinformatics repositories, for example) domains. Data integration appears with increasing frequency as the volume and the need to share existing data explodes. It has become the focus of extensive theoretical work, and numerous open problems remain unsolved.""
""Theory of database privacy and security "",,,,""Privacy is the ability of an individual or group to seclude themselves, or information about themselves, and thereby express themselves selectively. The boundaries and content of what is considered private differ among cultures and individuals, but share common themes. When something is private to a person, it usually means that something is inherently special or sensitive to them. The domain of privacy partially overlaps security (confidentiality), which can include the concepts of appropriate use, as well as protection of information. Privacy may also take the form of bodily integrity.
The right not to be subjected to unsanctioned invasion of privacy by the government, corporations or individuals is part of many countries' privacy laws, and in some cases, constitutions. Almost all countries have laws which in some way limit privacy. An example of this would be law concerning taxation, which normally require the sharing of information about personal income or earnings. In some countries individual privacy may conflict with freedom of speech laws and some laws may require public disclosure of information which would be considered private in other countries and cultures.
Privacy may be voluntarily sacrificed, normally in exchange for perceived benefits and very often with specific dangers and losses, although this is a very strategic view of human relationships. Research shows that people are more willing to voluntarily sacrifice privacy if the data gatherer is seen to be transparent as to what information is gathered and how it is used. In the business world, a person may volunteer personal details (often for advertising purposes) in order to gamble on winning a prize. A person may also disclose personal information as part of being an executive for a publicly traded company in the USA pursuant to federal securities law. Personal information which is voluntarily shared but subsequently stolen or misused can lead to identity theft.
The concept of universal individual privacy is a modern construct primarily associated with Western culture, British and North American in particular, and remained virtually unknown in some cultures until recent times. According to some researchers, this concept sets Anglo-American culture apart even from Western European cultures such as French or Italian. Most cultures, however, recognize the ability of individuals to withhold certain parts of their personal information from wider society—a figleaf over the genitals being an ancient example.
The distinction or overlap between secrecy and privacy is ontologically subtle, which is why the word "privacy" is an example of an untranslatable lexeme, and many languages do not have a specific word for "privacy". Such languages either use a complex description to translate the term (such as Russian combining the meaning of уединение—solitude, секретность—secrecy, and частная жизнь—private life) or borrow from English "privacy" (as Indonesian Privasi or Italian la privacy). The distinction hinges on the discreteness of interests of parties (persons or groups), which can have emic variation depending on cultural mores of individualism, collectivism, and the negotiation between individual and group rights. The difference is sometimes expressed humorously as "when I withhold information, it is privacy; when you withhold information, it is secrecy."
The December 15, 1890 Samuel Warren and Louis Brandeis publish the article of the law called "The right to privacy", considered one of the most influential papers in the history of American law.""
""Type structures "",,,,""In cosmology, Fermi balls are hypothetical objects that may have been created in the early history of the universe by spontaneous symmetry breaking. One paper has described them as "charged SLAC-bag type structures".""
""Denotational semantics "",,,,""In computer science, denotational semantics (initially known as mathematical semantics or Scott–Strachey semantics) is an approach of formalizing the meanings of programming languages by constructing mathematical objects (called denotations) that describe the meanings of expressions from the languages. Other approaches to providing formal semantics of programming languages include axiomatic semantics and operational semantics.
Broadly speaking, denotational semantics is concerned with finding mathematical objects called domains that represent what programs do. For example, programs (or program phrases) might be represented by partial functions or by games between the environment and the system.
An important tenet of denotational semantics is that semantics should be compositional: the denotation of a program phrase should be built out of the denotations of its subphrases.""
""Operational semantics "",,,,""Operational semantics are a category of formal programming language semantics in which certain desired properties of a program, such as correctness, safety or security, are verified by constructing proofs from logical statements about its execution and procedures, rather than by attaching mathematical meanings to its terms (denotational semantics). Operational semantics are classified in two categories: structural operational semantics (or small-step semantics) formally describe how the individual steps of a computation take place in a computer-based system. By opposition natural semantics (or big-step semantics) describe how the overall results of the executions are obtained. Other approaches to providing a formal semantics of programming languages include axiomatic semantics and denotational semantics.
The operational semantics for a programming language describes how a valid program is interpreted as sequences of computational steps. These sequences then are the meaning of the program. In the context of functional programs, the final step in a terminating sequence returns the value of the program. (In general there can be many return values for a single program, because the program could be nondeterministic, and even for a deterministic program there can be many computation sequences since the semantics may not specify exactly what sequence of operations arrives at that value.)
The concept of operational semantics was used for the first time in defining the semantics of Algol 68. The following statement is a quote from the revised ALGOL 68 report:

The meaning of a program in the strict language is explained in terms of a hypothetical computer which performs the set of actions which constitute the elaboration of that program. (Algol68, Section 2)

The first use of the term "operational semantics" in its present meaning is attributed to Dana Scott (Plotkin04). What follows is a quote from Scott's seminal paper on formal semantics, in which he mentions the "operational" aspects of semantics.

It is all very well to aim for a more ‘abstract’ and a ‘cleaner’ approach to semantics, but if the plan is to be any good, the operational aspects cannot be completely ignored. (Scott70)

Perhaps the first formal incarnation of operational semantics was the use of the lambda calculus to define the semantics of LISP by [John McCarthy. "Recursive Functions of Symbolic Expressions and Their Computation by Machine, Part I". Retrieved 2006-10-13. ]. Abstract machines in the tradition of the SECD machine are also closely related.""
""Axiomatic semantics "",,,,""Axiomatic semantics is an approach based on mathematical logic to proving the correctness of computer programs. It is closely related to Hoare logic.
Axiomatic semantics define the meaning of a command in a program by describing its effect on assertions about the program state. The assertions are logical statements - predicates with variables, where the variables define the state of the program.""
""Action semantics "",,,,""Action semantics is a framework for the formal specification of semantics of programming languages invented by David Watt and Peter D. Mosses in the 1990s. It is a mixture of denotational, operational and algebraic semantics.
Action Semantics aims to be pragmatic. Action-Semantic Descriptions (ASDs) are designed to scale up to handle realistic programming languages. This is aided by the extensibility and modifiability of ASDs. This helps to ensure that extensions and changes do not require too many changes in the description. This is in contrast to the typical case when extending denotational or operational semantics, which may require reformulation of the entire description.
The Action Semantics framework was originally developed at the University of Aarhus and the University of Glasgow. Groups and individuals around the world have since contributed further to the approach.""
""Categorical semantics "",,,,""Categorical logic is a branch of category theory within mathematics, adjacent to mathematical logic but more notable for its connections to theoretical computer science. In broad terms, categorical logic represents both syntax and semantics by a category, and an interpretation by a functor. The categorical framework provides a rich conceptual background for logical and type-theoretic constructions. The subject has been recognisable in these terms since around 1970.""
""Program specifications "",,,,""A functional specification (also, functional spec, specs, functional specifications document (FSD), functional requirements specification, or Program specification) in systems engineering and software development is the documentation that describes the requested behavior of an engineering system. The documentation typically describes what is needed by the system user as well as requested properties of inputs and outputs (e.g. of the software system). A functional specification is the more technical response to a matching requirements document, e.g. the Product Requirement Document "PRD". Thus it picks up the results of the requirements analysis stage. On more complex systems multiple levels of functional specifications will typically nest to each other, e.g. on the system level, on the module level and on the level of technical details.

""
""Pre- and post-conditions "",,,,""Ubercode is a high level programming language designed by Ubercode Software and released in 2005 for Microsoft Windows. Ubercode is influenced by Eiffel and BASIC. It is commercial software and can be tried out for free for 30 days. Ubercode has the following design goals:
Compilable language - compiled into Windows EXE files.
Automatic memory management - memory is allocated / freed automatically, and the language has no memory management primitives.
Pre and post conditions - these are run-time assertions which are attached to function declarations, as in Eiffel.
High-level data types - resizable arrays, lists and tables may contain arbitrary components.
Integrated file handling - primitives for transparent handling of text, binary, CSV, XML and dBase files.
Ease of use - language structure is relatively simple, making the language accessible to beginners.""
""Program verification "",,,,""In the context of hardware and software systems, formal verification is the act of proving or disproving the correctness of intended algorithms underlying a system with respect to a certain formal specification or property, using formal methods of mathematics.
Formal verification can be helpful in proving the correctness of systems such as: cryptographic protocols, combinational circuits, digital circuits with internal memory, and software expressed as source code.
The verification of these systems is done by providing a formal proof on an abstract mathematical model of the system, the correspondence between the mathematical model and the nature of the system being otherwise known by construction. Examples of mathematical objects often used to model systems are: finite state machines, labelled transition systems, Petri nets, vector addition systems, timed automata, hybrid automata, process algebra, formal semantics of programming languages such as operational semantics, denotational semantics, axiomatic semantics and Hoare logic.""
""Program analysis "",,,,""In computer science, program analysis is the process of automatically analyzing the behavior of computer programs regarding a property such as correctness, robustness, safety and liveness. Program analysis focuses on two major areas: program optimization and program correctness. The first focuses on improving the program’s performance while reducing the resource usage while the latter focuses on ensuring that the program does what It is supposed to do.
Program analysis can be performed without executing the program (static program analysis), during runtime (dynamic program analysis) or in a combination of both.

""
""Parsing "",,,,""Parsing or syntactic analysis is the process of analysing a string of symbols, either in natural language or in computer languages, conforming to the rules of a formal grammar. The term parsing comes from Latin pars (orationis), meaning part (of speech).
The term has slightly different meanings in different branches of linguistics and computer science. Traditional sentence parsing is often performed as a method of understanding the exact meaning of a sentence, sometimes with the aid of devices such as sentence diagrams. It usually emphasizes the importance of grammatical divisions such as subject and predicate.
Within computational linguistics the term is used to refer to the formal analysis by a computer of a sentence or other string of words into its constituents, resulting in a parse tree showing their syntactic relation to each other, which may also contain semantic and other information.
The term is also used in psycholinguistics when describing language comprehension. In this context, parsing refers to the way that human beings analyze a sentence or phrase (in spoken language or text) "in terms of grammatical constituents, identifying the parts of speech, syntactic relations, etc." This term is especially common when discussing what linguistic cues help speakers to interpret garden-path sentences.
Within computer science, the term is used in the analysis of computer languages, referring to the syntactic analysis of the input code into its component parts in order to facilitate the writing of compilers and interpreters.""
""Abstraction "",,,,""Abstraction in its main sense is a conceptual process by which general rules and concepts are derived from the usage and classification of specific examples, literal ("real" or "concrete") signifiers, first principles, or other methods. "An abstraction" is the product of this process—a concept that acts as a super-categorical noun for all subordinate concepts, and connects any related concepts as a group, field, or category.
Conceptual abstractions may be formed by filtering the information content of a concept or an observable phenomenon, selecting only the aspects which are relevant for a particular purpose. For example, abstracting a leather soccer ball to the more general idea of a ball selects only the information on general ball attributes and behavior, eliminating the other characteristics of that particular ball. In a type–token distinction, a type (e.g., a 'ball') is more abstract than its tokens (e.g., 'that leather soccer ball').
Abstraction in its secondary use is a material process, discussed in the themes below.
^ a b Suzanne K. Langer (1953), Feeling and Form: a theory of art developed from Philosophy in a New Key p. 90: "Sculptural form is a powerful abstraction from actual objects and the three-dimensional space which we construe ... through touch and sight."
^ Alfred Sohn-Rethel, Intellectual and manual labour: A critique of epistemology, Humanities Press, 1977""
""Permutations and combinations "",,,,""In combinatorics, the twelvefold way is a name given to a systematic classification of 12 related enumerative problems concerning two finite sets, which include the classical problems of counting permutations, combinations, multisets, and partitions either of a set or of a number. The idea of the classification is credited to Gian-Carlo Rota, and the name was suggested by Joel Spencer.""
""Generating functions "",,,,""In mathematics, a generating function describes an infinite sequence of numbers (an) by treating them like the coefficients of a series expansion. The sum of this infinite series is the generating function. Unlike an ordinary series, this formal series is allowed to diverge, meaning that the generating function is not always a true function and the "variable" is actually an indeterminate. Generating functions were first introduced by Abraham de Moivre in 1730, in order to solve the general linear recurrence problem. One can generalize to formal series in more than one indeterminate, to encode information about arrays of numbers indexed by several natural numbers.
There are various types of generating functions, including ordinary generating functions, exponential generating functions, Lambert series, Bell series, and Dirichlet series; definitions and examples are given below. Every sequence in principle has a generating function of each type (except that Lambert and Dirichlet series require indices to start at 1 rather than 0), but the ease with which they can be handled may differ considerably. The particular generating function, if any, that is most useful in a given context will depend upon the nature of the sequence and the details of the problem being addressed.
Generating functions are often expressed in closed form (rather than as a series), by some expression involving operations defined for formal series. These expressions in terms of the indeterminate x may involve arithmetic operations, differentiation with respect to x and composition with (i.e., substitution into) other generating functions; since these operations are also defined for functions, the result looks like a function of x. Indeed, the closed form expression can often be interpreted as a function that can be evaluated at (sufficiently small) concrete values of x, and which has the formal series as its series expansion; this explains the designation "generating functions". However such interpretation is not required to be possible, because formal series are not required to give a convergent series when a nonzero numeric value is substituted for x. Also, not all expressions that are meaningful as functions of x are meaningful as expressions designating formal series; for example, negative and fractional powers of x are examples of functions that do not have a corresponding formal power series.
Generating functions are not functions in the formal sense of a mapping from a domain to a codomain. Generating functions are sometimes called generating series, in that a series of terms can be said to be the generator of its sequence of term coefficients.""
""Combinatorial optimization "",,,,""In applied mathematics and theoretical computer science, combinatorial optimization is a topic that consists of finding an optimal object from a finite set of objects. In many such problems, exhaustive search is not feasible. It operates on the domain of those optimization problems, in which the set of feasible solutions is discrete or can be reduced to discrete, and in which the goal is to find the best solution. Some common problems involving combinatorial optimization are the traveling salesman problem ("TSP") and the minimum spanning tree problem ("MST").
Combinatorial optimization is a subset of mathematical optimization that is related to operations research, algorithm theory, and computational complexity theory. It has important applications in several fields, including artificial intelligence, machine learning, mathematics, auction theory, and software engineering.
Some research literature considers discrete optimization to consist of integer programming together with combinatorial optimization (which in turn is composed of optimization problems dealing with graph structures) although all of these topics have closely intertwined research literature. It often involves determining the way to efficiently allocate resources used to find solutions to mathematical problems.

""
""Combinatorics on words "",,,,""Combinatorics on words is a fairly new field of mathematics, branching from combinatorics, which focuses on the study of words and formal languages. The subject looks at letters or symbols, and the sequences they form. Combinatorics on words affects various areas of mathematical study, including algebra and computer science. There have been a wide range of contributions to the field. Some of the first work was on square-free words by Thue in the early 1900s. He and colleagues observed patterns within words and tried to explain them. As time went on, combinatorics on words became useful in the study of algorithms and coding. It led to developments in abstract algebra and answering open questions.""
""Enumeration "",,,,""An enumeration is a complete, ordered listing of all the items in a collection. The term is commonly used in mathematics and computer science to refer to a listing of all of the elements of a set. The precise requirements for an enumeration (for example, whether the set must be finite, or whether the list is allowed to contain repetitions) depend on the discipline of study and the context of a given problem.
Some sets can be enumerated by means of a natural ordering (such as 1, 2, 3, 4, ... for the set of positive integers), but in other cases it may be necessary to impose a (perhaps arbitrary) ordering. In some contexts, such as enumerative combinatorics, the term enumeration is used more in the sense of counting – with emphasis on determination of the number of elements that a set contains, rather than the production of an explicit listing of those elements.""
""Trees "",,,,""In botany, a tree is a perennial plant with an elongated stem, or trunk, supporting branches and leaves in most species. In some usages, the definition of a tree may be narrower, including only woody plants with secondary growth, plants that are usable as lumber or plants above a specified height. Trees are not a taxonomic group but include a variety of plant species that have independently evolved a woody trunk and branches as a way to tower above other plants to compete for sunlight. In looser senses, the taller palms, the tree ferns, bananas and bamboos are also trees. Trees tend to be long-lived, some reaching several thousand years old. The tallest known tree, a coast redwood named Hyperion, stands 115.6 m (379 ft) high. Trees have been in existence for 370 million years. It is estimated that there are just over 3 trillion mature trees in the world.
A tree typically has many secondary branches supported clear of the ground by the trunk. This trunk typically contains woody tissue for strength, and vascular tissue to carry materials from one part of the tree to another. For most trees it is surrounded by a layer of bark which serves as a protective barrier. Below the ground, the roots branch and spread out widely; they serve to anchor the tree and extract moisture and nutrients from the soil. Above ground, the branches divide into smaller branches and shoots. The shoots typically bear leaves, which capture light energy and convert it into sugars by photosynthesis, providing the food for the tree's growth and development. Flowers and fruit may also be present, but some trees, such as conifers, instead have pollen cones and seed cones; others, such as tree ferns, produce spores instead.
Trees play a significant role in reducing erosion and moderating the climate. They remove carbon dioxide from the atmosphere and store large quantities of carbon in their tissues. Trees and forests provide a habitat for many species of animals and plants. Tropical rainforests are one of the most biodiverse habitats in the world. Trees provide shade and shelter, timber for construction, fuel for cooking and heating, and fruit for food as well as having many other uses. In parts of the world, forests are shrinking as trees are cleared to increase the amount of land available for agriculture. Because of their longevity and usefulness, trees have always been revered and they play a role in many of the world's mythologies.
^ Crowther, T. W.; Glick, H. B.; Covey, K. R.; Bettigole, C.; Maynard, D. S.; Thomas, S. M.; Smith, J. R.; Hintler, G.; Duguid, M. C.; Amatulli, G.; Tuanmu, M.-N.; Jetz, W.; Salas, C.; Stam, C.; Piotto, D.; Tavani, R.; Green, S.; Bruce, G.; Williams, S. J.; Wiser, S. K.; Huber, M. O.; Hengeveld, G. M.; Nabuurs, G.-J.; Tikhonova, E.; Borchardt, P.; Li, C.-F.; Powrie, L. W.; Fischer, M.; Hemp, A.; et al. (2015). "Mapping tree density at a global scale". Nature 525 (7568): 201. doi:10.1038/nature14967. PMID 26331545. 
^ Greenfieldboyce, Nell (September 2, 2015). "Tree Counter Is Astonished By How Many Trees There Are". National Public Radio.""
""Hypergraphs "",,,,""In mathematics, a hypergraph is a generalization of a graph in which an edge can connect any number of vertices. Formally, a hypergraph  is a pair  where  is a set of elements called nodes or vertices, and  is a set of non-empty subsets of  called hyperedges or edges. Therefore,  is a subset of , where  is the power set of .
While graph edges are pairs of nodes, hyperedges are arbitrary sets of nodes, and can therefore contain an arbitrary number of nodes. However, it is often desirable to study hypergraphs where all hyperedges have the same cardinality; a k-uniform hypergraph is a hypergraph such that all its hyperedges have size k. (In other words, one such hypergraph is a collection of sets, each such set a hyperedge connecting k nodes.) So a 2-uniform hypergraph is a graph, a 3-uniform hypergraph is a collection of unordered triples, and so on.
A hypergraph is also called a set system or a family of sets drawn from the universal set X. The difference between a set system and a hypergraph is in the questions being asked. Hypergraph theory tends to concern questions similar to those of graph theory, such as connectivity and colorability, while the theory of set systems tends to ask non-graph-theoretical questions, such as those of Sperner theory.
There are variant definitions; sometimes edges must not be empty, and sometimes multiple edges, with the same set of nodes, are allowed.
Hypergraphs can be viewed as incidence structures. In particular, there is a bipartite "incidence graph" or "Levi graph" corresponding to every hypergraph, and conversely, most, but not all, bipartite graphs can be regarded as incidence graphs of hypergraphs.
Hypergraphs have many other names. In computational geometry, a hypergraph may sometimes be called a range space and then the hyperedges are called ranges. In cooperative game theory, hypergraphs are called simple games (voting games); this notion is applied to solve problems in social choice theory. In some literature edges are referred to as hyperlinks or connectors.
Special kinds of hypergraphs include, besides k-uniform ones, clutters, where no edge appears as a subset of another edge; and abstract simplicial complexes, which contain all subsets of every edge.
The collection of hypergraphs is a category with hypergraph homomorphisms as morphisms.""
""Random graphs "",,,,""In mathematics, random graph is the general term to refer to probability distributions over graphs. Random graphs may be described simply by a probability distribution, or by a random process which generates them. The theory of random graphs lies at the intersection between graph theory and probability theory. From a mathematical perspective, random graphs are used to answer questions about the properties of typical graphs. Its practical applications are found in all areas in which complex networks need to be modeled – a large number of random graph models are thus known, mirroring the diverse types of complex networks encountered in different areas. In a mathematical context, random graph refers almost exclusively to the Erdős–Rényi random graph model. In other contexts, any graph model may be referred to as a random graph.

""
""Graph coloring "",,,,""In graph theory, graph coloring is a special case of graph labeling; it is an assignment of labels traditionally called "colors" to elements of a graph subject to certain constraints. In its simplest form, it is a way of coloring the vertices of a graph such that no two adjacent vertices share the same color; this is called a vertex coloring. Similarly, an edge coloring assigns a color to each edge so that no two adjacent edges share the same color, and a face coloring of a planar graph assigns a color to each face or region so that no two faces that share a boundary have the same color.
Vertex coloring is the starting point of the subject, and other coloring problems can be transformed into a vertex version. For example, an edge coloring of a graph is just a vertex coloring of its line graph, and a face coloring of a plane graph is just a vertex coloring of its dual. However, non-vertex coloring problems are often stated and studied as is. That is partly for perspective, and partly because some problems are best studied in non-vertex form, as for instance is edge coloring.
The convention of using colors originates from coloring the countries of a map, where each face is literally colored. This was generalized to coloring the faces of a graph embedded in the plane. By planar duality it became coloring the vertices, and in this form it generalizes to all graphs. In mathematical and computer representations, it is typical to use the first few positive or nonnegative integers as the "colors". In general, one can use any finite set as the "color set". The nature of the coloring problem depends on the number of colors but not on what they are.
Graph coloring enjoys many practical applications as well as theoretical challenges. Beside the classical types of problems, different limitations can also be set on the graph, or on the way a color is assigned, or even on the color itself. It has even reached popularity with the general public in the form of the popular number puzzle Sudoku. Graph coloring is still a very active field of research.
Note: Many terms used in this article are defined in Glossary of graph theory.""
""Paths and connectivity problems "",,,,""In computer science and computational complexity theory, st-connectivity or STCON is a decision problem asking, for vertices s and t in a directed graph, if t is reachable from s.
Formally, the decision problem is given by
PATH = {〈D, s, t〉 | D is a directed graph with a path from vertex s to t}.""
""Graph enumeration "",,,,""In combinatorics, an area of mathematics, graph enumeration describes a class of combinatorial enumeration problems in which one must count undirected or directed graphs of certain types, typically as a function of the number of vertices of the graph. These problems may be solved either exactly (as an algebraic enumeration problem) or asymptotically. The pioneers in this area of mathematics were Pólya, Cayley  and Redfield.""
""Extremal graph theory "",,,,""Extremal graph theory is a branch of the mathematical field of graph theory. Extremal graph theory studies extremal (maximal or minimal) graphs which satisfy a certain property. Extremality can be taken with respect to different graph invariants, such as order, size or girth. More abstractly, it studies how global properties of a graph influence local substructures of the graph.""
""Graph algorithms "",,,,""The following is a list of algorithms along with one-line descriptions for each.""
""Approximation algorithms "",,,,""In computer science and operations research, approximation algorithms are algorithms used to find approximate solutions to optimization problems. Approximation algorithms are often associated with NP-hard problems; since it is unlikely that there can ever be efficient polynomial-time exact algorithms solving NP-hard problems, one settles for polynomial-time sub-optimal solutions. Unlike heuristics, which usually only find reasonably good solutions reasonably fast, one wants provable solution quality and provable run-time bounds. Ideally, the approximation is optimal up to a small constant factor (for instance within 5% of the optimal solution). Approximation algorithms are increasingly being used for problems where exact polynomial-time algorithms are known but are too expensive due to the input size. A typical example for an approximation algorithm is the one for vertex cover in graphs: find an uncovered edge and add both endpoints to the vertex cover, until none remain. It is clear that the resulting cover is at most twice as large as the optimal one. This is a constant factor approximation algorithm with a factor of 2.
NP-hard problems vary greatly in their approximability; some, such as the bin packing problem, can be approximated within any factor greater than 1 (such a family of approximation algorithms is often called a polynomial time approximation scheme or PTAS). Others are impossible to approximate within any constant, or even polynomial factor unless P = NP, such as the maximum clique problem.
NP-hard problems can often be expressed as integer programs (IP) and solved exactly in exponential time. Many approximation algorithms emerge from the linear programming relaxation of the integer program.
Not all approximation algorithms are suitable for all practical applications. They often use IP/LP/Semidefinite solvers, complex data structures or sophisticated algorithmic techniques which lead to difficult implementation problems. Also, some approximation algorithms have impractical running times even though they are polynomial time, for example O(n2156) . Yet the study of even very expensive algorithms is not a completely theoretical pursuit as they can yield valuable insights. A classic example is the initial PTAS for Euclidean TSP due to Sanjeev Arora which had prohibitive running time, yet within a year, Arora refined the ideas into a linear time algorithm. Such algorithms are also worthwhile in some applications where the running times and cost can be justified e.g. computational biology, financial engineering, transportation planning, and inventory management. In such scenarios, they must compete with the corresponding direct IP formulations.
Another limitation of the approach is that it applies only to optimization problems and not to "pure" decision problems like satisfiability, although it is often possible to conceive optimization versions of such problems, such as the maximum satisfiability problem (Max SAT).
Inapproximability has been a fruitful area of research in computational complexity theory since the 1990 result of Feige, Goldwasser, Lovász, Safra and Szegedy on the inapproximability of Independent Set. After Arora et al. proved the PCP theorem a year later, it has now been shown that Johnson's 1974 approximation algorithms for Max SAT, Set Cover, Independent Set and Coloring all achieve the optimal approximation ratio, assuming P != NP.
^ Zych, Anna; Bilò, Davide (2011). "New Reoptimization Techniques applied to Steiner Tree Problem". Electronic Notes in Discrete Mathematics 37: 387–392. doi:10.1016/j.endm.2011.05.066. ISSN 1571-0653.""
""Bayesian networks "",,,,""A Bayesian network, Bayes network, belief network, Bayes(ian) model or probabilistic directed acyclic graphical model is a probabilistic graphical model (a type of statistical model) that represents a set of random variables and their conditional dependencies via a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases.
Formally, Bayesian networks are DAGs whose nodes represent random variables in the Bayesian sense: they may be observable quantities, latent variables, unknown parameters or hypotheses. Edges represent conditional dependencies; nodes that are not connected (there is no path from one of the variables to the other in the bayesian network) represent variables that are conditionally independent of each other. Each node is associated with a probability function that takes, as input, a particular set of values for the node's parent variables, and gives (as output) the probability (or probability distribution, if applicable) of the variable represented by the node. For example, if  parent nodes represent  Boolean variables then the probability function could be represented by a table of  entries, one entry for each of the  possible combinations of its parents being true or false. Similar ideas may be applied to undirected, and possibly cyclic, graphs; such are called Markov networks.
Efficient algorithms exist that perform inference and learning in Bayesian networks. Bayesian networks that model sequences of variables (e.g. speech signals or protein sequences) are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.""
""Markov networks "",,,,""In the domain of physics and probability, a Markov random field (often abbreviated as MRF), Markov network or undirected graphical model is a set of random variables having a Markov property described by an undirected graph. In other words, a random field is said to be Markov random field if it satisfies Markov properties.
A Markov network or MRF is similar to a Bayesian network in its representation of dependencies; the differences being that Bayesian networks are directed and acyclic, whereas Markov networks are undirected and may be cyclic. Thus, a Markov network can represent certain dependencies that a Bayesian network cannot (such as cyclic dependencies); on the other hand, it can't represent certain dependencies that a Bayesian network can (such as induced dependencies). The underlying graph of a Markov random field may be finite or infinite.
When the joint probability density of the random variables is strictly positive, it is also referred to as a Gibbs random field, because, according to the Hammersley–Clifford theorem, it can then be represented by a Gibbs measure for an appropriate (locally defined) energy function. The prototypical Markov random field is the Ising model; indeed, the Markov random field was introduced as the general setting for the Ising model. In the domain of artificial intelligence, a Markov random field is used to model various low- to mid-level tasks in image processing and computer vision.""
""Factor graphs "",,,,""A factor graph is a bipartite graph representing the factorization of a function. In probability theory and its applications, factor graphs are used to represent factorization of a probability distribution function, enabling efficient computations, such as the computation of marginal distributions through the sum-product algorithm. One of the important success stories of factor graphs and the sum-product algorithm is the decoding of capacity-approaching error-correcting codes, such as LDPC and turbo codes.
Factor graphs generalize constraint graphs. A factor whose value is either 0 or 1 is called a constraint. A constraint graph is a factor graph where all factors are constraints. The max-product algorithm for factor graphs can be viewed as a generalization of the arc-consistency algorithm for constraint processing.""
""Decision diagrams "",,,,""In computer science, a binary decision diagram (BDD) or branching program is a data structure that is used to represent a Boolean function. On a more abstract level, BDDs can be considered as a compressed representation of sets or relations. Unlike other compressed representations, operations are performed directly on the compressed representation, i.e. without decompression. Other data structures used to represent a Boolean function include negation normal form (NNF), and propositional directed acyclic graph (PDAG).

""
""Causal networks "",,,,""A Bayesian network, Bayes network, belief network, Bayes(ian) model or probabilistic directed acyclic graphical model is a probabilistic graphical model (a type of statistical model) that represents a set of random variables and their conditional dependencies via a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases.
Formally, Bayesian networks are DAGs whose nodes represent random variables in the Bayesian sense: they may be observable quantities, latent variables, unknown parameters or hypotheses. Edges represent conditional dependencies; nodes that are not connected (there is no path from one of the variables to the other in the bayesian network) represent variables that are conditionally independent of each other. Each node is associated with a probability function that takes, as input, a particular set of values for the node's parent variables, and gives (as output) the probability (or probability distribution, if applicable) of the variable represented by the node. For example, if  parent nodes represent  Boolean variables then the probability function could be represented by a table of  entries, one entry for each of the  possible combinations of its parents being true or false. Similar ideas may be applied to undirected, and possibly cyclic, graphs; such are called Markov networks.
Efficient algorithms exist that perform inference and learning in Bayesian networks. Bayesian networks that model sequences of variables (e.g. speech signals or protein sequences) are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.""
""Stochastic differential equations "",,,,""A stochastic differential equation (SDE) is a differential equation in which one or more of the terms is a stochastic process, resulting in a solution which is also a stochastic process. SDEs are used to model various phenomena such as unstable stock prices or physical systems subject to thermal fluctuations. Typically, SDEs contain a variable which represents random white noise that is calculated as the derivative of Brownian motion or the Wiener process. However, it should be mentioned that other types of random behaviour are possible, such as jump processes.""
""Kernel density estimators "",,,,""In statistics, kernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable. Kernel density estimation is a fundamental data smoothing problem where inferences about the population are made, based on a finite data sample. In some fields such as signal processing and econometrics it is also termed the Parzen–Rosenblatt window method, after Emanuel Parzen and Murray Rosenblatt, who are usually credited with independently creating it in its current form.""
""Bayesian nonparametric models "",,,,""In probability theory and statistics, the Dirichlet process (DP) is one of the most popular Bayesian nonparametric models. It was introduced by Thomas Ferguson as a prior over probability distributions.
A Dirichlet process  is completely defined by its parameters:  (the base distribution or base measure) is an arbitrary distribution and  (the concentration parameter) is a positive real number (it is often denoted as ). According to the Bayesian paradigma these parameters should be chosen based on the available prior information on the domain.
The question is: how should we choose the prior parameters  of the DP, in particular the infinite dimensional one , in case of lack of prior information?
To address this issue, the only prior that has been proposed so far is the limiting DP obtained for , which has been introduced under the name of Bayesian bootstrap by Rubin; in fact it can be proven that the Bayesian bootstrap is asymptotically equivalent to the frequentist bootstrap introduced by Bradley Efron. The limiting Dirichlet process  has been criticized on diverse grounds. From an a-priori point of view, the main criticism is that taking  is far from leading to a noninformative prior. Moreover, a-posteriori, it assigns zero probability to any set that does not include the observations.
The imprecise Dirichlet process has been proposed to overcome these issues. The basic idea is to fix  but do not choose any precise base measure .
More precisely, the imprecise Dirichlet process (IDP) is defined as follows:

where  is the set of all probability measures. In other words, the IDP is the set of all Dirichlet processes (with a fixed ) obtained
by letting the base measure  to span the set of all probability measures.
^ Ferguson, Thomas (1973). "Bayesian analysis of some nonparametric problems". Annals of Statistics 1 (2): 209–230. doi:10.1214/aos/1176342360. MR 350949. 
^ a b Rubin D (1981). The Bayesian bootstrap. Ann. Stat. 9 130–134
^ Efron B (1979). Bootstrap methods: Another look at the jackknife. Ann. Stat. 7 1–26
^ Sethuraman, J.; Tiwari, R. C. (1981). "Convergence of Dirichlet measures and the interpretation of their parameter". Defense Technical Information Center. 
^ Benavoli, Alessio; Mangili, Francesca; Ruggeri, Fabrizio; Zaffalon, Marco. "Imprecise Dirichlet Process with application to the hypothesis test on the probability that X< Y". Arxiv. Retrieved 2014.""
""Maximum likelihood estimation "",,,,""In statistics, maximum-likelihood estimation (MLE) is a method of estimating the parameters of a statistical model given data.
The method of maximum likelihood corresponds to many well-known estimation methods in statistics. For example, one may be interested in the heights of adult female penguins, but be unable to measure the height of every single penguin in a population due to cost or time constraints. Assuming that the heights are normally distributed with some unknown mean and variance, the mean and variance can be estimated with MLE while only knowing the heights of some sample of the overall population. MLE would accomplish this by taking the mean and variance as parameters and finding particular parametric values that make the observed results the most probable given the model.
In general, for a fixed set of data and underlying statistical model, the method of maximum likelihood selects the set of values of the model parameters that maximizes the likelihood function. Intuitively, this maximizes the "agreement" of the selected model with the observed data, and for discrete random variables it indeed maximizes the probability of the observed data under the resulting distribution. Maximum-likelihood estimation gives a unified approach to estimation, which is well-defined in the case of the normal distribution and many other problems.""
""Bayesian computation "",,,,""Approximate Bayesian computation (ABC) constitutes a class of computational methods rooted in Bayesian statistics. In all model-based statistical inference, the likelihood function is of central importance, since it expresses the probability of the observed data under a particular statistical model, and thus quantifies the support data lend to particular values of parameters and to choices among different models. For simple models, an analytical formula for the likelihood function can typically be derived. However, for more complex models, an analytical formula might be elusive or the likelihood function might be computationally very costly to evaluate.
ABC methods bypass the evaluation of the likelihood function. In this way, ABC methods widen the realm of models for which statistical inference can be considered. ABC methods are mathematically well-founded, but they inevitably make assumptions and approximations whose impact needs to be carefully assessed. Furthermore, the wider application domain of ABC exacerbates the challenges of parameter estimation and model selection.
ABC has rapidly gained popularity over the last years and in particular for the analysis of complex problems arising in biological sciences, e.g. in population genetics, ecology, epidemiology, and systems biology.""
""Hypothesis testing and confidence interval computation "",,,,""In statistics, a confidence interval (CI) is a type of interval estimate of a population parameter. It is an observed interval (i.e., it is calculated from the observations), in principle different from sample to sample, that frequently includes the value of an unobservable parameter of interest if the experiment is repeated. How frequently the observed interval contains the parameter is determined by the confidence level or confidence coefficient. More specifically, the meaning of the term "confidence level" is that, if CI are constructed across many separate data analyses of replicated (and possibly different) experiments, the proportion of such intervals that contain the true value of the parameter will match the given confidence level. Whereas two-sided confidence limits form a confidence interval, their one-sided counterparts are referred to as lower or upper confidence bounds.
Confidence intervals consist of a range of values (interval) that act as good estimates of the unknown population parameter; however, the interval computed from a particular sample does not necessarily include the true value of the parameter. When we say, "we are 99% confident that the true value of the parameter is in our confidence interval", we express that 99% of the hypothetically observed confidence intervals will hold the true value of the parameter. After any particular sample is taken, the population parameter is either in the interval realized or not; it is not a matter of chance. The desired level of confidence is set by the researcher (not determined by data). If a corresponding hypothesis test is performed, the confidence level is the complement of respective level of significance, i.e. a 95% confidence interval reflects a significance level of 0.05. The confidence interval contains the parameter values that, when tested, should not be rejected with the same sample. Greater levels of variance yield larger confidence intervals, and hence less precise estimates of the parameter. Confidence intervals of difference parameters not containing 0 imply that there is a statistically significant difference between the populations.
In applied practice, confidence intervals are typically stated at the 95% confidence level. However, when presented graphically, confidence intervals can be shown at several confidence levels, for example 90%, 95% and 99%.
Certain factors may affect the confidence interval size including size of sample, level of confidence, and population variability. A larger sample size normally will lead to a better estimate of the population parameter.
Confidence intervals were introduced to statistics by Jerzy Neyman in a paper published in 1937.""
""Quantile regression "",,,,""Quantile regression is a type of regression analysis used in statistics and econometrics. Whereas the method of least squares results in estimates that approximate the conditional mean of the response variable given certain values of the predictor variables, quantile regression aims at estimating either the conditional median or other quantiles of the response variable.""
""Variable elimination "",,,,""Variable elimination (VE) is a simple and general exact inference algorithm in probabilistic graphical models, such as Bayesian networks and Markov random fields. It can be used for inference of maximum a posteriori (MAP) state or estimation of marginal distribution over a subset of variables. The algorithm has exponential time complexity, but could be efficient in practice for the low-treewidth graphs, if the proper elimination order is used.""
""Loopy belief propagation "",,,,""Belief propagation, also known as sum-product message passing, is a message passing algorithm for performing inference on graphical models, such as Bayesian networks and Markov random fields. It calculates the marginal distribution for each unobserved node, conditional on any observed nodes. Belief propagation is commonly used in artificial intelligence and information theory and has demonstrated empirical success in numerous applications including low-density parity-check codes, turbo codes, free energy approximation, and satisfiability.
The algorithm was first proposed by Judea Pearl in 1982, who formulated this algorithm on trees, and was later extended to polytrees. It has since been shown to be a useful approximate algorithm on general graphs.
If X={Xi} is a set of discrete random variables with a joint mass function p, the marginal distribution of a single Xi is simply the summation of p over all other variables:

However, this quickly becomes computationally prohibitive: if there are 100 binary variables, then one needs to sum over 299 ≈ 6.338 × 1029 possible values. By exploiting the polytree structure, belief propagation allows the marginals to be computed much more efficiently.

""
""Variational methods "",,,,""Calculus of variations is a field of mathematical analysis that deals with maximizing or minimizing functionals, which are mappings from a set of functions to the real numbers. Functionals are often expressed as definite integrals involving functions and their derivatives. The interest is in extremal functions that make the functional attain a maximum or minimum value – or stationary functions – those where the rate of change of the functional is zero.
A simple example of such a problem is to find the curve of shortest length connecting two points. If there are no constraints, the solution is obviously a straight line between the points. However, if the curve is constrained to lie on a surface in space, then the solution is less obvious, and possibly many solutions may exist. Such solutions are known as geodesics. A related problem is posed by Fermat's principle: light follows the path of shortest optical length connecting two points, where the optical length depends upon the material of the medium. One corresponding concept in mechanics is the principle of least action.
Many important problems involve functions of several variables. Solutions of boundary value problems for the Laplace equation satisfy the Dirichlet principle. Plateau's problem requires finding a surface of minimal area that spans a given contour in space: a solution can often be found by dipping a frame in a solution of soap suds. Although such experiments are relatively easy to perform, their mathematical interpretation is far from simple: there may be more than one locally minimizing surface, and they may have non-trivial topology.""
""Expectation maximization "",,,,""In statistics, an expectation–maximization (EM) algorithm is an iterative method for finding maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables. The EM iteration alternates between performing an expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step. These parameter-estimates are then used to determine the distribution of the latent variables in the next E step.""
""Gibbs sampling "",,,,""In statistics and in statistical physics, Gibbs sampling or a Gibbs sampler is a Markov chain Monte Carlo (MCMC) algorithm for obtaining a sequence of observations which are approximated from a specified multivariate probability distribution (i.e. from the joint probability distribution of two or more random variables), when direct sampling is difficult. This sequence can be used to approximate the joint distribution (e.g., to generate a histogram of the distribution); to approximate the marginal distribution of one of the variables, or some subset of the variables (for example, the unknown parameters or latent variables); or to compute an integral (such as the expected value of one of the variables). Typically, some of the variables correspond to observations whose values are known, and hence do not need to be sampled.
Gibbs sampling is commonly used as a means of statistical inference, especially Bayesian inference. It is a randomized algorithm (i.e. an algorithm that makes use of random numbers, and hence may produce different results each time it is run), and is an alternative to deterministic algorithms for statistical inference such as variational Bayes or the expectation-maximization algorithm (EM).
As with other MCMC algorithms, Gibbs sampling generates a Markov chain of samples, each of which is correlated with nearby samples. As a result, care must be taken if independent samples are desired (typically by thinning the resulting chain of samples by only taking every nth value, e.g. every 100th value). In addition (again, as in other MCMC algorithms), samples from the beginning of the chain (the burn-in period) may not accurately represent the desired distribution.""
""Metropolis-Hastings algorithm "",,,,""In statistics and in statistical physics, the Metropolis–Hastings algorithm is a Markov chain Monte Carlo (MCMC) method for obtaining a sequence of random samples from a probability distribution for which direct sampling is difficult. This sequence can be used to approximate the distribution (i.e., to generate a histogram), or to compute an integral (such as an expected value). Metropolis–Hastings and other MCMC algorithms are generally used for sampling from multi-dimensional distributions, especially when the number of dimensions is high. For single-dimensional distributions, other methods are usually available (e.g. adaptive rejection sampling) that can directly return independent samples from the distribution, and are free from the problem of auto-correlated samples that is inherent in MCMC methods.""
""Simulated annealing "",,,,""Simulated annealing (SA) is a probabilistic technique for approximating the global optimum of a given function. Specifically, it is a metaheuristic to approximate global optimization in a large search space. It is often used when the search space is discrete (e.g., all tours that visit a given set of cities). For problems where finding the precise global optimum is less important than finding an acceptable local optimum in a fixed amount of time, simulated annealing may be preferable to alternatives such as brute-force search or gradient descent.
Simulated annealing interprets slow cooling as a slow decrease in the probability of accepting worse solutions as it explores the solution space. Accepting worse solutions is a fundamental property of metaheuristics because it allows for a more extensive search for the optimal solution.
The method was independently described by Scott Kirkpatrick, C. Daniel Gelatt and Mario P. Vecchi in 1983, and by Vlado Černý in 1985. The method is an adaptation of the Metropolis–Hastings algorithm, a Monte Carlo method to generate sample states of a thermodynamic system, invented by M.N. Rosenbluth and published by N. Metropolis et al. in 1953.""
""Markov-chain Monte Carlo convergence measures "",,,,""A Markov chain (discrete-time Markov chain or DTMC), named after Andrey Markov, is a random process that undergoes transitions from one state to another on a state space. It must possess a property that is usually characterized as "memorylessness": the probability distribution of the next state depends only on the current state and not on the sequence of events that preceded it. This specific kind of "memorylessness" is called the Markov property. Markov chains have many applications as statistical models of real-world processes.

""
""Sequential Monte Carlo methods "",,,,""Particle filters or Sequential Monte Carlo (SMC) methods are a set of genetic-type particle Monte Carlo methodologies to solve the filtering problem. The term "particle filters" was first coined in 1996 by Del Moral in reference to mean field interacting particle methods used in fluid mechanics since the beginning of the 1960s. The terminology "sequential Monte Carlo" was proposed by Liu and Chen in 1998.
From the statistical and probabilistic point of view, particle filters can be interpreted as mean field particle interpretations of Feynman-Kac probability measures. These particle integration techniques were developed in molecular chemistry and computational physics by Theodore E. Harris and Herman Kahn in 1951, Marshall. N. Rosenbluth and Arianna. W. Rosenbluth in 1955 and more recently by Jack H. Hetherington in 1984. In computational physics, these Feynman-Kac type path particle integration methods are also used in Quantum Monte Carlo, and more specifically Diffusion Monte Carlo methods. Feynman-Kac interacting particle methods are also strongly related to mutation-selection genetic algorithms currently used in evolutionary computing to solve complex optimization problems.
The particle filter methodology is used to solve Hidden Markov Chain (HMM) and nonlinear filtering problems arising in signal processing and Bayesian statistical inference. The filtering problem consists in estimating the internal states in dynamical systems when partial observations are made, and random perturbations are present in the sensors as well as in the dynamical system. The objective is to compute the conditional probability (a.k.a. posterior distributions) of the states of some Markov process, given some noisy and partial observations. With the notable exception of linear-Gaussian signal-observation models (Kalman filter) or wider classes of models (Benes filter) Mireille Chaleyat-Maurel and Dominique Michel proved in 1984 that the sequence of posterior distributions of the random states of the signal given the observations (a.k.a. optimal filter) have no finitely recursive recursion. Various numerical methods based on fixed grid approximations, Markov Chain Monte Carlo techniques (MCMC), conventional linearization, extended Kalman filters, or determining the best linear system (in expect cost-error sense) have never really coped with large scale systems, unstable processes or when the nonlinearities are not sufficiently smooth.
Particle filtering methodology uses a genetic type mutation-selection sampling approach, with a set of particles (also called individuals, or samples) to represent the posterior distribution of some stochastic process given some noisy and/or partial observations. The state-space model can be nonlinear and the initial state and noise distributions can take any form required. Particle filter techniques provide a well-established methodology for generating samples from the required distribution without requiring assumptions about the state-space model or the state distributions. However, these methods do not perform well when applied to very high-dimensional systems.
Particle filters implement the prediction-updating transitions of the filtering equation directly by using a genetic type mutation-selection particle algorithm. The samples from the distribution are represented by a set of particles; each particle has a likelihood weight assigned to it that represents the probability of that particle being sampled from the probability density function. Weight disparity leading to weight collapse is a common issue encountered in these filtering algorithms; however it can be mitigated by including a resampling step before the weights become too uneven. Several adaptive resampling criteria can be used, including the variance of the weights and the relative entropy w.r.t. the uniform distribution. In the resampling step, the particles with negligible weights are replaced by new particles in the proximity of the particles with higher weights.
Particle filters and Feynman-Kac particle methodologies find application in signal and image processing, Bayesian inference, machine learning, risk analysis and rare event sampling, engineering and robotics, artificial intelligence, bioinformatics, phylogenetics, computational science, Economics and mathematical finance, molecular chemistry, computational physics, pharmacokinetic and other fields.""
""Bootstrapping "",,,,""In general parlance, bootstrapping usually refers to a self-starting process that is supposed to proceed without external input. In computer technology the term (usually shortened to booting) usually refers to the process of loading the basic software into the memory of a computer after power-on or general reset, especially the operating system which will then take care of loading other software as needed.
The term appears to have originated in the early 19th century United States (particularly in the phrase "pull oneself over a fence by one's bootstraps"), to mean an absurdly impossible action, an adynaton.
^ World Wide Words: Boot, Michael Quinion
^ "bootstraps--speculation/questions" (Mailing list). 2005-08-28. 
^ "figurative 'bootstraps'" (Mailing list). 2005-08-11.""
""Jackknifing "",,,,""Jackknifing means the folding of an articulated vehicle i.e. one towing a trailer, such that it resembles the acute angle of a folding pocket knife. If a vehicle towing a trailer skids, the trailer can push from behind until it spins around and faces backwards. This may be caused by equipment failure, improper braking, or adverse road conditions such as an icy road surface. In extreme circumstances, a driver may attempt to jackknife the vehicle deliberately in order to halt it following brake failure.""
""Random number generation "",,,,""A random number generator (RNG) is a computational or physical device designed to generate a sequence of numbers or symbols that can not be reasonably predicted better than by a random chance.
Various applications of randomness have led to the development of several different methods for generating random data, of which some have existed since ancient times, including dice, coin flipping, the shuffling of playing cards, the use of yarrow stalks (for divination) in the I Ching, and many other techniques. Because of the mechanical nature of these techniques, generating large numbers of sufficiently random numbers (important in statistics) required a lot of work and/or time. Thus, results would sometimes be collected and distributed as random number tables. Nowadays, after the advent of computational random-number generators, a growing number of government-run lotteries and lottery games have started using RNGs instead of more traditional drawing-methods. RNGs are also used to determine the odds of modern slot machines.
Several computational methods for random number generation exist. Many fall short of the goal of true randomness, although they may meet, with varying success, some of the statistical tests for randomness intended to measure how unpredictable their results are (that is, to what degree their patterns are discernible). However, carefully designed cryptographically secure computationally based methods of generating random numbers also exist, such as those based on the Yarrow algorithm, the Fortuna (PRNG), and others.""
""Queueing theory "",,,,""Queueing theory is the mathematical study of waiting lines, or queues. In queueing theory a model is constructed so that queue lengths and waiting time can be predicted. Queueing theory is generally considered a branch of operations research because the results are often used when making business decisions about the resources needed to provide a service.
Queueing theory has its origins in research by Agner Krarup Erlang when he created models to describe the Copenhagen telephone exchange. The ideas have since seen applications including telecommunication, traffic engineering, computing and the design of factories, shops, offices and hospitals.""
""Contingency table analysis "",,,,""The NAG Numerical Library is a software product developed and sold by The Numerical Algorithms Group. It is a software library of numerical analysis routines, containing more than 1,700 mathematical and statistical algorithms. Areas covered by the library include linear algebra, optimization, quadrature, the solution of ordinary and partial differential equations, regression analysis, and time series analysis.
Users of the NAG Library call its routines from within their applications in order to incorporate its mathematical or statistical functionality and to solve numerical problems - for example, finding the minimum or maximum of a function, fitting a curve or surface to data, or solving a differential equation. The Library is currently available in the form of the NAG C Library, the NAG Fortran Library, and the NAG Library for .NET, and its contents are accessible from several computing environments, including standard languages such as C, C++, Fortran, Visual Basic, Java and C#, as well as packages such as MATLAB, R, LabVIEW, Excel, Origin and Ch. Supported operating systems include the 32 bit and 64 bit versions of Windows, Linux and OS X, as well as Solaris, AIX and HP-UX.

""
""Robust regression "",,,,""In robust statistics, robust regression is a form of regression analysis designed to circumvent some limitations of traditional parametric and non-parametric methods. Regression analysis seeks to find the relationship between one or more independent variables and a dependent variable. Certain widely used methods of regression, such as ordinary least squares, have favourable properties if their underlying assumptions are true, but can give misleading results if those assumptions are not true; thus ordinary least squares is said to be not robust to violations of its assumptions. Robust regression methods are designed to be not overly affected by violations of assumptions by the underlying data-generating process.
In particular, least squares estimates for regression models are highly sensitive to (not robust against) outliers. While there is no precise definition of an outlier, outliers are observations which do not follow the pattern of the other observations. This is not normally a problem if the outlier is simply an extreme observation drawn from the tail of a normal distribution, but if the outlier results from non-normal measurement error or some other violation of standard ordinary least squares assumptions, then it compromises the validity of the regression results if a non-robust regression technique is used.""
""Time series analysis "",,,,""A time series is a sequence of data points made:
1) over a continuous time interval
2) out of successive measurements across that interval
3) using equal spacing between every two consecutive measurements
4) with each time unit within the time interval having at most one data point
Examples of time series are ocean tides, counts of sunspots, and the daily closing value of the Dow Jones Industrial Average.
Non-Examples: The height measurements of a group of people where each height is recorded over a period of time and each person has only one record in the data set.
Panel data is the general class, a multidimensional data set, whereas a time series data set is a one-dimensional panel (as is a cross-sectional dataset). Yet a data set may exhibit characteristics of both panel data and time series data. One way to tell is to ask what makes one data record unique from the other records. If the answer is the time data field, then this is a time series data set candidate. If determining a unique record requires a time data field and an additional identifier which is unrelated to time (student ID, stock symbol, country code), then it is panel data candidate. If the differentiation lies on the non-time identifier, then the data set is a cross-sectional data set candidate.
Time series are very frequently plotted via line charts. Time series are used in statistics, signal processing, pattern recognition, econometrics, mathematical finance, weather forecasting, intelligent transport and trajectory forecasting, earthquake prediction, electroencephalography, control engineering, astronomy, communications engineering, and largely in any domain of applied science and engineering which involves temporal measurements.
Time series analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data. Time series forecasting is the use of a model to predict future values based on previously observed values. While regression analysis is often employed in such a way as to test theories that the current values of one or more independent time series affect the current value of another time series, this type of analysis of time series is not called "time series analysis", which focuses on comparing values of a single time series or multiple dependent time series at different points in time.
Time series data have a natural temporal ordering. This makes time series analysis distinct from cross-sectional studies, in which there is no natural ordering of the observations (e.g. explaining people's wages by reference to their respective education levels, where the individuals' data could be entered in any order). Time series analysis is also distinct from spatial data analysis where the observations typically relate to geographical locations (e.g. accounting for house prices by the location as well as the intrinsic characteristics of the houses). A stochastic model for a time series will generally reflect the fact that observations close together in time will be more closely related than observations further apart. In addition, time series models will often make use of the natural one-way ordering of time so that values for a given period will be expressed as deriving in some way from past values, rather than from future values (see time reversibility.)
Time series analysis can be applied to real-valued, continuous data, discrete numeric data, or discrete symbolic data (i.e. sequences of characters, such as letters and words in the English language).""
""Survival analysis "",,,,""Survival analysis is a branch of statistics for analyzing the expected duration of time until one or more events happen, such as death in biological organisms and failure in mechanical systems. This topic is called reliability theory or reliability analysis in engineering, duration analysis or duration modelling in economics, and event history analysis in sociology. Survival analysis attempts to answer questions such as: what is the proportion of a population which will survive past a certain time? Of those that survive, at what rate will they die or fail? Can multiple causes of death or failure be taken into account? How do particular circumstances or characteristics increase or decrease the probability of survival?
To answer such questions, it is necessary to define "lifetime". In the case of biological survival, death is unambiguous, but for mechanical reliability, failure may not be well-defined, for there may well be mechanical systems in which failure is partial, a matter of degree, or not otherwise localized in time. Even in biological problems, some events (for example, heart attack or other organ failure) may have the same ambiguity. The theory outlined below assumes well-defined events at specific times; other cases may be better treated by models which explicitly account for ambiguous events.
More generally, survival analysis involves the modelling of time to event data; in this context, death or failure is considered an "event" in the survival analysis literature – traditionally only a single event occurs for each subject, after which the organism or mechanism is dead or broken. Recurring event or repeated event models relax that assumption. The study of recurring events is relevant in systems reliability, and in many areas of social sciences and medical research.""
""Renewal theory "",,,,""Renewal theory is the branch of probability theory that generalizes Poisson processes for arbitrary holding times. Applications include calculating the best strategy for replacing worn-out machinery in a factory and comparing the long-term benefits of different insurance policies.""
""Dimensionality reduction "",,,,""In machine learning and statistics, dimensionality reduction or dimension reduction is the process of reducing the number of random variables under consideration, via obtaining a set "uncorrelated" principle variables. It can be divided into feature selection and feature extraction.

""
""Cluster analysis "",,,,""Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters). It is a main task of exploratory data mining, and a common technique for statistical data analysis, used in many fields, including machine learning, pattern recognition, image analysis, information retrieval, bioinformatics and data compression.
Cluster analysis itself is not one specific algorithm, but the general task to be solved. It can be achieved by various algorithms that differ significantly in their notion of what constitutes a cluster and how to efficiently find them. Popular notions of clusters include groups with small distances among the cluster members, dense areas of the data space, intervals or particular statistical distributions. Clustering can therefore be formulated as a multi-objective optimization problem. The appropriate clustering algorithm and parameter settings (including values such as the distance function to use, a density threshold or the number of expected clusters) depend on the individual data set and intended use of the results. Cluster analysis as such is not an automatic task, but an iterative process of knowledge discovery or interactive multi-objective optimization that involves trial and failure. It will often be necessary to modify data preprocessing and model parameters until the result achieves the desired properties.
Besides the term clustering, there are a number of terms with similar meanings, including automatic classification, numerical taxonomy, botryology (from Greek βότρυς "grape") and typological analysis. The subtle differences are often in the usage of the results: while in data mining, the resulting groups are the matter of interest, in automatic classification the resulting discriminative power is of interest. This often leads to misunderstandings between researchers coming from the fields of data mining and machine learning, since they use the same terms and often the same algorithms, but have different goals.
Cluster analysis was originated in anthropology by Driver and Kroeber in 1932 and introduced to psychology by Zubin in 1938 and Robert Tryon in 1939 and famously used by Cattell beginning in 1943 for trait theory classification in personality psychology.""
""Statistical graphics "",,,,""Statistical graphics, also known as graphical techniques, are graphics in the field of statistics used to visualize quantitative data.""
""Exploratory data analysis "",,,,""In statistics, exploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task. Exploratory data analysis was promoted by John Tukey to encourage statisticians to explore the data, and possibly formulate hypotheses that could lead to new data collection and experiments. EDA is different from initial data analysis (IDA), which focuses more narrowly on checking assumptions required for model fitting and hypothesis testing, and handling missing values and making transformations of variables as needed. EDA encompasses IDA.""
""Markov processes "",,,,""In probability theory and statistics, a Markov process or Markoff process, named after the Russian mathematician Andrey Markov, is a stochastic process that satisfies the Markov property. A Markov process can be thought of as 'memoryless': loosely speaking, a process satisfies the Markov property if one can make predictions for the future of the process based solely on its present state just as well as one could knowing the process's full history. i.e., conditional on the present state of the system, its future and past are independent.""
""Nonparametric statistics "",,,,""Nonparametric statistics are statistics not based on parameterized families of probability distributions. They include both descriptive and inferential statistics. The typical parameters are the mean, variance, etc. Unlike parametric statistics, nonparametric statistics make no assumptions about the probability distributions of the variables being assessed. The difference between parametric models and non-parametric models is that the former has a fixed number of parameters, while the latter grows the number of parameters with the amount of training data. Note that the non-parametric model does not have any parameters: parameters are determined by the training data, not the model.

""
""Distribution functions "",,,,""This article describes the distribution function as used in physics. You may be looking for the related mathematical concepts of cumulative distribution function or probability density function.
In molecular kinetic theory in physics, a particle's distribution function is a function of seven variables, , which gives the number of particles per unit volume in single-particle phase space. It is the number of particles per unit volume having approximately the velocity  near the place  and time . The usual normalization of the distribution function is

Here, N is the total number of particles and n is the number density of particles - the number of particles per unit volume, or the density divided by the mass of individual particles.
A distribution function may be specialised with respect to a particular set of dimensions. E.g. take the quantum mechanical six-dimensional phase space,  and multiply by the total space volume, to give the momentum distribution i.e. the number of particles in the momentum phase space having approximately the momentum .
Particle distribution functions are often used in plasma physics to describe wave–particle interactions and velocity-space instabilities. Distribution functions are also used in fluid mechanics, statistical mechanics and nuclear physics.
The basic distribution function uses the Boltzmann constant  and temperature  with the number density to modify the normal distribution:

Related distribution functions may allow bulk fluid flow, in which case the velocity origin is shifted, so that the exponent's numerator is ;  is the bulk velocity of the fluid. Distribution functions may also feature non-isotropic temperatures, in which each term in the exponent is divided by a different temperature.
Plasma theories such as magnetohydrodynamics may assume the particles to be in thermodynamic equilibrium. In this case, the distribution function is Maxwellian. This distribution function allows fluid flow and different temperatures in the directions parallel to, and perpendicular to, the local magnetic field. More complex distribution functions may also be used since plasmas are rarely in thermal equilibrium.
The mathematical analog of a distribution is a measure; the time evolution of a measure on a phase space is the topic of study in dynamical systems.""
""Multivariate statistics "",,,,""Multivariate statistics is a subdivision of statistics encompassing the simultaneous observation and analysis of more than one outcome variable. The application of multivariate statistics is multivariate analysis.
Multivariate statistics concerns understanding the different aims and background of each of the different forms of multivariate analysis, and how they relate to each other. The practical implementation of multivariate statistics to a particular problem may involve several types of univariate and multivariate analyses in order to understand the relationships between variables and their relevance to the actual problem being studied.
In addition, multivariate statistics is concerned with multivariate probability distributions, in terms of both

how these can be used to represent the distributions of observed data;
how they can be used as part of statistical inference, particularly where several different quantities are of interest to the same analysis.

Certain types of problem involving multivariate data, for example simple linear regression and multiple regression, are not usually considered as special cases of multivariate statistics because the analysis is dealt with by considering the (univariate) conditional distribution of a single outcome variable given the other variables.""
""Solvers "",,,,""A soldier is one who fights as part of an organized land-based armed force. A soldier can be an enlisted person, a non-commissioned officer, or junior-commissioned officer, or an officer.""
""Statistical software "",,,,""Statistical software are specialized computer programs for analysis in statistics and econometrics.""
""Mathematical software performance "",,,,""The Ken Kennedy Award, established in 2009 by the Association for Computing Machinery and the IEEE Computer Society in memory of Ken Kennedy, is awarded annually and recognizes substantial contributions to programmability and productivity in computing and substantial community service or mentoring contributions. The award includes a $5,000 honorarium and the award recipient will be announced at the ACM - IEEE Supercomputing Conference.""
""Coding theory "",,,,""Coding theory is the study of the properties of codes and their fitness for a specific application. Codes are used for data compression, cryptography, error-correction and more recently also for network coding. Codes are studied by various scientific disciplines—such as information theory, electrical engineering, mathematics, linguistics, and computer science—for the purpose of designing efficient and reliable data transmission methods. This typically involves the removal of redundancy and the correction (or detection) of errors in the transmitted data.
There are four types of coding:
Data compression (or, source coding)
Error correction (or channel coding)
Cryptographic coding
Line coding
Data compression and error correction may be studied in combination.
Source encoding attempts to compress the data from a source in order to transmit it more efficiently. This practice is found every day on the Internet where the common Zip data compression is used to reduce the network load and make files smaller.
The second, channel encoding, adds extra data bits to make the transmission of data more robust to disturbances present on the transmission channel. The ordinary user may not be aware of many applications using channel coding. A typical music CD uses the Reed-Solomon code to correct for scratches and dust. In this application the transmission channel is the CD itself. Cell phones also use coding techniques to correct for the fading and noise of high frequency radio transmission. Data modems, telephone transmissions, and NASA all employ channel coding techniques to get the bits through, for example the turbo code and LDPC codes.""
""Interpolation "",,,,""In the mathematical field of numerical analysis, interpolation is a method of constructing new data points within the range of a discrete set of known data points.
In engineering and science, one often has a number of data points, obtained by sampling or experimentation, which represent the values of a function for a limited number of values of the independent variable. It is often required to interpolate (i.e. estimate) the value of that function for an intermediate value of the independent variable. This may be achieved by curve fitting or regression analysis.
A different problem which is closely related to interpolation is the approximation of a complicated function by a simple function. Suppose the formula for some given function is known, but too complex to evaluate efficiently. A few known data points from the original function can be used to create an interpolation based on a simpler function. Of course, when a simple function is used to estimate data points from the original, interpolation errors are usually present; however, depending on the problem domain and the interpolation method used, the gain in simplicity may be of greater value than the resultant loss in precision.
In the examples below if we consider x as a topological space and the function f forms a different kind of Banach spaces then the problem is treated as "interpolation of operators". The classical results about interpolation of operators are the Riesz–Thorin theorem and the Marcinkiewicz theorem. There are also many other subsequent results.""
""Numerical differentiation "",,,,""In numerical analysis, numerical differentiation describes algorithms for estimating the derivative of a mathematical function or function subroutine using values of the function and perhaps other knowledge about the function.""
""Interval arithmetic "",,,,""Interval arithmetic, interval mathematics, interval analysis, or interval computation, is a method developed by mathematicians since the 1950s and 1960s, as an approach to putting bounds on rounding errors and measurement errors in mathematical computation and thus developing numerical methods that yield reliable results. Very simply put, it represents each value as a range of possibilities. For example, instead of estimating the height of someone using standard arithmetic as 2.0 metres, using interval arithmetic we might be certain that that person is somewhere between 1.97 and 2.03 metres.
This concept is suitable for a variety of purposes. The most common use is to keep track of and handle rounding errors directly during the calculation and of uncertainties in the knowledge of the exact values of physical and technical parameters. The latter often arise from measurement errors and tolerances for components or due to limits on computational accuracy. Interval arithmetic also helps find reliable and guaranteed solutions to equations and optimization problems.
Mathematically, instead of working with an uncertain real  we work with the two ends of the interval  which contains . In interval arithmetic, any variable  lies between  and , or could be one of them. A function  when applied to  is also uncertain. In interval arithmetic  produces an interval  which is all the possible values for  for all .""
""Arbitrary-precision arithmetic "",,,,""In computer science, arbitrary-precision arithmetic, also called bignum arithmetic, multiple precision arithmetic, or sometimes infinite-precision arithmetic, indicates that calculations are performed on numbers whose digits of precision are limited only by the available memory of the host system. This contrasts with the faster fixed-precision arithmetic found in most arithmetic logic unit (ALU) hardware, which typically offers between 8 and 64 bits of precision.
Several modern programming languages have built-in support for bignums, and others have libraries available for arbitrary-precision integer and floating-point math. Rather than store values as a fixed number of binary bits related to the size of the processor register, these implementations typically use variable-length arrays of digits.
Arbitrary precision is used in applications where the speed of arithmetic is not a limiting factor, or where precise results with very large numbers are required. It should not be confused with the symbolic computation provided by many computer algebra systems, which represent numbers by expressions such as π·sin(2), and can thus represent any computable number with infinite precision.

""
""Automatic differentiation "",,,,""In mathematics and computer algebra, automatic differentiation (AD), also called algorithmic differentiation or computational differentiation, is a set of techniques to numerically evaluate the derivative of a function specified by a computer program. AD exploits the fact that every computer program, no matter how complicated, executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (exp, log, sin, cos, etc.). By applying the chain rule repeatedly to these operations, derivatives of arbitrary order can be computed automatically, accurately to working precision, and using at most a small constant factor more arithmetic operations than the original program.
Automatic differentiation is not:

Symbolic differentiation, nor
Numerical differentiation (the method of finite differences).
These classical methods run into problems: symbolic differentiation leads to inefficient code (unless carefully done) and faces the difficulty of converting a computer program into a single expression, while numerical differentiation can introduce round-off errors in the discretization process and cancellation. Both classical methods have problems with calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial derivatives of a function with respect to many inputs, as is needed for gradient-based optimization algorithms. Automatic differentiation solves all of these problems, at the expense of introducing more software dependencies.
^ Neidinger, Richard D. (2010). "Introduction to Automatic Differentiation and MATLAB Object-Oriented Programming" (PDF). SIAM Review 52 (3): 545–563. doi:10.1137/080743627. 
^ http://www.ec-securehost.com/SIAM/SE24.html""
""Mesh generation "",,,,""Mesh generation is the practice of generating a polygonal or polyhedral mesh that approximates a geometric domain. The term "grid generation" is often used interchangeably. Typical uses are for rendering to a computer screen or for physical simulation such as finite element analysis or computational fluid dynamics. The input model form can vary greatly but common sources are CAD, NURBS, B-rep, STL (file format) or a point cloud. The field is highly interdisciplinary, with contributions found in mathematics, computer science, and engineering.
Three-dimensional meshes created for finite element analysis need to consist of tetrahedra, pyramids, prisms or hexahedra. Those used for the finite volume method can consist of arbitrary polyhedra. Those used for finite difference methods usually need to consist of piecewise structured arrays of hexahedra known as multi-block structured meshes. A mesh is otherwise a discretization of a domain existing in one, two or three dimensions.""
""Discretization "",,,,""In mathematics, discretization concerns the process of transferring continuous functions, models, and equations into discrete counterparts. This process is usually carried out as a first step toward making them suitable for numerical evaluation and implementation on digital computers. Processing on a digital computer requires another process called quantization. Dichotomization is the special case of discretization in which the number of discrete classes is 2, which can approximate a continuous variable as a binary variable (creating a dichotomy for modeling purposes).
Euler–Maruyama method
Zero-order hold
Discretization is also related to discrete mathematics, and is an important component of granular computing. In this context, discretization may also refer to modification of variable or category granularity, as when multiple discrete variables are aggregated or multiple discrete categories fused.
Whenever continuous data is discretized, there is always some amount of discretization error. The goal is to reduce the amount to a level considered negligible for the modeling purposes at hand.

""
""Network optimization "",,,,""In optimization, 3-opt is a simple local search algorithm for solving the travelling salesman problem and related network optimization problems.
3-opt analysis involves deleting 3 connections (or edges) in a network (or tour), reconnecting the network in all other possible ways, and then evaluating each reconnection method to find the optimum one. This process is then repeated for a different set of 3 connections.""
""Linear programming "",,,,""Linear programming (LP; also called linear optimization) is a method to achieve the best outcome (such as maximum profit or lowest cost) in a mathematical model whose requirements are represented by linear relationships. Linear programming is a special case of mathematical programming (mathematical optimization).
More formally, linear programming is a technique for the optimization of a linear objective function, subject to linear equality and linear inequality constraints. Its feasible region is a convex polytope, which is a set defined as the intersection of finitely many half spaces, each of which is defined by a linear inequality. Its objective function is a real-valued affine (linear) function defined on this polyhedron. A linear programming algorithm finds a point in the polyhedron where this function has the smallest (or largest) value if such a point exists.
Linear programs are problems that can be expressed in canonical form as

where x represents the vector of variables (to be determined), c and b are vectors of (known) coefficients, A is a (known) matrix of coefficients, and  is the matrix transpose. The expression to be maximized or minimized is called the objective function (cTx in this case). The inequalities Ax ≤ b and x ≥ 0 are the constraints which specify a convex polytope over which the objective function is to be optimized. In this context, two vectors are comparable when they have the same dimensions. If every entry in the first is less-than or equal-to the corresponding entry in the second then we can say the first vector is less-than or equal-to the second vector.
Linear programming can be applied to various fields of study. It is widely used in business and economics, and is also utilized for some engineering problems. Industries that use linear programming models include transportation, energy, telecommunications, and manufacturing. It has proved useful in modeling diverse types of problems in planning, routing, scheduling, assignment, and design.""
""Semidefinite programming "",,,,""Semidefinite programming (SDP) is a subfield of convex optimization concerned with the optimization of a linear objective function (an objective function is a user-specified function that the user wants to minimize or maximize) over the intersection of the cone of positive semidefinite matrices with an affine space, i.e., a spectrahedron.
Semidefinite programming is a relatively new field of optimization which is of growing interest for several reasons. Many practical problems in operations research and combinatorial optimization can be modeled or approximated as semidefinite programming problems. In automatic control theory, SDPs are used in the context of linear matrix inequalities. SDPs are in fact a special case of cone programming and can be efficiently solved by interior point methods. All linear programs can be expressed as SDPs, and via hierarchies of SDPs the solutions of polynomial optimization problems can be approximated. Semidefinite programming has been used in the optimization of complex systems. In recent years, some quantum query complexity problems have been formulated in term of semidefinite programs.""
""Convex optimization "",,,,""Convex minimization, a subfield of optimization, studies the problem of minimizing convex functions over convex sets. The convexity property can make optimization in some sense "easier" than the general case - for example, any local minimum must be a global minimum.
Given a real vector space  together with a convex, real-valued function

defined on a convex subset  of , the problem is to find any point  in  for which the number  is smallest, i.e., a point  such that
 for all .
The convexity of  makes the powerful tools of convex analysis applicable. In finite-dimensional normed spaces, the Hahn–Banach theorem and the existence of subgradients lead to a particularly satisfying theory of necessary and sufficient conditions for optimality, a duality theory generalizing that for linear programming, and effective computational methods.
Convex minimization has applications in a wide range of disciplines, such as automatic control systems, estimation and signal processing, communications and networks, electronic circuit design, data analysis and modeling, statistics (optimal design), and finance. With recent improvements in computing and in optimization theory, convex minimization is nearly as straightforward as linear programming. Many optimization problems can be reformulated as convex minimization problems. For example, the problem of maximizing a concave function f can be re-formulated equivalently as a problem of minimizing the function -f, which is convex.""
""Quasiconvex programming and unimodality "",,,,""In mathematics, a quasiconvex function is a real-valued function defined on an interval or on a convex subset of a real vector space such that the inverse image of any set of the form  is a convex set. Informally, along any stretch of the curve the highest point is one of the endpoints. The negative of a quasiconvex function is said to be quasiconcave.
All convex functions are also quasiconvex, but not all quasiconvex functions are convex, so quasiconvexity is a generalization of convexity. Quasiconvexity and quasiconcavity extend to functions with multiple arguments the notion of unimodality of functions with a single real argument.""
""Stochastic control and optimization "",,,,""In queueing theory, a discipline within the mathematical theory of probability, mean value analysis (MVA) is a recursive technique for computing expected queue lengths, waiting time at queueing nodes and throughput in equilibrium for a closed separable system of queues. The first approximate techniques were published independently by Schweitzer and Bard, followed later by an exact version by Lavenberg and Reiser published in 1980.
It is based on the arrival theorem, which states that when one customer in an M-customer closed system arrives at a service facility he/she observes the rest of the system to be in the equilibrium state for a system with M − 1 customers.""
""Quadratic programming "",,,,""Quadratic programming (QP) is a special type of mathematical optimization problem. It is the problem of optimizing (minimizing or maximizing) a quadratic function of several variables subject to linear constraints on these variables.""
""Nonconvex optimization "",,,,""BARON is a computational system for solving nonconvex optimization problems to global optimality. Purely continuous, purely integer, and mixed-integer nonlinear problems can be solved with the software. BARON is available under the AIMMS and GAMS modeling languages on a variety of platforms. The GAMS/BARON solver is also available on the NEOS Server.
The development of the BARON algorithms and software has been recognized by the 2004 INFORMS Computing Society Prize and the 2006 Beale-Orchard-Hays Prize for excellence in computational mathematical programming from the Mathematical Optimization Society.""
""Submodular optimization and polymatroids "",,,,""In mathematics, a submodular set function (also known as a submodular function) is a set function whose value, informally, has the property that the difference in the incremental value of the function, that a single element makes when added to an input set, decreases as the size of the input set increases. Submodular functions have a natural diminishing returns property which makes them suitable for many applications, including approximation algorithms, game theory (as functions modeling user preferences) and electrical networks. Recently, submodular functions have also found immense utility in several real world problems in machine learning and artificial intelligence, including automatic summarization, multi-document summarization, feature selection, active learning, sensor placement, image collection summarization and many other domains.

""
""Integer programming "",,,,""An integer programming problem is a mathematical optimization or feasibility program in which some or all of the variables are restricted to be integers. In many settings the term refers to integer linear programming (ILP), in which the objective function and the constraints (other than the integer constraints) are linear.
Integer programming is NP-hard. A special case, 0-1 integer linear programming, in which unknowns are binary, and only the restrictions must be satisfied, is one of Karp's 21 NP-complete problems.""
""Ordinary differential equations "",,,,""In mathematics, an ordinary differential equation or ODE is a differential equation containing one or more functions of one independent variable and its derivatives. The term "ordinary" is used in contrast with the term partial differential equation which may be with respect to more than one independent variable.
ODEs that are linear differential equations have exact closed-form solutions that can be added and multiplied by coefficients. By contrast, ODEs that lack additive solutions are nonlinear, and solving them is far more intricate, as one can rarely represent them by elementary functions in closed form: Instead, exact and analytic solutions of ODEs are in series or integral form. Graphical and numerical methods, applied by hand or by computer, may approximate solutions of ODEs and perhaps yield useful information, often sufficing in the absence of exact, analytic solutions.""
""Partial differential equations "",,,,""In mathematics, a partial differential equation (PDE) is a differential equation that contains unknown multivariable functions and their partial derivatives. (A special case are ordinary differential equations (ODEs), which deal with functions of a single variable and their derivatives.) PDEs are used to formulate problems involving functions of several variables, and are either solved by hand, or used to create a relevant computer model.
PDEs can be used to describe a wide variety of phenomena such as sound, heat, electrostatics, electrodynamics, fluid flow, elasticity, or quantum mechanics. These seemingly distinct physical phenomena can be formalised similarly in terms of PDEs. Just as ordinary differential equations often model one-dimensional dynamical systems, partial differential equations often model multidimensional systems. PDEs find their generalisation in stochastic partial differential equations.""
""Differential algebraic equations "",,,,""In mathematics, differential-algebraic equations (DAEs) are a general form of (systems of) differential equations for vector–valued functions x in one independent variable t,

where  is a vector of dependent variables  and the system has as many equations, . They are distinct from ordinary differential equation (ODE) in that a DAE is not completely solvable for the derivatives of all components of the function x because these may not all appear (i.e. some equations are algebraic); technically the distinction between an implicit ODE system [that may be rendered explicit] and a DAE system is that the Jacobian matrix  is a singular matrix for a DAE system. This distinction between ODEs and DAEs is made because DAEs have different characteristics and are generally more difficult to solve.
In practical terms, the distinction between DAEs and ODEs is often that the solution of a DAE system depends on the derivatives of the input signal and not just the signal itself as in the case of ODEs; this issue is commonly encountered in systems with hysteresis, such as the Schmitt trigger.
This difference is more clearly visible if the system may be rewritten so that instead of x we consider a pair  of vectors of dependent variables and the DAE has the form

where , ,  and 
A DAE system of this form is called semi-explicit. Every solution of the second half g of the equation defines a unique direction for x via the first half f of the equations, while the direction for y is arbitrary. But not every point (x,y,t) is a solution of g. The variables in x and the first half f of the equations get the attribute differential. The components of y and the second half g of the equations are called the algebraic variables or equations of the system. [The term algebraic in the context of DAEs only means free of derivatives and is not related to (abstract) algebra.]
The solution of a DAE consists of two parts, first the search for consistent initial values and second the computation of a trajectory. To find consistent initial values it is often necessary to consider the derivatives of some of the component functions of the DAE. The highest order of a derivative that is necessary in this process is called the differentiation index. The equations derived in computing the index and consistent initial values may also be of use in the computation of the trajectory. A semi-explicit DAE system can be converted to an implicit one by decreasing the differentiation index by one, and vice versa.""
""Differential variational inequalities "",,,,""In mathematics, a differential variational inequality (DVI) is a dynamical system that incorporates ordinary differential equations and variational inequalities or complementarity problems.
DVIs are useful for representing models involving both dynamics and inequality constraints. Examples of such problems include, for example, mechanical impact problems, electrical circuits with ideal diodes, Coulomb friction problems for contacting bodies, and dynamic economic and related problems such as dynamic traffic networks and networks of queues (where the constraints can either be upper limits on queue length or that the queue length cannot become negative). DVIs are related to a number of other concepts including differential inclusions, projected dynamical systems, evolutionary inequalities, and parabolic variational inequalities.
Differential variational inequalities were first formally introduced by Pang and Stewart, whose definition should not be confused with the differential variational inequality used in Aubin and Cellina (1984).
Differential variational inequalities have the form to find  such that

for every  and almost all t; K a closed convex set, where

Closely associated with DVIs are dynamic/differential complementarity problems: if K is a closed convex cone, then the variational inequality is equivalent to the complementarity problem:""
""Lambda calculus "",,,,""Lambda calculus (also written as λ-calculus) is a formal system in mathematical logic for expressing computation based on function abstraction and application using variable binding and substitution. It was first introduced by mathematician Alonzo Church in the 1930s as part of an investigation into the foundations of mathematics. Lambda calculus is a universal model of computation equivalent to a Turing machine (Church-Turing thesis, 1937). Its namesake, Greek letter lambda (λ), is used in lambda terms (also called lambda expressions) to denote binding a variable in a function.
Lambda calculus may be typed and untyped. In typed lambda calculus functions can be applied only if they are capable of accepting the given input's "type" of data.
Lambda calculus has applications in many different areas in mathematics, philosophy, linguistics, and computer science. Lambda calculus has played an important role in the development of the theory of programming languages. Functional programming languages implement the lambda calculus. Lambda calculus also is a current research topic in Category theory.""
""Differential calculus "",,,,""In mathematics, differential calculus is a subfield of calculus concerned with the study of the rates at which quantities change. It is one of the two traditional divisions of calculus, the other being integral calculus.
The primary objects of study in differential calculus are the derivative of a function, related notions such as the differential, and their applications. The derivative of a function at a chosen input value describes the rate of change of the function near that input value. The process of finding a derivative is called differentiation. Geometrically, the derivative at a point is the slope of the tangent line to the graph of the function at that point, provided that the derivative exists and is defined at that point. For a real-valued function of a single real variable, the derivative of a function at a point generally determines the best linear approximation to the function at that point.
Differential calculus and integral calculus are connected by the fundamental theorem of calculus, which states that differentiation is the reverse process to integration.
Differentiation has applications to nearly all quantitative disciplines. For example, in physics, the derivative of the displacement of a moving body with respect to time is the velocity of the body, and the derivative of velocity with respect to time is acceleration. The derivative of the momentum of a body equals the force applied to the body; rearranging this derivative statement leads to the famous F = ma equation associated with Newton's second law of motion. The reaction rate of a chemical reaction is a derivative. In operations research, derivatives determine the most efficient ways to transport materials and design factories.
Derivatives are frequently used to find the maxima and minima of a function. Equations involving derivatives are called differential equations and are fundamental in describing natural phenomena. Derivatives and their generalizations appear in many fields of mathematics, such as complex analysis, functional analysis, differential geometry, measure theory and abstract algebra.""
""Integral calculus "",,,,""In mathematics, an integral assigns numbers to functions in a way that can describe displacement, area, volume, and other concepts that arise by combining infinitesimal data. Integration is one of the two main operations of calculus, with its inverse, differentiation, being the other. Given a function f of a real variable x and an interval [a, b] of the real line, the definite integral

is defined informally as the signed area of the region in the xy-plane that is bounded by the graph of f, the x-axis and the vertical lines x = a and x = b. The area above the x-axis adds to the total and that below the x-axis subtracts from the total.
Roughly speaking, the operation of integration is the reverse of differentiation. For this reason, the term integral may also refer to the related notion of the antiderivative, a function F whose derivative is the given function f. In this case, it is called an indefinite integral and is written:

The integrals discussed in this article are those termed definite integrals. It is the fundamental theorem of calculus that connects differentiation with the definite integral: if f is a continuous real-valued function defined on a closed interval [a, b], then, once an antiderivative F of f is known, the definite integral of f over that interval is given by

The principles of integration were formulated independently by Isaac Newton and Gottfried Leibniz in the late 17th century, who thought of the integral as an infinite sum of rectangles of infinitesimal width. A rigorous mathematical definition of the integral was given by Bernhard Riemann. It is based on a limiting procedure which approximates the area of a curvilinear region by breaking the region into thin vertical slabs. Beginning in the nineteenth century, more sophisticated notions of integrals began to appear, where the type of the function as well as the domain over which the integration is performed has been generalised. A line integral is defined for functions of two or three variables, and the interval of integration [a, b] is replaced by a certain curve connecting two points on the plane or in the space. In a surface integral, the curve is replaced by a piece of a surface in the three-dimensional space.

""
""Approximation "",,,,""An approximation is anything that is similar but not exactly equal to something else.""
""Integral equations "",,,,""In mathematics, an integral equation is an equation in which an unknown function appears under an integral sign.
There is a close connection between differential and integral equations, and some problems may be formulated either way. See, for example, Green's function, Fredholm theory, and Maxwell's equations.""
""Nonlinear equations "",,,,""In physics and other sciences, a nonlinear system, in contrast to a linear system, is a system which does not satisfy the superposition principle – meaning that the output of a nonlinear system is not directly proportional to the input.
In mathematics, a nonlinear system of equations is a set of simultaneous equations in which the unknowns (or the unknown functions in the case of differential equations) appear as variables of a polynomial of degree higher than one or in the argument of a function which is not a polynomial of degree one. In other words, in a nonlinear system of equations, the equation(s) to be solved cannot be written as a linear combination of the unknown variables or functions that appear in them. It does not matter if nonlinear known functions appear in the equations. In particular, a differential equation is linear if it is linear in terms of the unknown function and its derivatives, even if nonlinear in terms of the other variables appearing in it.
Typically, the behavior of a nonlinear system is described by a nonlinear system of equations.
Nonlinear problems are of interest to engineers, physicists and mathematicians and many other scientists because most systems are inherently nonlinear in nature.
As nonlinear equations are difficult to solve, nonlinear systems are commonly approximated by linear equations (linearization). This works well up to some accuracy and some range for the input values, but some interesting phenomena such as chaos and singularities are hidden by linearization. It follows that some aspects of the behavior of a nonlinear system appear commonly to be chaotic, unpredictable or counterintuitive. Although such chaotic behavior may resemble random behavior, it is absolutely not random.
For example, some aspects of the weather are seen to be chaotic, where simple changes in one part of the system produce complex effects throughout. This nonlinearity is one of the reasons why accurate long-term forecasts are impossible with current technology.
Some authors use the term nonlinear science for the study of nonlinear systems. This is disputed by others:

Using a term like nonlinear science is like referring to the bulk of zoology as the study of non-elephant animals.""
""Lambda calculus "",,,,""Lambda calculus (also written as λ-calculus) is a formal system in mathematical logic for expressing computation based on function abstraction and application using variable binding and substitution. It was first introduced by mathematician Alonzo Church in the 1930s as part of an investigation into the foundations of mathematics. Lambda calculus is a universal model of computation equivalent to a Turing machine (Church-Turing thesis, 1937). Its namesake, Greek letter lambda (λ), is used in lambda terms (also called lambda expressions) to denote binding a variable in a function.
Lambda calculus may be typed and untyped. In typed lambda calculus functions can be applied only if they are capable of accepting the given input's "type" of data.
Lambda calculus has applications in many different areas in mathematics, philosophy, linguistics, and computer science. Lambda calculus has played an important role in the development of the theory of programming languages. Functional programming languages implement the lambda calculus. Lambda calculus also is a current research topic in Category theory.""
""Differential calculus "",,,,""In mathematics, differential calculus is a subfield of calculus concerned with the study of the rates at which quantities change. It is one of the two traditional divisions of calculus, the other being integral calculus.
The primary objects of study in differential calculus are the derivative of a function, related notions such as the differential, and their applications. The derivative of a function at a chosen input value describes the rate of change of the function near that input value. The process of finding a derivative is called differentiation. Geometrically, the derivative at a point is the slope of the tangent line to the graph of the function at that point, provided that the derivative exists and is defined at that point. For a real-valued function of a single real variable, the derivative of a function at a point generally determines the best linear approximation to the function at that point.
Differential calculus and integral calculus are connected by the fundamental theorem of calculus, which states that differentiation is the reverse process to integration.
Differentiation has applications to nearly all quantitative disciplines. For example, in physics, the derivative of the displacement of a moving body with respect to time is the velocity of the body, and the derivative of velocity with respect to time is acceleration. The derivative of the momentum of a body equals the force applied to the body; rearranging this derivative statement leads to the famous F = ma equation associated with Newton's second law of motion. The reaction rate of a chemical reaction is a derivative. In operations research, derivatives determine the most efficient ways to transport materials and design factories.
Derivatives are frequently used to find the maxima and minima of a function. Equations involving derivatives are called differential equations and are fundamental in describing natural phenomena. Derivatives and their generalizations appear in many fields of mathematics, such as complex analysis, functional analysis, differential geometry, measure theory and abstract algebra.""
""Integral calculus "",,,,""In mathematics, an integral assigns numbers to functions in a way that can describe displacement, area, volume, and other concepts that arise by combining infinitesimal data. Integration is one of the two main operations of calculus, with its inverse, differentiation, being the other. Given a function f of a real variable x and an interval [a, b] of the real line, the definite integral

is defined informally as the signed area of the region in the xy-plane that is bounded by the graph of f, the x-axis and the vertical lines x = a and x = b. The area above the x-axis adds to the total and that below the x-axis subtracts from the total.
Roughly speaking, the operation of integration is the reverse of differentiation. For this reason, the term integral may also refer to the related notion of the antiderivative, a function F whose derivative is the given function f. In this case, it is called an indefinite integral and is written:

The integrals discussed in this article are those termed definite integrals. It is the fundamental theorem of calculus that connects differentiation with the definite integral: if f is a continuous real-valued function defined on a closed interval [a, b], then, once an antiderivative F of f is known, the definite integral of f over that interval is given by

The principles of integration were formulated independently by Isaac Newton and Gottfried Leibniz in the late 17th century, who thought of the integral as an infinite sum of rectangles of infinitesimal width. A rigorous mathematical definition of the integral was given by Bernhard Riemann. It is based on a limiting procedure which approximates the area of a curvilinear region by breaking the region into thin vertical slabs. Beginning in the nineteenth century, more sophisticated notions of integrals began to appear, where the type of the function as well as the domain over which the integration is performed has been generalised. A line integral is defined for functions of two or three variables, and the interval of integration [a, b] is replaced by a certain curve connecting two points on the plane or in the space. In a surface integral, the curve is replaced by a piece of a surface in the three-dimensional space.

""
""Point-set topology "",,,,""In mathematics, general topology is the branch of topology that deals with the basic set-theoretic definitions and constructions used in topology. It is the foundation of most other branches of topology, including differential topology, geometric topology, and algebraic topology. Another name for general topology is point-set topology.
The fundamental concepts in point-set topology are continuity, compactness, and connectedness:
Continuous functions, intuitively, take nearby points to nearby points.
Compact sets are those that can be covered by finitely many sets of arbitrarily small size.
Connected sets are sets that cannot be divided into two pieces that are far apart.
The words 'nearby', 'arbitrarily small', and 'far apart' can all be made precise by using open sets. If we change the definition of 'open set', we change what continuous functions, compact sets, and connected sets are. Each choice of definition for 'open set' is called a topology. A set with a topology is called a topological space.
Metric spaces are an important class of topological spaces where distances can be assigned a number called a metric. Having a metric simplifies many proofs, and many of the most common topological spaces are metric spaces.

""
""Algebraic topology "",,,,""Algebraic topology is a branch of mathematics that uses tools from abstract algebra to study topological spaces. The basic goal is to find algebraic invariants that classify topological spaces up to homeomorphism, though usually most classify up to homotopy equivalence.
Although algebraic topology primarily uses algebra to study topological problems, using topology to solve algebraic problems is sometimes also possible. Algebraic topology, for example, allows for a convenient proof that any subgroup of a free group is again a free group.""
""Geometric topology "",,,,""In mathematics, geometric topology is the study of manifolds and maps between them, particularly embeddings of one manifold into another.""
""Continuous functions "",,,,""In mathematics, a continuous function is, roughly speaking, a function for which sufficiently small changes in the input result in arbitrarily small changes in the output. Otherwise, a function is said to be a discontinuous function. A continuous function with a continuous inverse function is called a homeomorphism.
Continuity of functions is one of the core concepts of topology, which is treated in full generality below. The introductory portion of this article focuses on the special case where the inputs and outputs of functions are real numbers. In addition, this article discusses the definition for the more general case of functions between two metric spaces. In order theory, especially in domain theory, one considers a notion of continuity known as Scott continuity. Other forms of continuity do exist but they are not discussed in this article.
As an example, consider the function h(t), which describes the height of a growing flower at time t. This function is continuous. By contrast, if M(t) denotes the amount of money in a bank account at time t, then the function jumps whenever money is deposited or withdrawn, so the function M(t) is discontinuous.""
""Relational database model "",,,,""The relational model (RM) for database management is an approach to managing data using a structure and language consistent with first-order predicate logic, first described in 1969 by Edgar F. Codd. In the relational model of a database, all data is represented in terms of tuples, grouped into relations. A database organized in terms of the relational model is a relational database.

The purpose of the relational model is to provide a declarative method for specifying data and queries: users directly state what information the database contains and what information they want from it, and let the database management system software take care of describing data structures for storing the data and retrieval procedures for answering queries.
Most relational databases use the SQL data definition and query language; these systems implement what can be regarded as an engineering approximation to the relational model. A table in an SQL database schema corresponds to a predicate variable; the contents of a table to a relation; key constraints, other constraints, and SQL queries correspond to predicates. However, SQL databases deviate from the relational model in many details, and Codd fiercely argued against deviations that compromise the original principles.""
""Entity relationship models "",,,,""In software engineering, an entity–relationship model (ER model) is a data model for describing the data or information aspects of a business domain or its process requirements, in an abstract way that lends itself to ultimately being implemented in a database such as a relational database. The main components of ER models are entities (things) and the relationships that can exist among them.
Entity–relationship modeling was developed by Peter Chen and published in a 1976 paper. However, variants of the idea existed previously, and have been devised subsequently such as supertype and subtype data entities and commonality relationships.""
""Hierarchical data models "",,,,""A hierarchical database model is a data model in which the data is organized into a tree-like structure. The data is stored as records which are connected to one another through links. A record is a collection of fields, with each field containing only one value. The entity type of a record defines which fields the record contains.

A record in the hierarchical database model corresponds to a row (or tuple) in the relational database model and an entity type corresponds to a table (or relation).
The hierarchical database model mandates that each child record has only one parent, whereas each parent record can have one or more child records. In order to retrieve data from a hierarchical database the whole tree needs to be traversed starting from the root node. This model is recognized as the first database model created by IBM in the 1960s.""
""Network data models "",,,,""The network model is a database model conceived as a flexible way of representing objects and their relationships. Its distinguishing feature is that the schema, viewed as a graph in which object types are nodes and relationship types are arcs, is not restricted to being a hierarchy or lattice.""
""Physical data models "",,,,""A physical data model (or database design) is a representation of a data design which takes into account the facilities and constraints of a given database management system. In the lifecycle of a project it typically derives from a logical data model, though it may be reverse-engineered from a given database implementation. A complete physical data model will include all the database artifacts required to create relationships between tables or to achieve performance goals, such as indexes, constraint definitions, linking tables, partitioned tables or clusters. Analysts can usually use a physical data model to calculate storage estimates; it may include specific storage allocation details for a given database system.
As of 2012 seven main databases dominate the commercial marketplace: Informix, Oracle, Postgres, SQL Server, Sybase, DB2 and MySQL. Other RDBMS systems tend either to be legacy databases or used within academia such as universities or further education colleges. Physical data models for each implementation would differ significantly, not least due to underlying operating-system requirements that may sit underneath them. For example: SQL Server runs only on Microsoft Windows operating-systems, while Oracle and MySQL can run on Solaris, Linux and other UNIX-based operating-systems as well as on Windows. This means that the disk requirements, security requirements and many other aspects of a physical data model will be influenced by the RDBMS that a database administrator (or an organization) chooses to use.""
""Semi-structured data "",,,,""Semi-structured data is a form of structured data that does not conform with the formal structure of data models associated with relational databases or other forms of data tables, but nonetheless contains tags or other markers to separate semantic elements and enforce hierarchies of records and fields within the data. Therefore, it is also known as self-describing structure.
In semi-structured data, the entities belonging to the same class may have different attributes even though they are grouped together, and the attributes' order is not important.
Semi-structured data is increasingly occurring since the advent of the Internet where full-text documents and databases are not the only forms of data any more and different applications need a medium for exchanging information. In object-oriented databases, one often finds semi-structured data.""
""Data streams "",,,,""Flynn's taxonomy is a classification of computer architectures, proposed by Michael J. Flynn in 1966. The classification system has stuck, and has been used as a tool in design of modern processors and their functionalities. Since the rise of multiprocessing central processing units (CPUs), a multiprogramming context has evolved as an extension of the classification system.""
""Data provenance "",,,,""Provenance (from the French provenir, "to come from"), is the chronology of the ownership, custody or location of a historical object. The term was originally mostly used in relation to works of art, but is now used in similar senses in a wide range of fields, including archaeology, paleontology, archives, manuscripts, printed books, and science and computing. The primary purpose of tracing the provenance of an object or entity is normally to provide contextual and circumstantial evidence for its original production or discovery, by establishing, as far as practicable, its later history, especially the sequences of its formal ownership, custody, and places of storage. The practice has a particular value in helping authenticate objects. Comparative techniques, expert opinions, and the results of scientific tests may also be used to these ends, but establishing provenance is essentially a matter of documentation.
It has been argued that in archaeology (North American archaeology and anthropological archaeology throughout the world), when the US spelling provenience is used it has a related but subtly different sense to provenance. Archaeological researchers use provenience to refer to the three-dimensional location or find spot of an artifact or feature within an archaeological site, whereas provenance covers an object's complete documented history. Ideally, in modern excavations, the provenience or find spot is recorded (even videoed) with great precision, but in older cases only the general site or approximate area may be known, especially when an artifact was found outside a professional excavation and its specific position not recorded. Any given antiquity may therefore have both a provenience (where it was found) and a provenance (where it has been since it was found). In some cases, especially where there is an inscription, the provenance may include a history that predates its burial in the ground, as well as those relating to its history after rediscovery.""
""Incomplete data "",,,,""The Nelson–Aalen estimator is a non-parametric estimator of the cumulative hazard rate function in case of censored data or incomplete data. It is used in survival theory, reliability engineering and life insurance to estimate the cumulative number of expected events. An "event" can be the failure of a non-repairable component, the death of a human being, or any occurrence for which the experimental unit remains in the "failed" state (e.g., death) from the point at which it changed on. The estimator is given by

with  the number of events at  and  the total individuals at risk at .
The curvature of the Nelson–Aalen estimator gives an idea of the hazard rate shape. A concave shape is an indicator for infant mortality while a convex shape indicates wear out mortality.
It can be used for example when testing the homogeneity of Poisson processes.

""
""Temporal data "",,,,""A temporal database is a database with built-in support for handling data involving time, being related to the slowly changing dimension concept, for example a temporal data model and a temporal version of Structured Query Language (SQL).
More specifically the temporal aspects usually include valid time and transaction time. These attributes can be combined to form bitemporal data.
Valid time is the time period during which a fact is true with respect to the real world.
Transaction time is the time period during which a fact stored in the database is considered to be true.
Bitemporal data combines both Valid and Transaction Time.
It is possible to have timelines other than Valid Time and Transaction Time, such as Decision Time, in the database. In that case the database is called a multitemporal database as opposed to a bitemporal database. However, this approach introduces additional complexities such as dealing with the validity of (foreign) keys.
Temporal databases are in contrast to current databases, which store only facts which are believed to be true at the current time.

""
""Uncertainty "",,,,""Uncertainty is the situation which involves imperfect and / or unknown information. In other words, it is a term used in subtly different ways in a number of fields, including insurance, philosophy, physics, statistics, economics, finance, psychology, sociology, engineering, metrology, and information science. It applies to predictions of future events, to physical measurements that are already made, or to the unknown. Uncertainty arises in partially observable and/or stochastic environments, as well as due to ignorance and/or indolence.""
""Inconsistent data "",,,,""The Vaccine Adverse Event Reporting System (VAERS) is a United States program for vaccine safety, co-managed by the U.S. Centers for Disease Control and Prevention (CDC) and the Food and Drug Administration (FDA). VAERS is a postmarketing surveillance program, collecting information about adverse events (possible side effects) that occur after administration of vaccines.
VAERS, the Vaccine Safety Datalink, and the Clinical Immunization Safety Assessment (CISA) Network are tools by which the CDC and FDA measure vaccine safety to fulfill their duty as regulatory agencies charged with protecting the public. Some scientists would like to do a more scientifically rigorous job of this, noting that VAERS has several limitations, including unverified reports, underreporting, inconsistent data quality, and absence of an unvaccinated control group.""
""Multidimensional range search "",,,,""The UB-tree as proposed by Rudolf Bayer and Volker Markl is a balanced tree for storing and efficiently retrieving multidimensional data. It is basically a B+ tree (information only in the leaves) with records stored according to Z-order, also called Morton order. Z-order is simply calculated by bitwise interlacing the keys.
Insertion, deletion, and point query are done as with ordinary B+ trees. To perform range searches in multidimensional point data, however, an algorithm must be provided for calculating, from a point encountered in the data base, the next Z-value which is in the multidimensional search range.
The original algorithm to solve this key problem was exponential with the dimensionality and thus not feasible ("GetNextZ-address"). A solution to this "crucial part of the UB-tree range query" linear with the z-address bit length has been described later. This method has already been described in an older paper where using Z-order with search trees has first been proposed.

""
""Data scans "",,,,"""Timescape" is the 151st episode of the television series Star Trek: The Next Generation. The 25th episode of the sixth season.""
""Unidimensional range search "",,,,""In computer science, a trie, also called digital tree and sometimes radix tree or prefix tree (as they can be searched by prefixes), is an ordered tree data structure that is used to store a dynamic set or associative array where the keys are usually strings. Unlike a binary search tree, no node in the tree stores the key associated with that node; instead, its position in the tree defines the key with which it is associated. All the descendants of a node have a common prefix of the string associated with that node, and the root is associated with the empty string. Values are not necessarily associated with every node. Rather, values tend only to be associated with leaves, and with some inner nodes that correspond to keys of interest. For the space-optimized presentation of prefix tree, see compact prefix tree.
In the example shown, keys are listed in the nodes and values below them. Each complete English word has an arbitrary integer value associated with it. A trie can be seen as a tree-shaped deterministic finite automaton. Each finite language is generated by a trie automaton, and each trie can be compressed into a deterministic acyclic finite state automaton.
Though tries are usually keyed by character strings, they need not be. The same algorithms can be adapted to serve similar functions of ordered lists of any construct, e.g. permutations on a list of digits or shapes. In particular, a bitwise trie is keyed on the individual bits making up any fixed-length binary datum, such as an integer or memory address.""
""Data compression "",,,,""In signal processing, data compression, source coding, or bit-rate reduction involves encoding information using fewer bits than the original representation. Compression can be either lossy or lossless. Lossless compression reduces bits by identifying and eliminating statistical redundancy. No information is lost in lossless compression. Lossy compression reduces bits by identifying unnecessary information and removing it. The process of reducing the size of a data file is referred to as data compression. In the context of data transmission, it is called source coding (encoding done at the source of the data before it is stored or transmitted) in opposition to channel coding.
Compression is useful because it helps reduce resource usage, such as data storage space or transmission capacity. Because compressed data must be decompressed to use, this extra processing imposes computational or other costs through decompression; this situation is far from being a free lunch. Data compression is subject to a space–time complexity trade-off. For instance, a compression scheme for video may require expensive hardware for the video to be decompressed fast enough to be viewed as it is being decompressed, and the option to decompress the video in full before watching it may be inconvenient or require additional storage. The design of data compression schemes involves trade-offs among various factors, including the degree of compression, the amount of distortion introduced (when using lossy data compression), and the computational resources required to compress and decompress the data.""
""Data encryption "",,,,""In cryptography, encryption is the process of encoding messages or information in such a way that only authorized parties can read it. Encryption does not of itself prevent interception, but denies the message content to the interceptor. In an encryption scheme, the intended communication information or message, referred to as plaintext, is encrypted using an encryption algorithm, generating ciphertext that can only be read if decrypted. For technical reasons, an encryption scheme usually uses a pseudo-random encryption key generated by an algorithm. It is in principle possible to decrypt the message without possessing the key, but, for a well-designed encryption scheme, large computational resources and skill are required. An authorized recipient can easily decrypt the message with the key provided by the originator to recipients, but not to unauthorized interceptors.""
""Query optimization "",,,,""Query optimization is a function of many relational database management systems. The query optimizer attempts to determine the most efficient way to execute a given query by considering the possible query plans.
Generally, the query optimizer cannot be accessed directly by users: once queries are submitted to database server, and parsed by the parser, they are then passed to the query optimizer where optimization occurs. However, some database engines allow guiding the query optimizer with hints.
A query is a request for information from a database. It can be as simple as "finding the address of a person with SS# 123-45-6789," or more complex like "finding the average salary of all the employed married men in California between the ages 30 to 39, that earn less than their wives." Queries results are generated by accessing relevant database data and manipulating it in a way that yields the requested information. Since database structures are complex, in most cases, and especially for not-very-simple queries, the needed data for a query can be collected from a database by accessing it in different ways, through different data-structures, and in different orders. Each different way typically requires different processing time. Processing times of the same query may have large variance, from a fraction of a second to hours, depending on the way selected. The purpose of query optimization, which is an automated process, is to find the way to process a given query in minimum time. The large possible variance in time justifies performing query optimization, though finding the exact optimal way to execute a query, among all possibilities, is typically very complex, time consuming by itself, may be too costly, and often practically impossible. Thus query optimization typically tries to approximate the optimum by comparing several common-sense alternatives to provide in a reasonable time a "good enough" plan which typically does not deviate much from the best possible result.""
""Query planning "",,,,""A query plan (or query execution plan) is an ordered set of steps used to access data in a SQL relational database management system. This is a specific case of the relational model concept of access plans.
Since SQL is declarative, there are typically a large number of alternative ways to execute a given query, with widely varying performance. When a query is submitted to the database, the query optimizer evaluates some of the different, correct possible plans for executing the query and returns what it considers the best option. Because query optimizers are imperfect, database users and administrators sometimes need to manually examine and tune the plans produced by the optimizer to get better performance.""
""Join algorithms "",,,,""A SQL join clause combines records from two or more tables in a relational database. It creates a set that can be saved as a table or used as it is. A JOIN is a means for combining fields from two tables (or more) by using values common to each. ANSI-standard SQL specifies five types of JOIN: INNER, LEFT OUTER, RIGHT OUTER, FULL OUTER and CROSS. As a special case, a table (base table, view, or joined table) can JOIN to itself in a self-join.
A programmer writes a JOIN statement to identify records for joining. If the evaluated predicate is true, the combined record is then produced in the expected format, a record set or a temporary table.""
""Data locking "",,,,""A data logger (also datalogger or data recorder) is an electronic device that records data over time or in relation to location either with a built in instrument or sensor or via external instruments and sensors. Increasingly, but not entirely, they are based on a digital processor (or computer). They generally are small, battery powered, portable, and equipped with a microprocessor, internal memory for data storage, and sensors. Some data loggers interface with a personal computer, and use software to activate the data logger and view and analyze the collected data, while others have a local interface device (keypad, LCD) and can be used as a stand-alone device.
Data loggers vary between general purpose types for a range of measurement applications to very specific devices for measuring in one environment or application type only. It is common for general purpose types to be programmable; however, many remain as static machines with only a limited number or no changeable parameters. Electronic data loggers have replaced chart recorders in many applications.
One of the primary benefits of using data loggers is the ability to automatically collect data on a 24-hour basis. Upon activation, data loggers are typically deployed and left unattended to measure and record information for the duration of the monitoring period. This allows for a comprehensive, accurate picture of the environmental conditions being monitored, such as air temperature and relative humidity.
The cost of data loggers has been declining over the years as technology improves and costs are reduced. Simple single channel data loggers cost as little as $25. More complicated loggers may costs hundreds or thousands of dollars.""
""Transaction logging "",,,,""In the field of databases in computer science, a transaction log (also transaction journal, database log, binary log or audit trail) is a history of actions executed by a database management system to guarantee ACID properties over crashes or hardware failures. Physically, a log is a file listing changes to the database, stored in a stable storage format.
If, after a start, the database is found in an inconsistent state or not been shut down properly, the database management system reviews the database logs for uncommitted transactions and rolls back the changes made by these transactions. Additionally, all transactions that are already committed but whose changes were not yet materialized in the database are re-applied. Both are done to ensure atomicity and durability of transactions.
This term is not to be confused with other, human-readable logs that a database management system usually provides.
In computer storage, a journal is a chronological record of data processing operations that may be used to construct or reinstate an historical or alternative version of a computer system or computer file.
In database management systems, a journal is the record of data altered by a given process.""
""Database recovery "",,,,""Database theory encapsulates a broad range of topics related to the study and research of the theoretical realm of databases and database management systems.
Theoretical aspects of data management include, among other areas, the foundations of query languages, computational complexity and expressive power of queries, finite model theory, database design theory, dependency theory, foundations of concurrency control and database recovery, deductive databases, temporal and spatial databases, real time databases, managing uncertain data and probabilistic databases, and Web data.
Most research work has traditionally been based on the relational model, since this model is usually considered the simplest and most foundational model of interest. Corresponding results for other data models, such as object-oriented or semi-structured models, or, more recently, graph data models and XML, are often derivable from those for the relational model.
A central focus of database theory is on understanding the complexity and power of query languages and their connection to logic. Starting from relational algebra and first-order logic (which are equivalent by Codd's theorem) and the insight that important queries such as graph reachability are not expressible in this language, more powerful language based on logic programming and fixpoint logic such as datalog were studied. Another focus was on the foundations of query optimization and data integration. Here most work studied conjunctive queries, which admit query optimization even under constraints using the chase algorithm.
The main research conferences in the area are the ACM Symposium on Principles of Database Systems (PODS) and the International Conference on Database Theory (ICDT).""
""Key-value stores "",,,,""The Apache Gora open-source software framework provides an in-memory data model and persistence for big data. Gora supports persisting to column stores, key/value stores, document stores and RDBMSs, and analyzing the data with extensive Apache Hadoop MapReduce support.""
""MapReduce-based systems "",,,,""RCFile (Record Columnar File) is a data placement structure that determines how to store relational tables on computer clusters. It is designed for systems using the MapReduce framework. The RCFile structure is a systematic combination of multiple components including data storage format, data compression approach, and optimization techniques for data reading. It is able to meet all the four requirements of data placement: (1) fast data loading, (2) fast query processing, (3) highly efficient storage space utilization, and (4) a strong adaptivity to dynamic data access patterns.
RCFile is a result of basic research with collaborative efforts from Facebook, Ohio State University, and Institute of Computing Technology, Chinese Academy of Sciences. A research paper entitled “RCFile: a Fast and Space-efficient Data Placement Structure in MapReduce-based Warehouse systems” was published and presented in ICDE’ 11. The data placement structure and its implementation presented in the paper have been widely adopted in the open source community, big data analytics industries, and application users. See the section of Impacts.""
""Database views "",,,,""In database theory, a view is the result set of a stored query on the data, which the database users can query just as they would in a persistent database collection object. This pre-established query command is kept in the database dictionary. Unlike ordinary base tables in a relational database, a view does not form part of the physical schema: as a result set, it is a virtual table computed or collated dynamically from data in the database when access to that view is requested. Changes applied to the data in a relevant underlying table are reflected in the data shown in subsequent invocations of the view. In some NoSQL databases, views are the only way to query data.
Views can provide advantages over tables:
Views can represent a subset of the data contained in a table. Consequently, a view can limit the degree of exposure of the underlying tables to the outer world: a given user may have permission to query the view, while denied access to the rest of the base table.
Views can join and simplify multiple tables into a single virtual table.
Views can act as aggregated tables, where the database engine aggregates data (sum, average, etc.) and presents the calculated results as part of the data.
Views can hide the complexity of data. For example, a view could appear as Sales2000 or Sales2001, transparently partitioning the actual underlying table.
Views take very little space to store; the database contains only the definition of a view, not a copy of all the data that it presents.
Depending on the SQL engine used, views can provide extra security.
Just as a function (in programming) can provide abstraction, so can a database view. In another parallel with functions, database users can manipulate nested views, thus one view can aggregate data from other views. Without the use of views, the normalization of databases above second normal form would become much more difficult. Views can make it easier to create lossless join decomposition.
Just as rows in a base table lack any defined ordering, rows available through a view do not appear with any default sorting. A view is a relational table, and the relational model defines a table as a set of rows. Since sets are not ordered — by definition — neither are the rows of a view. Therefore, an ORDER BY clause in the view definition is meaningless; the SQL standard (SQL:2003) does not allow an ORDER BY clause in the subquery of a CREATE VIEW command, just as it is refused in a CREATE TABLE statement. However, sorted data can be obtained from a view, in the same way as any other table — as part of a query statement on that view. Nevertheless, some DBMS (such as Oracle Database) do not abide by this SQL standard restriction.""
""Integrity checking "",,,,""A checksum or hash sum is a small-size datum from a block of digital data for the purpose of detecting errors which may have been introduced during its transmission or storage. It is usually applied to an installation file after it is received from the download server. By themselves checksums are often used to verify data integrity, but should not be relied upon to also verify data authenticity.
The actual procedure which yields the checksum, given a data input is called a checksum function or checksum algorithm. Depending on its design goals, a good checksum algorithm will usually output a significantly different value, even for small changes made to the input. This is especially true of cryptographic hash functions, which may be used to detect many data corruption errors and verify overall data integrity; if the computed checksum for the current data input matches the stored value of a previously computed checksum, there is a very high probability the data has not been accidentally altered or corrupted.
Checksum functions are related to hash functions, fingerprints, randomization functions, and cryptographic hash functions. However, each of those concepts has different applications and therefore different design goals. For instance a function returning the start of a string can provide a hash appropriate for some applications but will never be a suitable checksum. Checksums are used as cryptographic primitives in larger authentication algorithms. For cryptographic systems with these two specific design goals, see HMAC.
Check digits and parity bits are special cases of checksums, appropriate for small blocks of data (such as Social Security numbers, bank account numbers, computer words, single bytes, etc.). Some error-correcting codes are based on special checksums which not only detect common errors but also allow the original data to be recovered in certain cases.""
""Deadlocks "",,,,""In concurrent programming, a deadlock is a situation in which two or more competing actions are each waiting for the other to finish, and thus neither ever does.
In a transactional database, a deadlock happens when two processes each within its own transaction updates two rows of information but in the opposite order. For example, process A updates row 1 then row 2 in the exact timeframe that process B updates row 2 then row 1. Process A can't finish updating row 2 until process B is finished, but process B cannot finish updating row 1 until process A is finished. No matter how much time is allowed to pass, this situation will never resolve itself and because of this, database management systems will typically kill the transaction of the process that has done the least amount of work.
In an operating system, a deadlock is a situation which occurs when a process or thread enters a waiting state because a resource requested is being held by another waiting process, which in turn is waiting for another resource held by another waiting process. If a process is unable to change its state indefinitely because the resources requested by it are being used by another waiting process, then the system is said to be in a deadlock.
Deadlock is a common problem in multiprocessing systems, parallel computing and distributed systems, where software and hardware locks are used to handle shared resources and implement process synchronization.
In telecommunication systems, deadlocks occur mainly due to lost or corrupt signals instead of resource contention.""
""Distributed database recovery "",,,,""The National Industrial Recovery Act (NIRA) was a law passed by the United States Congress in 1933 to authorize the President to regulate industry in an attempt to raise prices after severe deflation and stimulate economic recovery. It also established a national public works program known as the Public Works Administration (PWA, not to be confused with the WPA of 1935). The National Recovery Administration (NRA) portion was widely hailed in 1933, but by 1934 business' opinion of the act had soured. By March 1934 the "NRA was engaged chiefly in drawing up these industrial codes for all industries to adopt." However, the NIRA was declared unconstitutional by the Supreme Court in 1935 and not replaced.
The legislation was enacted in June 1933 during the Great Depression in the United States as part of President Franklin D. Roosevelt's New Deal legislative program. Section 7(a) of the bill, which protected collective bargaining rights for unions, proved contentious (especially in the Senate), but both chambers eventually passed the legislation. President Roosevelt signed the bill into law on June 16, 1933. The Act had two main sections (or "titles"). Title I was devoted to industrial recovery, authorizing the promulgation of industrial codes of fair competition, guaranteed trade union rights, permitted the regulation of working standards, and regulated the price of certain refined petroleum products and their transportation. Title II established the Public Works Administration, outlined the projects and funding opportunities it could engage in. Title II also provided funding for the Act.
The Act was implemented by the NRA and the Public Works Administration (PWA). Very large numbers of regulations were generated under the authority granted to the NRA by the Act, which led to a significant loss of political support for Roosevelt and the New Deal. The NIRA was set to expire in June 1935, but in a major constitutional ruling the U.S. Supreme Court held Title I of the Act unconstitutional on May 27, 1935, in Schechter Poultry Corp. v. United States, 295 U.S. 495 (1935). The National Industrial Recovery Act is widely considered a policy failure, both in the 1930s and by historians today. Disputes over the reasons for this failure continue. Among the suggested causes are that the Act promoted economically harmful monopolies, that the Act lacked critical support from the business community, and that it was poorly administered. The Act encouraged union organizing, which led to significant labor unrest. The NIRA had no mechanisms for handling these problems, which led Congress to pass the National Labor Relations Act in 1935. The Act was also a major force behind a major modification of the law criminalizing making false statements.

""
""Main memory engines "",,,,""An in-memory database (IMDB; also main memory database system or MMDB or memory resident database) is a database management system that primarily relies on main memory for computer data storage. It is contrasted with database management systems that employ a disk storage mechanism. Main memory databases are faster than disk-optimized databases since the internal optimization algorithms are simpler and execute fewer CPU instructions. Accessing data in memory eliminates seek time when querying the data, which provides faster and more predictable performance than disk.
Applications where response time is critical, such as those running telecommunications network equipment and mobile advertising networks, often use main-memory databases. IMDBs have gained a lot of traction, especially in the data analytics space, starting in the mid-2000s - mainly due to less expensive RAM.
With the introduction of non-volatile random access memory technology, in-memory databases will be able to run at full speed and maintain data in the event of power failure.""
""Online analytical processing engines "",,,,""The Analytical Engine was a proposed mechanical general-purpose computer designed by English mathematician and computer pioneer Charles Babbage. It was first described in 1837 as the successor to Babbage's difference engine, a design for a mechanical computer. The Analytical Engine incorporated an arithmetic logic unit, control flow in the form of conditional branching and loops, and integrated memory, making it the first design for a general-purpose computer that could be described in modern terms as Turing-complete. In other words, the logical structure of the Analytical Engine was essentially the same as that which has dominated computer design in the electronic era.
Babbage was never able to complete construction of any of his machines due to conflicts with his chief engineer and inadequate funding. It was not until the 1940s that the first general-purpose computers were actually built, more than a century after Babbage had proposed the pioneering Analytical Engine in 1837.""
""Stream management "",,,,""A Data stream management system (DSMS) is a computer program to manage continuous data streams. It is similar to a database management system (DBMS), which is, however, designed for static data in conventional databases. A DSMS also offers a flexible query processing so that the information need can be expressed using queries. However, in contrast to a DBMS, a DSMS executes a continuous query that is not only performed once, but is permanently installed. Therefore, the query is continuously executed until it is explicitly uninstalled. Since most DSMS are data-driven, a continuous query produces new results as long as new data arrive at the system. This basic concept is similar to Complex event processing so that both technologies are partially coalescing.""
""Structured Query Language "",,,,""SQL (/ˈɛs kjuː ˈɛl/, or /ˈsiːkwəl/; Structured Query Language) is a special-purpose programming language designed for managing data held in a relational database management system (RDBMS), or for stream processing in a relational data stream management system (RDSMS).
Originally based upon relational algebra and tuple relational calculus, SQL consists of a data definition language, data manipulation language, and a data control language. The scope of SQL includes data insert, query, update and delete, schema creation and modification, and data access control. Although SQL is often described as, and to a great extent is, a declarative language (4GL), it also includes procedural elements.
SQL was one of the first commercial languages for Edgar F. Codd's relational model, as described in his influential 1970 paper, "A Relational Model of Data for Large Shared Data Banks." Despite not entirely adhering to the relational model as described by Codd, it became the most widely used database language.
SQL became a standard of the American National Standards Institute (ANSI) in 1986, and of the International Organization for Standardization (ISO) in 1987. Since then, the standard has been revised to include a larger set of features. Despite the existence of such standards, though, most SQL code is not completely portable among different database systems without adjustments.""
""XPath "",,,,""XPath, the XML Path Language, is a query language for selecting nodes from an XML document. In addition, XPath may be used to compute values (e.g., strings, numbers, or Boolean values) from the content of an XML document. XPath was defined by the World Wide Web Consortium (W3C).
^ "XML and Semantic Web W3C Standards Timeline" (PDF). 2012-02-04.""
""XQuery "",,,,""XQuery is a query and functional programming language that queries and transforms collections of structured and unstructured data, usually in the form of XML, text and with vendor-specific extensions for other data formats (JSON, binary, etc.). The language is developed by the XML Query working group of the W3C. The work is closely coordinated with the development of XSLT by the XSL Working Group; the two groups share responsibility for XPath, which is a subset of XQuery.
XQuery 1.0 became a W3C Recommendation on January 23, 2007.
XQuery 3.0 became a W3C Recommendation on April 8, 2014.

"The mission of the XML Query project is to provide flexible query facilities to extract data from real and virtual documents on the World Wide Web, therefore finally providing the needed interaction between the Web world and the database world. Ultimately, collections of XML files will be accessed like databases".""
""MapReduce languages "",,,,""Jaql (JAQL) is a functional data processing and query language most commonly used for JSON query processing on BigData.
It started as an Open Source project at Google but the latest release was on 7/12/2010. IBM took it over as primary data processing language for their Hadoop software package BigInsights.
Although having been developed for JSON it supports a variety of other data sources like CSV, TSV, XML.
A comparison to other BigData query languages like PIG Latin and Hive QL illustrates performance and usability aspects of these technologies.
JAQL supports Lazy Evaluation, so expressions are only materialized when needed.""
""Call level interfaces "",,,,""The Call Level Interface (CLI) is an application programming interface (API) and software standard to embed Structured Query Language (SQL) code in a host program as defined in a joint standard by the International Organization for Standardization (ISO) and International Electrotechnical Commission (IEC): ISO/IEC 9075-3:2003. The Call Level Interface defines how a program should send SQL queries to the database management system (DBMS) and how the returned recordsets should be handled by the application in a consistent way. Developed in the early 1990s, the API was defined only for the programming languages C and COBOL.
The interface is part of what The Open Group, publishes in a part of the X/Open Portability Guide, termed the Common Application Environment, which is intended to be a wide standard for programming open applications, i.e., applications from different programming teams and different vendors that can interoperate efficiently. SQL/CLI provides an international standard implementation-independent CLI to access SQL databases. Client-server tools can easily access databases through dynamic-link libraries (DLL). It supports and encourages a rich set of client-server tools.
The most widespread use of the CLI standard is the basis of the Open Database Connectivity (ODBC) specification, which is widely used to allow applications to transparently access database systems from different vendors. The current version of the API, ODBC 3.52, incorporates features from both the ISO and X/Open standards. Examples of languages that support Call Level Interface are ANSI C, C#, Visual Basic .NET (VB.NET), Java, Pascal, and Fortran.""
""Database utilities and tools "",,,,""PC Tools was a collection of software utilities developed by Central Point Software.""
""Database performance evaluation "",,,,""The International Conference on Systems Engineering (ICSEng) is the series of International Conferences, jointly organized on a rotational basis among three institutions:
University of Nevada Las Vegas, USA - International Conference on Systems Engineering (ICSEng)
Wrocław University of Technology, Poland - International Conference on Systems Science (ICSS)
Coventry University, United Kingdom - International Conference on Systems Engineering (ICSE)
Conference covers the Systems Engineering with the focus on applications and was first held in 1974 in Wrocław (Poland) as 1st ICSS. In a current form it was founded by Zdzisław Bubnicki, William Wells and Glyn James. The 23rd edition of ICSEng will be held in Las Vegas, USA 
^ Transactions of the Institute of Measurement and Control, 2001, Editorial
^ Niederliński, Antoni (2006). "Professor Zdzisław Bubnicki in my memory", Control and Cybernetics, vol. 35, no. 2, 2006 (source text)
^ nature.com, Nature Events Directory, (posting )""
""Autonomous database administration "",,,,""Khanty-Mansi Autonomous Okrug — Yugra or Khanty-Mansiysk Autonomous Okrug – Ugra (Russian: Ха́нты-Манси́йский автоно́мный о́круг — Югра́, Khanty-Mansiysky avtonomny okrug – Yugra), is a federal subject of Russia (an autonomous okrug of Tyumen Oblast). Population: 1,532,243 (2010 Census).
The people native to the region are the Khanty and the Mansi, known collectively as Ob Ugric people. The local languages, Khanty language and Mansi language, enjoy special status in the autonomous okrug and along with their distant relative Hungarian are part of the Ugric branch of the Finno-Ugric languages. Russian remains the only official language.
In 2012, the majority (51%) of the oil produced in Russia comes from Khanty–Mansi Autonomous Okrug, giving the region great economic importance.""
""Data dictionaries "",,,,""A data dictionary, or Metadata Repository, as defined in the IBM Dictionary of Computing, is a "centralized repository of information about data such as meaning, relationships to other data, origin, usage, and format." The term can have one of several closely related meanings pertaining to databases and database management systems (DBMS):
A document describing a database or collection of databases
An integral component of a DBMS that is required to determine its structure
A piece of middleware that extends or supplants the native data dictionary of a DBMS""
""Extraction, transformation and loading "",,,,""In computing, Extract, Transform and Load (ETL) refers to a process in database usage and especially in data warehousing that:
Extracts data from homogeneous or heterogeneous data sources
Transforms the data for storing it in the proper format or structure for the purposes of querying and analysis
Loads it into the final target (database, more specifically, operational data store, data mart, or data warehouse)
Usually all the three phases execute in parallel since the data extraction takes time, so while the data is being pulled another transformation process executes, processing the already received data and prepares the data for loading and as soon as there is some data ready to be loaded into the target, the data loading kicks off without waiting for the completion of the previous phases.
ETL systems commonly integrate data from multiple applications (systems), typically developed and supported by different vendors or hosted on separate computer hardware. The disparate systems containing the original data are frequently managed and operated by different employees. For example, a cost accounting system may combine data from payroll, sales, and purchasing.""
""Data exchange "",,,,""Data exchange is the process of taking data structured under a source schema and actually transforming it into data structured under a target schema, so that the target data is an accurate representation of the source data. Data exchange is similar to the related concept of data integration except that data is actually restructured (with possible loss of content) in data exchange. There may be no way to transform an instance given all of the constraints. Conversely, there may be numerous ways to transform the instance (possibly infinitely many), in which case a "best" choice of solutions has to be identified and justified.""
""Data cleaning "",,,,""Data cleansing, data cleaning or data scrubbing is the process of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database. Used mainly in databases, the term refers to identifying incomplete, incorrect, inaccurate, irrelevant, etc. parts of the data and then replacing, modifying, or deleting this dirty data or coarse data. Data cleansing may be performed interactively with data wrangling tools, or as batch processing through scripting.
After cleansing, a data set will be consistent with other similar data sets in the system. The inconsistencies detected or removed may have been originally caused by user entry errors, by corruption in transmission or storage, or by different data dictionary definitions of similar entities in different stores.
Data cleansing differs from data validation in that validation almost invariably means data is rejected from the system at entry and is performed at entry time, rather than on batches of data.
The actual process of data cleansing may involve removing typographical errors or validating and correcting values against a known list of entities. The validation may be strict (such as rejecting any address that does not have a valid postal code) or fuzzy (such as correcting records that partially match existing, known records).
Some data cleansing solutions will clean data by cross checking with a validated data set. Also data enhancement, where data is made more complete by adding related information, is a common data cleansing practice. For example, appending addresses with phone numbers related to that address.
Data cleansing may also involve activities like, harmonization of data, and standardization of data. For example, harmonization of short codes (st, rd, etc.) to actual words (street, road, etcetera). Standardization of data is a means of changing a reference data set to a new standard, ex, use of standard codes.

""
""Wrappers (data mining) "",,,,""Wrapper in data mining is a program that extracts content of a particular information source and translates it into a relational form. Many web pages present structured data - telephone directories, product catalogs, etc. formatted for human browsing using HTML language. Structured data are typically descriptions of objects retrieved from underlying databases and displayed in Web pages following some fixed templates. Software systems using such resources must translate HTML content into a relational form. Wrappers are commonly used as such translators. Formally, a wrapper is a function from a page to the set of tuples it contains.
^ Nicholas Kushmerick, Daniel S. Weld, Robert Doorenbos, Wrapper Induction for Information Extraction Proceedings of the International Joint Conference on Artificial Intelligence, 1997""
""Mediators and data integration "",,,,""Query Rewriting is a technique used in mediation based data integration systems for translating the queries formulated over the mediated schema to a query over the various sources by making use of the view definitions. Mediation based data integration system hides from the end user the underlying heterogeneity of the various data providing sources linked to it by providing a uniform query interface in the form of a mediated schema. This schema is also referred to as the global schema whereas the schema of the various data sources is collectively referred to as the local schema. The local schema and the mediated schema are mapped to each other using view definitions. The queries formulated on the mediated schema cannot be directly used to query the sources. Therefore query rewriting translates such a query formulated over the global schema to a query over the various data sources. Examples include bucket algorithm, Minicon algorithm, inverse rules algorithm. This rewritten query is then evaluated to obtain the query response making use of the data obtained by querying the data sources.""
""Entity resolution "",,,,""Record linkage (RL) refers to the task of finding records in a data set that refer to the same entity across different data sources (e.g., data files, books, websites, databases). Record linkage is necessary when joining data sets based on entities that may or may not share a common identifier (e.g., database key, URI, National identification number), as may be the case due to differences in record shape, storage location, and/or curator style or preference. A data set that has undergone RL-oriented reconciliation may be referred to as being cross-linked. Record Linkage is called Data Linkage in many jurisdictions, but is the same process.""
""Data warehouses "",,,,""In computing, a data warehouse (DW or DWH), also known as an enterprise data warehouse (EDW), is a system used for reporting and data analysis. DWs are central repositories of integrated data from one or more disparate sources. They store current and historical data and are used for creating analytical reports for knowledge workers throughout the enterprise. Examples of reports could range from annual and quarterly comparisons and trends to detailed daily sales analysis.
The data stored in the warehouse is uploaded from the operational systems (such as marketing, sales, etc., shown in the figure to the right). The data may pass through an operational data store for additional operations before it is used in the DW for reporting.""
""Federated databases "",,,,""A federated database system is a type of meta-database management system (DBMS), which transparently maps multiple autonomous database systems into a single federated database. The constituent databases are interconnected via a computer network and may be geographically decentralized. Since the constituent database systems remain autonomous, a federated database system is a contrastable alternative to the (sometimes daunting) task of merging several disparate databases. A federated database, or virtual database, is a composite of all constituent databases in a federated database system. There is no actual data integration in the constituent disparate databases as a result of data federation.
Through data abstraction, federated database systems can provide a uniform user interface, enabling users and clients to store and retrieve data from multiple noncontiguous databases with a single query—even if the constituent databases are heterogeneous. To this end, a federated database system must be able to decompose the query into subqueries for submission to the relevant constituent DBMS's, after which the system must composite the result sets of the subqueries. Because various database management systems employ different query languages, federated database systems can apply wrappers to the subqueries to translate them into the appropriate query languages.""
""Application servers "",,,,""An application server is a software framework that provides both facilities to create web applications and a server environment to run them.
Application Server Frameworks contain a comprehensive service layer model. An application server acts as a set of components accessible to the software developer through an API defined by the platform itself. For Web applications, these components are usually performed in the same running environment as their web server(s), and their main job is to support the construction of dynamic pages. However, many application servers target much more than just Web page generation: they implement services like clustering, fail-over, and load-balancing, so developers can focus on implementing the business logic.
In the case of Java application servers, the server behaves like an extended virtual machine for running applications, transparently handling connections to the database on one side, and, often, connections to the Web client on the other.
Other uses of the term may refer to the services that a server makes available or the computer hardware on which the services run.
^ "What is an App Server?". theserverside.com. Retrieved 2010-02-28. an application server provides an environment where applications can run, no matter what the applications are or what they do 
^ Stefano Ceri, Piero Fraternali, Aldo Bongio, Marco Brambilla, Sara Comai, Maristella Matella. "Designing Data Intensive Web Applications, 2Q03".""
""Object-relational mapping facilities "",,,,""This is a list of well-known object-relational mapping software. It is not up-to-date or all-inclusive.""
""Data federation tools "",,,,""The Federation of American Scientists (FAS) is a 501(c)(3) organization with the stated intent of using science and scientific analysis to attempt to make the world more secure. FAS was founded in 1945 by scientists who worked on the Manhattan Project to develop the first atomic bombs.
With 100 sponsors, FAS claims that it promotes a safer and more secure world by developing and advancing solutions to important science and technology security policy problems by educating the public and policy makers, and promoting transparency through research and analysis to maximize impact on policy. FAS projects are organized in three main programs: Nuclear Security, Government Secrecy and Biosecurity.""
""Data replication tools "",,,,""Self-replication is any behavior of a dynamical system that yields construction of an identical copy of itself. Biological cells, given suitable environments, reproduce by cell division. During cell division, DNA is replicated and can be transmitted to offspring during reproduction. Biological viruses can replicate, but only by commandeering the reproductive machinery of cells through a process of infection. Harmful prion proteins can replicate by converting normal proteins into rogue forms. Computer viruses reproduce using the hardware and software already present on computers. Self-replication in robotics has been an area of research and a subject of interest in science fiction. Any self-replicating mechanism which does not make a perfect copy will experience genetic variation and will create variants of itself. These variants will be subject to natural selection, since some will be better at surviving in their current environment than others and will out-breed them.""
""Message queues "",,,,""Microsoft Message Queuing or MSMQ is a message queue implementation developed by Microsoft and deployed in its Windows Server operating systems since Windows NT 4 and Windows 95. The latest Windows 8 also includes this component. In addition to its mainstream server platform support, MSMQ has been incorporated into Microsoft Embedded platforms since 1999 and the release of Windows CE 3.0.""
""Service buses "",,,,""An enterprise service bus (ESB) is a software architecture model used for designing and implementing communication between mutually interacting software applications in a service-oriented architecture (SOA). As a software architectural model for distributed computing, it is a specialty variant of the more general client server model and promotes agility and flexibility with regard to communication between applications. Its primary use is in enterprise application integration (EAI) of heterogeneous and complex landscapes.""
""Enterprise application integration tools "",,,,""Business performance management is a set of management and analytic processes that enables the management of an organization's performance to achieve one or more pre-selected goals. Synonyms for "business performance management" include "corporate performance management (CPM)" and "enterprise performance management".
Business performance management is contained within approaches to business process management.
Business performance management has three main activities:
selection of goals,
consolidation of measurement information relevant to an organization’s progress against these goals, and
interventions made by managers in light of this information with a view to improving future performance against these goals.
Although presented here sequentially, typically all three activities will run concurrently, with interventions by managers affecting the choice of goals, the measurement information monitored, and the activities being undertaken by the organization.
Because business performance management activities in large organizations often involve the collation and reporting of large volumes of data, many software vendors, particularly those offering business intelligence tools, market products intended to assist in this process. As a result of this marketing effort, business performance management is often incorrectly understood as an activity that necessarily relies on software systems to work, and many definitions of business performance management explicitly suggest software as being a definitive component of the approach.
This interest in business performance management from the software community is sales-driven - "The biggest growth area in operational BI analysis is in the area of business performance management."
Since 1992, business performance management has been strongly influenced by the rise of the balanced scorecard framework. It is common for managers to use the balanced scorecard framework to clarify the goals of an organization, to identify how to track them, and to structure the mechanisms by which interventions will be triggered. These steps are the same as those that are found in BPM, and as a result balanced scorecard is often used as the basis for business performance management activity with organizations.
In the past, owners have sought to drive strategy down and across their organizations, transform these strategies into actionable metrics and use analytics to expose the cause-and-effect relationships that, if understood, could give insight into decision-making.

""
""Magnetic disks "",,,,""Magnetic storage or magnetic recording is the storage of data on a magnetised medium. Magnetic storage uses different patterns of magnetisation in a magnetisable material to store data and is a form of non-volatile memory. The information is accessed using one or more read/write heads.
As of 2013, magnetic storage media, primarily hard disks, are widely used to store computer data as well as audio and video signals. In the field of computing, the term magnetic storage is preferred and in the field of audio and video production, the term magnetic recording is more commonly used. The distinction is less technical and more a matter of preference. Other examples of magnetic storage media include floppy disks, magnetic recording tape, and magnetic stripes on credit cards.""
""Magnetic tapes "",,,,""Magnetic tape is a medium for magnetic recording, made of a thin magnetizable coating on a long, narrow strip of plastic film. It was developed in Germany, based on magnetic wire recording. Devices that record and play back audio and video using magnetic tape are tape recorders and video tape recorders. A device that stores computer data on magnetic tape is a tape drive (tape unit, streamer).
Magnetic tape revolutionized broadcast and recording. When all radio was live, it allowed programming to be recorded. At a time when gramophone records were recorded in one take, it allowed recordings to be made in multiple parts, which were then mixed and edited with tolerable loss in quality. It is a key technology in early computer development, allowing unparalleled amounts of data to be mechanically created, stored for long periods, and to be rapidly accessed.
Nowadays other technologies can perform the functions of magnetic tape. In many cases these technologies are replacing tape. Despite this, innovation in the technology continues and companies like Sony and IBM continue to produce new magnetic tape drives.
Over years, magnetic tape can suffer from deterioration called sticky-shed syndrome. Caused by absorption of moisture into the binder of the tape, it can render the tape unusable.

""
""Flash memory "",,,,""Flash memory is an electronic non-volatile computer storage medium that can be electrically erased and reprogrammed.
Introduced by Toshiba in 1984, flash memory was developed from EEPROM (electrically erasable programmable read-only memory). There are two main types of flash memory, which are named after the NAND and NOR logic gates. The individual flash memory cells exhibit internal characteristics similar to those of the corresponding gates.
Whereas EPROMs had to be completely erased before being rewritten, NAND type flash memory may be written and read in blocks (or pages) which are generally much smaller than the entire device. NOR type flash allows a single machine word (byte) to be written—​​to an erased location—​​or read independently.
The NAND type is primarily used in memory cards, USB flash drives, solid-state drives (those produced in 2009 or later), and similar products, for general storage and transfer of data. NAND or NOR flash memory is also often used to store configuration data in numerous digital products, a task previously made possible by EEPROM or battery-powered static RAM. One key disadvantage of flash memory is that it can endure relatively small number of write cycles in a specific block.
Example applications of both types of flash memory include personal computers, PDAs, digital audio players, digital cameras, mobile phones, synthesizers, video games, scientific instrumentation, industrial robotics, medical electronics, and so on. In addition to being non-volatile, flash memory offers fast read access times, as fast as dynamic RAM, although not as fast as static RAM or ROM. Its mechanical shock resistance helps explain its popularity over hard disks in portable devices, as does its high durability, being able to withstand high pressure, temperature, immersion in water, etc.
Although flash memory is technically a type of EEPROM, the term "EEPROM" is generally used to refer specifically to non-flash EEPROM which is erasable in small blocks, typically bytes. Because erase cycles are slow, the large block sizes used in flash memory erasing give it a significant speed advantage over non-flash EEPROM when writing large amounts of data. As of 2013, flash memory costs much less than byte-programmable EEPROM and has become the dominant memory type wherever a system requires a significant amount of non-volatile, solid-state storage.""
""Phase change memory "",,,,""Phase-change memory (also known as PCM, PCME, PRAM, PCRAM, Ovonic Unified Memory, Chalcogenide RAM and C-RAM) is a type of non-volatile random-access memory. PRAMs exploit the unique behaviour of chalcogenide glass. In the older generation of PCM heat produced by the passage of an electric current through a heating element generally made of TiN would be used to either quickly heat and quench the glass, making it amorphous, or to hold it in its crystallization temperature range for some time, thereby switching it to a crystalline state. PCM also has the ability to achieve a number of distinct intermediary states, thereby having the ability to hold multiple bits in a single cell, but the difficulties in programming cells in this way has prevented these capabilities from being implemented in other technologies (most notably flash memory) with the same capability. Newer PCM technology has been trending in two different directions. One group have been directing a lot of research towards attempting to find viable material alternatives to Ge2Sb2Te5 (GST), with mixed success. Another have developed the use of a GeTe - Sb2Te3 superlattice to achieve non-thermal phase changes by simply changing the co-ordination state of the Germanium atoms with a laser pulse. This new Interfacial Phase Change Memory (IPCM) has had many successes and continues to be the site of much active research.
Leon Chua has argued that all two-terminal non-volatile memory devices, including PCM, should be considered memristors. Stan Williams of HP Labs has also argued that PCM should be considered a memristor. However, this terminology has been challenged and the potential applicability of memristor theory to any physically realizable device is open to question.""
""Disk arrays "",,,,""Typically a disk array provides increased availability, resiliency, and maintainability by using existing components (controllers, power supplies, fans, etc.), often up to the point where all single points of failure (SPOFs) are eliminated from the design. Additionally, disk array components are often hot-swappable.
Typically, disk arrays are divided into categories:
Network attached storage (NAS) arrays
Storage area network (SAN) arrays:
Modular SAN arrays
Monolithic SAN arrays
Utility Storage Arrays

Storage virtualization
Primary vendors of storage systems include Coraid, Inc., DataDirect Networks, Dell, EMC Corporation, Hewlett-Packard, Hitachi Data Systems, IBM, Infortrend, NetApp, Oracle Corporation, Panasas and other companies that often act as OEMs for the above vendors and do not themselves market the storage components they manufacture.""
""Tape libraries "",,,,""In computer storage, a tape library, sometimes called a tape silo, tape robot or tape jukebox, is a storage device which contains one or more tape drives, a number of slots to hold tape cartridges, a barcode reader to identify tape cartridges and an automated method for loading tapes (a robot).
One of the earliest examples was the IBM 3850 Mass Storage System (MSS), announced in 1974.""
""Heap (data structure) "",,,,""In computer science, a heap is a specialized tree-based data structure that satisfies the heap property: If A is a parent node of B then the key of node A is ordered with respect to the key of node B with the same ordering applying across the heap. A heap can be classified further as either a "max heap" or a "min heap". In a max heap, the keys of parent nodes are always greater than or equal to those of the children and the highest key is in the root node. In a min heap, the keys of parent nodes are less than or equal to those of the children and the lowest key is in the root node. Heaps are crucial in several efficient graph algorithms such as Dijkstra's algorithm, and in the sorting algorithm heapsort. A common implementation of a heap is the binary heap, in which the tree is a complete binary tree (see figure).
In a heap, the highest (or lowest) priority element is always stored at the root. A heap is not a sorted structure and can be regarded as partially ordered. As visible from the heap-diagram, there is no particular relationship among nodes on any given level, even among the siblings. When a heap is a complete binary tree, it has a smallest possible height—a heap with N nodes always has log N height. A heap is a useful data structure when you need to remove the object with the highest (or lowest) priority.
Note that, as shown in the graphic, there is no implied ordering between siblings or cousins and no implied sequence for an in-order traversal (as there would be in, e.g., a binary search tree). The heap relation mentioned above applies only between nodes and their parents, grandparents, etc. The maximum number of children each node can have depends on the type of heap, but in many types it is at most two, which is known as a binary heap.
The heap is one maximally efficient implementation of an abstract data type called a priority queue, and in fact priority queues are often referred to as "heaps", regardless of how they may be implemented.
A heap data structure should not be confused with the heap which is a common name for the pool of memory from which dynamically allocated memory is allocated. The term was originally used only for the data structure.""
""Indexed file organization "",,,,""ISAM stands for Indexed Sequential Access Method, a method for indexing data for fast retrieval. ISAM was originally developed by IBM for mainframe computers. Today the term is used for several related concepts:
Specifically, the IBM ISAM product and the algorithm it employs.
A database system where an application developer directly uses an application programming interface to search indexes in order to locate records in data files. In contrast, a relational database uses a query optimizer which automatically selects indexes.
An indexing algorithm that allows both sequential and keyed access to data. Most databases use some variation of the B-Tree for this purpose, although the original IBM ISAM and VSAM implementations did not do so.
Most generally, any index for a database. Indexes are used by almost all databases, both relational and otherwise.
In an ISAM system, data is organized into records which are composed of fixed length fields. Records are stored sequentially, originally to speed access on a tape system. A secondary set of hash tables known as indexes contain "pointers" into the tables, allowing individual records to be retrieved without having to search the entire data set. This is a departure from the contemporaneous navigational databases, in which the pointers to other data were stored inside the records themselves. The key improvement in ISAM is that the indexes are small and can be searched quickly, thereby allowing the database to access only the records it needs. Additionally modifications to the data do not require changes to other data, only the table and indexes in question.
When an ISAM file is created, index nodes are fixed, and their pointers do not change during inserts and deletes that occur later (only content of leaf nodes change afterwards). As a consequence of this, if inserts to some leaf node exceed the node's capacity, new records are stored in overflow chains. If there are many more inserts than deletions from a table, these overflow chains can gradually become very large, and this affects the time required for retrieval of a record.
Relational databases can easily be built on an ISAM framework with the addition of logic to maintain the validity of the links between the tables. Typically the field being used as the link, the foreign key, will be indexed for quick lookup. While this is slower than simply storing the pointer to the related data directly in the records, it also means that changes to the physical layout of the data do not require any updating of the pointers—the entry will still be valid.
ISAM is very simple to understand and implement, as it primarily consists of direct, sequential access to a database file. It is also very inexpensive. The tradeoff is that each client machine must manage its own connection to each file it accesses. This, in turn, leads to the possibility of conflicting inserts into those files, leading to an inconsistent database state. This is typically solved with the addition of a client-server framework which marshals client requests and maintains ordering. This is the basic concept behind a database management system (DBMS), which is a client layer over the underlying data store.
ISAM was replaced at IBM with a methodology called VSAM (Virtual Storage Access Method). Still later, IBM developed DB2 which, as of 2004, IBM promotes as their primary database management system. VSAM is the physical access method used in DB2.
The OpenVMS operating system uses the Files-11 file system in conjunction with RMS (Record Management Services). RMS provides an additional layer between the application and the files on disk that provides a consistent method of data organization and access across multiple 3GL and 4GL languages. RMS provides 4 different methods of accessing data; Sequential, Relative Record Number Access, Record File Address Access, and Indexed Access.
The Indexed Access method of reading or writing data only provides the desired outcome if in fact the file is organized as an ISAM file with the appropriate, previously defined keys. Access to data via the previously defined key(s) is extremely fast. Multiple keys, overlapping keys and key compression within the hash tables are supported. A utility to define/redefine keys in existing files is provided. Records can be deleted, although "garbage collection" is done via a separate utility.
^ Chin, Y.H. (1975). "Analysis of VSAM's free-space behavior". VLDB '75: Proceedings of the 1st International Conference on Very Large Data Bases: 514–515. 
^ Bogue, Robert L. (2004-02-13). "Explore the differences between ISAM and relational databases". Retrieved 17 October 2014. 
^ Larson, Per-Åke (1981). "Analysis of index-sequential files with overflow chaining". ACM Transactions on Database Systems 6 (4). 
^ Ramakrishnan Raghu, Gehrke Johannes - Database Management Systems, McGraw-Hill Higher Education (2000), 2nd edition (en) page 252""
""Linked lists "",,,,""In computer science, a linked list is a linear collection of data elements, called nodes pointing to the next node by means of pointer. It is a data structure consisting of a group of nodes which together represent a sequence. Under the simplest form, each node is composed of data and a reference (in other words, a link) to the next node in the sequence; more complex variants add additional links. This structure allows for efficient insertion or removal of elements from any position in the sequence.

Linked lists are among the simplest and most common data structures. They can be used to implement several other common abstract data types, including lists (the abstract data type), stacks, queues, associative arrays, and S-expressions, though it is not uncommon to implement the other data structures directly without using a list as the basis of implementation.
The principal benefit of a linked list over a conventional array is that the list elements can easily be inserted or removed without reallocation or reorganization of the entire structure because the data items need not be stored contiguously in memory or on disk, while an array has to be declared in the source code, before compiling and running the program. Linked lists allow insertion and removal of nodes at any point in the list, and can do so with a constant number of operations if the link previous to the link being added or removed is maintained during list traversal.
On the other hand, simple linked lists by themselves do not allow random access to the data, or any form of efficient indexing. Thus, many basic operations — such as obtaining the last node of the list (assuming that the last node is not maintained as separate node reference in the list structure), or finding a node that contains a given datum, or locating the place where a new node should be inserted — may require sequential scanning of most or all of the list elements. The advantages and disadvantages of using linked lists are given below.

""
""B-trees "",,,,""In computer science, a B-tree is a self-balancing tree data structure that keeps data sorted and allows searches, sequential access, insertions, and deletions in logarithmic time. The B-tree is a generalization of a binary search tree in that a node can have more than two children (Comer 1979, p. 123). Unlike self-balancing binary search trees, the B-tree is optimized for systems that read and write large blocks of data. B-trees are a good example of a data structure for external memory. It is commonly used in databases and filesystems.""
""Vnodes "",,,,""A virtual file system (VFS) or virtual filesystem switch is an abstraction layer on top of a more concrete file system. The purpose of a VFS is to allow client applications to access different types of concrete file systems in a uniform way. A VFS can, for example, be used to access local and network storage devices transparently without the client application noticing the difference. It can be used to bridge the differences in Windows, Mac OS and Unix filesystems, so that applications can access files on local file systems of those types without having to know what type of file system they are accessing.
A VFS specifies an interface (or a "contract") between the kernel and a concrete file system. Therefore, it is easy to add support for new file system types to the kernel simply by fulfilling the contract. The terms of the contract might change incompatibly from release to release, which would require that concrete file system support be recompiled, and possibly modified before recompilation, to allow it to work with a new release of the operating system; or the supplier of the operating system might make only backward-compatible changes to the contract, so that concrete file system support built for a given release of the operating system would work with future versions of the operating system.""
""Inodes "",,,,""In a Unix-style file system, the inode is a data structure used to represent a filesystem object, which can be one of various things including a file or a directory. Each inode stores the attributes and disk block location(s) of the filesystem object's data. Filesystem object attributes may include manipulation metadata (e.g. change, access, modify time), as well as owner and permission data (e.g. group-id, user-id, permissions).
Directories are lists of names assigned to inodes. The directory contains an entry for itself, its parent, and each of its children.

""
""Slotted pages "",,,,""The Delaney Card (a.k.a. Visual Seating Plan) is a method of classroom management. This small one-by-three-inch card has been used extensively in the New York metropolitan area since the 1950s. Each Delaney Card contains the name of one student in class, and lists their names, telephone numbers, addresses and other vital information of each student.

""
""Fixed length attributes "",,,,""Ganesha (/ɡəˈneɪʃə/; Sanskrit: गणेश, Gaṇeśa;  listen ), also known as Ganapati and Vinayaka, is one of the best-known and most worshipped deities in the Hindu pantheon. His image is found throughout India, Sri Lanka and Nepal. Hindu sects worship him regardless of affiliations. Devotion to Ganesha is widely diffused and extends to Jains and Buddhists.
Although he is known by many attributes, Ganesha's elephant head makes him easy to identify. Ganesha is widely revered as the remover of obstacles, the patron of arts and sciences and the deva of intellect and wisdom. As the god of beginnings, he is honoured at the start of rituals and ceremonies. Ganesha is also invoked as patron of letters and learning during writing sessions. Several texts relate mythological anecdotes associated with his birth and exploits and explain his distinct iconography.
Ganesha emerged as a distinct deity in the 4th and 5th centuries CE, during the Gupta Period, although he inherited traits from Vedic and pre-Vedic precursors. He was formally included among the five primary deities of Smartism (a Hindu denomination) in the 9th century. A sect of devotees called the Ganapatya arose, who identified Ganesha as the supreme deity. The principal scriptures dedicated to Ganesha are the Ganesha Purana, the Mudgala Purana, and the Ganapati Atharvashirsa.""
""Variable length attributes "",,,,""NTFS (New Technology File System) is a proprietary file system developed by Microsoft. Starting with Windows NT 3.1, it is the default file system of Windows NT family.
NTFS has several technical improvements over FAT and HPFS (High Performance File System), the file systems that it superseded, such as improved support for metadata, and the use of advanced data structures to improve performance, reliability, and disk space utilization, plus additional extensions, such as security access control lists (ACL) and file system journaling.""
""Null values in records "",,,,""Null is a special marker used in Structured Query Language (SQL) to indicate that a data value does not exist in the database. Introduced by the creator of the relational database model, E. F. Codd, SQL Null serves to fulfill the requirement that all true relational database management systems (RDBMS) support a representation of "missing information and inapplicable information". Codd also introduced the use of the lowercase Greek omega (ω) symbol to represent Null in database theory. NULL is also an SQL reserved keyword used to identify the Null special marker.
For people new to the subject, a good way to remember what null means is to remember that in terms of information, "lack of a value" is not the same thing as "a value of zero"; similarly, "lack of an answer" is not the same thing as "an answer of no". For example, consider the question "How many books does Juan own?" The answer may be "zero" (we know that he owns none) or "null" (we do not know how many he owns, or doesn't own). In a database table, the column reporting this answer would start out with no value (marked by Null), and it would not be updated with the value "zero" until we have ascertained that Juan owns no books.
SQL null is a state (unknown) and not a value. This usage is quite different from most programming languages, where null means not assigned to a particular instance.

""
""Horizontal partitioning "",,,,""A partition is a division of a logical database or its constituent elements into distinct independent parts. Database partitioning is normally done for manageability, performance or availability reasons, as for load balancing.""
""Vertical partitioning "",,,,""A partition is a division of a logical database or its constituent elements into distinct independent parts. Database partitioning is normally done for manageability, performance or availability reasons, as for load balancing.""
""Column based storage "",,,,""A correlation database is a database management system (DBMS) that is data-model-independent and designed to efficiently handle unplanned, ad hoc queries in an analytical system environment.
Unlike row-oriented relational database management systems, which use a records-based storage approach, or column-oriented databases which use a column-based storage method, a correlation database uses a value-based storage (VBS) architecture in which each unique data value is stored only once and an auto-generated indexing system maintains the context for all values.
^ Raab, David M."Analytical Database Options". Information Management Magazine 1 July 2008.""
""Hybrid storage layouts "",,,,""A hybrid vehicle uses two or more distinct types of power, such as internal combustion engine+electric motor, e.g. in diesel-electric trains using diesel engines and electricity from overhead lines, and submarines that use diesels when surfaced and batteries when submerged. Other means to store energy include pressurized fluid, in hydraulic hybrids.""
""RAID "",,,,""RAID (originally redundant array of inexpensive disks, now commonly redundant array of independent disks) is a data storage virtualization technology that combines multiple physical disk drive components into a single logical unit for the purposes of data redundancy, performance improvement, or both.
Data is distributed across the drives in one of several ways, referred to as RAID levels, depending on the required level of redundancy and performance. The different schemas, or data distribution layouts, are named by the word RAID followed by a number, for example RAID 0 or RAID 1. Each schema, or a RAID level, provides a different balance among the key goals: reliability, availability, performance, and capacity. RAID levels greater than RAID 0 provide protection against unrecoverable sector read errors, as well as against failures of whole physical drives.""
""Remote replication "",,,,""RIS, Remote Installation Services is a Microsoft-supplied server that allows PXE BIOS-enabled computers to remotely execute boot environment variables.
These variables are likely computers that are on a company's (or that company's client's) network. RIS is used to create installation images of operating systems or computer configurations, which can be used to demonstrate the installation process to users whose machines have been granted access to the RIS server. This eliminates the need to use a CD-ROM for installing an operating system. 
^ "Remote Installation Services". Microsoft. Retrieved 2012-07-03.""
""Cloud based storage "",,,,""A file hosting service, cloud storage service, online file storage provider, or cyberlocker is an Internet hosting service specifically designed to host user files. It allows users to upload files that could then be accessed over the internet from a different computer, tablet, smart phone or other networked device, by the same user or possibly by other users, after a password or other authentication is provided. Typically, the services allow HTTP access, and sometimes FTP access. Related services are content-displaying hosting services (i.e. video and image), virtual storage, and remote backup.""
""Storage area networks "",,,,""A storage area network (SAN) is a network which provides access to consolidated, block level data storage. SANs are primarily used to enhance storage devices, such as disk arrays, tape libraries, and optical jukeboxes, accessible to servers so that the devices appear to the operating system as locally attached devices. A SAN typically has its own network of storage devices that are generally not accessible through the local area network (LAN) by other devices. The cost and complexity of SANs dropped in the early 2000s to levels allowing wider adoption across both enterprise and small to medium-sized business environments.
A SAN does not provide file abstraction, only block-level operations. However, file systems built on top of SANs do provide file-level access, and are known as shared-disk file systems.""
""Direct attached storage "",,,,""Direct-attached storage (DAS) is digital storage directly attached to the computer accessing it, as opposed to storage accessed over a computer network. Examples of DAS include hard drives, optical disc drives, and storage on external drives. The name "DAS" is a retronym to contrast with storage area network (SAN) and network-attached storage (NAS).""
""Network attached storage "",,,,""Network-attached storage (NAS) is a file-level computer data storage server connected to a computer network providing data access to a heterogeneous group of clients. NAS is specialized for serving files either by its hardware, software, or configuration. It is often manufactured as a computer appliance – a purpose-built specialized computer. NAS systems are networked appliances which contain one or more storage drives, often arranged into logical, redundant storage containers or RAID.
Network-attached storage removes the responsibility of file serving from other servers on the network. They typically provide access to files using network file sharing protocols such as NFS, SMB/CIFS, or AFP. As of 2010, NAS devices began gaining popularity as a convenient method of sharing files among multiple computers. Potential benefits of dedicated network-attached storage, compared to general-purpose servers also serving files, include faster data access, easier administration, and simple configuration.
The hard disk drives with "NAS" in their name are functionally similar to other drives but may have different firmware, vibration tolerance, or power dissipation to make them more suitable for use in RAID arrays, which are sometimes used in NAS implementations. For example, some NAS versions of drives support a command extension to allow extended error recovery to be disabled. In a non-RAID application, it may be important for a disk drive to go to great lengths to successfully read a problematic storage block, even if it takes several seconds. In an appropriately configured RAID array, a single bad block on a single drive can be recovered completely via the redundancy encoded across the RAID set. If a drive spends several seconds executing extensive retries it might cause the RAID controller to flag the drive as "down" whereas if it simply replied promptly that the block of data had a checksum error, the RAID controller would use the redundant data on the other drives to correct the error and continue without any problem. Such a "NAS" SATA hard disk drive can be used as an internal PC hard drive, without any problems or adjustments needed, as it simply supports additional options and may possibly be built to a higher quality standard (particularly if accompanied by a higher quoted MTBF figure and higher price) than a regular consumer drive.""
""Distributed storage "",,,,""A clustered file system is a file system which is shared by being simultaneously mounted on multiple servers. There are several approaches to clustering, most of which do not employ a clustered file system (only direct attached storage for each node). Clustered file systems can provide features like location-independent addressing and redundancy which improve reliability or reduce the complexity of the other parts of the cluster. Parallel file systems are a type of clustered file system that spread data across multiple storage nodes, usually for redundancy or performance.""
""Hierarchical storage management "",,,,""Hierarchical storage management (HSM) is a data storage technique, which automatically moves data between high-cost and low-cost storage media. HSM systems exist because high-speed storage devices, such as hard disk drive arrays, are more expensive (per byte stored) than slower devices, such as optical discs and magnetic tape drives. While it would be ideal to have all data available on high-speed devices all the time, this is prohibitively expensive for many organizations. Instead, HSM systems store the bulk of the enterprise's data on slower devices, and then copy data to faster disk drives when needed. In effect, HSM turns the fast disk drives into caches for the slower mass storage devices. The HSM system monitors the way data is used and makes best guesses as to which data can safely be moved to slower devices and which data should stay on the fast devices.
HSM may also be used where more robust storage is available for long-term archiving, but this is slow to access. This may be as simple as an off-site backup, for protection against a building fire.
HSM is a long-established concept, dating back to the beginnings of commercial data processing. The techniques used though have changed significantly as new technology becomes available, for both storage and for long-distance communication of large data sets. The scale of measures such as 'size' and 'access time' have changed dramatically. Despite this, many of the underlying concepts keep returning to favour years later, although at much larger or faster scales.

""
""Storage virtualization "",,,,""In computer science, storage virtualization uses virtualization to enable better functionality and more advanced features in computer data storage systems.
Broadly speaking, a "storage system" is also known as a storage array or disk array or a filer. Storage systems typically use special hardware and software along with disk drives in order to provide very fast and reliable storage for computing and data processing. Storage systems are complex, and may be thought of as a special purpose computer designed to provide storage capacity along with advanced data protection features. Disk drives are only one element within a storage system, along with hardware and special purpose embedded software within the system.
Storage systems can provide either block accessed storage, or file accessed storage. Block access is typically delivered over Fibre Channel, iSCSI, SAS, FICON or other protocols. File access is often provided using NFS or CIFS protocols.
Within the context of a storage system, there are two primary types of virtualization that can occur:
Block virtualization used in this context refers to the abstraction (separation) of logical storage (partition) from physical storage so that it may be accessed without regard to physical storage or heterogeneous structure. This separation allows the administrators of the storage system greater flexibility in how they manage storage for end users.
File virtualization addresses the NAS challenges by eliminating the dependencies between the data accessed at the file level and the location where the files are physically stored. This provides opportunities to optimize storage use and server consolidation and to perform non-disruptive file migrations.""
""Information lifecycle management "",,,,""Information Lifecycle Management (sometimes abbreviated ILM) refers to a wide-ranging set of strategies for administering storage systems on computing devices.
ILM is the practice of applying certain policies to effective information management. This practice has been used by Records and Information Management (RIM) Professionals for over three decades and had its basis in the management of information in paper or other physical forms (microfilm, negatives, photographs, audio or video recordings and other assets).
ILM includes every phase of a "record" from its beginning to its end. And while it is generally applied to information that rises to the classic definition of a record (Records management), it applies to any and all informational assets. During its existence, information can become a record by being identified as documenting a business transaction or as satisfying a business need. In this sense ILM has been part of the overall approach of ECM Enterprise content management.
However, in a more general perspective the term "business" must be taken in a broad sense, and not forcibly tied to direct commercial or enterprise contexts. While most records are thought of as having a relationship to enterprise business, not all do. Much recorded information serves to document an event or a critical point in history. Examples of these are birth, death, medical/health and educational records. e-Science, for example, is an emerging area where ILM has become relevant.
In the year 2004, the Storage Networking Industry Association, on behalf of the information technology (IT) and information storage industries, attempted to assign a new broader definition to Information Lifecycle Management (ILM). The oft-quoted definition that it released that October at the Storage Networking World conference in Orlando, Florida, stated that "ILM consists of the policies, processes, practices, and tools used to align the business value of information with the most appropriate and cost effective IT infrastructure from the time information is conceived through its final disposition." In this view, information is aligned with business processes through management policies and service levels associated with applications, metadata, information, and data.""
""Version management "",,,,""A component of software configuration management, version control, also known as revision control or source control, is the management of changes to documents, computer programs, large web sites, and other collections of information. Changes are usually identified by a number or letter code, termed the "revision number", "revision level", or simply "revision". For example, an initial set of files is "revision 1". When the first change is made, the resulting set is "revision 2", and so on. Each revision is associated with a timestamp and the person making the change. Revisions can be compared, restored, and with some types of files, merged.
The need for a logical way to organize and control revisions has existed for almost as long as writing has existed, but revision control became much more important, and complicated, when the era of computing began. The numbering of book editions and of specification revisions are examples that date back to the print-only era. Today, the most capable (as well as complex) revision control systems are those used in software development, where a team of people may change the same files.
Version control systems (VCS) most commonly run as stand-alone applications, but revision control is also embedded in various types of software such as word processors and spreadsheets, e.g., Google Docs and Sheets and in various content management systems, e.g., Wikipedia's Page history. Revision control allows for the ability to revert a document to a previous revision, which is critical for allowing editors to track each other's edits, correct mistakes, and defend against vandalism and spamming.
Software tools for revision control are essential for the organization of multi-developer projects.""
""Storage power management "",,,,""Semiconductor Research Corporation (SRC) is a technology research consortium. A non-profit founded in 1982 and based in North Carolina, USA,
SRC comprises a few programs:
Global Research Collaboration (GRC) drives near-term materials, interconnect, devices, design, and tools progress.
Focus Center Research Programs (FCRP) supports future generations of IC requirements.
Nanoelectronics Research Initiative (NRI) is responsible for determining the post-CMOS information element by 2020.
Energy Research Initiative (ERI) supports research in photovoltaics, smart grid, electrical energy storage and power management.
SRC Education Alliance (SRCEA) is a private foundation supporting science and engineering students and encouraging them to pursue careers in the semiconductor industry.
Semiconductor Technology Advanced Research Network (STARnet) is a collaborative of universities providing exploratory research on semiconductor, system and design technology for the U.S. microelectronics and defense industries.""
""Thin provisioning "",,,,""In computing, thin provisioning involves using virtualization technology to give the appearance of having more physical resources than are actually available. If a system always has enough resource to simultaneously support all of the virtualized resources, then it is not thin provisioned. The term thin provisioning is applied to disk layer in this article, but could refer to an allocation scheme for any resource. For example, real memory in a computer is typically thin-provisioned to running tasks with some form of address translation technology doing the virtualization. Each task acts as if it has real memory allocated. The sum of the allocated virtual memory assigned to tasks typically exceeds the total of real memory.
The efficiency of thin or thick/fat provisioning is a function of the use case, not of the technology. Thick provisioning is typically more efficient when the amount of resource used very closely approximates to the amount of resource allocated. Thin provisioning offers more efficiency where the amount of resource used is much smaller than allocated, so that the benefit of providing only the resource needed exceeds the cost of the virtualization technology used.
Just-in-time allocation differs from thin provisioning. Most file systems back files just-in-time but are not thin provisioned. Overallocation also differs from thin provisioning; resources can be over-allocated / oversubscribed without using virtualization technology, for example overselling seats on a flight without allocating actual seats at time of sale, avoiding having each consumer having a claim on a specific seat number.
Thin provisioning is a mechanism that applies to large-scale centralized computer disk-storage systems, SANs, and storage virtualization systems. Thin provisioning allows space to be easily allocated to servers, on a just-enough and just-in-time basis. Thin provisioning is called "sparse volumes" in some contexts.

""
""Intranets "",,,,""An intranet is a private network accessible only to an organization's staff. Generally a wide range of information and services from the organization's internal IT systems are available that would not be available to the public from the Internet. A company-wide intranet can constitute an important focal point of internal communication and collaboration, and provide a single starting point to access internal and external resources. In its simplest form an intranet is established with the technologies for local area networks (LANs) and wide area networks (WANs).
Intranets began to appear in a range of larger organizations from 1994. The launch of a free webserver from Microsoft in 1996 helped make the technology accessible to a wider market.

""
""Extranets "",,,,""An extranet is a website that allows controlled access to partners, vendors and suppliers or an authorized set of customers – normally to a subset of the information accessible from an organization's intranet. An extranet is similar to a DMZ in that it provides access to needed services for authorised parties, without granting access to an organization's entire network.
Historically the term was occasionally also used in the sense of two organisations sharing their internal networks over a VPN.""
""Enterprise resource planning "",,,,""Enterprise resource planning (ERP) is a category of business-management software—typically a suite of integrated applications—that an organization can use to collect, store, manage and interpret data from many business activities, including:
product planning, purchase
manufacturing or service delivery
marketing and sales
inventory management
shipping and payment
ERP provides an integrated view of core business processes, often in real-time, using common databases maintained by a database management system. ERP systems track business resources—cash, raw materials, production capacity—and the status of business commitments: orders, purchase orders, and payroll. The applications that make up the system share data across various departments (manufacturing, purchasing, sales, accounting, etc.) that provide the data. ERP facilitates information flow between all business functions, and manages connections to outside stakeholders.
Enterprise system software is a multibillion-dollar industry that produces components that support a variety of business functions. IT investments have become the largest category of capital expenditure in United States-based businesses over the past decade. Though early ERP systems focused on large enterprises, smaller enterprises increasingly use ERP systems.
The ERP system is considered a vital organizational tool because it integrates varied organizational systems and facilitates error-free transactions and production. However, developing an ERP system differs from traditional system development. ERP systems run on a variety of computer hardware and network configurations, typically using a database as an information repository.""
""Enterprise applications "",,,,""Enterprise software, also known as enterprise application software (EAS), is computer software used to satisfy the needs of an organization rather than individual users. Such organizations would include businesses, schools, interest-based user groups, clubs, charities, or governments. Enterprise software is an integral part of a (computer based) Information System.
Services provided by enterprise software are typically business-oriented tools such as online shopping and online payment processing, interactive product catalogue, automated billing systems, security, enterprise content management, IT service management, customer relationship management, enterprise resource planning, business intelligence, project management, collaboration, human resource management, manufacturing, occupational health and safety, enterprise application integration, and enterprise forms automation.
As enterprises have similar departments and systems in common, enterprise software is often available as a suite of customizable programs. Generally, the complexity of these tools requires specialist capabilities and specific knowledge.""
""Data centers "",,,,""A data center is a facility used to house computer systems and associated components, such as telecommunications and storage systems. It generally includes redundant or backup power supplies, redundant data communications connections, environmental controls (e.g., air conditioning, fire suppression) and various security devices. Large data centers are industrial scale operations using as much electricity as a small town.""
""Blogs "",,,,""A blog (a truncation of the expression weblog) is a discussion or informational site published on the World Wide Web consisting of discrete entries ("posts") typically displayed in reverse chronological order (the most recent post appears first). Until 2009, blogs were usually the work of a single individual, occasionally of a small group, and often covered a single subject. More recently, "multi-author blogs" (MABs) have developed, with posts written by large numbers of authors and professionally edited. MABs from newspapers, other media outlets, universities, think tanks, advocacy groups, and similar institutions account for an increasing quantity of blog traffic. The rise of Twitter and other "microblogging" systems helps integrate MABs and single-author blogs into societal newstreams. Blog can also be used as a verb, meaning to maintain or add content to a blog.
The emergence and growth of blogs in the late 1990s coincided with the advent of web publishing tools that facilitated the posting of content by non-technical users. (Previously, a knowledge of such technologies as HTML and FTP had been required to publish content on the Web.)
A majority are interactive, allowing visitors to leave comments and even message each other via GUI widgets on the blogs, and it is this interactivity that distinguishes them from other static websites. In that sense, blogging can be seen as a form of social networking service. Indeed, bloggers do not only produce content to post on their blogs, but also build social relations with their readers and other bloggers. However, there are high-readership blogs which do not allow comments, such as Daring Fireball.
Many blogs provide commentary on a particular subject; others function as more personal online diaries; others function more as online brand advertising of a particular individual or company. A typical blog combines text, images, and links to other blogs, Web pages, and other media related to its topic. The ability of readers to leave comments in an interactive format is an important contribution to the popularity of many blogs. Most blogs are primarily textual, although some focus on art (art blogs), photographs (photoblogs), videos (video blogs or "vlogs"), music (MP3 blogs), and audio (podcasts). Microblogging is another type of blogging, featuring very short posts. In education, blogs can be used as instructional resources. These blogs are referred to as edublogs.
On 16 February 2011, there were over 156 million public blogs in existence. On 20 February 2014, there were around 172 million Tumblr and 75.8 million WordPress blogs in existence worldwide. According to critics and other bloggers, Blogger is the most popular blogging service used today. However, Blogger does not offer public statistics. Technorati has 1.3 million blogs as of February 22, 2014.""
""Wikis "",,,,""A wiki (/ˈwɪki/ WIK-ee) is a website which allows collaborative modification of its content and structure directly from the web browser. In a typical wiki, text is written using a simplified markup language (known as "wiki markup"), and often edited with the help of a rich-text editor.
A wiki is run using wiki software, otherwise known as a wiki engine. There are dozens of different wiki engines in use, both standalone and part of other software, such as bug tracking systems. Some wiki engines are open source, whereas others are proprietary. Some permit control over different functions (levels of access); for example, editing rights may permit changing, adding or removing material. Others may permit access without enforcing access control. Other rules may also be imposed to organize content. A wiki engine is a type of content management system, but it differs from most other such systems, including blog software, in that the content is created without any defined owner or leader, and wikis have little implicit structure, allowing structure to emerge according to the needs of the users.
The encyclopedia project Wikipedia is by far the most popular wiki-based website, and is in fact one of the most widely viewed sites of any kind of the world, having been ranked in the top ten since 2007. Wikipedia is not a single wiki but rather a collection of hundreds of wikis, one for each language. There are at least tens of thousands of other wikis in use, both public and private, including wikis functioning as knowledge management resources, notetaking tools, community websites and intranets.
Ward Cunningham, the developer of the first wiki software, WikiWikiWeb, originally described it as "the simplest online database that could possibly work". "Wiki" (pronounced [ˈwiki]) is a Hawaiian word meaning "quick".

""
""Reputation systems "",,,,""A reputation system computes and publishes reputation scores for a set of objects (e.g. service providers, services, goods or entities) within a community or domain, based on a collection of opinions that other entities hold about the objects. The opinions are typically passed as ratings to a central place where all perceptions, opinions and ratings can be accumulated. A reputation center uses a specific reputation algorithm to dynamically compute the reputation scores based on the received ratings. Reputation is a sign of trustworthiness manifested as testimony by other people. New expectations and realities about the transparency, availability, and privacy of people and institutions are emerging. Reputation management – the selective exposure of personal information and activities – is an important element to how people function in networks as they establish credentials, build trust with others, and gather information to deal with problems or make decisions.
Reputation systems are related to recommender systems and collaborative filtering, but with the difference that reputation systems produce scores based on explicit ratings from the community, whereas recommender systems use some external set of entities and events (such as the purchase of books, movies, or music) to generate marketing recommendations to users. The role of reputation systems is to facilitate trust, and often functions by making the reputation more visible.

""
""Open source software "",,,,""Open-source software (OSS) is computer software with its source code made available with a license in which the copyright holder provides the rights to study, change, and distribute the software to anyone and for any purpose. Open-source software may be developed in a collaborative public manner. Open-source software is the most prominent example of open-source development.
The open-source model, or collaborative competition development from multiple independent sources, generates an increasingly more diverse scope of design perspective than any one company is capable of developing and sustaining long term. A report by the Standish Group (from 2008) states that adoption of open-source software models has resulted in savings of about $60 billion per year to consumers.
^ St. Laurent, Andrew M. (2008). Understanding Open Source and Free Software Licensing. O'Reilly Media. p. 4. ISBN 9780596553951. 
^ Verts, William T. (2008-01-13). "Open source software". World Book Online Reference Center. Archived from the original on January 1, 2011. 
^ Rothwell, Richard (2008-08-05). "Creating wealth with free software". Free Software Magazine. Retrieved 2008-09-08. 
^ "Standish Newsroom — Open Source" (Press release). Boston. 2008-04-16. Retrieved 2008-09-08.""
""Social networking sites "",,,,""A social networking service (also social networking site or SNS) is a platform to build social networks or social relations among people who share similar interests, activities, backgrounds or real-life connections. A social network service consists of a representation of each user (often a profile), his or her social links, and a variety of additional services such as career services. Social network sites are web-based services that allow individuals to create a public profile, create a list of users with whom to share connections, and view and cross the connections within the system. Most social network services are web-based and provide means for users to interact over the Internet, such as e-mail and instant messaging. Social network sites are varied and they incorporate new information and communication tools such as mobile connectivity, photo/video/sharing and blogging. Online community services are sometimes considered a social network service, though in a broader sense, social network service usually means an individual-centered service whereas online community services are group-centered. Social networking sites allow users to share ideas, pictures, posts, activities, events, and interests with people in their network.
According to the Oxford Dictionary, a "social network" is a dedicated website or other application that enables users to communicate with each other by posting information, comments, messages, images, etc. The main types of social networking services are those that contain category places (such as former school year or classmates), means to connect with friends (usually with self-description pages), and a recommendation system linked to trust. Popular methods now combine many of these, with American-based services such as Facebook, Google+, LinkedIn, Instagram, Pinterest, Vine, Tumblr, and Twitter widely used worldwide; Wechat, Sina Weibo, and Tencent QQ in China; Nexopia in Canada; Badoo, Bebo, Vkontakte (Russia), Delphi, Draugiem.lv (Latvia), iWiW (Hungary), Nasza-Klasa (Poland), Soup (Austria), Glocals in Switzerland, Skyrock, The Sphere, StudiVZ (Germany), Tagged, Tuenti (mostly in Spain), Myspace, Xanga and XING in parts of Europe; Hi5 in South America and Central America; Mxit in Africa; CarnivalPics based in Nigeria; Cyworld, Mixi, Renren, Friendster, Sina Weibo and Wretch in Asia and the Pacific Islands. Social network services can be split into three types: socializing social network services are primarily for socializing with existing friends (e.g., Facebook); networking social network services are primarily for non-social interpersonal communication (e.g., LinkedIn); and social navigation social network services are primarily for helping users to find specific information or resources (e.g., Goodreads for books).
There have been attempts to standardize these services to avoid the need to duplicate entries of friends and interests (see the FOAF standard). A study reveals that India has recorded world's largest growth in terms of social media users in 2013. A 2013 survey found that 73% of U.S. adults use social networking sites.""
""Social tagging systems "",,,,""Many have argued that social tagging or collaborative tagging systems can provide navigational cues or “way-finders”  for other users to explore information. The notion is that, given that social tags are labels that users create to represent topics extracted from Web documents, interpretation of these tags should allow other users to predict contents of different documents efficiently. Social tags are arguably more important in exploratory search, in which the users may engage in iterative cycles of goal refinement and exploration of new information (as opposed to simple fact-retrievals), and interpretation of information contents by others will provide useful cues for people to discover topics that are relevant. One significant challenge that arises in social tagging systems is the rapid increase in the number and diversity of the tags. As opposed to structured annotation systems, tags provide users an unstructured, open-ended mechanism to annotate and organize web-content. As users are free to create any tag to describe any resource, it leads to what is referred to as the vocabulary problem. Because users may use different words to describe the same document or extract different topics from the same document based on their own background knowledge, the lack of a top-down mediation may lead to an increase in the use of incoherent tags to represent the information resources in the system. In other words, the inherent "unstructuredness" of social tags may hinder their potential as navigational cues for searchers because the diversities of users and motivation may lead to diminishing tag-topic relations as the system grows. However, a number of studies have shown that structures do emerge at the semantic level -- indicating that there are cohesive forces that are driving the emergent structures in a social tagging system.""
""Synchronous editors "",,,,""Solid Edge is a 3D CAD, parametric feature (history based) and synchronous technology solid modeling software. It runs on Microsoft Windows and provides solid modeling, assembly modelling and 2D orthographic view functionality for mechanical designers. Through third party applications it has links to many other Product Lifecycle Management (PLM) technologies.
Originally developed and released by Intergraph in 1996 using the ACIS geometric modeling kernel it later changed to using the Parasolid kernel. In 1998 it was purchased and further developed by UGS Corp (the purchase date corresponds to the kernel swap).
In 2007, UGS was acquired by the Automation & Drives Division of Siemens AG. UGS company was renamed Siemens PLM Software on October 1, 2007. Since Sep 2006 Siemens also offers a free 2D version called Solid Edge 2D Drafting.
Solid Edge is available in either Classic or Premium. The "Premium" package includes all of the features of "Classic" in addition to mechanical and electrical routing software, and powerful engineering simulation capabilities for CAE (Computer Aided Engineering). 
Solid Edge is a direct competitor to SolidWorks, PTC Creo, and Autodesk Inventor.""
""Asynchronous editors "",,,,""In computer science, asynchronous I/O is a form of input/output processing that permits other processing to continue before the transmission has finished.
Input and output (I/O) operations on a computer can be extremely slow compared to the processing of data. An I/O device can incorporate mechanical devices that must physically move, such as a hard drive seeking a track to read or write; this is often orders of magnitude slower than the switching of electric current. For example, during a disk operation that takes ten milliseconds to perform, a processor that is clocked at one gigahertz could have performed ten million instruction-processing cycles.
A simple approach to I/O would be to start the access and then wait for it to complete. But such an approach (called synchronous I/O or blocking I/O) would block the progress of a program while the communication is in progress, leaving system resources idle. When a program makes many I/O operations, this means that the processor can spend almost all of its time idle waiting for I/O operations to complete.
Alternatively, it is possible to start the communication and then perform processing that does not require that the I/O be completed. This approach is called asynchronous input/output. Any task that depends on the I/O having completed (this includes both using the input values and critical operations that claim to assure that a write operation has been completed) still needs to wait for the I/O operation to complete, and thus is still blocked, but other processing that does not have a dependency on the I/O operation can continue.
Many operating system functions exist to implement asynchronous I/O at many levels. In fact, one of the main functions of all but the most rudimentary of operating systems is to perform at least some form of basic asynchronous I/O, though this may not be particularly apparent to the operator or programmer. In the simplest software solution, the hardware device status is polled at intervals to detect whether the device is ready for its next operation. (For example, the CP/M operating system was built this way. Its system call semantics did not require any more elaborate I/O structure than this, though most implementations were more complex, and thereby more efficient.) Direct memory access (DMA) can greatly increase the efficiency of a polling-based system, and hardware interrupts can eliminate the need for polling entirely. Multitasking operating systems can exploit the functionality provided by hardware interrupts, whilst hiding the complexity of interrupt handling from the user. Spooling was one of the first forms of multitasking designed to exploit asynchronous I/O. Finally, multithreading and explicit asynchronous I/O APIs within user processes can exploit asynchronous I/O further, at the cost of extra software complexity.
Asynchronous I/O is used to improve throughput, latency, and/or responsiveness.""
""Location based services "",,,,""Location-based services (LBS) are a general class of computer program-level services that use location data to control features. As such LBS is an information service and has a number of uses in social networking today as an entertainment service, which is accessible with mobile devices through the mobile network and which uses information on the geographical position of the mobile device. This has become more and more important with the expansion of the smartphone and tablet markets as well.
LBS are used in a variety of contexts, such as health, indoor object search, entertainment, work, personal life, etc.
LBS is critical to many businesses as well as government organizations to drive real insight from data tied to a specific location where activities take place. The spatial patterns that location-related data and services can provide is one on its most powerful and useful aspect where location is a common denominator in all of these activities and can be leveraged to better understand patterns and relationships.
LBS include services to identify a location of a person or object, such as discovering the nearest banking cash machine (a.k.a. ATM) or the whereabouts of a friend or employee. LBS include parcel tracking and vehicle tracking services. LBS can include mobile commerce when taking the form of coupons or advertising directed at customers based on their current location. They include personalized weather services and even location-based games. They are an example of telecommunication convergence.
This concept of location based systems is not compliant with the standardized concept of real-time locating systems (RTLS) and related local services, as noted in ISO/IEC 19762-5 and ISO/IEC 24730-1. While networked computing devices generally do very well to inform consumers of days old data, the computing devices themselves can also be tracked, even in real-time. LBS privacy issues arise in that context, and are documented below.""
""Geographic information systems "",,,,""A geographic information system or geographical information system (GIS) or geospatial information system is a system designed to capture, store, manipulate, analyze, manage, and present all types of spatial or geographical data. The acronym GIS is sometimes used for geographic information science (GIScience) to refer to the academic discipline that studies geographic information systems and is a large domain within the broader academic discipline of geoinformatics. What goes beyond a GIS is a spatial data infrastructure, a concept that has no such restrictive boundaries.
In a general sense, the term describes any information system that integrates, stores, edits, analyzes, shares, and displays geographic information. GIS applications are tools that allow users to create interactive queries (user-created searches), analyze spatial information, edit data in maps, and present the results of all these operations. Geographic information science is the science underlying geographic concepts, applications, and systems.
GIS is a broad term that can refer to a number of different technologies, processes, and methods. It is attached to many operations and has many applications related to engineering, planning, management, transport/logistics, insurance, telecommunications, and business. For that reason, GIS and location intelligence applications can be the foundation for many location-enabled services that rely on analysis and visualization.
GIS can relate unrelated information by using location as the key index variable. Locations or extents in the Earth space–time may be recorded as dates/times of occurrence, and x, y, and z coordinates representing, longitude, latitude, and elevation, respectively. All Earth-based spatial–temporal location and extent references should, ideally, be relatable to one another and ultimately to a "real" physical location or extent. This key characteristic of GIS has begun to open new avenues of scientific inquiry.""
""Sensor networks "",,,,""Wireless sensor networks (WSN), sometimes called wireless sensor and actuator networks (WSAN), are spatially distributed autonomous sensors to monitor physical or environmental conditions, such as temperature, sound, pressure, etc. and to cooperatively pass their data through the network to a main location. The more modern networks are bi-directional, also enabling control of sensor activity. The development of wireless sensor networks was motivated by military applications such as battlefield surveillance; today such networks are used in many industrial and consumer applications, such as industrial process monitoring and control, machine health monitoring, and so on.
The WSN is built of "nodes" – from a few to several hundreds or even thousands, where each node is connected to one (or sometimes several) sensors. Each such sensor network node has typically several parts: a radio transceiver with an internal antenna or connection to an external antenna, a microcontroller, an electronic circuit for interfacing with the sensors and an energy source, usually a battery or an embedded form of energy harvesting. A sensor node might vary in size from that of a shoebox down to the size of a grain of dust, although functioning "motes" of genuine microscopic dimensions have yet to be created. The cost of sensor nodes is similarly variable, ranging from a few to hundreds of dollars, depending on the complexity of the individual sensor nodes. Size and cost constraints on sensor nodes result in corresponding constraints on resources such as energy, memory, computational speed and communications bandwidth. The topology of the WSNs can vary from a simple star network to an advanced multi-hop wireless mesh network. The propagation technique between the hops of the network can be routing or flooding.
In computer science and telecommunications, wireless sensor networks are an active research area with numerous workshops and conferences arranged each year, for example IPSN, SenSys, and EWSN.""
""Data streaming "",,,,""In Connection-oriented communication, a data stream is a sequence of digitally encoded coherent signals (packets of data or data packets) used to transmit or receive information that is in the process of being transmitted.""
""Global positioning systems "",,,,""The Global Positioning System (GPS) is a space-based navigation system that provides location and time information in all weather conditions, anywhere on or near the Earth where there is an unobstructed line of sight to four or more GPS satellites. The system provides critical capabilities to military, civil, and commercial users around the world. The United States government created the system, maintains it, and makes it freely accessible to anyone with a GPS receiver.
The US began the GPS project in 1973 to overcome the limitations of previous navigation systems, integrating ideas from several predecessors, including a number of classified engineering design studies from the 1960s. The U.S. Department of Defense (DoD) developed the system, which originally used 24 satellites. It became fully operational in 1995. Roger L. Easton, Ivan A. Getting and Bradford Parkinson are credited with inventing it.
Advances in technology and new demands on the existing system have now led to efforts to modernize the GPS and implement the next generation of GPS Block IIIA satellites and Next Generation Operational Control System (OCX). Announcements from Vice President Al Gore and the White House in 1998 initiated these changes. In 2000, the U.S. Congress authorized the modernization effort, GPS III.
In addition to GPS, other systems are in use or under development. The Russian Global Navigation Satellite System (GLONASS) was developed contemporaneously with GPS, but suffered from incomplete coverage of the globe until the mid-2000s. There are also the planned European Union Galileo positioning system, India's Indian Regional Navigation Satellite System, China's BeiDou Navigation Satellite System, and the Japanese Quasi-Zenith Satellite System.""
""Data warehouses "",,,,""In computing, a data warehouse (DW or DWH), also known as an enterprise data warehouse (EDW), is a system used for reporting and data analysis. DWs are central repositories of integrated data from one or more disparate sources. They store current and historical data and are used for creating analytical reports for knowledge workers throughout the enterprise. Examples of reports could range from annual and quarterly comparisons and trends to detailed daily sales analysis.
The data stored in the warehouse is uploaded from the operational systems (such as marketing, sales, etc., shown in the figure to the right). The data may pass through an operational data store for additional operations before it is used in the DW for reporting.""
""Expert systems "",,,,""In artificial intelligence, an expert system is a computer system that emulates the decision-making ability of a human expert. Expert systems are designed to solve complex problems by reasoning about knowledge, represented primarily as if–then rules rather than through conventional procedural code. The first expert systems were created in the 1970s and then proliferated in the 1980s. Expert systems were among the first truly successful forms of AI software.
An expert system is divided into two sub-systems: the inference engine and the knowledge base. The knowledge base represents facts and rules. The inference engine applies the rules to the known facts to deduce new facts. Inference engines can also include explanation and debugging capabilities.""
""Data analytics "",,,,""Analysis of data is a process of inspecting, cleaning, transforming, and modeling data with the goal of discovering useful information, suggesting conclusions, and supporting decision-making. Data analysis has multiple facets and approaches, encompassing diverse techniques under a variety of names, in different business, science, and social science domains.
Data mining is a particular data analysis technique that focuses on modeling and knowledge discovery for predictive rather than purely descriptive purposes. Business intelligence covers data analysis that relies heavily on aggregation, focusing on business information. In statistical applications, some people divide data analysis into descriptive statistics, exploratory data analysis (EDA), and confirmatory data analysis (CDA). EDA focuses on discovering new features in the data and CDA on confirming or falsifying existing hypotheses. Predictive analytics focuses on application of statistical models for predictive forecasting or classification, while text analytics applies statistical, linguistic, and structural techniques to extract and classify information from textual sources, a species of unstructured data. All are varieties of data analysis.
Data integration is a precursor to data analysis, and data analysis is closely linked to data visualization and data dissemination. The term data analysis is sometimes used as a synonym for data modeling.""
""Online analytical processing "",,,,""In computing, online analytical processing, or OLAP (/ˈoʊlæp/), is an approach to answering multi-dimensional analytical (MDA) queries swiftly. OLAP is part of the broader category of business intelligence, which also encompasses relational database, report writing and data mining. Typical applications of OLAP include business reporting for sales, marketing, management reporting, business process management (BPM), budgeting and forecasting, financial reporting and similar areas, with new applications coming up, such as agriculture. The term OLAP was created as a slight modification of the traditional database term online transaction processing (OLTP).
OLAP tools enable users to analyze multidimensional data interactively from multiple perspectives. OLAP consists of three basic analytical operations: consolidation (roll-up), drill-down, and slicing and dicing. Consolidation involves the aggregation of data that can be accumulated and computed in one or more dimensions. For example, all sales offices are rolled up to the sales department or sales division to anticipate sales trends. By contrast, the drill-down is a technique that allows users to navigate through the details. For instance, users can view the sales by individual products that make up a region’s sales. Slicing and dicing is a feature whereby users can take out (slicing) a specific set of data of the OLAP cube and view (dicing) the slices from different viewpoints. These viewpoints are sometimes called dimensions (such as looking at the same sales by salesperson or by date or by customer or by product or by region, etc.)
Databases configured for OLAP use a multidimensional data model, allowing for complex analytical and ad hoc queries with a rapid execution time. They borrow aspects of navigational databases, hierarchical databases and relational databases.""
""Mobile information processing systems "",,,,""Windows Mobile is a family of mobile operating systems developed by Microsoft for smartphones and Pocket PCs.
Its origins dated back to Windows CE in 1996, though Windows Mobile itself first appeared in 2000 as PocketPC 2000. It was renamed "Windows Mobile" in 2003, at which point it came in several versions (similar to the desktop versions of Windows) and was aimed at business and enterprise consumers. By 2007, it was the most popular smartphone software in the U.S., but this popularity faded in the following years. In February 2010, facing competition from rival OSs including iOS and Android, Microsoft announced Windows Phone to supersede Windows Mobile. As a result, Windows Mobile has been deprecated. Windows Phone is incompatible with Windows Mobile devices and software. The last version of Windows Mobile, released after the announcement of Windows Phone, was 6.5.5. After this, Microsoft ceased development on Windows Mobile, in order to concentrate on Windows Phone.
^ Lextrait, Vincent (February 2010). "The Programming Languages Beacon, v10.0". Retrieved February 12, 2010. 
^ Evers, Joris (January 6, 2005). "Microsoft to phase out Pocket PC, Smartphone brands | Hardware". InfoWorld. IDG. Retrieved July 14, 2011. 
^ 
^ Nicholas Kolakowski (March 15, 2010). "Microsoft Explains Windows Phone Lack of Compatibility". eWeek. 
^ "Windows Phone: A New Kind of Phone (36:47 min. in)". Microsoft. June 13, 2010. Retrieved September 9, 2010.""
""Process control systems "",,,,""Industrial control system (ICS) is a general term that encompasses several types of control systems used in industrial production, including supervisory control and data acquisition (SCADA) systems, distributed control systems (DCS), and other smaller control system configurations such as programmable logic controllers (PLC) often found in the industrial sectors and critical infrastructures.
ICSs are typically used in industries such as electrical, water, oil, gas and data. Based on data received from remote stations, automated or operator-driven supervisory commands can be pushed to remote station control devices, which are often referred to as field devices. Field devices control local operations such as opening and closing valves and breakers, collecting data from sensor systems, and monitoring the local environment for alarm conditions.""
""Multimedia databases "",,,,""A Multimedia database (MMDB) is a collection of related multimedia data. The multimedia data include one or more primary media data types such as text, images, graphic objects (including drawings, sketches and illustrations) animation sequences, audio and video.
A Multimedia Database Management System (MMDBMS) is a framework that manages different types of data potentially represented in a wide diversity of formats on a wide array of media sources. It provides support for multimedia data types, and facilitate for creation, storage, access, query and control of a multimedia database.""
""Multimedia content creation "",,,,""AMD FirePro is AMD's brand of graphics cards intended for use in workstations running professional Computer-aided design (CAD), Computer-generated imagery (CGI), Digital content creation (DCC) applications. The GPU chips on FirePro-branded graphics cards are identical to the ones used on Radeon-branded graphics cards. The end products (i.e. the graphics card) differentiate substantially by the provided graphics device drivers and through the available professional support for the software.
Competitors include Nvidia's Quadro-branded and to some extent Tesla-branded product series and Intel's Xeon Phi-branded products.""
""Massively multiplayer online games "",,,,""A massively multiplayer online game (MMOG or MMO) is an online game which is capable of supporting large numbers of players simultaneously in the same instance (or world). MMOs usually have at least one persistent world, however some games differ. These games can be found for most network-capable platforms, including the personal computer, video game console, or smartphones and other mobile devices.
MMOs can enable players to cooperate and compete with each other on a large scale, and sometimes to interact meaningfully with people around the world. They include a variety of gameplay types, representing many video game genres.

""
""Data cleaning "",,,,""Data cleansing, data cleaning or data scrubbing is the process of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database. Used mainly in databases, the term refers to identifying incomplete, incorrect, inaccurate, irrelevant, etc. parts of the data and then replacing, modifying, or deleting this dirty data or coarse data. Data cleansing may be performed interactively with data wrangling tools, or as batch processing through scripting.
After cleansing, a data set will be consistent with other similar data sets in the system. The inconsistencies detected or removed may have been originally caused by user entry errors, by corruption in transmission or storage, or by different data dictionary definitions of similar entities in different stores.
Data cleansing differs from data validation in that validation almost invariably means data is rejected from the system at entry and is performed at entry time, rather than on batches of data.
The actual process of data cleansing may involve removing typographical errors or validating and correcting values against a known list of entities. The validation may be strict (such as rejecting any address that does not have a valid postal code) or fuzzy (such as correcting records that partially match existing, known records).
Some data cleansing solutions will clean data by cross checking with a validated data set. Also data enhancement, where data is made more complete by adding related information, is a common data cleansing practice. For example, appending addresses with phone numbers related to that address.
Data cleansing may also involve activities like, harmonization of data, and standardization of data. For example, harmonization of short codes (st, rd, etc.) to actual words (street, road, etcetera). Standardization of data is a means of changing a reference data set to a new standard, ex, use of standard codes.

""
""Collaborative filtering "",,,,""Collaborative filtering (CF) is a technique used by some recommender systems. Collaborative filtering has two senses, a narrow one and a more general one. In general, collaborative filtering is the process of filtering for information or patterns using techniques involving collaboration among multiple agents, viewpoints, data sources, etc. Applications of collaborative filtering typically involve very large data sets. Collaborative filtering methods have been applied to many different kinds of data including: sensing and monitoring data, such as in mineral exploration, environmental sensing over large areas or multiple sensors; financial data, such as financial service institutions that integrate many financial sources; or in electronic commerce and web applications where the focus is on user data, etc. The remainder of this discussion focuses on collaborative filtering for user data, although some of the methods and approaches may apply to the other major applications as well.
In the newer, narrower sense, collaborative filtering is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating). The underlying assumption of the collaborative filtering approach is that if a person A has the same opinion as a person B on an issue, A is more likely to have B's opinion on a different issue x than to have the opinion on x of a person chosen randomly. For example, a collaborative filtering recommendation system for television tastes could make predictions about which television show a user should like given a partial list of that user's tastes (likes or dislikes). Note that these predictions are specific to the user, but use information gleaned from many users. This differs from the simpler approach of giving an average (non-specific) score for each item of interest, for example based on its number of votes.""
""Association rules "",,,,""Association rule learning is a method for discovering interesting relations between variables in large databases. It is intended to identify strong rules discovered in databases using some measures of interestingness. Based on the concept of strong rules, Rakesh Agrawal et al. introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets. For example, the rule  found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as, e.g., promotional pricing or product placements. In addition to the above example from market basket analysis association rules are employed today in many application areas including Web usage mining, intrusion detection, Continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.
^ Piatetsky-Shapiro, Gregory (1991), Discovery, analysis, and presentation of strong rules, in Piatetsky-Shapiro, Gregory; and Frawley, William J.; eds., Knowledge Discovery in Databases, AAAI/MIT Press, Cambridge, MA.
^ Agrawal, R.; Imieliński, T.; Swami, A. (1993). "Mining association rules between sets of items in large databases". Proceedings of the 1993 ACM SIGMOD international conference on Management of data - SIGMOD '93. p. 207. doi:10.1145/170035.170072. ISBN 0897915925.""
""Nearest-neighbor search "",,,,""Nearest neighbor search (NNS), also known as proximity search, similarity search or closest point search, is an optimization problem for finding closest (or most similar) points. Closeness is typically expressed in terms of a dissimilarity function: the less similar the objects, the larger the function values. Formally, the nearest-neighbor (NN) search problem is defined as follows: given a set S of points in a space M and a query point q ∈ M, find the closest point in S to q. Donald Knuth in vol. 3 of The Art of Computer Programming (1973) called it the post-office problem, referring to an application of assigning to a residence the nearest post office. A direct generalization of this problem is a k-NN search, where we need to find the k closest points.
Most commonly M is a metric space and dissimilarity is expressed as a distance metric, which is symmetric and satisfies the triangle inequality. Even more common, M is taken to be the d-dimensional vector space where dissimilarity is measured using the Euclidean distance, Manhattan distance or other distance metric. However, the dissimilarity function can be arbitrary. One example are asymmetric Bregman divergences, for which the triangle inequality does not hold.
^ Cayton, Lawerence (2008). "Fast nearest neighbor retrieval for bregman divergences.". Proceedings of the 25th international conference on Machine learning: 112–119.""
""Data stream mining "",,,,""Data Stream Mining is the process of extracting knowledge structures from continuous, rapid data records. A data stream is an ordered sequence of instances that in many applications of data stream mining can be read only once or a small number of times using limited computing and storage capabilities. Examples of data streams include computer network traffic, phone conversations, ATM transactions, web searches, and sensor data. Data stream mining can be considered a subfield of data mining, machine learning, and knowledge discovery.
In many data stream mining applications, the goal is to predict the class or value of new instances in the data stream given some knowledge about the class membership or values of previous instances in the data stream. Machine learning techniques can be used to learn this prediction task from labeled examples in an automated fashion. Often, concepts from the field of incremental learning, a generalization of Incremental heuristic search are applied to cope with structural changes, on-line learning and real-time demands. In many applications, especially operating within non-stationary environments, the distribution underlying the instances or the rules underlying their labeling may change over time, i.e. the goal of the prediction, the class to be predicted or the target value to be predicted, may change over time. This problem is referred to as concept drift.""
""Digital libraries and archives "",,,,""Bangalore (/ˈbæŋɡəlɔːr/; Kannada: ಬೆಂಗಳೂರು, IPA: [beŋɡaɭuːru]) is the capital and IT city of the Indian state of Karnataka.
Bangalore, or Bengaluru, as it is known today, was founded by Kempe Gowda, who built a mud fort at the site in 1537. It has developed over the years into an industrial and technological hub in India. An unofficial website of the heritage of the city, including the built, natural and cultural history of the city and the region is at www.bangaloreheritage.in.""
""Computational advertising "",,,,""Andrei Zary Broder (Hebrew: אנדרי זרי ברודר‎) is a Distinguished Scientist at Google. Previously he was a Research Fellow and Vice President of Computational Advertising for Yahoo!. Prior to Yahoo he worked for AltaVista as the vice president of research, and for IBM Research as a Distinguished Engineer and CTO of IBM's Institute for Search and Text Analysis.""
""Computing platforms "",,,,""A computing platform is, in the most general sense, whatever a pre-existing piece of computer software or code object is designed to run within, obeying its constraints, and making use of its facilities.
The term computing platform can refer to different abstraction levels, including a certain hardware architecture, an operating system (OS), and runtime libraries. In total it can be said to be the stage on which computer programs can run.
Binary executables have to be compiled for a specific hardware platform, since different central processor units have different machine codes. In addition, operating systems and runtime libraries allow re-use of code and provide abstraction layers which allow the same high-level source code to run on differently configured hardware. For example, there are many kinds of data storage device, and any individual computer can have a different configuration of storage devices; but the application is able to call a generic save or write function provided by the OS and runtime libraries, which then handle the details themselves. A platform can be seen both as a constraint on the application development process – the application is written for such-and-such a platform – and an assistance to the development process, in that they provide low-level functionality ready-made.""
""Web crawling "",,,,""Not to be confused with offline reader. For the search engine of the same name, see WebCrawler.

A Web crawler is an Internet bot which systematically browses the World Wide Web, typically for the purpose of Web indexing.
Web search engines and some other sites use Web crawling or spidering software to update their web content or indexes of others sites' web content. Web crawlers can copy all the pages they visit for later processing by a search engine which indexes the downloaded pages so the users can search much more efficiently.
Crawlers consume resources on the systems they visit and often visit sites without tacit approval. Issues of schedule, load, and "politeness" come into play when large collections of pages are accessed. Mechanisms exist for public sites not wishing to be crawled to make this known to the crawling agent.
As the number of pages on the internet is extremely large, even the largest crawlers fall short of making a complete index.
Crawlers can validate hyperlinks and HTML code. They can also be used for web scraping (see also data-driven programming).""
""Web indexing "",,,,""Web indexing (or Internet indexing) refers to various methods for indexing the contents of a website or of the Internet as a whole. Individual websites or intranets may use a back-of-the-book index, while search engines usually use keywords and metadata to provide a more useful vocabulary for Internet or onsite searching. With the increase in the number of periodicals that have articles online, web indexing is also becoming important for periodical websites.
Back-of-the-book-style web indexes may be called "web site A-Z indexes". The implication with "A-Z" is that there is an alphabetical browse view or interface. This interface differs from that of a browse through layers of hierarchical categories (also known as a taxonomy) which are not necessarily alphabetical, but are also found on some web sites. Although an A-Z index could be used to index multiple sites, rather than the multiple pages of a single site, this is unusual.
Metadata web indexing involves assigning keywords or phrases to web pages or web sites within a metadata tag (or "meta-tag") field, so that the web page or web site can be retrieved with a search engine that is customized to search the keywords field. This may or may not involve using keywords restricted to a controlled vocabulary list. This method is commonly used by search engine indexing.

""
""Page and site ranking "",,,,""Doorway pages are web pages that are created for spamdexing. This is for spamming the index of a search engine by inserting results for particular phrases with the purpose of sending visitors to a different page. They are also known as bridge pages, portal pages, jump pages, gateway pages, entry pages and by other names. Doorway pages that redirect visitors without their knowledge use some form of cloaking. This usually falls under Black Hat SEO.

""
""Spam detection "",,,,""Various anti-spam techniques are used to prevent email spam (unsolicited bulk email).
No technique is a complete solution to the spam problem, and each has trade-offs between incorrectly rejecting legitimate email (false positives) vs. not rejecting all spam (false negatives) - and the associated costs in time and effort.
Anti-spam techniques can be broken into four broad categories: those that require actions by individuals, those that can be automated by email administrators, those that can be automated by email senders and those employed by researchers and law enforcement officials.""
""Collaborative filtering "",,,,""Collaborative filtering (CF) is a technique used by some recommender systems. Collaborative filtering has two senses, a narrow one and a more general one. In general, collaborative filtering is the process of filtering for information or patterns using techniques involving collaboration among multiple agents, viewpoints, data sources, etc. Applications of collaborative filtering typically involve very large data sets. Collaborative filtering methods have been applied to many different kinds of data including: sensing and monitoring data, such as in mineral exploration, environmental sensing over large areas or multiple sensors; financial data, such as financial service institutions that integrate many financial sources; or in electronic commerce and web applications where the focus is on user data, etc. The remainder of this discussion focuses on collaborative filtering for user data, although some of the methods and approaches may apply to the other major applications as well.
In the newer, narrower sense, collaborative filtering is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating). The underlying assumption of the collaborative filtering approach is that if a person A has the same opinion as a person B on an issue, A is more likely to have B's opinion on a different issue x than to have the opinion on x of a person chosen randomly. For example, a collaborative filtering recommendation system for television tastes could make predictions about which television show a user should like given a partial list of that user's tastes (likes or dislikes). Note that these predictions are specific to the user, but use information gleaned from many users. This differs from the simpler approach of giving an average (non-specific) score for each item of interest, for example based on its number of votes.""
""Social recommendation "",,,,""Yahoo is a computer software and web search engine company founded on March 1, 1995. The company is a public corporation and its headquarters is located in Sunnyvale, California. It was founded by Stanford University graduate students Jerry Yang and David Filo in 1994. According to web traffic analysis companies, Yahoo has been one of the most visited websites on the Internet, with more than 130 million unique users per month in the United States alone. As of October 2007, the global network of Yahoo receives 3.4 billion page views per day on average, making it one of the most visited US websites.
Yahoo's first acquisition was the purchase of Net Controls, a web search engine company, in September 1997 for US$1.4 million. As of April 2008, the company's largest acquisition is the purchase of Broadcast.com, an Internet radio company, for $5.7 billion, making Broadcast.com co-founder Mark Cuban a billionaire. Most of the companies acquired by Yahoo are based in the United States; 78 of the companies are from the United States, and 15 are based in a foreign country. As of July 2015, Yahoo has acquired 114 companies, with Polyvore being the latest.""
""Personalization "",,,,""Personalization, sometimes known as customization, consists of tailoring a service or a product to accommodate specific individuals, sometimes tied to groups or segments of individuals. A wide variety of organizations use personalization to improve customer satisfaction, digital sales conversion, marketing results, branding, and improved website metrics as well as for advertising. Personalization is a key element in social media and recommender systems.""
""Social tagging "",,,,""A folksonomy is a system in which users apply public tags to online items, typically to aid them in re-finding those items. This can give rise to a classification system based on those tags and their frequencies, in contrast to a taxonomic classification specified by the owners of the content when it is published. This practice is also known as collaborative tagging, social classification, social indexing, and social tagging. However, these terms have slightly different meanings than folksonomy. Folksonomy was originally “the result of personal free tagging of information [...] for one’s own retrieval.”. Social tagging is the application of tags in an open online environment where the tags of other users are available to others. Collaborative tagging (also known as group tagging) is tagging performed by a group of users. This type of folksonomy is commonly used in cooperative and collaborative projects such as research, content repositories, and social bookmarking.
The term was coined by Thomas Vander Wal in 2004 as a portmanteau of folk and taxonomy. Folksonomies became popular as part of social software applications such as social bookmarking and photograph annotation that enable users to collectively classify and find information via shared tags. Some websites include tag clouds as a way to visualize tags in a folksonomy.
Folksonomies can be used for K-12 education, business, and higher education. More specifically, folksonomies may be implemented for social bookmarking, teacher resource repositories, e-learning systems, collaborative learning, collaborative research, and professional development.""
""Sponsored search advertising "",,,,""Yahoo Search Marketing is a keyword-based "Pay per click" or "Sponsored search" Internet advertising service provided by Yahoo.
Yahoo began offering this service after acquiring Overture Services, Inc. (formerly GoTo.com). GoTo.com was an Idealab spin off and was the first company to successfully provide a pay-for-placement search service following previous attempts that were not well received.""
""Display advertising "",,,,""Display advertising is advertising on websites. It includes many different formats and contains items such as text, images, flash, video, and audio. The main purpose of display advertising is to deliver general advertisements and brand messages to site visitors.
According to eMarketer, Facebook and Twitter will take 33% of display ad spending market share by 2017. Also, desktop display advertising has eclipsed search ad buying in 2014, with mobile ad spending to overtake display in 2015. 

""
""Site wrapping "",,,,""Job wrapping is a term used commonly to describe a process by which jobs can be captured from employer website and posted to the job boards that the employer wants to advertise them.
Corporate recruiters and HR professionals who send job listings to multiple Internet employment sites can sometimes delegate those chores to the employment sites themselves under an arrangement called "job wrapping". Job wrap ensures that employer job openings and updates get wrapped up regularly and posted on the job boards that they have designated.
The term "job wrapping" is synonymous with "spidering", "scraping", or "mirroring". Job wrapping is generally done by a third party vendor.
Popular job search sites known to do job wrapping include Linkedin, monster.com, ZipRecruiter, Simply Hired, Incruit and Indeed.""
""Deep web "",,,,""The deep web, deep net, invisible web, or hidden web are parts of the World Wide Web whose contents are not indexed by standard search engines for any reason. The deep web is opposite to the surface web.
The deep web includes many very common uses such as web mail, online banking but also paid for services with a paywall such as video on demand, and many more.
The dark web is the part of the deep web that requires special software to access, such as encryption software and peer-peer software, rather than just an internet protocol stack and conventional web browser.
Computer scientist Mike Bergman is credited with coining the term deep web in 2000.

""
""Search results deduplication "",,,,""Quantum Corporation is a manufacturer of tape drive, tape automation, and disk-based data deduplication backup, recovery and deduplication storage products for physical, virtual and cloud environments. It also sells scalable file storage systems and archive software and appliances for managing data. The company's headquarters is in San Jose, California. From its founding in 1980 until 2001, it was also a major disk storage manufacturer (usually second-place in market share behind Seagate), and was based in Milpitas, California. Quantum sold its hard disk drive business to Maxtor in 2001.""
""Web log analysis "",,,,""Web log analysis software (also called a web log analyzer) is a kind of web analytics software that parses a server log file from a web server, and based on the values contained in the log file, derives indicators about when, how, and by whom a web server is visited. Reports are usually generated immediately, but data extracted from the log files can alternatively be stored in a database, allowing various reports to be generated on demand.
Features supported by log analysis packages may include "hit filters", which use pattern matching to examine selected log data.

""
""Traffic analysis "",,,,""Traffic analysis is the process of intercepting and examining messages in order to deduce information from patterns in communication. It can be performed even when the messages are encrypted and cannot be decrypted. In general, the greater the number of messages observed, or even intercepted and stored, the more can be inferred from the traffic. Traffic analysis can be performed in the context of military intelligence, counter-intelligence, or pattern-of-life analysis, and is a concern in computer security.
Traffic analysis tasks may be supported by dedicated computer software programs. Advanced traffic analysis techniques may include various forms of social network analysis.

""
""Email "",,,,""Electronic mail, most commonly called email or e-mail since around 1993, is a method of exchanging digital messages from an author to one or more recipients. Email operates across the Internet or other computer networks.
Some early email systems required the author and the recipient to both be online at the same time, in common with instant messaging. Today's email systems are based on a store-and-forward model. Email servers accept, forward, deliver, and store messages. Neither the users nor their computers are required to be online simultaneously; they need connect only briefly, typically to a mail server, for as long as it takes to send or receive messages.
Historically, the term electronic mail was used generically for any electronic document transmission. For example, several writers in the early 1970s used the term to describe fax document transmission. As a result, it is difficult to find the first citation for the use of the term with the more specific meaning it has today.
An Internet email message consists of three components, the message envelope, the message header, and the message body. The message header contains control information, including, minimally, an originator's email address and one or more recipient addresses. Usually descriptive information is also added, such as a subject header field and a message submission date/time stamp.
Originally an ASCII text-only communications medium, Internet email was extended by Multipurpose Internet Mail Extensions (MIME) to carry text in other character sets and multi-media content attachments. International email, with internationalized email addresses using UTF-8, has been standardized, but as of 2016 not widely adopted.
Electronic mail predates the inception of the Internet and was in fact a crucial tool in creating it, but the history of modern, global Internet email services reaches back to the early ARPANET. Standards for encoding email messages were proposed as early as 1973 (RFC 561). Conversion from ARPANET to the Internet in the early 1980s produced the core of the current services. An email message sent in the early 1970s looks quite similar to a basic text message sent on the Internet today.
Email is an information and communications technology. It uses technology to communicate a digital message over the Internet. Users use email differently, based on how they think about it. There are many software platforms available to send and receive. Popular email platforms include Gmail, Hotmail, Yahoo! Mail, Outlook, and many others.
Network-based email was initially exchanged on the ARPANET in extensions to the File Transfer Protocol (FTP), but is now carried by the Simple Mail Transfer Protocol (SMTP), first published as Internet standard 10 (RFC 821) in 1982. In the process of transporting email messages between systems, SMTP communicates delivery parameters using a message envelope separate from the message (header and body) itself.""
""Blogs "",,,,""A blog (a truncation of the expression weblog) is a discussion or informational site published on the World Wide Web consisting of discrete entries ("posts") typically displayed in reverse chronological order (the most recent post appears first). Until 2009, blogs were usually the work of a single individual, occasionally of a small group, and often covered a single subject. More recently, "multi-author blogs" (MABs) have developed, with posts written by large numbers of authors and professionally edited. MABs from newspapers, other media outlets, universities, think tanks, advocacy groups, and similar institutions account for an increasing quantity of blog traffic. The rise of Twitter and other "microblogging" systems helps integrate MABs and single-author blogs into societal newstreams. Blog can also be used as a verb, meaning to maintain or add content to a blog.
The emergence and growth of blogs in the late 1990s coincided with the advent of web publishing tools that facilitated the posting of content by non-technical users. (Previously, a knowledge of such technologies as HTML and FTP had been required to publish content on the Web.)
A majority are interactive, allowing visitors to leave comments and even message each other via GUI widgets on the blogs, and it is this interactivity that distinguishes them from other static websites. In that sense, blogging can be seen as a form of social networking service. Indeed, bloggers do not only produce content to post on their blogs, but also build social relations with their readers and other bloggers. However, there are high-readership blogs which do not allow comments, such as Daring Fireball.
Many blogs provide commentary on a particular subject; others function as more personal online diaries; others function more as online brand advertising of a particular individual or company. A typical blog combines text, images, and links to other blogs, Web pages, and other media related to its topic. The ability of readers to leave comments in an interactive format is an important contribution to the popularity of many blogs. Most blogs are primarily textual, although some focus on art (art blogs), photographs (photoblogs), videos (video blogs or "vlogs"), music (MP3 blogs), and audio (podcasts). Microblogging is another type of blogging, featuring very short posts. In education, blogs can be used as instructional resources. These blogs are referred to as edublogs.
On 16 February 2011, there were over 156 million public blogs in existence. On 20 February 2014, there were around 172 million Tumblr and 75.8 million WordPress blogs in existence worldwide. According to critics and other bloggers, Blogger is the most popular blogging service used today. However, Blogger does not offer public statistics. Technorati has 1.3 million blogs as of February 22, 2014.""
""Web conferencing "",,,,""Web conferencing may be used as an umbrella term for various types of online collaborative services including web seminars ("webinars"), webcasts, and peer-level web meetings. It may also be used in a more narrow sense to refer only to the peer-level web meeting context, in an attempt to disambiguate it from the other types of collaborative sessions. Terminology related to these technologies is inexact, and no generally agreed upon source or standards organization exists to provide an established usage reference.
In general, web conferencing is made possible by Internet technologies, particularly on TCP/IP connections. Services may allow real-time point-to-point communications as well as multicast communications from one sender to many receivers. It offers data streams of text-based messages, voice and video chat to be shared simultaneously, across geographically dispersed locations. Applications for web conferencing include meetings, training events, lectures, or presentations from a web-connected computer to other web-connected computers.""
""Social networks "",,,,""A social network is a social structure made up of a set of social actors (such as individuals or organizations), sets of dyadic ties, and other social interactions between actors. The social network perspective provides a set of methods for analyzing the structure of whole social entities as well as a variety of theories explaining the patterns observed in these structures. The study of these structures uses social network analysis to identify local and global patterns, locate influential entities, and examine network dynamics.
Social networks and the analysis of them is an inherently interdisciplinary academic field which emerged from social psychology, sociology, statistics, and graph theory. Georg Simmel authored early structural theories in sociology emphasizing the dynamics of triads and "web of group affiliations". Jacob Moreno is credited with developing the first sociograms in the 1930s to study interpersonal relationships. These approaches were mathematically formalized in the 1950s and theories and methods of social networks became pervasive in the social and behavioral sciences by the 1980s. Social network analysis is now one of the major paradigms in contemporary sociology, and is also employed in a number of other social and formal sciences. Together with other complex networks, it forms part of the nascent field of network science.""
""Answer ranking "",,,,""David John Nutt (born 16 April 1951) DM FRCP FRCPsych FMedSci is a British psychiatrist and neuropsychopharmacologist specialising in the research of drugs that affect the brain and conditions such as addiction, anxiety and sleep. He was until 2009 a professor at the University of Bristol heading their Psychopharmacology Unit. Since then he has been the Edmond J Safra chair in Neuropsychopharmacology at Imperial College, London. Nutt was a member of the Committee on Safety of Medicines, and was President of the European College of Neuropsychopharmacology. His book "Drugs without the hot air" (UIT press) won the Transmission Prize for Communicating Science in 2014.

""
""Reputation systems "",,,,""A reputation system computes and publishes reputation scores for a set of objects (e.g. service providers, services, goods or entities) within a community or domain, based on a collection of opinions that other entities hold about the objects. The opinions are typically passed as ratings to a central place where all perceptions, opinions and ratings can be accumulated. A reputation center uses a specific reputation algorithm to dynamically compute the reputation scores based on the received ratings. Reputation is a sign of trustworthiness manifested as testimony by other people. New expectations and realities about the transparency, availability, and privacy of people and institutions are emerging. Reputation management – the selective exposure of personal information and activities – is an important element to how people function in networks as they establish credentials, build trust with others, and gather information to deal with problems or make decisions.
Reputation systems are related to recommender systems and collaborative filtering, but with the difference that reputation systems produce scores based on explicit ratings from the community, whereas recommender systems use some external set of entities and events (such as the purchase of books, movies, or music) to generate marketing recommendations to users. The role of reputation systems is to facilitate trust, and often functions by making the reputation more visible.

""
""Digital cash "",,,,""Electronic money, or e-money, is the money balance recorded electronically on a stored-value card. These cards have microprocessors embedded which can be loaded with a monetary value. Another form of electronic money is network money, software that allows the transfer of value on computer networks, particularly the internet. Electronic money is a floating claim on a private bank or other financial institution that is not linked to any particular account. Examples of electronic money are bank deposits, electronic funds transfer, direct deposit, payment processors, and digital currencies.
Electronic money can either be centralized, where there is a central point of control over the money supply, or decentralized, where the control over the money supply can come from various sources. Electronic money that is decentralized is also known as digital currencies. The major difference between E-money and digital currencies is that E-money doesn't change the value of the fiat currency (USD, EUR) it represents, but digital currency isn't equivalent to any fiat currency. In other words, all digital currency is Electronic money, but Electronic money isn't necessarily digital currency. Many mobile sub-systems have been introduced in the past few years including Google Wallet and Apple Pay.""
""E-commerce infrastructure "",,,,""Founded in January 2002, the Center for E-Commerce Infrastructure Development (CECID) is a research and development center in the University of Hong Kong committed to promoting e-commerce infrastructure development and standardization. A member of OASIS, W3C, RosettaNet, and the ebXML Asia Committee, CECID actively takes part in the development and implementation of international standards, such as Universal Business Language, Web Services, and RosettaNet. Through participation in these international and regional standards bodies, CECID follows closely the latest developments in e-commerce technology standards and promotes Hong Kong's e-commerce technology to technical communities overseas.
CECID's operation is primarily financed by R&D grants from the Innovation and Technology Commission of the Hong Kong Government for its two flagship research projects, namely Project Phoenix and Project Pyxis. In its completed Project Phoenix, CECID has produced several software packages that implement major ebXML specifications. These software packages include Hermes Message Service Handler, ebMail, and ebXMLRR Registry/Repository and are currently released under open source licenses on the freebXML.org website that CECID established in 2002. Commenced in 2004, Project Pyxis targets to develop enabling technology for e-business interoperability between trading partners and within large enterprises using various complementary and competing Web Services standards.""
""Electronic data interchange "",,,,""Electronic Data Interchange (EDI) is an electronic communication method that provides standards for exchanging data via any electronic means. By adhering to the same standard, two different companies or organizations, even in two different countries, can electronically exchange documents (such as purchase orders, invoices, shipping notices, and many others). EDI has existed for more than 30 years, and there are many EDI standards (including X12, EDIFACT, ODETTE, etc.), some of which address the needs of specific industries or regions. It also refers specifically to a family of standards. In 1996, the National Institute of Standards and Technology defined electronic data interchange as "the computer-to-computer interchange of strictly formatted messages that represent documents other than monetary instruments. EDI implies a sequence of messages between two parties, either of whom may serve as originator or recipient. The formatted data representing the documents may be transmitted from originator to recipient via telecommunications or physically transported on electronic storage media." It distinguishes mere electronic communication or data exchange, specifying that "in EDI, the usual processing of received messages is by computer only. Human intervention in the processing of a received message is typically intended only for error conditions, for quality review, and for special situations. For example, the transmission of binary or textual data is not EDI as defined here unless the data are treated as one or more data elements of an EDI message and are not normally intended for human interpretation as part of online data processing."
EDI can be formally defined as the transfer of structured data, by agreed message standards, from one computer system to another without human intervention.""
""Electronic funds transfer "",,,,""Electronic funds transfer (EFT) is the electronic transfer of money from one bank account to another, either within a single financial institution or across multiple institutions, through computer-based systems and without the direct intervention of bank staff. EFTs are known by a number of names. In the United States, they may be referred to as electronic checks or e-checks.
The term covers a number of different payment systems, for example:
cardholder-initiated transactions, using a payment card such as a credit or debit card
direct deposit payment initiated by the payer
direct debit payments for which a business debits the consumer's bank accounts for payment for goods or services
wire transfer via an international banking network such as SWIFT
electronic bill payment in online banking, which may be delivered by EFT or paper check
transactions involving stored value of electronic money, possibly in a private currency.""
""Online shopping "",,,,""Online shopping (sometimes known as e-tail from "electronic retail" or e-shopping) is a form of electronic commerce which allows consumers to directly buy goods or services from a seller over the Internet using a web browser. Alternative names are: e-web-store, e-shop, e-store, Internet shop, web-shop, web-store, online store, online storefront and virtual store. Mobile commerce (or m-commerce) describes purchasing from an online retailer's mobile optimized online site or app.
An online shop evokes the physical analogy of buying products or services at a bricks-and-mortar retailer or shopping center; the process is called business-to-consumer (B2C) online shopping. In the case where a business buys from another business, the process is called business-to-business (B2B) online shopping. The largest of these online retailing corporations are Alibaba, Amazon.com, and eBay.
^ "The Alibaba phenomenon". The Economics. March 23, 2013.""
""Online banking "",,,,""Online banking also known as internet banking, e-banking, or virtual banking, is an electronic payment system that enables customers of a bank or other financial institution to conduct a range of financial transactions through the financial institution's website. The online banking system will typically connect to or be part of the core banking system operated by a bank and is in contrast to branch banking that was the traditional way customers access banking services. Basically Online Banking, Internet Banking & E-Banking is the same thing. There is no difference fundamentally and in mechanism.
To access a financial institution's online banking facility, a customer with internet access would need to register with the institution for the service, and set up a password and other credentials for customer verification. The credentials for online banking is normally not the same as for telephone or mobile banking. Financial institutions now routinely allocate customers numbers, whether or not customers have indicated an intention to access their online banking facility. Customers' numbers are normally not the same as account numbers, because a number of customer accounts can be linked to the one customer number. The customer number can be linked to any account that the customer controls, such as cheque, savings, loan, credit card and other accounts.
The customer visits the financial institution's secure website, and enters the online banking facility using the customer number and credentials previously set up. The types of financial transactions which a customer may transact through online banking usually includes obtaining account balances and list of latest transactions, electronic bill payments, and funds transfers between a customer's or another's accounts. Most also enable a customer to download copies of statements, which can be printed at the customer's premises (with some banks charging a fee for mailing hardcopies of bank statements). Some banks also enable customers to download transactions directly into the customer's accounting software. The facility may also enable the customer to order cheque-books, statements, report loss of credit cards, stop payment on a cheque, advise change of address, and other routine transactions.

""
""Secure online transactions "",,,,""Vincent Aron Cate (born 1963) is a cryptography software developer based in Anguilla. He graduated from the University of California, Berkeley and ran an Atari hardware business in the 1980s before beginning a Ph.D. programme at Carnegie Mellon University, but dropped out and moved to Anguilla to pursue business opportunities there. In his new home, he would go on to establish an internet service provider, a computer club for young students, and an annual cryptography conference. A former U.S. citizen, he gave up his citizenship in 1998 in protest of U.S. laws on the export of cryptography.""
""Online auctions "",,,,""An online auction is an auction which is held over the internet. Online auctions come in many different formats, but most popularly they are ascending English auctions, descending Dutch auctions, first-price sealed-bid, Vickrey auctions, or sometimes even a combination of multiple auctions, taking elements of one and forging them with another. The scope and reach of these auctions have been propelled by the Internet to a level beyond what the initial purveyors had anticipated. This is mainly because online auctions break down and remove the physical limitations of traditional auctions such as geography, presence, time, space, and a small target audience. This influx in reachability has also made it easier to commit unlawful actions within an auction. In 2002, online auctions were projected to account for 30% of all online e-commerce due to the rapid expansion of the popularity of the form of electronic commerce.
^ a b Bapna, R.; Goes, P.; Gupta, A. (2001). "Insights and analyses of online auctions". Communications of the ACM 44 (11): 42. doi:10.1145/384150.384160. 
^ Albert, M. R. (2002). "E-Buyer Beware: Why Online Auction Fraud Should Be Regulated". American Business Law Journal 39 (4): 575. doi:10.1111/j.1744-1714.2002.tb00306.x. 
^ Vakrat, Y.; Seidmann, A. (2000). "Implications of the bidders' arrival process on the design of online auctions". Proceedings of the 33rd Annual Hawaii International Conference on System Sciences. p. 7. doi:10.1109/HICSS.2000.926822. ISBN 0-7695-0493-0.""
""Wikis "",,,,""A wiki (/ˈwɪki/ WIK-ee) is a website which allows collaborative modification of its content and structure directly from the web browser. In a typical wiki, text is written using a simplified markup language (known as "wiki markup"), and often edited with the help of a rich-text editor.
A wiki is run using wiki software, otherwise known as a wiki engine. There are dozens of different wiki engines in use, both standalone and part of other software, such as bug tracking systems. Some wiki engines are open source, whereas others are proprietary. Some permit control over different functions (levels of access); for example, editing rights may permit changing, adding or removing material. Others may permit access without enforcing access control. Other rules may also be imposed to organize content. A wiki engine is a type of content management system, but it differs from most other such systems, including blog software, in that the content is created without any defined owner or leader, and wikis have little implicit structure, allowing structure to emerge according to the needs of the users.
The encyclopedia project Wikipedia is by far the most popular wiki-based website, and is in fact one of the most widely viewed sites of any kind of the world, having been ranked in the top ten since 2007. Wikipedia is not a single wiki but rather a collection of hundreds of wikis, one for each language. There are at least tens of thousands of other wikis in use, both public and private, including wikis functioning as knowledge management resources, notetaking tools, community websites and intranets.
Ward Cunningham, the developer of the first wiki software, WikiWikiWeb, originally described it as "the simplest online database that could possibly work". "Wiki" (pronounced [ˈwiki]) is a Hawaiian word meaning "quick".

""
""Simple Object Access Protocol (SOAP) "",,,,""Middleware is computer software that provides services to software applications beyond those available from the operating system. It can be described as "software glue". Middleware makes it easier for software developers to perform communication and input/output, so they can focus on the specific purpose of their application.
Middleware includes Web servers, application servers, content management systems, and similar tools that support application development and delivery. It is especially integral to information technology based on Extensible Markup Language (XML), Simple Object Access Protocol (SOAP), Web services, SOA, Web 2.0 infrastructure, and Lightweight Directory Access Protocol (LDAP)
^ "What is Middleware?". Middleware.org. Defining Technology. 2008. Retrieved 2013-08-11.""
""RESTful web services "",,,,""JAX-RS: Java API for RESTful Web Services (JAX-RS) is a Java programming language API that provides support in creating web services according to the Representational State Transfer (REST) architectural pattern. JAX-RS uses annotations, introduced in Java SE 5, to simplify the development and deployment of web service clients and endpoints.
From version 1.1 on, JAX-RS is an official part of Java EE 6. A notable feature of being an official part of Java EE is that no configuration is necessary to start using JAX-RS. For non-Java EE 6 environments a (small) entry in the web.xml deployment descriptor is required.""
""Web Services Description Language (WSDL) "",,,,""Semantic Annotations for WSDL and XML Schema (SAWSDL) is a 2007 published technical recommendation of W3C in the context of Semantic Web framework:
SAWSDL defines a set of extension attributes for the Web Services Description Language (WSDL) and XML Schema definition language. Application of the attributes shall allow for description of additional semantics of WSDL components. The specification defines how semantic annotation is accomplished using references to conceptual semantic models, e.g. ontologies. Semantic annotations for WSDL and XML Schema (SAWSDL) does not specify a language for representing the semantic models. Instead it provides mechanisms by which concepts from the semantic models can be referred using annotations (citation abbreviated).""
""Universal Description Discovery and Integration (UDDI) "",,,,""The following is a list of web service protocols.
BEEP - Blocks Extensible Exchange Protocol
E-Business XML
Hessian
JSON-RPC
JSON-WSP
SOAP - outgrowth of XML-RPC, originally an acronym for Simple Object Access Protocol
Universal Description, Discovery, and Integration (UDDI)
Web Processing Service (WPS)
WSFL - Web Services Flow Language (superseded by BPEL)
WSCL - Web Services Conversation Language
XINS Standard Calling Convention - HTTP parameters in (GET/POST/HEAD), POX out
XLANG - XLANG-Specification (superseded by BPEL)
XML-RPC - XML Remote Procedure Call""
""Service discovery and interfaces "",,,,""Innovative Interfaces, Inc. (abbreviated III) is a software company specializing in integrated systems for library management. Their key products include Sierra, Polaris, Millennium, and Virtua, with customers in 66 countries.
The company's software is used by various types of libraries including academic, public, school, medical, law, and special libraries as well as consortia. In September 2014 Sierra was installed at 1494 libraries (with 3435 facilities), Polaris at 1339 (with 2808 facilities), Millennium at 1316 (with 2640 facilities), and Virtua at 224 (with 490 facilities).
Founded in 1978 by Jerry Kline and Steve Silberstein in Berkeley California, the initial product was a system to interface OCLC data with a library's cataloging system. Huntsman Gay Global Capital and JMI Equity invested in the company in 2012, the same year Kim Massana, formerly president of Thomson Reuters Elite, was appointed CEO. The equity firms purchased the company outright the next year. Backed by the new investors, a wave of hiring increased staff in 2013. The company also made several acquisitions within the next two years: SkyRiver Technology Solutions (which maintains partnerships with 3M, EBSCO Information Services, OverDrive, Inc., and Bibliotheca), Polaris Library Systems, and VTLS Inc. Bert Winemiller took over as CEO for a brief period in 2015 before the company named James Tallman as their new CEO in January 2016.
III's interface for library users is the Encore Discovery Solution that provides web searching and access of library resources along with features such as ranked relevancy, facets to focus searches, tagging, ratings, and book reviews.""
""Resource Description Framework (RDF) "",,,,""Embedded RDF (eRDF) is a syntax for writing HTML in such a way that the information in the HTML document can be extracted (with an eRDF parser or XSLT style sheet) into Resource Description Framework (RDF). This can be of great use for searching within data.
It was invented by Ian Davis in 2005, and partly inspired by microformats, a simplified approach to semantically annotate data in websites. This specification is obsolete, superseded by RDFa, Microdata, and JSON-LD.""
""Web Ontology Language (OWL) "",,,,""WebOnt is the W3C's Semantic Web Activity Working group on Web Ontology Language. The result of this group's effort has been in producing OWL - the Web Ontology Language.
OWL took DAML+OIL as a starting language (which itself is built on top of RDF and XML) and provided extensions that are compatible with description logics.
OWL is the emerging expressive web ontology language.""
""Extensible Markup Language (XML) "",,,,""In computing, a news aggregator, also termed a feed aggregator, feed reader, news reader, RSS reader or simply aggregator, is client software or a web application which aggregates syndicated web content such as online newspapers, blogs, podcasts, and video blogs (vlogs) in one location for easy viewing. RSS is synchronized subscription system. Basically, RSS uses extensible markup language (XML) to structure pieces of information to be aggregated in a feed reader that displays the information in a user-friendly interface. The updates distributed include, for example, journal tables of contents, podcasts, videos,and news items.""
""Document structure "",,,,""Document Structure Description, or DSD, is a schema language for XML, that is, a language for describing valid XML documents. It's an alternative to DTD or the W3C XML Schema.
An example of DSD in its simplest form:

<dsd xmlns="http://www.brics.dk/DSD/2.0"
 xmlns:my="http://example.com">
 
 <if><element name="my:foo"/>
  <declare>
   <attribute name="first"/>
   <attribute name="second"/>
   <contents>
    <element name="my:bar"/>
   </contents>
  </declare>
 </if>
 
 <if><element name="my:bar"/>
  <declare>
   <contents>
   </contents>
  </declare>
 </if>
 
</dsd>

This says that element named "foo" in the XML namespace "http://example.com" may have two attributes, named "first" and "second". A "foo" element may not have any character data. It must contain one subelement, named "bar", also in the "http://example.com" namespace. A "bar" element is not allowed any attributes, character data or subelements.
One XML document that would be valid according to the above DSD would be:

<foo xmlns="http://example.com" second="2">
 <bar/>
</foo>""
""Document topic models "",,,,""Vector space model or term vector model is an algebraic model for representing text documents (and any objects, in general) as vectors of identifiers, such as, for example, index terms. It is used in information filtering, information retrieval, indexing and relevancy rankings. Its first use was in the SMART Information Retrieval System.""
""Content analysis and feature selection "",,,,""On the Origin of Species, published on 24 November 1859, is a work of scientific literature by Charles Darwin which is considered to be the foundation of evolutionary biology. Darwin's book introduced the scientific theory that populations evolve over the course of generations through a process of natural selection. It presented a body of evidence that the diversity of life arose by common descent through a branching pattern of evolution. Darwin included evidence that he had gathered on the Beagle expedition in the 1830s and his subsequent findings from research, correspondence, and experimentation.
Various evolutionary ideas had already been proposed to explain new findings in biology. There was growing support for such ideas among dissident anatomists and the general public, but during the first half of the 19th century the English scientific establishment was closely tied to the Church of England, while science was part of natural theology. Ideas about the transmutation of species were controversial as they conflicted with the beliefs that species were unchanging parts of a designed hierarchy and that humans were unique, unrelated to other animals. The political and theological implications were intensely debated, but transmutation was not accepted by the scientific mainstream.
The book was written for non-specialist readers and attracted widespread interest upon its publication. As Darwin was an eminent scientist, his findings were taken seriously and the evidence he presented generated scientific, philosophical, and religious discussion. The debate over the book contributed to the campaign by T. H. Huxley and his fellow members of the X Club to secularise science by promoting scientific naturalism. Within two decades there was widespread scientific agreement that evolution, with a branching pattern of common descent, had occurred, but scientists were slow to give natural selection the significance that Darwin thought appropriate. During "the eclipse of Darwinism" from the 1880s to the 1930s, various other mechanisms of evolution were given more credit. With the development of the modern evolutionary synthesis in the 1930s and 1940s, Darwin's concept of evolutionary adaptation through natural selection became central to modern evolutionary theory, and it has now become the unifying concept of the life sciences.

""
""Data encoding and canonicalization "",,,,""In computer science, canonicalization (sometimes standardization or normalization) is a process for converting data that has more than one possible representation into a "standard", "normal", or canonical form. This can be done to compare different representations for equivalence, to count the number of distinct data structures, to improve the efficiency of various algorithms by eliminating repeated calculations, or to make it possible to impose a meaningful sorting order.

""
""Ontologies "",,,,""In computer science and information science, an ontology is a formal naming and definition of the types, properties, and interrelationships of the entities that really or fundamentally exist for a particular domain of discourse. It is thus a practical application of philosophical ontology, with a taxonomy.
An ontology compartmentalizes the variables needed for some set of computations and establishes the relationships between them.
The fields of artificial intelligence, the Semantic Web, systems engineering, software engineering, biomedical informatics, library science, enterprise bookmarking, and information architecture all create ontologies to limit complexity and to organize information. The ontology can then be applied to problem solving.""
""Dictionaries "",,,,""A dictionary is a collection of words in one or more specific languages, often alphabetically (or by radical and stroke for ideographic languages), with usage of information, definitions, etymologies, phonetics, pronunciations, translation, and other information; or a book of words in one language with their equivalents in another, also known as a lexicon. It is a lexicographical product designed for utility and function, curated with selected data, presented in a way that shows inter-relationship among the data.
A broad distinction is made between general and specialized dictionaries. Specialized dictionaries do not contain information about words that are used in language for general purposes—words used by ordinary people in everyday situations. Lexical items that describe concepts in specific fields are usually called terms instead of words, although there is no consensus whether lexicology and terminology are two different fields of study. In theory, general dictionaries are supposed to be semasiological, mapping word to definition, while specialized dictionaries are supposed to be onomasiological, first identifying concepts and then. establishing the terms used to designate them. In practice, the two approaches are used for both types. There are other types of dictionaries that don't fit neatly in the above distinction, for instance bilingual (translation) dictionaries, dictionaries of synonyms (thesauri), or rhyming dictionaries. The word dictionary (unqualified) is usually understood to refer to a monolingual general-purpose dictionary.
A different dimension on which dictionaries (usually just general-purpose ones) are sometimes distinguished is whether they are prescriptive or descriptive, the latter being in theory largely based on linguistic corpus studies—this is the case of most modern dictionaries. However, this distinction cannot be upheld in the strictest sense. The choice of headwords is considered itself of prescriptive nature; for instance, dictionaries avoid having too many taboo words in that position. Stylistic indications (e.g. ‘informal’ or ‘vulgar’) present in many modern dictionaries is considered less than objectively descriptive as well.

Although the first recorded dictionaries date back to Sumerian times (these were bilingual dictionaries), the systematic study of dictionaries as objects of scientific interest themselves is a 20th-century enterprise, called lexicography, and largely initiated by Ladislav Zgusta. The birth of the new discipline was not without controversy, the practical dictionary-makers being sometimes accused by others of "astonishing" lack of method and critical-self reflection.""
""Thesauri "",,,,""In general usage, a thesaurus is a reference work that lists words grouped together according to similarity of meaning (containing synonyms and sometimes antonyms), in contrast to a dictionary, which provides definitions for words, and generally lists them in alphabetical order. The main purpose of such reference works is to help the user "to find the word, or words, by which [an] idea may be most fitly and aptly expressed" – to quote Peter Mark Roget, architect of the best known thesaurus in the English language.
Although including synonyms, a thesaurus should not be taken as a complete list of all the synonyms for a particular word. The entries are also designed for drawing distinctions between similar words and assisting in choosing exactly the right word. Unlike a dictionary, a thesaurus entry does not give the definition of words.
In library science and information science thesauri have been widely used to specify domain models. Recently, thesauri have been implemented with Simple Knowledge Organization System (SKOS).""
""Query reformulation "",,,,""Query expansion (QE) is the process of reformulating a seed query to improve retrieval performance in information retrieval operations. In the context of web search engines, query expansion involves evaluating a user's input (what words were typed into the search query area, and sometimes other types of data) and expanding the search query to match additional documents. Query expansion involves techniques such as:
Finding synonyms of words, and searching for the synonyms as well
Finding all the various morphological forms of words by stemming each word in the search query
Fixing spelling errors and automatically searching for the corrected form or suggesting it in the results
Re-weighting the terms in the original query
Query expansion is a methodology studied in the field of computer science, particularly within the realm of natural language processing and information retrieval.""
""Personalization "",,,,""Personalization, sometimes known as customization, consists of tailoring a service or a product to accommodate specific individuals, sometimes tied to groups or segments of individuals. A wide variety of organizations use personalization to improve customer satisfaction, digital sales conversion, marketing results, branding, and improved website metrics as well as for advertising. Personalization is a key element in social media and recommender systems.""
""Task models "",,,,""ConcurTaskTrees (CTT) is a notation for task model specifications useful to support design of interactive applications specifically tailored for user interface model-based design.
The main features of ConcurTaskTrees are:
Hierarchical structure
Graphical syntax
Concurrent notation
In Human-Computer Interaction, in the field of task analysis, task models indicate the logical activities that an application should support to reach users’ goals.
ConcurTaskTrees has been mapped into Unified Modeling Language.""
""Collaborative search "",,,,""Collaborative search engines (CSE) are Web search engines and enterprise searches within company intranets that let users combine their efforts in information retrieval (IR) activities, share information resources collaboratively using knowledge tags, and allow experts to guide less experienced people through their searches. Collaboration partners do so by providing query terms, collective tagging, adding comments or opinions, rating search results, and links clicked of former (successful) IR activities to users having the same or a related information need.""
""Rank aggregation "",,,,""The Kemeny–Young method is a voting system that uses preferential ballots and pairwise comparison counts to identify the most popular choices in an election. It is a Condorcet method because if there is a Condorcet winner, it will always be ranked as the most popular choice.
This method assigns a score for each possible sequence, where each sequence considers which choice might be most popular, which choice might be second-most popular, which choice might be third-most popular, and so on down to which choice might be least-popular. The sequence that has the highest score is the winning sequence, and the first choice in the winning sequence is the most popular choice. (As explained below, ties can occur at any ranking level.)
The Kemeny–Young method is also known as the Kemeny rule, VoteFair popularity ranking, the maximum likelihood method, and the median relation.""
""Probabilistic retrieval models "",,,,""""
""Language models "",,,,""A statistical language model is a probability distribution over sequences of words. Given such a sequence, say of length m, it assigns a probability  to the whole sequence. Having a way to estimate the relative likelihood of different phrases is useful in many natural language processing applications. Language modeling is used in speech recognition, machine translation, part-of-speech tagging, parsing, handwriting recognition, information retrieval and other applications.
In speech recognition, the computer tries to match sounds with word sequences. The language model provides context to distinguish between words and phrases that sound similar. For example, in American English, the phrases "recognize speech" and "wreck a nice beach" are pronounced almost the same but mean very different things. These ambiguities are easier to resolve when evidence from the language model is incorporated with the pronunciation model and the acoustic model.
Language models are used in information retrieval in the query likelihood model. Here a separate language model is associated with each document in a collection. Documents are ranked based on the probability of the query Q in the document's language model . Commonly, the unigram language model is used for this purpose—otherwise known as the bag of words model.
Data sparsity is a major problem in building language models. Most possible word sequences will not be observed in training. One solution is to make the assumption that the probability of a word only depends on the previous n words. This is known as an n-gram model or unigram model when n = 1.""
""Similarity measures "",,,,""In statistics and related fields, a similarity measure or similarity function is a real-valued function that quantifies the similarity between two objects. Although no single definition of a similarity measure exists, usually such measures are in some sense the inverse of distance metrics: they take on large values for similar objects and either zero or a negative value for very dissimilar objects. E.g., in the context of cluster analysis, Frey and Dueck suggest defining a similarity measure

where  is the squared Euclidean distance.
Cosine similarity is a commonly used similarity measure for real-valued vectors, used in (among other fields) information retrieval to score the similarity of documents in the vector space model. In machine learning, common kernel functions such as the RBF kernel can be viewed as similarity functions.""
""Learning to rank "",,,,""Learning to rank or machine-learned ranking (MLR) is the application of machine learning, typically supervised, semi-supervised or reinforcement learning, in the construction of ranking models for information retrieval systems. Training data consists of lists of items with some partial order specified between items in each list. This order is typically induced by giving a numerical or ordinal score or a binary judgment (e.g. "relevant" or "not relevant") for each item. The ranking model's purpose is to rank, i.e. produce a permutation of items in new, unseen lists in a way which is "similar" to rankings in the training data in some sense.
Learning to rank is a relatively new research area which has emerged in the past decade.""
""Combination, fusion and federated search "",,,,""Brazil (/brəˈzɪl/; Portuguese: Brasil [bɾaˈziw]), officially the Federative Republic of Brazil (Portuguese: República Federativa do Brasil,  listen ) is the largest sovereign state in Latin America. A federal republic, Brazil is the world's fifth-largest country, by both geographical area and total population. It is the largest Portuguese-speaking country in the world, and the only one in the Americas.
Bounded by the Atlantic Ocean on the east, Brazil has a coastline of 7,491 km (4,655 mi). It borders all other South American countries except Ecuador and Chile and occupies 47.3 percent of the continent of South America. Its Amazon River basin includes a vast tropical forest, home to diverse wildlife, a variety of ecological systems, and extensive natural resources spanning numerous protected habitats. This unique environmental heritage makes Brazil one of 17 megadiverse countries, and is the subject of significant global interest and debate regarding deforestation and environmental protection.
Brazil was inhabited by numerous tribal nations prior to the landing in 1500 of explorer Pedro Álvares Cabral, who claimed the area for the Portuguese Empire. Brazil remained a Portuguese colony until 1808, when the capital of the empire was transferred from Lisbon to Rio de Janeiro. In 1815, the colony was elevated to the rank of kingdom upon the formation of the United Kingdom of Portugal, Brazil and the Algarves. Independence was achieved in 1822 with the creation of the Empire of Brazil, a unitary state governed under a constitutional monarchy and a parliamentary system. The ratification of the first constitution in 1824 led to the formation of a bicameral legislature, now called the National Congress. The country became a presidential republic in 1889 following a military coup d'état. An authoritarian military junta came to power in 1964 and ruled until 1985, after which civilian governance resumed. Brazil's current constitution, formulated in 1988, defines it as a federal republic. The federation is composed of the union of the Federal District, the 26 states, and the 5,570 municipalities. Brazil has the world's highest population of Roman Catholics and is the world's most populous Catholic-majority country.
Brazil is a developed country. Brazil's economy is the world's ninth-largest by nominal GDP and seventh-largest by GDP (PPP) as of 2015. A member of the BRICS group, Brazil until 2010 had one of the world's fastest growing major economies, with its economic reforms giving the country new international recognition and influence. Brazil's national development bank plays an important role for the country's economic growth. Brazil is a founding member of the United Nations, the G20, BRICS, Unasul, Mercosul, Organization of American States, Organization of Ibero-American States, CPLP and the Latin Union. Brazil is a regional power in Latin America and a middle power in international affairs, with some analysts identifying it as an emerging global power. One of the world's major breadbaskets, Brazil has been the largest producer of coffee for the last 150 years.
^ Exército Brasileiro. "Hino à Bandeira Nacional" (in Portuguese). Retrieved January 29, 2014. 
^ "Demographics". Brazilian Government. Archived from the original on 17 November 2011. Retrieved 8 October 2011. 
^ Libras, Brazilian Portuguese sign language, is officially recognized as a legal means of communication in "Lei 10.436". Brazilian Government. Retrieved 23 July 2015. 
^ "Caracteristicas da População e dos Domicílios do Censo Demográfico 2010 — Cor ou raça" (PDF). Retrieved 7 April 2012. 
^ "Brazil". IBGE population estimates.
^ a b 
^ Country Comparison to the World: Gini Index – Brazil The World Factbook. Retrieved on 3 April 2012.
^ "2015 Human Development Report Summary" (PDF). United Nations Development Programme. 2015. Retrieved 14 December 2015. 
^ The European Portuguese pronunciation is IPA: [bɾɐˈziɫ]
^ José María Bello (1966). A History of Modern Brazil: 1889–1964. Stanford University Press. p. 56. ISBN 978-0-8047-0238-6. 
^ S. George Philander (2012). Encyclopedia of Global Warming and Climate Change, Second Edition. Princeton University. p. 148. ISBN 978-1-4129-9261-9. 
^ John J. Crocitti; Monique Vallance (2011). Brazil Today: An Encyclopedia of Life in the Republic. South Dakota State University. p. 23. ISBN 978-0-313-34673-6. 
^ a b "Geography of Brazil". The World Factbook. Central Intelligence Agency. 2008. Retrieved 3 June 2008. 
^ "BRAZIL – Land". Archived from the original on 23 October 2014. 
^ "Brazilian Federal Constitution" (in Portuguese). Presidency of the Republic. 1988. Retrieved 3 June 2008.  "Brazilian Federal Constitution". v-brazil.com. 2007. Retrieved 3 June 2008. Unofficial translate 
^ IMF, October 2015
^ "CIA – The World Factbook – Country Comparisons – GDP (purchasing power parity)". Cia.gov. Retrieved 25 January 2011. 
^ Clendenning, Alan (17 April 2008). "Booming Brazil could be world power soon". USA Today – The Associated Press. p. 2. Retrieved 12 December 2008. 
^ Fernando J. Cardim de Carvalho (January 2013). "Relative insulation". D+C Development and Cooperation/ dandc.eu. 
^ Clare Ribando Seelke (2010). Brazil-U. S. Relations. Congressional Research Service. p. 1. ISBN 978-1-4379-2786-3. 
^ Jorge Dominguez; Byung Kook Kim (2013). Between Compliance and Conflict: East Asia Latin America and the New Pax Americana. Center for International Affairs, Harvard University. pp. 98–99. ISBN 978-1-136-76983-2. 
^""
""Information retrieval diversity "",,,,""PRESANS is an open innovation company organizing global online calls for expertise.""
""Top-k retrieval in databases "",,,,""Information retrieval (IR) is the activity of obtaining information resources relevant to an information need from a collection of information resources. Searches can be based on metadata or on full-text (or other content-based) indexing.
Automated information retrieval systems are used to reduce what has been called "information overload". Many universities and public libraries use IR systems to provide access to books, journals and other documents. Web search engines are the most visible IR applications.""
""Novelty in information retrieval "",,,,""In information science and information retrieval, relevance denote how well a retrieved document or set of documents meets the information need of the user. Relevance may include concerns such as timeliness, authority or novelty of the result.""
""Question answering "",,,,""Question answering (QA) is a computer science discipline within the fields of information retrieval and natural language processing (NLP), which is concerned with building systems that automatically answer questions posed by humans in a natural language.
A QA implementation, usually a computer program, may construct its answers by querying a structured database of knowledge or information, usually a knowledge base. More commonly, QA systems can pull answers from an unstructured collection of natural language documents.
Some examples of natural language document collections used for QA systems include:
a local collection of reference texts
internal organization documents and web pages
compiled newswire reports
a set of Wikipedia pages
a subset of World Wide Web pages
QA research attempts to deal with a wide range of question types including: fact, list, definition, How, Why, hypothetical, semantically constrained, and cross-lingual questions.
Closed-domain question answering deals with questions under a specific domain (for example, medicine or automotive maintenance), and can be seen as an easier task because NLP systems can exploit domain-specific knowledge frequently formalized in ontologies. Alternatively, closed-domain might refer to a situation where only a limited type of questions are accepted, such as questions asking for descriptive rather than procedural information. QA systems in the context of machine reading applications have also been constructed in the medical domain, for instance related to Alzheimers disease 
Open-domain question answering deals with questions about nearly anything, and can only rely on general ontologies and world knowledge. On the other hand, these systems usually have much more data available from which to extract the answer.

""
""Document filtering "",,,,""An IFilter is a plugin that allows Microsoft's search engines to index various file formats (as documents, email attachments, database records, audio metadata etc.) so that they become searchable. Without an appropriate IFilter, contents of a file cannot be parsed and indexed by the search engine.
They can be obtained as standalone packages or bundled with certain software such as Adobe Reader, LibreOffice, Microsoft Office and OpenOffice.
It also refers to the software interface needed to implement such plugins.""
""Recommender systems "",,,,""Recommender systems or recommendation systems (sometimes replacing "system" with a synonym such as platform or engine) are a subclass of information filtering system that seek to predict the 'rating' or 'preference' that a user would give to an item.
Recommender systems have become extremely common in recent years, and are applied in a variety of applications. The most popular ones are probably movies, music, news, books, research articles, search queries, social tags, and products in general. However, there are also recommender systems for experts, collaborators, jokes, restaurants, financial services, life insurance, persons (online dating), and Twitter followers.""
""Information extraction "",,,,""Information extraction (IE) is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents. In most of the cases this activity concerns processing human language texts by means of natural language processing (NLP). Recent activities in multimedia document processing like automatic annotation and content extraction out of images/audio/video could be seen as information extraction.
Due to the difficulty of the problem, current approaches to IE focus on narrowly restricted domains. An example is the extraction from news wire reports of corporate mergers, such as denoted by the formal relation:
,
from an online news sentence such as:
"Yesterday, New York based Foo Inc. announced their acquisition of Bar Corp."
A broad goal of IE is to allow computation to be done on the previously unstructured data. A more specific goal is to allow logical reasoning to draw inferences based on the logical content of the input data. Structured data is semantically well-defined data from a chosen target domain, interpreted with respect to category and context.
Information Extraction is the part of a greater puzzle which deals with the problem of devising automatic methods for text management, beyond its transmission, storage and display. The discipline of information retrieval (IR) has developed automatic methods, typically of a statistical flavor, for indexing large document collections and classifying documents. Another complementary approach is that of natural language processing (NLP) which has solved the problem of modelling human language processing with considerable success when taking into account the magnitude of the task. In terms of both difficulty and emphasis, IE deals with tasks in between both IR and NLP. In terms of input, IE assumes the existence of a set of documents in which each document follows a template, i.e. describes one or more entities or events in a manner that is similar to those in other documents but differing in the details. An example, consider a group of newswire articles on Latin American terrorism with each article is presumed to be based upon one or more terroristic acts. We also define for any given IE task a template, which is a(or a set of) case frame(s) to hold the information contained in a single document. For the terrorism example, a template would have slots corresponding to the perpetrator, victim, and weapon of the terroristic act, and the date on which the event happened. An IE system for this problem is required to “understand” an attack article only enough to find data corresponding to the slots in this template.""
""Sentiment analysis "",,,,""Sentiment analysis (also known as opinion mining) refers to the use of natural language processing, text analysis and computational linguistics to identify and extract subjective information in source materials. Sentiment analysis is widely applied to reviews and social media for a variety of applications, ranging from marketing to customer service.
Generally speaking, sentiment analysis aims to determine the attitude of a speaker or a writer with respect to some topic or the overall contextual polarity of a document. The attitude may be his or her judgment or evaluation (see appraisal theory), affective state (that is to say, the emotional state of the author when writing), or the intended emotional communication (that is to say, the emotional effect the author wishes to have on the reader).""
""Expert search "",,,,""Areas where information retrieval techniques are employed include (the entries are in alphabetical order within each category):""
""Near-duplicate and plagiarism detection "",,,,""Plagiarism detection is the process of locating instances of plagiarism within a work or document. The widespread use of computers and the advent of the Internet has made it easier to plagiarize the work of others. Most cases of plagiarism are found in academia, where documents are typically essays or reports. However, plagiarism can be found in virtually any field, including scientific papers, art designs, and source code.
Detection of plagiarism can be either manual or software-assisted. Manual detection requires substantial effort and excellent memory, and is impractical in cases where too many documents must be compared, or original documents are not available for comparison. Software-assisted detection allows vast collections of documents to be compared to each other, making successful detection much more likely.
The practice of plagiarizing by use of sufficient word substitutions to elude detection software is known as rogeting.""
""Clustering and classification "",,,,""Apache Mahout is a project of the Apache Software Foundation to produce free implementations of distributed or otherwise scalable machine learning algorithms focused primarily in the areas of collaborative filtering, clustering and classification. Many of the implementations use the Apache Hadoop platform. Mahout also provides Java libraries for common maths operations (focused on linear algebra and statistics) and primitive Java collections. Mahout is a work in progress; the number of implemented algorithms has grown quickly, but various algorithms are still missing.
While Mahout's core algorithms for clustering, classification and batch based collaborative filtering are implemented on top of Apache Hadoop using the map/reduce paradigm, it does not restrict contributions to Hadoop-based implementations. Contributions that run on a single node or on a non-Hadoop cluster are also welcomed. For example, the 'Taste' collaborative-filtering recommender component of Mahout was originally a separate project and can run stand-alone without Hadoop.
Starting with the release 0.10.0, the project shifts its focus to building backend-independent programming environment, code named "Samsara". The environment consists of an algebraic backend-independent optimizer and an algebraic Scala DSL unifying in-memory and distributed algebraic operators. At the time of this writing supported algebraic platforms are Apache Spark and H20, with Apache Flink physical operator set support still being in progress. Support for MapReduce algorithms is being gradually phased out.""
""Business intelligence "",,,,""Business intelligence (BI) can be described as "a set of techniques and tools for the acquisition and transformation of raw data into meaningful and useful information for business analysis purposes". The term "data surfacing" is also more often associated with BI functionality. BI technologies are capable of handling large amounts of unstructured data to help identify, develop and otherwise create new strategic business opportunities. The goal of BI is to allow for the easy interpretation of these large volumes of data. Identifying new opportunities and implementing an effective strategy based on insights can provide businesses with a competitive market advantage and long-term stability.
BI technologies provide historical, current and predictive views of business operations. Common functions of business intelligence technologies are reporting, online analytical processing, analytics, data mining, process mining, complex event processing, business performance management, benchmarking, text mining, predictive analytics and prescriptive analytics.
BI can be used to support a wide range of business decisions ranging from operational to strategic. Basic operating decisions include product positioning or pricing. Strategic business decisions include priorities, goals and directions at the broadest level. In all cases, BI is most effective when it combines data derived from the market in which a company operates (external data) with data from company sources internal to the business such as financial and operations data (internal data). When combined, external and internal data can provide a more complete picture which, in effect, creates an "intelligence" that cannot be derived by any singular set of data. Amongst myriad uses, BI tools empower organisations to gain insight into new markets, assess demand and suitability of products and services for different market segments and gauge the impact of marketing efforts.""
""Relevance assessment "",,,,""In information science and information retrieval, relevance denote how well a retrieved document or set of documents meets the information need of the user. Relevance may include concerns such as timeliness, authority or novelty of the result.""
""Retrieval effectiveness "",,,,""The Binary Independence Model (BIM) is a probabilistic information retrieval technique that makes some simple assumptions to make the estimation of document/query similarity probability feasible.""
""Retrieval efficiency "",,,,""Terry Archer Welch was an American computer scientist. Along with Abraham Lempel and Jacob Ziv, he developed the lossless Lempel–Ziv–Welch (LZW) compression algorithm, which was published in 1984.""
""Presentation of retrieval results "",,,,""Retrieval-induced forgetting (or RIF) is a memory phenomenon where remembering causes forgetting of other information in memory. The phenomenon was first demonstrated in 1994, although the concept of RIF has been previously discussed in the context of retrieval inhibition.
RIF is demonstrated through a three-phase experiment consisting of study, practice of some studied material, and a final test of all studied material. Such experiments have also used multiple kinds of final tests including recall using only category cues, recall using category and word stems, and recognition tests. The effect has been produced using many different kinds of materials, can be produced in group settings, and is reduced in special populations such as individuals with attention deficit hyperactivity disorder (ADHD) or schizophrenia.
Although RIF occurs as a consequence of conscious remembering through explicit retrieval, the actual forgetting is thought to occur implicitly, below the level of awareness. Cognitive psychologists continue to debate why RIF occurs, and how it relates to the larger picture of memory and general cognition. In particular, researchers are divided on the idea of whether the forgetting is caused by a process that actively inhibits information, or due to interference from other information in memory. Inhibition associated with RIF has been looked at as similar to forms of physical inhibition. RIF has also been tied to memory retrieval strategies, with disrupting such strategies affecting the phenomenon.

""
""Search engine indexing "",,,,""Search engine indexing collects, parses, and stores data to facilitate fast and accurate information retrieval. Index design incorporates interdisciplinary concepts from linguistics, cognitive psychology, mathematics, informatics, and computer science. An alternate name for the process in the context of search engines designed to find web pages on the Internet is web indexing.
Popular engines focus on the full-text indexing of online, natural language documents. Media types such as video and audio and graphics are also searchable.
Meta search engines reuse the indices of other services and do not store a local index, whereas cache-based search engines permanently store the index along with the corpus. Unlike full-text indices, partial-text services restrict the depth indexed to reduce index size. Larger services typically perform indexing at a predetermined time interval due to the required time and processing costs, while agent-based search engines index in real time.""
""Search index compression "",,,,""The objective of image compression is to reduce irrelevance and redundancy of the image data in order to be able to store or transmit data in an efficient form.""
""Distributed retrieval "",,,,""A metasearch engine (or aggregator) is a search tool that uses another search engine's data to produce their own results from the Internet. Metasearch engines take input from a user and simultaneously send out queries to third party search engines for results. Sufficient data is gathered, formatted by their ranks and presented to the users.
Information stored on the World Wide Web is constantly expanding, making it increasingly impossible for a single search engine to index the entire web for resources. A metasearch engine is a solution to overcome this limitation. By combining multiple results from different search engines, a metasearch engine is able to enhance the user’s experience for retrieving information, as less effort is required in order to access more materials. A metasearch engine is efficient, as it is capable of generating a large volume of data, however, scores of websites stored on search engines are all different: this can draw in irrelevant documents. Other problems such as spamming also significantly reduce the accuracy of the search. The process of fusion aims to tackle this issue and improve the engineering of a metasearch engine. There are many types of metasearch engines available to allow users to access specialised information in a particular field. These include Savvysearch engine and Metaseek engine.""
""Peer-to-peer retrieval "",,,,""Search engine indexing collects, parses, and stores data to facilitate fast and accurate information retrieval. Index design incorporates interdisciplinary concepts from linguistics, cognitive psychology, mathematics, informatics, and computer science. An alternate name for the process in the context of search engines designed to find web pages on the Internet is web indexing.
Popular engines focus on the full-text indexing of online, natural language documents. Media types such as video and audio and graphics are also searchable.
Meta search engines reuse the indices of other services and do not store a local index, whereas cache-based search engines permanently store the index along with the corpus. Unlike full-text indices, partial-text services restrict the depth indexed to reduce index size. Larger services typically perform indexing at a predetermined time interval due to the required time and processing costs, while agent-based search engines index in real time.""
""Retrieval on mobile devices "",,,,""Mobile document access (MDA) is a methodology by which a mobile computer such as a cell phone or PDA, can retrieve, store, and otherwise access electronic documents and/or images of paper documents not specifically created for a mobile computing device. The term has some overlap with the concepts of Mobile content, Content Management Systems and Data conversion, and sometimes utilize the Mobile Web.""
""Adversarial retrieval "",,,,""Adversarial information retrieval (adversarial IR) is a topic in information retrieval related to strategies for working with a data source where some portion of it has been manipulated maliciously. Tasks can include gathering, indexing, filtering, retrieving and ranking information from such a data source. Adversarial IR includes the study of methods to detect, isolate, and defeat such manipulation.
On the Web, the predominant form of such manipulation is search engine spamming (also known as spamdexing), which involves employing various techniques to disrupt the activity of web search engines, usually for financial gain. Examples of spamdexing are link-bombing, comment or referrer spam, spam blogs (splogs), malicious tagging. Reverse engineering of ranking algorithms, advertisement blocking, click fraud, and web content filtering may also be considered forms of adversarial data manipulation.
Activities intended to poison the supply of useful data make search engines less useful for users. If search engines are more exclusionary they risk becoming more like directories and less dynamic.""
""Link and co-citation analysis "",,,,""Citation analysis is the examination of the frequency, patterns, and graphs of citations in articles and books. It uses citations in scholarly works to establish links to other works or other researchers. Citation analysis is one of the most widely used methods of bibliometrics. For example, bibliographic coupling and co-citation are association measures based on citation analysis (shared citations or shared references).
Automated citation indexing has changed the nature of citation analysis research, allowing millions of citations to be analyzed for large-scale patterns and knowledge discovery. The first example of automated citation indexing was CiteSeer, later to be followed by Google Scholar.
Today citation analysis tools are easily available to compute various impact measures for scholars based on data from citation indices. These have various applications, from the identification of expert referees to review papers and grant proposals, to providing transparent data in support of academic merit review, tenure, and promotion decisions. This competition for limited resources may lead to ethical questionable behavior to increase citations.
A great deal of criticism has been made of the practice of naively using citation analyses to compare the impact of different scholarly articles without taking into account other factors which may affect citation patterns. Among these criticisms, a recurrent one focuses on “field-dependent factors”, which refers to the fact that citation practices vary from one area of science to another, and even between fields of research within a discipline.
^ Rubin, Richard (2010). Foundations of library and information science (3rd ed.). New York: Neal-Schuman Publishers. ISBN 978-1-55570-690-6. 
^ Garfield, E. Citation Indexing - Its Theory and Application in Science, Technology and Humanities Philadelphia:ISI Press, 1983.
^ "Dimension of Citation Analysis". Retrieved 1 July 2012.  by Loet Leydesdorff and Olga Amsterdamska
^ Giles, C. Lee; Bollacker, Kurt D.; Lawrence, Steve (1998), "CiteSeer: an automatic citation indexing system.", Digital libraries 98 : the Third ACM Conference on Digital Libraries, June 23–26, 1998, Pittsburgh, PA (New York: Association for Computing Machinery): 89–98, doi:10.1145/276675.276685, ISBN 0-89791-965-3, retrieved July 7, 2011 
^ Examples include subscription-based tools based on proprietary data, such as Web of Science and Scopus, and free tools based on open data, such as Scholarometer by Filippo Menczer and his team.
^ Kaur, Jasleen; Diep Thi Hoang; Xiaoling Sun; Lino Possamai; Mohsen JafariAsbagh; Snehal Patil; Filippo Menczer (2012). "Scholarometer: A Social Framework for Analyzing Impact across Disciplines". PLOS ONE 7 (9): e43235. doi:10.1371/journal.pone.0043235. 
^ Hoang, D.; Kaur, J.; Menczer, F. (2010), "Crowdsourcing Scholarly Data", Proceedings of the WebSci10: Extending the Frontiers of Society On-Line, April 26-27th, 2010, Raleigh, NC: US 
^ Anderson, M.S. van; Ronning, E.A. van; de Vries, R.; Martison, B.C. (2007). "The perverse effects of competition on scientists’ work and relationship". Science and Engineering Ethics 4 (13): 437–461. doi:10.1007/s11948-007-9042-5. 
^ Wesel, M. van (2016). "Evaluation by Citation: Trends in Publication Behavior, Evaluation Criteria, and the Strive for High Impact Publications". Science and Engineering Ethics 22 (1): 199–225. doi:10.1007/s11948-015-9638-0. 
^ Bornmann, L.; Daniel, H. D. (2008). "What do citation counts measure? A review of studies on citing behavior". Journal of Documentation 64 (1): 45–80. doi:10.1108/00220410810844150. 
^ Anauati, Maria Victoria and Galiani, Sebastian and Gálvez, Ramiro H., Quantifying the Life Cycle of Scholarly Articles Across Fields of Economic Research (November 11, 2014). Available at SSRN: http://ssrn.com/abstract=2523078""
""Structured text search "",,,,""Enamine Ltd is a chemical company headquartered in Kiev, Ukraine. Located in Eastern Europe, it is a provider of high throughput screening compounds, chemical building blocks and fine chemicals. It also offers integrated services of custom synthesis, lead optimization and molecular modelling to support customers in the design of new drugs and other bioactive products.
Enamine focuses on three main technologies: combinatorial chemistry, chemoinformatics, organic synthesis research.
Enamine operates an e-commerce service, which provides a way for navigating a database of above 10 million building blocks and screening compounds by means of structure search, text search, CAS number, and by loading chemical data files. The service supports online ordering and billing systems.""
""Mathematics retrieval "",,,,""The Information Retrieval Facility (IRF), founded 2006 and located in Vienna, Austria, was a research platform for networking and collaboration for professionals in the field of information retrieval. It ceased operations in 2012.
The IRF had members in the following categories:
Researchers in information retrieval (IR) or related scientific areas
Industrial/corporate information management professionals
Patent authorities and governmental institutions
Students of one of the above""
""Chemical and biochemical retrieval "",,,,""Cheminformatics (also known as chemoinformatics, chemioinformatics and chemical informatics) is the use of computer and informational techniques applied to a range of problems in the field of chemistry. These in silico techniques are used in, for example, pharmaceutical companies in the process of drug discovery. These methods can also be used in chemical and allied industries in various other forms.""
""Multilingual and cross-lingual retrieval "",,,,""Cross-language information retrieval (CLIR) is a subfield of information retrieval dealing with retrieving information written in a language different from the language of the user's query. For example, a user may pose their query in English but retrieve relevant documents written in French. To do so, most of CLIR systems use translation techniques. CLIR techniques can be classified into different categories based on different translation resources:
Dictionary-based CLIR techniques
Parallel corpora based CLIR techniques
Comparable corpora based CLIR techniques
Machine translator based CLIR techniques
The first workshop on CLIR was held in Zürich during the SIGIR-96 conference. Workshops have been held yearly since 2000 at the meetings of the Cross Language Evaluation Forum (CLEF).
The term "cross-language information retrieval" has many synonyms, of which the following are perhaps the most frequent: cross-lingual information retrieval, translingual information retrieval, multilingual information retrieval. The term "multilingual information retrieval" refers to CLIR in general, but it also has a specific meaning of cross-language information retrieval where a document collection is multilingual.
Google Search had a cross-language search feature that was removed in 2013.

""
""Image search "",,,,""An image retrieval system is a computer system for browsing, searching and retrieving images from a large database of digital images. Most traditional and common methods of image retrieval utilize some method of adding metadata such as captioning', keywords, or descriptions to the images so that retrieval can be performed over the annotation words. Manual image annotation is time-consuming, laborious and expensive; to address this, there has been a large amount of research done on automatic image annotation. Additionally, the increase in social web applications and the semantic web have inspired the development of several web-based image annotation tools.
The first microcomputer-based image database retrieval system was developed at MIT, in the 1990s, by Banireddy Prasaad, Amar Gupta, Hoo-min Toong, and Stuart Madnick.
A 2008 survey article documented progresses after 2007.""
""Video search "",,,,""A video search engine is a web-based search engine which crawls the web for video content. Some video search engines parse externally hosted content while others allow content to be uploaded and hosted on their own servers. Some engines also allow users to search by video format type and by length of the clip. The video search results are usually accompanied by a thumbnail view of the video.
Video search engines are computer programs designed to find videos stored on digital devices, either through Internet servers or in storage units from the same computer. These searches can be made through audiovisual indexing, which can extract information from audiovisual material and record it as metadata, which will be tracked by search engines.""
""Speech / audio search "",,,,""HTML5 Audio is a subject of the HTML5 specification, incorporating audio input, playback, and synthesis, as well as speech to text, in the browser.""
""Music retrieval "",,,,""Music information retrieval (MIR) is the interdisciplinary science of retrieving information from music. MIR is a small but growing field of research with many real-world applications. Those involved in MIR may have a background in musicology, psychology, academic music study, signal processing, machine learning or some combination of these.""
""Enterprise search "",,,,""Enterprise search is the practice of making content from multiple enterprise-type sources, such as databases and intranets, searchable to a defined audience.
"Enterprise Search" is used to describe the software of search information within an enterprise (though the search function and its results may still be public). Enterprise search can be contrasted with web search, which applies search technology to documents on the open web, and desktop search, which applies search technology to the content on a single computer.
Enterprise search systems index data and documents from a variety of sources such as: file systems, intranets, document management systems, e-mail, and databases. Many enterprise search systems integrate structured and unstructured data in their collections. Enterprise search systems also use access controls to enforce a security policy on their users.
Enterprise search can be seen as a type of vertical search of an enterprise.""
""Desktop search "",,,,""Desktop search tools search within a user's own computer files as opposed to searching the Internet. These tools are designed to find information on the user's PC, including web browser history, e-mail archives, text documents, sound files, images, and video.
One of the main advantages of desktop search programs is that search results are displayed quickly due to the use of proper indexes.
A variety of desktop search programs are now available; see this list for examples.
Desktop search emerged as a concern for large firms for two main reasons: untapped productivity and security. On the one hand, users need to be able to quickly find relevant files, but on the other hand, they shouldn't have access to restricted files. According to analyst firm Gartner, up to 80% of some companies' data is locked up inside unstructured data — the information stored on an end user's PC, the directories (folders) and files they've created on a network, documents stored in repositories such as corporate intranets and a multitude of other locations. Moreover, many companies have structured or unstructured information stored in older file formats to which they don't have ready access.
Companies doing business in the United States are frequently required under regulatory mandates like Sarbanes-Oxley, HIPAA and FERPA to make sure that access to sensitive information is 100% controlled. This creates a challenge for IT organizations, which may not have a desktop search standard, or lack strict central control over end users downloading tools from the Internet. Some consumer-oriented desktop search tools make it possible to generate indexes outside the corporate firewall and share those indexes with unauthorized users. In some cases, end users are able to index — but not preview — items they should not even know exist.
Historically, full desktop search comes from the work of Apple Computer's Advanced Technology Group, resulting in the underlying AppleSearch technology in the early 1990s. It was used to build the Sherlock search engine and then developed into Spotlight, which brought automated, non-timer-based full indexing into the operating system.""
""Web and social media search "",,,,""A web feed (or news feed) is a data format used for providing users with frequently updated content. Content distributors syndicate a web feed, thereby allowing users to subscribe to it. Making a collection of web feeds accessible in one spot is known as aggregation, which is performed by a news aggregator. A web feed is also sometimes referred to as a syndicated feed.
A typical scenario of web feed use is: a content provider publishes a feed link on their site which end users can register with an aggregator program (also called a feed reader or a news reader) running on their own machines; doing this is usually as simple as dragging the link from the web browser to the aggregator. When instructed, the aggregator asks all the servers in its feed list if they have new content; if so, the aggregator either makes a note of the new content or downloads it. Aggregators can be scheduled to check for new content periodically. Web feeds are an example of pull technology, although they may appear to push content to the user.
The kinds of content delivered by a web feed are typically HTML (webpage content) or links to webpages and other kinds of digital media. Often when websites provide web feeds to notify users of content updates, they only include summaries in the web feed rather than the full content itself.
Web feeds are operated by many news websites, weblogs, schools, and podcasters.""
""Key management "",,,,""Key management is the management of cryptographic keys in a cryptosystem. This includes dealing with the generation, exchange, storage, use, and replacement of keys. It includes cryptographic protocol design, key servers, user procedures, and other relevant protocols.
Key management concerns keys at the user level, either between users or systems. This is in contrast to key scheduling; key scheduling typically refers to the internal handling of key material within the operation of a cipher.
Successful key management is critical to the security of a cryptosystem. In practice it is arguably the most difficult aspect of cryptography because it involves system policy, user training, organizational and departmental interactions, and coordination between all of these elements.""
""Digital signatures "",,,,""A digital signature is a mathematical scheme for demonstrating the authenticity of a digital message or documents. A valid digital signature gives a recipient reason to believe that the message was created by a known sender, that the sender cannot deny having sent the message (authentication and non-repudiation), and that the message was not altered in transit (integrity).
Digital signatures are a standard element of most cryptographic protocol suites, and are commonly used for software distribution, financial transactions, contract management software, and in other cases where it is important to detect forgery or tampering.""
""Public key encryption "",,,,""Public-key cryptography, or asymmetric cryptography, is any cryptographic system that uses two kinds of keys: public keys may be disseminated widely, while private keys are known only to the owner. In a public-key encryption system, any person can encrypt a message using the public key (better imagined as a lock) of the receiver and leave it on a public server or transmit it on a public network. Such a message can be decrypted only with the receiver's private key.
Public-key cryptography systems often rely on cryptographic algorithms based on mathematical problems that currently admit no efficient solution—particularly those inherent in certain integer factorization, discrete logarithm, and elliptic curve relationships. It is computationally easy for a user to generate a public and private key-pair and to use it for encryption and decryption. The strength lies in the "impossibility" (computational impracticality) for a properly generated private key to be determined from its corresponding public key. Thus the public key may be published without compromising security. Security depends only on keeping the private key private. Public key algorithms, unlike symmetric key algorithms, do not require a secure channel for the initial exchange of one (or more) secret keys between the parties.
Because of the computational complexity of asymmetric encryption, it is usually used only for small blocks of data, typically the transfer of a symmetric encryption key (e.g. a session key). This symmetric key is then used to encrypt the rest of the potentially long message sequence. The symmetric encryption/decryption is based on simpler algorithms and is much faster.
Message authentication involves hashing the message to produce a "digest," and encrypting the digest with the private key to produce a digital signature. Thereafter anyone can verify this signature by (1) computing the hash of the message, (2) decrypting the signature with the signer's public key, and (3) comparing the computed digest with the decrypted digest. Equality between the digests confirms the message is unmodified since it was signed, and that the signer, and no one else, intentionally performed the signature operation — presuming the signer's private key has remained secret. The security of such procedure depends on a hash algorithm of such quality that it is computationally impossible to alter or find a substitute message that produces the same digest - but studies have shown that even with the MD5 and SHA-1 algorithms, producing an altered or substitute message is not impossible. The current hashing standard for encryption is SHA-2. The message itself can also be used in place of the digest.
Public-key algorithms are fundamental security ingredients in cryptosystems, applications and protocols. They underpin various Internet standards, such as Transport Layer Security (TLS), S/MIME, PGP, and GPG. Some public key algorithms provide key distribution and secrecy (e.g., Diffie–Hellman key exchange), some provide digital signatures (e.g., Digital Signature Algorithm), and some provide both (e.g., RSA).
Public-key cryptography finds application in, among others, the information technology security discipline, information security. Information security (IS) is concerned with all aspects of protecting electronic information assets against security threats. Public-key cryptography is used as a method of assuring the confidentiality, authenticity and non-repudiability of electronic communications and data storage.""
""Hash functions and message authentication codes "",,,,""Lars Ramkilde Knudsen (born 21 February 1962) is a Danish researcher in cryptography, particularly interested in the design and analysis of block ciphers, hash functions and message authentication codes (MACs).""
""Cryptanalysis and other attacks "",,,,""In cryptography, partitioning cryptanalysis is a form of cryptanalysis for block ciphers. Developed by Carlo Harpes in 1995, the attack is a generalization of linear cryptanalysis. Harpes originally replaced the bit sums (affine transformations) of linear cryptanalysis with more general balanced Boolean functions. He demonstrated a toy cipher that exhibits resistance against ordinary linear cryptanalysis but is susceptible to this sort of partitioning cryptanalysis. In its full generality, partitioning cryptanalysis works by dividing the sets of possible plaintexts and ciphertexts into efficiently-computable partitions such that the distribution of ciphertexts is significantly non-uniform when the plaintexts are chosen uniformly from a given block of the partition. Partitioning cryptanalysis has been shown to be more effective than linear cryptanalysis against variants of DES and CRYPTON. A specific partitioning attack called mod n cryptanalysis uses the congruence classes modulo some integer for partitions.""
""Information-theoretic techniques "",,,,""In computer science, the cell-probe model is a model of computation similar to the Random-access machine, except that all operations are free except memory access. This model is useful for proving lower bounds of algorithms for data structure problems.""
""Mathematical foundations of cryptography "",,,,""Cryptography or cryptology (from Greek κρυπτός kryptós, "hidden, secret"; and γράφειν graphein, "writing", or -λογία -logia, "study", respectively) is the practice and study of techniques for secure communication in the presence of third parties called adversaries. More generally, cryptography is about constructing and analyzing protocols that prevent third parties or the public from reading private messages; various aspects in information security such as data confidentiality, data integrity, authentication, and non-repudiation are central to modern cryptography. Modern cryptography exists at the intersection of the disciplines of mathematics, computer science, and electrical engineering. Applications of cryptography include ATM cards, computer passwords, and electronic commerce.
Cryptography prior to the modern age was effectively synonymous with encryption, the conversion of information from a readable state to apparent nonsense. The originator of an encrypted message (Alice) shared the decoding technique needed to recover the original information only with intended recipients (Bob), thereby precluding unwanted persons (Eve) from doing the same. The cryptography literature often uses Alice ("A") for the sender, Bob ("B") for the intended recipient, and Eve ("eavesdropper") for the adversary. Since the development of rotor cipher machines in World War I and the advent of computers in World War II, the methods used to carry out cryptology have become increasingly complex and its application more widespread.
Modern cryptography is heavily based on mathematical theory and computer science practice; cryptographic algorithms are designed around computational hardness assumptions, making such algorithms hard to break in practice by any adversary. It is theoretically possible to break such a system, but it is infeasible to do so by any known practical means. These schemes are therefore termed computationally secure; theoretical advances, e.g., improvements in integer factorization algorithms, and faster computing technology require these solutions to be continually adapted. There exist information-theoretically secure schemes that provably cannot be broken even with unlimited computing power—an example is the one-time pad—but these schemes are more difficult to implement than the best theoretically breakable but computationally secure mechanisms.
The growth of cryptographic technology has raised a number of legal issues in the information age. Cryptography's potential for use as a tool for espionage and sedition has led many governments to classify it as a weapon and to limit or even prohibit its use and export. In some jurisdictions where the use of cryptography is legal, laws permit investigators to compel the disclosure of encryption keys for documents relevant to an investigation. Cryptography also plays a major role in digital rights management and copyright infringement of digital media.
^ Liddell, Henry George; Scott, Robert; Jones, Henry Stuart; McKenzie, Roderick (1984). A Greek-English Lexicon. Oxford University Press. 
^ Rivest, Ronald L. (1990). "Cryptography". In J. Van Leeuwen. Handbook of Theoretical Computer Science 1. Elsevier. 
^ Bellare, Mihir; Rogaway, Phillip (21 September 2005). "Introduction". Introduction to Modern Cryptography. p. 10. 
^ 
^ Biggs, Norman (2008). Codes: An introduction to Information Communication and Cryptography. Springer. p. 171. 
^ 
^ 
^""
""Security requirements "",,,,""Requirements analysis in systems engineering and software engineering, encompasses those tasks that go into determining the needs or conditions to meet for a new or altered product or project, taking account of the possibly conflicting requirements of the various stakeholders, analyzing, documenting, validating and managing software or system requirements.
Requirements analysis is critical to the failure of a systems or software project. The requirements should be documented, actionable, measurable, testable, traceable, related to identified business needs or opportunities, and defined to a level of detail sufficient for system design.""
""Formal security models "",,,,""In computer science, an Access Control Matrix or Access Matrix is an abstract, formal security model of protection state in computer systems, that characterizes the rights of each subject with respect to every object in the system. It was first introduced by Butler W. Lampson in 1971.
An access matrix can be envisioned as a rectangular array of cells, with one row per subject and one column per object. The entry in a cell - that is, the entry for a particular subject-object pair - indicates the access mode that the subject is permitted to exercise on the object. Each column is equivalent to an access control list for the object; and each row is equivalent to an access profile for the subject.""
""Logic and verification "",,,,""Tobias Nipkow (born 1958) is a German computer scientist. He received his Diplom (MSc) in computer science from the Technische Hochschule Darmstadt in 1982, and his Ph.D. from the University of Manchester in 1987. He worked at MIT from 1987, changed to Cambridge University in 1989, and to Technical University Munich in 1992, where he was appointed professor for programming theory. He is chair of the Logic and Verification group since 2011.
He is known for his work in interactive and automatic theorem proving, in particular for the Isabelle proof assistant; he is the editor of the Journal of Automated Reasoning. Moreover, he focuses on programming language semantics, type systems and functional programming.
^ Brief vita""
""Biometrics "",,,,""Biometrics refers to metrics related to human characteristics. Biometrics authentication (or realistic authentication) is used in computer science as a form of identification and access control. It is also used to identify individuals in groups that are under surveillance.
Biometric identifiers are the distinctive, measurable characteristics used to label and describe individuals. Biometric identifiers are often categorized as physiological versus behavioral characteristics. Physiological characteristics are related to the shape of the body. Examples include, but are not limited to fingerprint, palm veins, face recognition, DNA, palm print, hand geometry, iris recognition, retina and odour/scent. Behavioral characteristics are related to the pattern of behavior of a person, including but not limited to typing rhythm, gait, and voice. Some researchers have coined the term behaviometrics to describe the latter class of biometrics.
More traditional means of access control include token-based identification systems, such as a driver's license or passport, and knowledge-based identification systems, such as a password or personal identification number. Since biometric identifiers are unique to individuals, they are more reliable in verifying identity than token and knowledge-based methods; however, the collection of biometric identifiers raises privacy concerns about the ultimate use of this information.
According to a CSO article the biometrics market will be worth US$13.8 billion in 2015.""
""Graphical / visual passwords "",,,,""A password is a word or string of characters used for user authentication to prove identity or access approval to gain access to a resource (example: an access code is a type of password), which should be kept secret from those not allowed access.
The use of passwords is known to be ancient. Sentries would challenge those wishing to enter an area or approaching it to supply a password or watchword, and would only allow a person or group to pass if they knew the password. In modern times, user names and passwords are commonly used by people during a log in process that controls access to protected computer operating systems, mobile phones, cable TV decoders, automated teller machines (ATMs), etc. A typical computer user has passwords for many purposes: logging into accounts, retrieving e-mail, accessing applications, databases, networks, web sites, and even reading the morning newspaper online.
Despite the name, there is no need for passwords to be actual words; indeed passwords which are not actual words may be harder to guess, a desirable property. Some passwords are formed from multiple words and may more accurately be called a passphrase. The terms passcode and passkey are sometimes used when the secret information is purely numeric, such as the personal identification number (PIN) commonly used for ATM access. Passwords are generally short enough to be easily memorized and typed.
Most organizations specify a password policy that sets requirements for the composition and usage of passwords, typically dictating minimum length, required categories (e.g. upper and lower case, numbers, and special characters), prohibited elements (e.g. own name, date of birth, address, telephone number). Some governments have national authentication frameworks that define requirements for user authentication to government services, including requirements for passwords.""
""Multi-factor authentication "",,,,""Multi-factor authentication (MFA) is a method of computer access control in which a user is only granted access after successfully presenting several separate pieces of evidence to an authentication mechanism - typically at least two of the following categories: knowledge (something they know); possession (something they have), and inherence (something they are).""
""Access control "",,,,""In the fields of physical security and information security, access control is the selective restriction of access to a place or other resource. The act of accessing may mean consuming, entering, or using. Permission to access a resource is called authorization.
Locks and login credentials are two analogous mechanisms of access control.""
""Pseudonymity, anonymity and untraceability "",,,,""An anonymizer or an anonymous proxy is a tool that attempts to make activity on the Internet untraceable. It is a proxy server computer that acts as an intermediary and privacy shield between a client computer and the rest of the Internet. It accesses the Internet on the user's behalf, protecting personal information by hiding the client computer's identifying information.""
""Digital rights management "",,,,""Digital rights management (DRM) schemes are various access control technologies that are used to restrict usage of proprietary hardware and copyrighted works. DRM technologies try to control the use, modification, and distribution of copyrighted works (such as software and multimedia content), as well as systems within devices that enforce these policies.
The use of digital rights management is not universally accepted. Proponents of DRM argue that it is necessary to prevent intellectual property from being copied freely, just as physical locks are needed to prevent personal property from being stolen, that it can help the copyright holder maintain artistic control, and that it can ensure continued revenue streams. Those opposed to DRM contend there is no evidence that DRM helps prevent copyright infringement, arguing instead that it serves only to inconvenience legitimate customers, and that DRM helps big business stifle innovation and competition. Furthermore, works can become permanently inaccessible if the DRM scheme changes or if the service is discontinued. DRM can also restrict users from exercising their legal rights under the copyright law, such as backing up copies of CDs or DVDs, lending materials out through a library, accessing works in the public domain, or using copyrighted materials for research and education under the fair use doctrine, and under French law. The Electronic Frontier Foundation (EFF) and the Free Software Foundation (FSF) consider the use of DRM systems to be an anti-competitive practice.
Worldwide, many laws have been created which criminalize the circumvention of DRM, communication about such circumvention, and the creation and distribution of tools used for such circumvention. Such laws are part of the Copyright Directive, the Digital Millennium Copyright Act, and DADVSI.
The term DRM is also sometimes referred to as "copy protection", "technical protection measures", "copy prevention", or "copy control", although the correctness of doing so is disputed.""
""Authorization "",,,,""Authorization or authorisation is the function of specifying access rights to resources related to information security and computer security in general and to access control in particular. More formally, "to authorize" is to define an access policy. For example, human resources staff is normally authorized to access employee records and this policy is usually formalized as access control rules in a computer system. During operation, the system uses the access control rules to decide whether access requests from (authenticated) consumers shall be approved (granted) or disapproved (rejected). Resources include individual files or an item's data, computer programs, computer devices and functionality provided by computer applications. Examples of consumers are computer users, computer programs and other devices on the computer.""
""Intrusion detection systems "",,,,""An intrusion detection system (IDS) is a device or software application that monitors network or system activities for malicious activities or policy violations and produces electronic reports to a management station. IDS come in a variety of "flavors" and approach the goal of detecting suspicious traffic in different ways. There are network based (NIDS) and host based (HIDS) intrusion detection systems. NIDS is a network security system focusing on the attacks that come from the inside of the network (authorized users). Some systems may attempt to stop an intrusion attempt but this is neither required nor expected of a monitoring system. Intrusion detection and prevention systems (IDPS) are primarily focused on identifying possible incidents, logging information about them, and reporting attempts. In addition, organizations use IDPSes for other purposes, such as identifying problems with security policies, documenting existing threats and deterring individuals from violating security policies. IDPSes have become a necessary addition to the security infrastructure of nearly every organization.
IDPSes typically record information related to observed events, notify security administrators of important observed events and produce reports. Many IDPSes can also respond to a detected threat by attempting to prevent it from succeeding. They use several response techniques, which involve the IDPS stopping the attack itself, changing the security environment (e.g. reconfiguring a firewall) or changing the attack's content.""
""Spoofing attacks "",,,,""In the context of network security, a spoofing attack is a situation in which one person or program successfully masquerades as another by falsifying data, thereby gaining an illegitimate advantage.""
""Phishing "",,,,""Phishing is the attempt to acquire sensitive information such as usernames, passwords, and credit card details (and sometimes, indirectly, money), often for malicious reasons, by masquerading as a trustworthy entity in an electronic communication. The word is a neologism created as a homophone of fishing due to the similarity of using a bait in an attempt to catch a victim. Communications purporting to be from popular social web sites, auction sites, banks, online payment processors or IT administrators are commonly used to lure unsuspecting victims. Phishing emails may contain links to websites that are infected with malware. Phishing is typically carried out by email spoofing or instant messaging, and it often directs users to enter details at a fake website whose look and feel are almost identical to the legitimate one. Phishing is an example of social engineering techniques used to deceive users, and exploits the poor usability of current web security technologies. Attempts to deal with the growing number of reported phishing incidents include legislation, user training, public awareness, and technical security measures. Many websites have now created secondary tools for applications, like maps for games, but they should be clearly marked as to who wrote them, and users should not use the same passwords anywhere on the internet.
Phishing is a continual threat, and the risk is even larger in social media such as Facebook, Twitter, and Google+. Hackers could create a clone of a website and tell you to enter personal information, which is then emailed to them. Hackers commonly take advantage of these sites to attack people using them at their workplace, homes, or in public in order to take personal and security information that can affect the user or company (if in a workplace environment). Phishing takes advantage of the trust that the user may have since the user may not be able to tell that the site being visited, or program being used, is not real; therefore, when this occurs, the hacker has the chance to gain the personal information of the targeted user, such as passwords, usernames, security codes, and credit card numbers, among other things.""
""Embedded systems security "",,,,""Mocana (founded 2004) is a San Francisco-based company that focuses on and embedded system security. One of its main products, Security of Things Platform, is a high-performance, ultra-optimized, OS-independent, high-assurance security solution for any device class. This decoupling of security implementation from the rest of mobile application development allows for easier development of software for devices comprising the "Internet of Things", in which numerous independent networked devices communicate with each other in various ways. Mocana was originally launched as an embedded systems security company, but more recently the company has shifted its focus to protecting mobile devices and the apps and data on them.""
""Hardware-based security protocols "",,,,""In computing, a firewall is a network security system that monitors and controls the incoming and outgoing network traffic based on predetermined security rules. A firewall typically establishes a barrier between a trusted, secure internal network and another outside network, such as the Internet, that is assumed not to be secure or trusted. Firewalls are often categorized as either network firewalls or host-based firewalls. Network firewalls are a software appliance running on general purpose hardware or hardware-based firewall computer appliances that filter traffic between two or more networks. Host-based firewalls provide a layer of software on one host that controls network traffic in and out of that single machine. Firewall appliances may also offer other functionality to the internal network they protect such as acting as a DHCP or VPN server for that network.""
""Side-channel analysis and countermeasures "",,,,""In cryptography, a side-channel attack is any attack based on information gained from the physical implementation of a cryptosystem, rather than brute force or theoretical weaknesses in the algorithms (compare cryptanalysis). For example, timing information, power consumption, electromagnetic leaks or even sound can provide an extra source of information, which can be exploited to break the system. Some side-channel attacks require technical knowledge of the internal operation of the system on which the cryptography is implemented, although others such as differential power analysis are effective as black-box attacks. Many powerful side-channel attacks are based on statistical methods pioneered by Paul Kocher.
Attempts to break a cryptosystem by deceiving or coercing people with legitimate access are not typically called side-channel attacks: see social engineering and rubber-hose cryptanalysis. For attacks on computer systems themselves (which are often used to perform cryptography and thus contain cryptographic keys or plaintexts), see computer security. The rise of Web 2.0 applications and software-as-a-service has also significantly raised the possibility of side-channel attacks on the web, even when transmissions between a web browser and server are encrypted (e.g., through HTTPS or WiFi encryption), according to researchers from Microsoft Research and Indiana University.""
""Hardware reverse engineering "",,,,""Engineering is the application of mathematics, empirical evidence and scientific, economic, social, and practical knowledge in order to invent, innovate, design, build, maintain, research, and improve structures, machines, tools, systems, components, materials, and processes.
The discipline of engineering is extremely broad, and encompasses a range of more specialized fields of engineering, each with a more specific emphasis on particular areas of applied science, technology and types of application.
The term Engineering is derived from the Latin ingenium, meaning "cleverness" and ingeniare, meaning "to contrive, devise".""
""Mobile platform security "",,,,""Windows Mobile is a family of mobile operating systems developed by Microsoft for smartphones and Pocket PCs.
Its origins dated back to Windows CE in 1996, though Windows Mobile itself first appeared in 2000 as PocketPC 2000. It was renamed "Windows Mobile" in 2003, at which point it came in several versions (similar to the desktop versions of Windows) and was aimed at business and enterprise consumers. By 2007, it was the most popular smartphone software in the U.S., but this popularity faded in the following years. In February 2010, facing competition from rival OSs including iOS and Android, Microsoft announced Windows Phone to supersede Windows Mobile. As a result, Windows Mobile has been deprecated. Windows Phone is incompatible with Windows Mobile devices and software. The last version of Windows Mobile, released after the announcement of Windows Phone, was 6.5.5. After this, Microsoft ceased development on Windows Mobile, in order to concentrate on Windows Phone.
^ Lextrait, Vincent (February 2010). "The Programming Languages Beacon, v10.0". Retrieved February 12, 2010. 
^ Evers, Joris (January 6, 2005). "Microsoft to phase out Pocket PC, Smartphone brands | Hardware". InfoWorld. IDG. Retrieved July 14, 2011. 
^ 
^ Nicholas Kolakowski (March 15, 2010). "Microsoft Explains Windows Phone Lack of Compatibility". eWeek. 
^ "Windows Phone: A New Kind of Phone (36:47 min. in)". Microsoft. June 13, 2010. Retrieved September 9, 2010.""
""Trusted computing "",,,,""Trusted Computing (TC) is a technology developed and promoted by the Trusted Computing Group. The term is taken from the field of trusted systems and has a specialized meaning. With Trusted Computing, the computer will consistently behave in expected ways, and those behaviors will be enforced by computer hardware and software. Enforcing this behavior is achieved by loading the hardware with a unique encryption key inaccessible to the rest of the system.
TC is controversial as the hardware is not only secured for its owner, but also secured against its owner. Such controversy has led opponents of trusted computing, such as free software activist Richard Stallman, to refer to it instead as treacherous computing, even to the point where some scholarly articles have begun to place scare quotes around "trusted computing".
Trusted Computing proponents such as International Data Corporation, the Enterprise Strategy Group and Endpoint Technologies Associates claim the technology will make computers safer, less prone to viruses and malware, and thus more reliable from an end-user perspective. In addition, they also claim that Trusted Computing will allow computers and servers to offer improved computer security over that which is currently available. Opponents often claim this technology will be used primarily to enforce digital rights management policies and not to increase computer security.
Chip manufacturers Intel and AMD, hardware manufacturers such as HP and Dell, and operating system providers such as Microsoft all plan to include Trusted Computing in coming generations of products. The U.S. Army requires that every new PC it purchases comes with a Trusted Platform Module (TPM). As of July 3, 2007, so does virtually the entire United States Department of Defense.""
""Virtualization and security "",,,,""openQRM is a free and open-source cloud computing management platform for managing heterogeneous data center infrastructures. The openQRM platform manages a data center's infrastructure to build private, public and hybrid IaaS (Infrastructure as a Service) clouds. openQRM orchestrates a multiplicity of storage, network, virtualization, monitoring, and security implementations technologies to deploy multi-tier services (e.g. compute clusters) as virtual machines on distributed infrastructures, combining both data center resources and remote cloud resources, according to allocation policies. According to the European Commission's report about the future of cloud computing from a group of experts:
The openQRM platform emphasizes a separation of hardware (physical servers and virtual machines) from software (operating system server-images). Hardware is treated agnostically as a computing resource which should be replaceable without the need to reconfigure the software.
Supported virtualization solutions include KVM, Linux-VServer, OpenVZ, VMware ESX and Xen. Virtual machines of these types are managed transparently via openQRM.
P2V (physical to virtual), V2P (virtual to physical), and V2V (virtual to virtual) migration are possible as well as transitioning from one virtualization technology to another with the same VM
openQRM is sponsored by openQRM Enterprise.

""
""Browser security "",,,,""Browser security is the application of Internet security  to web browsers in order to protect networked data and computer systems from breaches of privacy or malware. Security exploits of browsers often use JavaScript - sometimes with cross-site scripting (XSS) - sometimes with a secondary payload using Adobe Flash. Security exploits can also take advantage of vulnerabilities (security holes) that are commonly exploited in all browsers (including Mozilla Firefox, Google Chrome, Opera, Microsoft Internet Explorer, and Safari).
^ Internet security overview, retrieved 2015-07-06 
^ Maone, Giorgio. "NoScript :: Add-ons for Firefox". Mozilla Add-ons. Mozilla Foundation. 
^ NC (Social Science Research Network). "BetterPrivacy :: Add-ons for Firefox". Mozilla Add-ons. Mozilla Foundation. 
^ Keizer, Greg. Firefox 3.5 Vulnerability Confirmed. Retrieved 19 November 2010.
^ Messmer, Ellen and NetworkWorld. "Google Chrome Tops 'Dirty Dozen' Vulnerable Apps List". Retrieved 19 November 2010.
^ Skinner, Carrie-Ann. Opera Plugs "Severe" Browser Hole. Retrieved 19 November 2010.
^ Bradly, Tony. "It's Time to Finally Drop Internet Explorer 6" . Retrieved 19 November 2010.
^ "Browser". Mashable. Retrieved 2 September 2011.""
""Information flow control "",,,,""Information flow in an information theoretical context is the transfer of information from a variable  to a variable  in a given process.
Not all flows may be desirable. For example, a system should not leak any secret (partially or not) to public observers.""
""Denial-of-service attacks "",,,,""In computing, a denial-of-service (DoS) attack is an attempt to make a machine or network resource unavailable to its intended users, such as to temporarily or indefinitely interrupt or suspend services of a host connected to the Internet. A distributed denial-of-service (DDoS) is where the attack source is more than one, often thousands of, unique IP addresses. It is analogous to a group of people crowding the entry door or gate to a shop or business, and not letting legitimate parties enter into the shop or business, disrupting normal operations.
Criminal perpetrators of DoS attacks often target sites or services hosted on high-profile web servers such as banks, credit card payment gateways; but motives of revenge, blackmail or activism can be behind other attacks.""
""Penetration testing "",,,,""A penetration test, or sometimes pentest, is a software attack on a computer system that looks for security weaknesses, potentially gaining access to the computer's features and data.
The process typically identifies the target systems and a particular goal—then reviews available information and undertakes various means to attain the goal. A penetration test target may be a white box (which provides background and system information) or black box (which provides only basic or no information except the company name). A penetration test can help determine whether a system is vulnerable to attack, if the defenses were sufficient, and which defenses (if any) the test defeated.
Security issues that the penetration test uncovers should be reported to the system owner. Penetration test reports may also assess potential impacts to the organization and suggest countermeasures to reduce risk.
The goals of penetration tests are:
Determine feasibility of a particular set of attack vectors
Identify high-risk vulnerabilities from a combination of lower-risk vulnerabilities exploited in a particular sequence
Identify vulnerabilities that may be difficult or impossible to detect with automated network or application vulnerability scanning software
Assess the magnitude of potential business and operational impacts of successful attacks
Test the ability of network defenders to detect and respond to attacks
Provide evidence to support increased investments in security personnel and technology
Penetration tests are a component of a full security audit. For example, the Payment Card Industry Data Security Standard requires penetration testing on a regular schedule, and after system changes.""
""Vulnerability scanners "",,,,""A vulnerability scanner is a computer program designed to assess computers, computer systems, networks or applications for weaknesses.
They can be run either as part of vulnerability management by those tasked with protecting systems - or by black hat attackers looking to gain unauthorized access.

""
""File system security "",,,,""Program Files is the directory name of a standard folder in Microsoft Windows operating systems in which applications that are not part of the operating system are conventionally installed. Typically, each application installed under the 'Program Files' directory will have a subdirectory for its application-specific resources. Shared resources, for example resources used by multiple applications from one company, are typically stored in the 'Common Program Files' directory.
In a standard Windows installation, the 'Program Files' directory will be at %SystemDrive%\Program Files (or the localized equivalent thereof), and the 'Common Program Files' (or the localized equivalent thereof) will be a subdirectory under 'Program Files'. In Windows Vista and later, the paths to the 'Program Files' and 'Common Program Files' directories are not localized on disk. Instead, the localized names are NTFS junction points to the non-localized locations. Additionally, the Windows shell localizes the name of the Program Files folder depending on the system's user interface display language.
Both 'Program Files' and 'Common Program Files' can be moved. At system startup, the actual paths to 'Program Files' and 'Common Program Files' are loaded from the Windows registry, where they are stored in the ProgramFilesDir and CommonFilesDir values under HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion. They are then made accessible to the rest of the system via the volatile environment variables %ProgramFiles%, and %CommonProgramFiles%. Applications can also obtain the locations of these paths by querying the Setup API using dirids, or through Windows Management Instrumentation, or by querying the shell using CSIDLs, or ShellSpecialFolderConstants. These are all localization-independent methods.
x86-64 versions of Windows have two folders for application files; 'Program Files' folder serves as the default installation target for 64-bit programs, while the 'Program Files (x86)' folder is the default installation target for 32-bit programs that need WoW64 emulation layer. While 64-bit Windows versions also have a %ProgramFiles(x86)% environment variable, the dirids and CSIDLs are not different between 32-bit and 64-bit environments; the APIs merely return different results, depending on whether the calling process is emulated or not.
To be backwards compatible with the 8.3 limitations of the old File Allocation Table filenames, the names 'Program Files', 'Program Files (x86)' and 'Common Program Files' are shortened by the system to progra~N and common~N, where N is a digit, a sequence number that on a clean install will be 1 (or 1 and 2 when both 'Program Files' and 'Program Files (x86)' are present).
If Windows is installed on an NTFS volume, by default, the 'Program Files' folder can only be modified by members of the 'Administrators' user groups. This can be an issue for programs created for Windows 9x. Those operating systems had no file system security, and programs could therefore also store their data in 'Program Files'. Programs that store their data in 'Program Files' will usually not run correctly on Windows NT systems with normal user privileges unless security is lowered for the affected subdirectories. Windows Vista addressed this issue by introducing File and Registry Virtualization.""
""Security protocols "",,,,""SPORE, the Security Protocols Open Repository, is an online library of security protocols with comments and links to papers. Each protocol is downloadable in a variety of formats, including rules for use with automatic protocol verification tools. All protocols are described using BAN logic or the style used by Clark and Jacob, and their goals. The database includes details on formal proofs or known attacks, with references to comments, analysis & papers. A large number of protocols are listed, including many which have been shown to be insecure.
It is a continuation of the seminal work by John Clark and Jeremy Jacob.
They seek contributions for new protocols, links and comments.""
""Web protocol security "",,,,""The File Transfer Protocol (FTP) is a standard network protocol used to transfer computer files between a client and server on a computer network.
FTP is built on a client-server model architecture and uses separate control and data connections between the client and the server. FTP users may authenticate themselves with a clear-text sign-in protocol, normally in the form of a username and password, but can connect anonymously if the server is configured to allow it. For secure transmission that protects the username and password, and encrypts the content, FTP is often secured with SSL/TLS (FTPS). SSH File Transfer Protocol (SFTP) is sometimes also used instead, but is technologically different.
The first FTP client applications were command-line programs developed before operating systems had graphical user interfaces, and are still shipped with most Windows, Unix, and Linux operating systems. Many FTP clients and automation utilities have since been developed for desktops, servers, mobile devices, and hardware, and FTP has been incorporated into productivity applications, such as Web page editors.""
""Mobile and wireless security "",,,,""The Center for Technological Research of Crete (CTR-Crete) (Greek: Κέντρο Τεχνολογικής Έρευνας (ΚΤΕ) Κρήτης) in Heraklion was founded according to the presidential decree No. 143/Φ.Ε.Κ. 123/20-6-2001 and is under the supervision and financing of the Ministry of National Education and Religious Affairs (Greece). It is a Private Legal Body, self-governed within the framework of articles 11 and 12 par. 1 of 1771/1998 Greek law and its internal regulation. There are 55 staff member, including 25 research scientists, 15 assistant researchers, and 15 associate researchers. CTR-Crete is affiliated with the Technological Educational Institute (TEI) of Crete and includes eight Sectors of Technology Transfer and Research operating in all four regional units of Crete.""
""Denial-of-service attacks "",,,,""In computing, a denial-of-service (DoS) attack is an attempt to make a machine or network resource unavailable to its intended users, such as to temporarily or indefinitely interrupt or suspend services of a host connected to the Internet. A distributed denial-of-service (DDoS) is where the attack source is more than one, often thousands of, unique IP addresses. It is analogous to a group of people crowding the entry door or gate to a shop or business, and not letting legitimate parties enter into the shop or business, disrupting normal operations.
Criminal perpetrators of DoS attacks often target sites or services hosted on high-profile web servers such as banks, credit card payment gateways; but motives of revenge, blackmail or activism can be behind other attacks.""
""Data anonymization and sanitization "",,,,""Data anonymization is a type of information sanitization whose intent is privacy protection. It is the process of either encrypting or removing personally identifiable information from data sets, so that the people whom the data describe remain anonymous. The Privacy Technology Focus Group defines it as "technology that converts clear text data into a nonhuman readable and irreversible form, including but not limited to preimage resistant hashes (e.g., one-way hashes) and encryption techniques in which the decryption key has been discarded." Data anonymization enables the transfer of information across a boundary, such as between two departments within an agency or between two agencies, while reducing the risk of unintended disclosure, and in certain environments in a manner that enables evaluation and analytics post-anonymization. In the context of medical data, anonymized data refers to data from which the patient cannot be identified by the recipient of the information. The name, address, and full post code must be removed together with any other information which, in conjunction with other data held by or disclosed to the recipient, could identify the patient. De-anonymization is the reverse process in which anonymous data is cross-referenced with other data sources to re-identify the anonymous data source. Generalization and perturbation are the two popular anonymization approaches for relational data.""
""Information accountability and usage control "",,,,""Accountability software, or Internet accountability software is a program that allows Leaders who are influencing people to have a process to help accomplish a goal of doing or not doing something. The software may be used to help other people or yourself. The software is a support tool.
The software can centralize information with who has what goal, what is the current objective, what have they done, and methods to communicate between the leader and person and maybe with others in a group, and other desired information. The software can perform functions
A University of Scranton study published by Dan Diamond in Forbes  found 92% of people did not accomplish their New Year's resolution. "One reason is the person did not have someone to help them such as an accountable partner and there was no software to centralize information."
It is also a type of Internet usage monitoring software marketed by groups opposed to pornography, such as Christian groups in the United States. To try to avoid pornography use, some individuals install accountability software, and filtering software, on their own computers, smartphones, and tablets. Others install such software on their children's computers and devices.
Conservative American states like Utah have more paying pornography subscribers per capita than blue states, and are more likely to self-diagnose as "addicts", and best-selling Christian anti-masturbation books encourage Evangelical men to stop using internet pornography and focus sexual attention on their wives. Sarah Diefendorf, a sociologist at the University of Washington, found that Evangelical men who took an abstinence pledge before marriage "still struggle with issues like excessive pornography viewing, masturbation" when married.
"Internet accountability" is a neologism used to describe a commitment to refrain from using Internet pornography. Accountability software may monitor Internet use on a personal computer, or Internet use by a specific user on a computer. These software applications then generate reports of Internet use viewable by a third party, sometimes called an accountability partner.
"Internet accountability" refers to an accountability system that is provided using the internet using a personal computer, tablet, and/or smartphone.
"Accountable Partner" is a person or business working with someone to help them accomplish a goal.""
""Database activity monitoring "",,,,""Database activity monitoring (DAM) is a database security technology for monitoring and analyzing database activity that operates independently of the database management system (DBMS) and does not rely on any form of native (DBMS-resident) auditing or native logs such as trace or transaction logs. DAM is typically performed continuously and in real-time.
Database activity monitoring and prevention (DAMP) is an extension to DAM that goes beyond monitoring and alerting to also block unauthorized activities.
DAM helps businesses address regulatory compliance mandates like the Payment Card Industry Data Security Standard (PCI DSS), the Health Insurance Portability and Accountability Act (HIPAA), the Sarbanes-Oxley Act (SOX), U.S. government regulations such as NIST 800-53, and EU regulations.
DAM is also an important technology for protecting sensitive databases from external attacks by cybercriminals. According to the 2009 Verizon Business’ Data Breach Investigations Report—based on data analyzed from Verizon Business’ caseload of 90 confirmed breaches involving 285 million compromised records during 2008—75 percent of all breached records came from compromised database servers.
According to Gartner, “DAM provides privileged user and application access monitoring that is independent of native database logging and audit functions. It can function as a compensating control for privileged user separation-of-duties issues by monitoring administrator activity. The technology also improves database security by detecting unusual database read and update activity from the application layer. Database event aggregation, correlation and reporting provide a database audit capability without the need to enable native database audit functions (which become resource-intensive as the level of auditing is increased).”
According to a survey by the Independent Oracle User Group (IOUG), “Most organizations do not have mechanisms in place to prevent database administrators and other privileged database users from reading or tampering with sensitive information in financial, HR, or other business applications. Most are still unable to even detect such breaches or incidents.”
Forrester refers to this category as “database auditing and real-time protection”.""
""Software security engineering "",,,,""Gary McGraw is an American computer scientist, author, and researcher.""
""Web application security "",,,,""Web application security is a branch of Information Security that deals specifically with security of websites, web applications and web services. At a high level, Web application security draws on the principles of application security but applies them specifically to Internet and Web systems.

""
""Social network security and privacy "",,,,""Social network is a social structure made up of a set of social actors (such as individuals or organizations), sets of dyadic ties, and other social interactions between actors. Privacy concerns with social networking services is a subset of data privacy, involving the right of mandating personal privacy concerning storing, re-purposing, provision to third parties, and displaying of information pertaining to oneself via the Internet.
Social network security and privacy issues result from the astronomical amounts of information these sites process each day. Features that invite users to participate in—messages, invitations, photos, open platform applications and other applications are often the venues for others to gain access to a user's private information. In addition, the technologies needed to deal with user’s information may intrude their privacy. More specifically, In the case of Facebook. Adrienne Felt, a Ph.D. candidate at Berkeley, made small headlines last year when she exposed a potentially devastating hole in the framework of Facebook's third-party application programming interface (API). It made it easier for people to lose their privacy. Felt and her co-researchers found that third-party platform applications on Facebook are provided with far more user information than it is needed. This potential privacy breach is actually built into the systematic framework of Facebook. Unfortunately, the flaws render the system to be almost indefensible. "The question for social networks is resolving the difference between mistakes in implementation and what the design of the application platform is intended to allow," said David Evans, Assistant Professor of Computer Science at the University of Virginia. Moreover, there is also the question of who should be hold responsible for the lack of user privacy? According Evan, the answer to the question is not likely to be found, because a better regulated API would be required for Facebook "to break a lot of applications, [especially when] a lot of companies are trying to make money off [these] applications." Felt agrees with her conclusion, because "there are marketing businesses built on top of the idea that third parties can get access to data and user information on Facebook."""
""Software reverse engineering "",,,,""Rigi is an interactive graph editor tool for software reverse engineering using the white box method, i.e. necessitating source code, thus it is mainly aimed at program comprehension. Rigi is distributed by its main author, Hausi A. Müller and the Rigi research group at the University of Victoria.
Rigi provides interactive links from the graphs it produces to the source code, but not vice versa. Rigi renders trees and grid-layout graphs using its own internal engine, but relies on University of Passau's GraphEd for more advanced layouts.
The public version of Rigi has built-in parsers ("fact extractors") for C and Cobol, and can leverage the C++ parser of IBM Visual Age. It can also accept external data in an RSF format (it introduced), so external parses can also feed it data, for example SHriMP tool's Java parser. Some efforts were made to integrate Rigi in Microsoft Visual Studio .NET. Early versions of Bauhaus were also built on top of Rigi; the author of this latter tool notes that the combination was rather slow for graphs having more than 500 nodes. Rigi was reportedly used to analyze some (undisclosed) embedded software at Nokia, in the range of hundreds of thousands of lines of code, and was met with positive feedback from the Nokia engineers.
Active development of Rigi has ceased in 1999, with the last official version released in 2003. A 2008 paper noted that "Rigi is a mature tool that is still used in research and popular in teaching, but it is currently no longer actively evolved and is in bug-fix mode."""
""Economics of security and privacy "",,,,""Jeffrey Hunker (1957-2013) was an American cyber security consultant and writer.""
""Social aspects of security and privacy "",,,,""Privacy is the ability of an individual or group to seclude themselves, or information about themselves, and thereby express themselves selectively. The boundaries and content of what is considered private differ among cultures and individuals, but share common themes. When something is private to a person, it usually means that something is inherently special or sensitive to them. The domain of privacy partially overlaps security (confidentiality), which can include the concepts of appropriate use, as well as protection of information. Privacy may also take the form of bodily integrity.
The right not to be subjected to unsanctioned invasion of privacy by the government, corporations or individuals is part of many countries' privacy laws, and in some cases, constitutions. Almost all countries have laws which in some way limit privacy. An example of this would be law concerning taxation, which normally require the sharing of information about personal income or earnings. In some countries individual privacy may conflict with freedom of speech laws and some laws may require public disclosure of information which would be considered private in other countries and cultures.
Privacy may be voluntarily sacrificed, normally in exchange for perceived benefits and very often with specific dangers and losses, although this is a very strategic view of human relationships. Research shows that people are more willing to voluntarily sacrifice privacy if the data gatherer is seen to be transparent as to what information is gathered and how it is used. In the business world, a person may volunteer personal details (often for advertising purposes) in order to gamble on winning a prize. A person may also disclose personal information as part of being an executive for a publicly traded company in the USA pursuant to federal securities law. Personal information which is voluntarily shared but subsequently stolen or misused can lead to identity theft.
The concept of universal individual privacy is a modern construct primarily associated with Western culture, British and North American in particular, and remained virtually unknown in some cultures until recent times. According to some researchers, this concept sets Anglo-American culture apart even from Western European cultures such as French or Italian. Most cultures, however, recognize the ability of individuals to withhold certain parts of their personal information from wider society—a figleaf over the genitals being an ancient example.
The distinction or overlap between secrecy and privacy is ontologically subtle, which is why the word "privacy" is an example of an untranslatable lexeme, and many languages do not have a specific word for "privacy". Such languages either use a complex description to translate the term (such as Russian combining the meaning of уединение—solitude, секретность—secrecy, and частная жизнь—private life) or borrow from English "privacy" (as Indonesian Privasi or Italian la privacy). The distinction hinges on the discreteness of interests of parties (persons or groups), which can have emic variation depending on cultural mores of individualism, collectivism, and the negotiation between individual and group rights. The difference is sometimes expressed humorously as "when I withhold information, it is privacy; when you withhold information, it is secrecy."
The December 15, 1890 Samuel Warren and Louis Brandeis publish the article of the law called "The right to privacy", considered one of the most influential papers in the history of American law.""
""Usability in security and privacy "",,,,""Privacy engineering is an emerging discipline within, at least, the software or information systems domain which aims to provide tools and techniques such that the engineered systems provide acceptable levels of privacy. An acceptable level of privacy is defined in terms of compliance against the functional and non-functional requirements set out through a privacy policy.

""
""User studies "",,,,""In software development and product management, a user story is a description consisting of one or more sentences in the everyday or business language of the end user or user of a system that captures what a user does or needs to do as part of his or her job function. User stories are used with agile software development methodologies as the basis for defining the functions a business system must provide, and to facilitate requirements management. It captures the "who", "what" and "why" of a requirement in a simple, concise way, often limited in detail by what can be hand-written on a small paper notecard.
A user story encapsulates the action of one function making it possible for software developers to create a vertical slice of their work.""
""Usability testing "",,,,""Usability testing is a technique used in user-centered interaction design to evaluate a product by testing it on users. This can be seen as an irreplaceable usability practice, since it gives direct input on how real users use the system. This is in contrast with usability inspection methods where experts use different methods to evaluate a user interface without involving users.
Usability testing focuses on measuring a human-made product's capacity to meet its intended purpose. Examples of products that commonly benefit from usability testing are foods, consumer products, web sites or web applications, computer interfaces, documents, and devices. Usability testing measures the usability, or ease of use, of a specific object or set of objects, whereas general human-computer interaction studies attempt to formulate universal principles.""
""Heuristic evaluations "",,,,""A heuristic evaluation is a usability inspection method for computer software that helps to identify usability problems in the user interface (UI) design. It specifically involves evaluators examining the interface and judging its compliance with recognized usability principles (the "heuristics"). These evaluation methods are now widely taught and practiced in the new media sector, where UIs are often designed in a short space of time on a budget that may restrict the amount of money available to provide for other types of interface testing.

""
""Laboratory experiments "",,,,""Tabernanthine is an alkaloid found in Tabernanthe iboga.
It has been used in laboratory experiments to study how addiction affects the brain.
Tabernanthine persistently reduced the self-administration of cocaine and morphine in rats.

""
""Hypertext / hypermedia "",,,,""Michael Groden (born 1947) is Professor of English at The University of Western Ontario.
Born in Buffalo, New York, Groden received a B.A. from Dartmouth College (magna cum laude) in 1969 and an M.A. and Ph.D. from Princeton University in 1975. He is known for his involvement in the envisioning and development of James Joyce's Ulysses as hypertext and hypermedia with William H. Quillian and other scholars from around the world.
In 2007, he was made a Fellow of the Royal Society of Canada.""
""Mixed / augmented reality "",,,,""The International Symposium on Mixed and Augmented Reality (ISMAR) is the leading international academic conference in the field of Augmented Reality and Mixed Reality. The symposium is organized and supported by IEEE Computer Society and IEEE VGTC [1]. The first ISMAR conference was held in 2002 in Darmstadt, Germany. The creation of the conference emerged from the fusion of two former academic events dedicated to this research field: the International Symposium on Augmented Reality (ISAR) and the International Symposium on Mixed Reality (ISMR) [2].""
""Command line interfaces "",,,,""A command-line interface or command language interpreter (CLI), also known as command-line user interface, console user interface, and character user interface (CUI), is a means of interacting with a computer program where the user (or client) issues commands to the program in the form of successive lines of text (command lines).
The CLI was the primary means of interaction with most computer systems until the introduction of the video display terminal in the mid-1960s, and continued to be used throughout the 1970s and 1980s on OpenVMS, Unix systems and personal computer systems including MS-DOS, CP/M and Apple DOS. The interface is usually implemented with a command line shell, which is a program that accepts commands as text input and converts commands to appropriate operating system functions.
Command-line interfaces to computer operating systems are less widely used by casual computer users, who favor graphical user interfaces.
Alternatives to the command line include, but are not limited to text user interface menus (see IBM AIX SMIT for example), keyboard shortcuts, and various other desktop metaphors centered on the pointer (usually controlled with a mouse). Examples of this include the Windows versions 1, 2, 3, 3.1, and 3.11 (an OS shell that runs in DOS), DosShell, and Mouse Systems PowerPanel.
Command-line interfaces are often preferred by more advanced computer users, as they often provide a more concise and powerful means to control a program or operating system.
Programs with command-line interfaces are generally easier to automate via scripting.
Command line interfaces for software other than operating systems include a number of programming languages such as Tcl/Tk, PHP and others, as well as utilities such as the compression utilities WinZip and UltimateZip, and some FTP and ssh/telnet clients.
^ "Text mode programs (CUI: Console User Interface)". Wine User Guide. Retrieved Sep 22, 2013.""
""Graphical user interfaces "",,,,""In computer science, a graphical user interface or GUI,  is a type of interface that allows users to interact with electronic devices through graphical icons and visual indicators such as secondary notation, as opposed to text-based interfaces, typed command labels or text navigation. GUIs were introduced in reaction to the perceived steep learning curve of command-line interfaces (CLIs), which require commands to be typed on the keyboard.
The actions in a GUI are usually performed through direct manipulation of the graphical elements. In addition to computers, GUIs can be found in hand-held devices such as MP3 players, portable media players, gaming devices, smartphones and smaller household, office and industrial equipment. The term "GUI" tends not to be applied to other low-resolution types of interfaces with display resolutions, such as video games (where HUD is preferred), or not restricted to flat screens, like volumetric displays because the term is restricted to the scope of two-dimensional display screens able to describe generic information, in the tradition of the computer science research at the PARC (Palo Alto Research Center).""
""Virtual reality "",,,,""Virtual reality or virtual realities (VR), also known as immersive multimedia or computer-simulated reality, is a computer technology that replicates an environment, real or imagined, and simulates a user's physical presence and environment in a way that allows the user to interact with it. Virtual realities artificially create sensory experience, which can include sight, touch, hearing, and smell.
Most up-to-date virtual realities are displayed either on a computer screen or with a special virtual reality headset, and some simulations include additional sensory information and focus on real sound through speakers or headphones targeted towards VR users. Some advanced haptic systems now include tactile information, generally known as force feedback in medical, gaming and military applications. Furthermore, virtual reality covers remote communication environments which provide virtual presence of users with the concepts of telepresence and telexistence or a virtual artifact (VA) either through the use of standard input devices such as a keyboard and mouse, or through multimodal devices such as a wired glove or omnidirectional treadmills. The immersive environment can be similar to the real world in order to create a lifelike experience—for example, in simulations for pilot or combat training—or it can differ significantly from reality, such as in VR games.""
""Natural language interfaces "",,,,""Natural language user interfaces (LUI or NLUI) are a type of computer human interface where linguistic phenomena such as verbs, phrases and clauses act as UI controls for creating, selecting and modifying data in software applications.
In interface design natural language interfaces are sought after for their speed and ease of use, but most suffer the challenges to understanding wide varieties of ambiguous input. Natural language interfaces are an active area of study in the field of natural language processing and computational linguistics. An intuitive general natural language interface is one of the active goals of the Semantic Web.
Text interfaces are "natural" to varying degrees. Many formal (un-natural) programming languages incorporate idioms of natural human language. Likewise, a traditional keyword search engine could be described as a "shallow" natural language user interface.""
""Collaborative interaction "",,,,""The bilateral relations between the Republic of Iraq and the Republic of India have traditionally been friendly and collaborative. Cultural interaction and economic trade between ancient India and Mesopotamia date back to 1800 BCE. The 1952 Treaty of Friendship established and strengthened ties between contemporary India and Iraq. By the 1970s, Iraq was regarded as one of India's closest allies in the Middle East.
Ties between India and Iraq were disrupted during the Iran–Iraq War, the 1991 Gulf War and the 2003 Iraq War. However, the bilateral relations normalised after the establishment of democratic government in Iraq.

""
""Graphics input devices "",,,,""DirectFB (Direct Frame Buffer) is a software library with a small memory footprint that provides graphics acceleration, input device handling and abstraction layer, and integrated windowing system with support for translucent windows and multiple display layers on top of the Linux framebuffer without requiring any kernel modifications. DirectFB is free and open-source software subject to the terms of the GNU Lesser General Public License (LGPL).
The library allows developers an alternative to a full X Window System (X11) server used in Unix-like operating systems. DirectFB allows applications to talk directly to video hardware through a direct API, speeding up and simplifying graphic operations.
It is often used by games and embedded systems developers to circumvent the overhead of a full X Window System server implementation. In 2006, a version of DirectFB was included in the software stack for the CE Linux Forum's Audio/Video Graphics Specification V2.
DirectFB can host XDirectFB, a rootless X server implementation that uses DirectFB windows for X11 top-level windows. XDirectFB is an interface that mimics the X11 interface through the DirectFB API to simplify running applications written for X11 on DirectFB.
DirectFBGL is an OpenGL extension for DirectFB/XDirectFB that uses Direct Rendering Infrastructure (DRI) in the Mesa 3D library to support OpenGL hardware acceleration.
Among the products that use DirectFB are LinuxTV, Ben Nanonote, the unreleased Palm Foleo mobile companion, the webOS operating system, Panasonic Viera Connect TVs and the Philips TVs based on jointSPACE.
As of 4 October 2015, DirectFB's website is down; it is believed that it went down in August 2015, and the last recorded Internet Archive record is 1 Aug 2015.""
""Sound-based input / output "",,,,""The Yamaha DX7 is an FM synthesis based digital synthesizer manufactured by the Yamaha Corporation from 1983 to 1989. It was the first commercially successful digital synthesizer. Its distinctive sound can be heard on many recordings, especially pop music from the 1980s. The monotimbral, 16-note polyphonic DX7 was the moderately priced model of the DX series keyboard synthesizers that included the larger and more elaborate DX1 and DX5; the feature-reduced DX9; and the smaller and not directly compatible DX100, DX11, and DX21. Over 200,000 of the original DX7 were made, and it remains one of the best-selling synthesizers of all time.""
""Pointing devices "",,,,""A pointing device is an input interface (specifically a human interface device) that allows a user to input spatial (i.e., continuous and multi-dimensional) data to a computer. CAD systems and graphical user interfaces (GUI) allow the user to control and provide data to the computer using physical gestures by moving a hand-held mouse or similar device across the surface of the physical desktop and activating switches on the mouse. Movements of the pointing device are echoed on the screen by movements of the pointer (or cursor) and other visual changes. Common gestures are point and click and drag and drop.
While the most common pointing device by far is the mouse, many more devices have been developed. A "rodent" is a technical term referring to a device which generates mouse-like input. However, the term "mouse" is commonly used as a metaphor for devices that move the cursor.
For most pointing devices, Fitts's law can be used to predict the speed with which users can point at a given target position.""
""Touch screens "",,,,""A touchscreen is an input device normally layered on the top of an electronic visual display of an information processing system. A user can give input or control the information processing system through simple or multi-touch gestures by touching the screen with a special stylus and/or one or more fingers. Some touchscreens use ordinary or specially coated gloves to work while others use a special stylus/pen only. The user can use the touchscreen to react to what is displayed and to control how it is displayed; for example, zooming to increase the text size.
The touchscreen enables the user to interact directly with what is displayed, rather than using a mouse, touchpad, or any other intermediate device (other than a stylus, which is optional for most modern touchscreens).
Touchscreens are common in devices such as game consoles, personal computers, tablet computers, electronic voting machines, and smartphones. They can also be attached to computers or, as terminals, to networks. They also play a prominent role in the design of digital appliances such as personal digital assistants (PDAs)es and some books (E-books).
The popularity of smartphones, tablets, and many types of information appliances is driving the demand and acceptance of common touchscreens for portable and functional electronics. Touchscreens are found in the medical field and in heavy industry, as well as for automated teller machines (ATMs), and kiosks such as museum displays or room automation, where keyboard and mouse systems do not allow a suitably intuitive, rapid, or accurate interaction by the user with the display's content.
Historically, the touchscreen sensor and its accompanying controller-based firmware have been made available by a wide array of after-market system integrators, and not by display, chip, or motherboard manufacturers. Display manufacturers and chip manufacturers worldwide have acknowledged the trend toward acceptance of touchscreens as a highly desirable user interface component and have begun to integrate touchscreens into the fundamental design of their products.""
""Haptic devices "",,,,""Haptic or kinesthetic communication recreates the sense of touch by applying forces, vibrations, or motions to the user. This mechanical stimulation can be used to assist in the creation of virtual objects in a computer simulation, to control such virtual objects, and to enhance the remote control of machines and devices (telerobotics). Haptic devices may incorporate tactile sensors that measure forces exerted by the user on the interface.
Most researchers distinguish three sensory systems related to sense of touch in humans: cutaneous, kinesthetic and haptic. All perceptions mediated by cutaneous and/or kinesthetic sensibility are referred to as tactual perception. The sense of touch may be classified as passive and active, and the term "haptic" is often associated with active touch to communicate or recognize objects.
Haptic technology has made it possible to investigate how the human sense of touch works by allowing the creation of carefully controlled haptic virtual objects.
The word haptic, from the Greek: ἁπτικός (haptikos), means "pertaining to the sense of touch" and comes from the Greek verb ἅπτεσθαι haptesthai, meaning "to contact" or "to touch".

""
""HCI theory, concepts and models "",,,,""Human–computer interaction (HCI) researches the design and use of computer technology, focusing particularly on the interfaces between people (users) and computers. Researchers in the field of HCI both observe the ways in which humans interact with computers and design technologies that let humans interact with computers in novel ways.
As a field of research, Human-Computer Interaction is situated at the intersection of computer science, behavioral sciences, design, media studies, and several other fields of study. The term was popularized by Stuart K. Card and Allen Newell of Carnegie Mellon University and Thomas P. Moran of IBM Research in their seminal 1983 book, The Psychology of Human-Computer Interaction, although the authors first used the term in 1980 and the first known use was in 1975. The term connotes that, unlike other tools with only limited uses (such as a hammer, useful for driving nails, but not much else), a computer has many uses and this takes place as an open-ended dialog between the user and the computer. The notion of dialog likens human-computer interaction to human-to-human interaction, an analogy the discussion of which is crucial to theoretical considerations in the field.""
""Auditory feedback "",,,,""Auditory feedback is an aid used by humans to control speech production and singing. It is assumed that auditory feedback, alongside other feedback mechanisms such as somatosensory feedback and visual feedback, helps to verify whether the current production of a passage of speech or singing is in accord with a person's acoustic-auditory intention. From the viewpoint of movement sciences and neurosciences the acoustic-auditory speech signal can be interpreted as the result of movements (skilled actions) of speech articulators (the lower jaw, lips, tongue, etc.), and thus auditory feedback can be interpreted as a feedback mechanism controlling skilled actions in the same way that visual feedback controls limb movements (e.g. reaching movements).

""
""Text input "",,,,""An input method (or input method editor, commonly abbreviated IME) is an operating system component or program that allows any data, such as keyboard strokes or mouse movements, to be received as input. In this way users can enter characters and symbols not found on their input devices. Using an input method is obligatory for any language that has more graphemes than there are keys on the keyboard.
For instance, on the computer, this allows the user of Latin keyboards to input Chinese, Japanese, Korean and Indic characters; on many hand-held devices, such as mobile phones, it enables using the numeric keypad to enter Latin alphabet characters (or any other alphabet characters) or a screen display to be touched to do so. On some operating systems, an input method is also used to define the behaviour of the dead keys.""
""Gestural input "",,,,""Oblong Industries is a computer user-interface company based in Los Angeles, CA which develops conference room telecollaboration systems which respond to gestural input.""
""User interface management systems "",,,,""A User Interface Management System (UIMS) is a mechanism for cleanly separating process or business logic from Graphical user interface (GUI) code in a computer program. UIMS are designed to support N-tier architectures by strictly defining and enforcing the boundary between the business logic and the GUI. A fairly rigid Software architecture is nearly always implied by the UIMS, and most often only one paradigm of separation is supported in a single UIMS. A UIMS may also have libraries and systems such as graphical tools for the creation of user interface resources or data stores.
Generally, you cannot easily use multiple UIMS systems at the same time, so choosing the correct model for your UIMS is a critical design decision in any project. The choice of system is dependent upon the system(s) you wish to create user interfaces for, and the general style of your application. For example, if you want to create a web based front end, or just a standalone application or both that would be an important factor in choosing. If you want to deploy to the Macintosh, Windows and Linux, that would further influence your choice of a UIMS system.
There are many UIMS approaches described in research papers. However, there are not very many systems available commercially or through open source.""
""User interface programming "",,,,""State management refers to the management of the state of one or more user interface controls such as text fields, OK buttons, radio buttons, etc. in a graphical user interface. In this user interface programming technique, the state of one UI control depends on the state of other UI controls. For example, a state managed UI control such as a button will be in the enabled state when input fields have valid input values and the button will be in the disabled state when the input fields are empty or have invalid values.""
""User interface toolkits "",,,,""A widget toolkit, widget library, GUI toolkit, or UX library is a library or a collection of libraries containing a set of graphical control elements (called widgets) used to construct the graphical user interface (GUI) of programs.
Most widget toolkits additionally include their own rendering engine. This engine can be specific to a certain operating system or windowing system or contain back-ends to interface with more multiple ones and also with rendering APIs such as OpenGL, OpenVG, or EGL. The look and feel of the graphical control elements can be hard-coded or decoupled, allowing the graphical control elements to be themed/skinned.
Being written in a specific programming language, the widget toolkit may be used from other languages employing bindings. Graphical user interface builders such as e.g. Glade Interface Designer facilitate the authoring of GUIs in a WYSIWYG manner employing a user interface markup language such as in this case GtkBuilder.
The GUI of a program is commonly constructed in a cascading manner, with graphical control elements being added directly to on top of one another.
Most widget toolkits use event-driven programming as a model for interaction. The toolkit handles user events, for example when the user clicks on a button. When an event is detected, it is passed on to the application where it is dealt with. The design of those toolkits has been criticized for promoting an oversimplified model of event-action, leading programmers to create error-prone, difficult to extend and excessively complex application code. Finite State Machines and Hierarchical State Machines have been proposed as high-level models to represent the interactive state changes for reactive programs.""
""Empirical studies in HCI "",,,,""Human–computer interaction (HCI) researches the design and use of computer technology, focusing particularly on the interfaces between people (users) and computers. Researchers in the field of HCI both observe the ways in which humans interact with computers and design technologies that let humans interact with computers in novel ways.
As a field of research, Human-Computer Interaction is situated at the intersection of computer science, behavioral sciences, design, media studies, and several other fields of study. The term was popularized by Stuart K. Card and Allen Newell of Carnegie Mellon University and Thomas P. Moran of IBM Research in their seminal 1983 book, The Psychology of Human-Computer Interaction, although the authors first used the term in 1980 and the first known use was in 1975. The term connotes that, unlike other tools with only limited uses (such as a hammer, useful for driving nails, but not much else), a computer has many uses and this takes place as an open-ended dialog between the user and the computer. The notion of dialog likens human-computer interaction to human-to-human interaction, an analogy the discussion of which is crucial to theoretical considerations in the field.""
""User interface design "",,,,""User interface design (UI) or user interface engineering is the design of user interfaces for machines and software, such as computers, home appliances, mobile devices, and other electronic devices, with the focus on maximizing the user experience. The goal of user interface design is to make the user's interaction as simple and efficient as possible, in terms of accomplishing user goals (user-centered design).
Good user interface design facilitates finishing the task at hand without drawing unnecessary attention to itself. Graphic design and typography are utilized to support its usability, influencing how the user performs certain interactions and improving the aesthetic appeal of the design; design aesthetics may enhance or detract from the ability of users to use the functions of the interface. The design process must balance technical functionality and visual elements (e.g., mental model) to create a system that is not only operational but also usable and adaptable to changing user needs.
Interface design is involved in a wide range of projects from computer systems, to cars, to commercial planes; all of these projects involve much of the same basic human interactions yet also require some unique skills and knowledge. As a result, designers tend to specialize in certain types of projects and have skills centered on their expertise, whether that be software design, user research, web design, or industrial design.""
""User centered design "",,,,""User-centered design (UCD) is a framework of processes (not restricted to interfaces or technologies) in which the needs, wants, and limitations of end users of a product, service or process are given extensive attention at each stage of the design process. User-centered design can be characterized as a multi-stage problem solving process that not only requires designers to analyse and foresee how users are likely to use a product, but also to test the validity of their assumptions with regard to user behavior in real world tests with actual users at each stage of the process from requirements, concepts, pre-production models, mid production and post production creating a circle of proof back to and confirming or modifying the original requirements. Such testing is necessary as it is often very difficult for the designers of a product to understand intuitively what a first-time user of their design experiences, and what each user's learning curve may look like.
The chief difference from other product design philosophies is that user-centered design tries to optimize the product around how users can, want, or need to use the product, rather than forcing the users to change their behavior to accommodate the product.""
""Activity centered design "",,,,""Activity-centered design (ACD), which is an approach to interaction design, does not focus on the goals and preferences of the user, but on the activity a user would perform with a given piece of technology. ACD has its theoretical underpinnings in activity theory, from which activities can be defined as actions taken by a user to achieve a goal.
When working with activity-centered design, the designers use research to get insights of the users. Observations and interviews are typical approaches to learn more about the users behavior. By mapping users' activities and tasks, the designer may notice missing tasks for the activity to become more easy to perform, and thus design solutions to accomplish those tasks.
^ Donald, N. (July 01, 2005). Human-centered design considered harmful. Interactions, 12.4, 14-19.
^ Kaptelinin V, Nardi B (1997). Activity Theory: Basic Concepts and Applications CHI 97 Electronic Publications: Tutorials.""
""Scenario-based design "",,,,""Design fiction is a method of critical design that uses fictional and narrative scenarios to envision, explain and raise questions about possible futures for design and the society.        

^ Joshua Tanenbaum. "What is design fiction?"
^ Julian Bleecker. "A Short Essay on Design, Science, Fact, and Fiction:"
^ Julian Bleecker. "A Short Slideshow on Design, Science, Fact, and Fiction: "
^ David Kirby. "The Future is Now:"
^ Bruce Sterling. "On Design Fictions:"
^ Bruce Sterling. "Interactions Article on Design Fictions:"
^ Bruce Sterling. "Curated list of Design Fiction videos: "
^ Paul Dourish and Genevieve Bell. "Resistance is Futile: Reading Science Fiction alongside Ubiquitous Computing: "
^ Joshua Tanenbaum, Karen Tanenbaum, Ron Wakkary. "Steampunk as Design Fiction:"
^ Stuart Reeves. "Envisioning Ubiquitous Computing: "
^ Johnson, Brian David (April 20, 2011), "Science Fiction Prototyping: Designing the Future with Science Fiction". Morgan & Claypool. ISBN 978-1608456550""
""Participatory design "",,,,""Participatory design (originally co-operative design, now often co-design) is an approach to design attempting to actively involve all stakeholders (e.g. employees, partners, customers, citizens, end users) in the design process to help ensure the result meets their needs and is usable. Participatory design is an approach which is focused on processes and procedures of design and is not a design style. The term is used in a variety of fields e.g. software design, urban design, architecture, landscape architecture, product design, sustainability, graphic design, planning, and even medicine as a way of creating environments that are more responsive and appropriate to their inhabitants' and users' cultural, emotional, spiritual and practical needs. It is one approach to placemaking.
Recent research suggests that designers create more innovative concepts and ideas when working within a co-design environment with others than they do when creating ideas on their own.
Participatory design has been used in many settings and at various scales. For some, this approach has a political dimension of user empowerment and democratization. For others, it is seen as a way of abrogating design responsibility and innovation by designers.
In several Scandinavian countries, during the 1960s and 1970s, participatory design was rooted in work with trade unions; its ancestry also includes action research and sociotechnical design.
^ Mitchell, V., Ross, T., May, A., Sims, R., & Parker, C. (2015). Empirical investigation of the impact of using co-design methods when generating proposals for sustainable travel solutions. CoDesign. http://doi.org/10.1080/15710882.2015.1091894
^ Web Page on Participatory Design on the site of CPSR. Retrieved 13 April 2006.""
""Contextual design "",,,,""Contextual design (CD) is a user-centered design process developed by Hugh Beyer and Karen Holtzblatt. It incorporates ethnographic methods for gathering data relevant to the product via field studies, rationalizing workflows, and designing human-computer interfaces. In practice, this means that researchers aggregate data from customers in the field where people are living and applying these findings into a final product. Contextual design can be seen as an alternative to engineering and feature driven models of creating new systems.""
""Interface design prototyping "",,,,""The user interface, in the industrial design field of human–machine interaction, is the space where interactions between humans and machines occur. The goal of this interaction is to allow effective operation and control of the machine from the human end, whilst the machine simultaneously feeds back information that aids the operators' decision-making process. Examples of this broad concept of user interfaces include the interactive aspects of computer operating systems, hand tools, heavy machinery operator controls, and process controls. The design considerations applicable when creating user interfaces are related to or involve such disciplines as ergonomics and psychology.
Generally, the goal of user interface design is to produce a user interface which makes it easy (self-explanatory), efficient, and enjoyable (user-friendly) to operate a machine in the way which produces the desired result. This generally means that the operator needs to provide minimal input to achieve the desired output, and also that the machine minimizes undesired outputs to the human.
With the increased use of personal computers and the relative decline in societal awareness of heavy machinery, the term user interface is generally assumed to mean the graphical user interface, while industrial control panel and machinery control design discussions more commonly refer to human-machine interfaces.
Other terms for user interface include human–computer interface and man–machine interface (MMI).""
""Interaction design theory, concepts and paradigms "",,,,""In design, human–computer interaction, and software development, interaction design, often abbreviated IxD, is defined as "the practice of designing interactive digital products, environments, systems, and services." Like many other design fields interaction design also has an interest in form but its main focus is on behaviour. What clearly marks interaction design as a design field as opposed to a science or engineering field is that it involves synthesising and imagining things as they might be, more so than focusing on how things are.
Interaction design is heavily focused on satisfying the needs and desires of the majority of people who will use the product; other disciplines like software engineering have a heavy focus on designing for technical stakeholders of a project.""
""Empirical studies in interaction design "",,,,""Empirical research is research using empirical evidence. It is a way of gaining knowledge by means of direct and indirect observation or experience. Empiricism values such research more than other kinds. Empirical evidence (the record of one's direct observations or experiences) can be analyzed quantitatively or qualitatively. Through quantifying the evidence or making sense of it in qualitative form, a researcher can answer empirical questions, which should be clearly defined and answerable with the evidence collected (usually called data). Research design varies by field and by the question being investigated. Many researchers combine qualitative and quantitative forms of analysis to better answer questions which cannot be studied in laboratory settings, particularly in the social sciences and in education.
In some fields, quantitative research may begin with a research question (e.g., "Does listening to vocal music during the learning of a word list have an effect on later memory for these words?") which is tested through experimentation. Usually, a researcher has a certain theory regarding the topic under investigation. Based on this theory some statements, or hypotheses, will be proposed (e.g., "Listening to vocal music has a negative effect on learning a word list."). From these hypotheses predictions about specific events are derived (e.g., "People who study a word list while listening to vocal music will remember fewer words on a later memory test than people who study a word list in silence."). These predictions can then be tested with a suitable experiment. Depending on the outcomes of the experiment, the theory on which the hypotheses and predictions were based will be supported or not, or may need to be modified and then subjected to further testing.""
""Social content sharing "",,,,""Nahum Sharfman was an Israeli tech entrepreneur who, along with Amir Ashkenazi, founded Shopping.com. He was also the Director of social content sharing site eSnips Ltd.
Prior to shopping.com, Sharfman co-founded the IT security company Commtouch for which he served as the Chairman of the Board from its inception in February 1991 to November 1997. Commtouch IPO'ed in 2000.
Sharfman and his wife, Nava, died in a plane crash on the Mount Ainos in the Ioanian island of Cephallonia, Greece while en route to Corfu on April 29, 2009.""
""Collaborative content creation "",,,,""This is a list of notable content management systems that are used to organize and facilitate collaborative content creation. Many of them are built on top of separate content management frameworks.

""
""Collaborative filtering "",,,,""Collaborative filtering (CF) is a technique used by some recommender systems. Collaborative filtering has two senses, a narrow one and a more general one. In general, collaborative filtering is the process of filtering for information or patterns using techniques involving collaboration among multiple agents, viewpoints, data sources, etc. Applications of collaborative filtering typically involve very large data sets. Collaborative filtering methods have been applied to many different kinds of data including: sensing and monitoring data, such as in mineral exploration, environmental sensing over large areas or multiple sensors; financial data, such as financial service institutions that integrate many financial sources; or in electronic commerce and web applications where the focus is on user data, etc. The remainder of this discussion focuses on collaborative filtering for user data, although some of the methods and approaches may apply to the other major applications as well.
In the newer, narrower sense, collaborative filtering is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating). The underlying assumption of the collaborative filtering approach is that if a person A has the same opinion as a person B on an issue, A is more likely to have B's opinion on a different issue x than to have the opinion on x of a person chosen randomly. For example, a collaborative filtering recommendation system for television tastes could make predictions about which television show a user should like given a partial list of that user's tastes (likes or dislikes). Note that these predictions are specific to the user, but use information gleaned from many users. This differs from the simpler approach of giving an average (non-specific) score for each item of interest, for example based on its number of votes.""
""Social recommendation "",,,,""Yahoo is a computer software and web search engine company founded on March 1, 1995. The company is a public corporation and its headquarters is located in Sunnyvale, California. It was founded by Stanford University graduate students Jerry Yang and David Filo in 1994. According to web traffic analysis companies, Yahoo has been one of the most visited websites on the Internet, with more than 130 million unique users per month in the United States alone. As of October 2007, the global network of Yahoo receives 3.4 billion page views per day on average, making it one of the most visited US websites.
Yahoo's first acquisition was the purchase of Net Controls, a web search engine company, in September 1997 for US$1.4 million. As of April 2008, the company's largest acquisition is the purchase of Broadcast.com, an Internet radio company, for $5.7 billion, making Broadcast.com co-founder Mark Cuban a billionaire. Most of the companies acquired by Yahoo are based in the United States; 78 of the companies are from the United States, and 15 are based in a foreign country. As of July 2015, Yahoo has acquired 114 companies, with Polyvore being the latest.""
""Social networks "",,,,""A social network is a social structure made up of a set of social actors (such as individuals or organizations), sets of dyadic ties, and other social interactions between actors. The social network perspective provides a set of methods for analyzing the structure of whole social entities as well as a variety of theories explaining the patterns observed in these structures. The study of these structures uses social network analysis to identify local and global patterns, locate influential entities, and examine network dynamics.
Social networks and the analysis of them is an inherently interdisciplinary academic field which emerged from social psychology, sociology, statistics, and graph theory. Georg Simmel authored early structural theories in sociology emphasizing the dynamics of triads and "web of group affiliations". Jacob Moreno is credited with developing the first sociograms in the 1930s to study interpersonal relationships. These approaches were mathematically formalized in the 1950s and theories and methods of social networks became pervasive in the social and behavioral sciences by the 1980s. Social network analysis is now one of the major paradigms in contemporary sociology, and is also employed in a number of other social and formal sciences. Together with other complex networks, it forms part of the nascent field of network science.""
""Social tagging "",,,,""A folksonomy is a system in which users apply public tags to online items, typically to aid them in re-finding those items. This can give rise to a classification system based on those tags and their frequencies, in contrast to a taxonomic classification specified by the owners of the content when it is published. This practice is also known as collaborative tagging, social classification, social indexing, and social tagging. However, these terms have slightly different meanings than folksonomy. Folksonomy was originally “the result of personal free tagging of information [...] for one’s own retrieval.”. Social tagging is the application of tags in an open online environment where the tags of other users are available to others. Collaborative tagging (also known as group tagging) is tagging performed by a group of users. This type of folksonomy is commonly used in cooperative and collaborative projects such as research, content repositories, and social bookmarking.
The term was coined by Thomas Vander Wal in 2004 as a portmanteau of folk and taxonomy. Folksonomies became popular as part of social software applications such as social bookmarking and photograph annotation that enable users to collectively classify and find information via shared tags. Some websites include tag clouds as a way to visualize tags in a folksonomy.
Folksonomies can be used for K-12 education, business, and higher education. More specifically, folksonomies may be implemented for social bookmarking, teacher resource repositories, e-learning systems, collaborative learning, collaborative research, and professional development.""
""Computer supported cooperative work "",,,,""The term computer-supported cooperative work (CSCW) was first coined by Irene Greif and Paul M. Cashman in 1984, at a workshop attended by individuals interested in using technology to support people in their work. At about this same time, in 1987 Dr. Charles Findley presented the concept of Collaborative Learning-Work. According to Carstensen and Schmidt, CSCW addresses "how collaborative activities and their coordination can be supported by means of computer systems." On the one hand, many authors consider that CSCW and groupware are synonyms. On the other hand, different authors claim that while groupware refers to real computer-based systems, CSCW focuses on the study of tools and techniques of groupware as well as their psychological, social, and organizational effects. The definition of Wilson (1991) expresses the difference between these two concepts:

CSCW [is] a generic term, which combines the understanding of the way people work in groups with the enabling technologies of computer networking, and associated hardware, software, services and techniques.""
""Social navigation "",,,,""Social translucence is a term that was proposed by Thomas Erickson and Wendy Kellogg to refer to "design digital systems that support coherent behavior by making participants and their activities visible to one another".
Social translucence represents a tool for transparency, which function is to
stimulate online participation
facilitate collaboration (via collaborative filtering but also by helping the construction of trust)
facilitate navigation (social navigation)
Social transluscence is in particular a core element in Online social networking such as Facebook or LinkedIn, in which they intervene in the possibility for people to expose their online identity, but also in the creation of awareness of other people activities, that are for instance present in the activity feeds that these systems make available.
Social translucence mechanisms have been made available in many web 2.0 systems such as:
Online communities
Online social networking
Wikis (McDonald et al. Zachry) (Chi, Suh & Kittur 2008)""
""Social media "",,,,""Social media are computer-mediated tools that allow people or companies to create, share, or exchange information, career interests, ideas, and pictures/videos in virtual communities and networks. Social media is defined as "a group of Internet-based applications that build on the ideological and technological foundations of Web 2.0, and that allow the creation and exchange of user-generated content." Furthermore, social media depend on mobile and web-based technologies to create highly interactive platforms through which individuals and communities share, co-create, discuss, and modify user-generated content. They introduce substantial and pervasive changes to communication between businesses, organizations, communities, and individuals. These changes are the focus of the emerging field of technoself studies. Social media differ from traditional or industrial media in many ways, including quality, reach, frequency, usability, immediacy, and permanence. Social media operate in a dialogic transmission system (many sources to many receivers). This is in contrast to traditional media that operates under a monologic transmission model (one source to many receivers).
"Social media has been broadly defined to refer to 'the many relatively inexpensive and widely accessible electronic tools that enable anyone to publish and access information, collaborate on a common effort, or build relationships.'"
There are many effects that stem from Internet usage. According to Nielsen, Internet users continue to spend more time with social media sites than any other type of site. At the same time, the total time spent on social media in the U.S. across PC and mobile devices increased by 99 percent to 121 billion minutes in July 2012 compared to 66 billion minutes in July 2011. For content contributors, the benefits of participating in social media have gone beyond simply social sharing to building reputation and bringing in career opportunities and monetary income, as discussed in Tang, Gu, and Whinston (2012).

""
""Social network analysis "",,,,""Social network analysis (SNA) is the process of investigating social structures through the use of network and graph theories. It characterizes networked structures in terms of nodes (individual actors, people, or things within the network) and the ties or edges (relationships or interactions) that connect them. Examples of social structures commonly visualized through social network analysis include social media networks, friendship and acquaintance networks, collaboration graphs, kinship, disease transmission,and sexual relationships. These networks are often visualized through sociograms in which nodes are represented as points and ties are represented as lines.
Social network analysis has emerged as a key technique in modern sociology. It has also gained a significant following in anthropology, biology, communication studies, economics, geography, history, information science, organizational studies, political science, social psychology, development studies, and sociolinguistics and is now commonly available as a consumer tool.""
""Ethnographic studies "",,,,""Anthropology of media (also anthropology of mass media, media anthropology) is an area of study within social or cultural anthropology that emphasizes ethnographic studies as a means of understanding producers, audiences, and other cultural and social aspects of mass media.""
""Blogs "",,,,""A blog (a truncation of the expression weblog) is a discussion or informational site published on the World Wide Web consisting of discrete entries ("posts") typically displayed in reverse chronological order (the most recent post appears first). Until 2009, blogs were usually the work of a single individual, occasionally of a small group, and often covered a single subject. More recently, "multi-author blogs" (MABs) have developed, with posts written by large numbers of authors and professionally edited. MABs from newspapers, other media outlets, universities, think tanks, advocacy groups, and similar institutions account for an increasing quantity of blog traffic. The rise of Twitter and other "microblogging" systems helps integrate MABs and single-author blogs into societal newstreams. Blog can also be used as a verb, meaning to maintain or add content to a blog.
The emergence and growth of blogs in the late 1990s coincided with the advent of web publishing tools that facilitated the posting of content by non-technical users. (Previously, a knowledge of such technologies as HTML and FTP had been required to publish content on the Web.)
A majority are interactive, allowing visitors to leave comments and even message each other via GUI widgets on the blogs, and it is this interactivity that distinguishes them from other static websites. In that sense, blogging can be seen as a form of social networking service. Indeed, bloggers do not only produce content to post on their blogs, but also build social relations with their readers and other bloggers. However, there are high-readership blogs which do not allow comments, such as Daring Fireball.
Many blogs provide commentary on a particular subject; others function as more personal online diaries; others function more as online brand advertising of a particular individual or company. A typical blog combines text, images, and links to other blogs, Web pages, and other media related to its topic. The ability of readers to leave comments in an interactive format is an important contribution to the popularity of many blogs. Most blogs are primarily textual, although some focus on art (art blogs), photographs (photoblogs), videos (video blogs or "vlogs"), music (MP3 blogs), and audio (podcasts). Microblogging is another type of blogging, featuring very short posts. In education, blogs can be used as instructional resources. These blogs are referred to as edublogs.
On 16 February 2011, there were over 156 million public blogs in existence. On 20 February 2014, there were around 172 million Tumblr and 75.8 million WordPress blogs in existence worldwide. According to critics and other bloggers, Blogger is the most popular blogging service used today. However, Blogger does not offer public statistics. Technorati has 1.3 million blogs as of February 22, 2014.""
""Wikis "",,,,""A wiki (/ˈwɪki/ WIK-ee) is a website which allows collaborative modification of its content and structure directly from the web browser. In a typical wiki, text is written using a simplified markup language (known as "wiki markup"), and often edited with the help of a rich-text editor.
A wiki is run using wiki software, otherwise known as a wiki engine. There are dozens of different wiki engines in use, both standalone and part of other software, such as bug tracking systems. Some wiki engines are open source, whereas others are proprietary. Some permit control over different functions (levels of access); for example, editing rights may permit changing, adding or removing material. Others may permit access without enforcing access control. Other rules may also be imposed to organize content. A wiki engine is a type of content management system, but it differs from most other such systems, including blog software, in that the content is created without any defined owner or leader, and wikis have little implicit structure, allowing structure to emerge according to the needs of the users.
The encyclopedia project Wikipedia is by far the most popular wiki-based website, and is in fact one of the most widely viewed sites of any kind of the world, having been ranked in the top ten since 2007. Wikipedia is not a single wiki but rather a collection of hundreds of wikis, one for each language. There are at least tens of thousands of other wikis in use, both public and private, including wikis functioning as knowledge management resources, notetaking tools, community websites and intranets.
Ward Cunningham, the developer of the first wiki software, WikiWikiWeb, originally described it as "the simplest online database that could possibly work". "Wiki" (pronounced [ˈwiki]) is a Hawaiian word meaning "quick".

""
""Reputation systems "",,,,""A reputation system computes and publishes reputation scores for a set of objects (e.g. service providers, services, goods or entities) within a community or domain, based on a collection of opinions that other entities hold about the objects. The opinions are typically passed as ratings to a central place where all perceptions, opinions and ratings can be accumulated. A reputation center uses a specific reputation algorithm to dynamically compute the reputation scores based on the received ratings. Reputation is a sign of trustworthiness manifested as testimony by other people. New expectations and realities about the transparency, availability, and privacy of people and institutions are emerging. Reputation management – the selective exposure of personal information and activities – is an important element to how people function in networks as they establish credentials, build trust with others, and gather information to deal with problems or make decisions.
Reputation systems are related to recommender systems and collaborative filtering, but with the difference that reputation systems produce scores based on explicit ratings from the community, whereas recommender systems use some external set of entities and events (such as the purchase of books, movies, or music) to generate marketing recommendations to users. The role of reputation systems is to facilitate trust, and often functions by making the reputation more visible.

""
""Open source software "",,,,""Open-source software (OSS) is computer software with its source code made available with a license in which the copyright holder provides the rights to study, change, and distribute the software to anyone and for any purpose. Open-source software may be developed in a collaborative public manner. Open-source software is the most prominent example of open-source development.
The open-source model, or collaborative competition development from multiple independent sources, generates an increasingly more diverse scope of design perspective than any one company is capable of developing and sustaining long term. A report by the Standish Group (from 2008) states that adoption of open-source software models has resulted in savings of about $60 billion per year to consumers.
^ St. Laurent, Andrew M. (2008). Understanding Open Source and Free Software Licensing. O'Reilly Media. p. 4. ISBN 9780596553951. 
^ Verts, William T. (2008-01-13). "Open source software". World Book Online Reference Center. Archived from the original on January 1, 2011. 
^ Rothwell, Richard (2008-08-05). "Creating wealth with free software". Free Software Magazine. Retrieved 2008-09-08. 
^ "Standish Newsroom — Open Source" (Press release). Boston. 2008-04-16. Retrieved 2008-09-08.""
""Social networking sites "",,,,""A social networking service (also social networking site or SNS) is a platform to build social networks or social relations among people who share similar interests, activities, backgrounds or real-life connections. A social network service consists of a representation of each user (often a profile), his or her social links, and a variety of additional services such as career services. Social network sites are web-based services that allow individuals to create a public profile, create a list of users with whom to share connections, and view and cross the connections within the system. Most social network services are web-based and provide means for users to interact over the Internet, such as e-mail and instant messaging. Social network sites are varied and they incorporate new information and communication tools such as mobile connectivity, photo/video/sharing and blogging. Online community services are sometimes considered a social network service, though in a broader sense, social network service usually means an individual-centered service whereas online community services are group-centered. Social networking sites allow users to share ideas, pictures, posts, activities, events, and interests with people in their network.
According to the Oxford Dictionary, a "social network" is a dedicated website or other application that enables users to communicate with each other by posting information, comments, messages, images, etc. The main types of social networking services are those that contain category places (such as former school year or classmates), means to connect with friends (usually with self-description pages), and a recommendation system linked to trust. Popular methods now combine many of these, with American-based services such as Facebook, Google+, LinkedIn, Instagram, Pinterest, Vine, Tumblr, and Twitter widely used worldwide; Wechat, Sina Weibo, and Tencent QQ in China; Nexopia in Canada; Badoo, Bebo, Vkontakte (Russia), Delphi, Draugiem.lv (Latvia), iWiW (Hungary), Nasza-Klasa (Poland), Soup (Austria), Glocals in Switzerland, Skyrock, The Sphere, StudiVZ (Germany), Tagged, Tuenti (mostly in Spain), Myspace, Xanga and XING in parts of Europe; Hi5 in South America and Central America; Mxit in Africa; CarnivalPics based in Nigeria; Cyworld, Mixi, Renren, Friendster, Sina Weibo and Wretch in Asia and the Pacific Islands. Social network services can be split into three types: socializing social network services are primarily for socializing with existing friends (e.g., Facebook); networking social network services are primarily for non-social interpersonal communication (e.g., LinkedIn); and social navigation social network services are primarily for helping users to find specific information or resources (e.g., Goodreads for books).
There have been attempts to standardize these services to avoid the need to duplicate entries of friends and interests (see the FOAF standard). A study reveals that India has recorded world's largest growth in terms of social media users in 2013. A 2013 survey found that 73% of U.S. adults use social networking sites.""
""Social tagging systems "",,,,""Many have argued that social tagging or collaborative tagging systems can provide navigational cues or “way-finders”  for other users to explore information. The notion is that, given that social tags are labels that users create to represent topics extracted from Web documents, interpretation of these tags should allow other users to predict contents of different documents efficiently. Social tags are arguably more important in exploratory search, in which the users may engage in iterative cycles of goal refinement and exploration of new information (as opposed to simple fact-retrievals), and interpretation of information contents by others will provide useful cues for people to discover topics that are relevant. One significant challenge that arises in social tagging systems is the rapid increase in the number and diversity of the tags. As opposed to structured annotation systems, tags provide users an unstructured, open-ended mechanism to annotate and organize web-content. As users are free to create any tag to describe any resource, it leads to what is referred to as the vocabulary problem. Because users may use different words to describe the same document or extract different topics from the same document based on their own background knowledge, the lack of a top-down mediation may lead to an increase in the use of incoherent tags to represent the information resources in the system. In other words, the inherent "unstructuredness" of social tags may hinder their potential as navigational cues for searchers because the diversities of users and motivation may lead to diminishing tag-topic relations as the system grows. However, a number of studies have shown that structures do emerge at the semantic level -- indicating that there are cohesive forces that are driving the emergent structures in a social tagging system.""
""Synchronous editors "",,,,""Solid Edge is a 3D CAD, parametric feature (history based) and synchronous technology solid modeling software. It runs on Microsoft Windows and provides solid modeling, assembly modelling and 2D orthographic view functionality for mechanical designers. Through third party applications it has links to many other Product Lifecycle Management (PLM) technologies.
Originally developed and released by Intergraph in 1996 using the ACIS geometric modeling kernel it later changed to using the Parasolid kernel. In 1998 it was purchased and further developed by UGS Corp (the purchase date corresponds to the kernel swap).
In 2007, UGS was acquired by the Automation & Drives Division of Siemens AG. UGS company was renamed Siemens PLM Software on October 1, 2007. Since Sep 2006 Siemens also offers a free 2D version called Solid Edge 2D Drafting.
Solid Edge is available in either Classic or Premium. The "Premium" package includes all of the features of "Classic" in addition to mechanical and electrical routing software, and powerful engineering simulation capabilities for CAE (Computer Aided Engineering). 
Solid Edge is a direct competitor to SolidWorks, PTC Creo, and Autodesk Inventor.""
""Asynchronous editors "",,,,""In computer science, asynchronous I/O is a form of input/output processing that permits other processing to continue before the transmission has finished.
Input and output (I/O) operations on a computer can be extremely slow compared to the processing of data. An I/O device can incorporate mechanical devices that must physically move, such as a hard drive seeking a track to read or write; this is often orders of magnitude slower than the switching of electric current. For example, during a disk operation that takes ten milliseconds to perform, a processor that is clocked at one gigahertz could have performed ten million instruction-processing cycles.
A simple approach to I/O would be to start the access and then wait for it to complete. But such an approach (called synchronous I/O or blocking I/O) would block the progress of a program while the communication is in progress, leaving system resources idle. When a program makes many I/O operations, this means that the processor can spend almost all of its time idle waiting for I/O operations to complete.
Alternatively, it is possible to start the communication and then perform processing that does not require that the I/O be completed. This approach is called asynchronous input/output. Any task that depends on the I/O having completed (this includes both using the input values and critical operations that claim to assure that a write operation has been completed) still needs to wait for the I/O operation to complete, and thus is still blocked, but other processing that does not have a dependency on the I/O operation can continue.
Many operating system functions exist to implement asynchronous I/O at many levels. In fact, one of the main functions of all but the most rudimentary of operating systems is to perform at least some form of basic asynchronous I/O, though this may not be particularly apparent to the operator or programmer. In the simplest software solution, the hardware device status is polled at intervals to detect whether the device is ready for its next operation. (For example, the CP/M operating system was built this way. Its system call semantics did not require any more elaborate I/O structure than this, though most implementations were more complex, and thereby more efficient.) Direct memory access (DMA) can greatly increase the efficiency of a polling-based system, and hardware interrupts can eliminate the need for polling entirely. Multitasking operating systems can exploit the functionality provided by hardware interrupts, whilst hiding the complexity of interrupt handling from the user. Spooling was one of the first forms of multitasking designed to exploit asynchronous I/O. Finally, multithreading and explicit asynchronous I/O APIs within user processes can exploit asynchronous I/O further, at the cost of extra software complexity.
Asynchronous I/O is used to improve throughput, latency, and/or responsiveness.""
""Empirical studies in collaborative and social computing "",,,,""Sociology is the study of social behavior or society, including its origins, development, organization, networks, and institutions. It is a social science that uses various methods of empirical investigation and critical analysis to develop a body of knowledge about social order, disorder, and change. Many sociologists aim to conduct research that may be applied directly to social policy and welfare, while others focus primarily on refining the theoretical understanding of social processes. Subject matter ranges from the micro level of individual agency and interaction to the macro level of systems and the social structure.
The traditional focuses of sociology include social stratification, social class, social mobility, religion, secularization, law, sexuality and deviance. As all spheres of human activity are affected by the interplay between social structure and individual agency, sociology has gradually expanded its focus to further subjects, such as health, medical, military and penal institutions, the Internet, education, and the role of social activity in the development of scientific knowledge.
The range of social scientific methods has also expanded. Social researchers draw upon a variety of qualitative and quantitative techniques. The linguistic and cultural turns of the mid-twentieth century led to increasingly interpretative, hermeneutic, and philosophic approaches towards the analysis of society. Conversely, the end of the 1990s and the beginning of 2000s have seen the rise of new analytically, mathematically and computationally rigorous techniques, such as agent-based modelling and social network analysis.
Social research informs politicians and policy makers, educators, planners, lawmakers, administrators, developers, business magnates, managers, social workers, non-governmental organizations, non-profit organizations, and people interested in resolving social issues in general. There is often a great deal of crossover between social research, market research, and other statistical fields.
^ sociology. (n.d.). The American Heritage Science Dictionary the name was created by dewight D Eisenhower . Retrieved 13 July 2013, from Dictionary.com website: http://dictionary.reference.com/browse/sociology
^ http://www.dummies.com/how-to/content/sociology-for-dummies-cheat-sheet.html
^ http://www.pasadena.edu/studentservices/counseling/graduation/documents/aa-t_sociology.pdf
^ https://www.colgate.edu/docs/default-source/default-document-library/sociology-a-21st-century-major.pdf?sfvrsn=0
^ http://www.asanet.org/introtosociology/Documents/Field%20of%20sociology033108.htm#whatissociology
^ 
^ 
^ 
^ 
^ 
^ Lynn R. Kahle, Pierre Valette-Florence (2012). Marketplace Lifestyles in an Age of Social Media. New York: M.E. Sharpe, Inc. ISBN 978-0-7656-2561-8.""
""Collaborative and social computing devices "",,,,""Collaborative software or groupware is an application software designed to help people involved in a common task to achieve their goals. One of the earliest definitions of collaborative software is 'intentional group processes plus software to support them.'
Collaborative software is a broad concept that overlaps considerably with computer-supported cooperative work (CSCW). According to Carstensen and Schmidt (1999) groupware is part of CSCW. The authors claim that CSCW, and thereby groupware, addresses "how collaborative activities and their coordination can be supported by means of computer systems." Software products such as email, calendaring, text chat, wiki, and bookmarking belong to this category whenever used for group work, whereas the more general term social software applies to systems used outside the workplace, for example, online dating services and social networking sites like Twitter and Facebook.
The use of collaborative software in the work space creates a collaborative working environment (CWE).
Finally, collaborative software relates to the notion of collaborative work systems, which are conceived as any form of human organization that emerges any time that collaboration takes place, whether it is formal or informal, intentional or unintentional. Whereas the groupware or collaborative software pertains to the technological elements of computer-supported cooperative work, collaborative work systems become a useful analytical tool to understand the behavioral and organizational variables that are associated to the broader concept of CSCW.""
""Ubiquitous computing "",,,,""Ubiquitous computing (or "ubicomp") is a concept in software engineering and computer science where computing is made to appear anytime and everywhere. In contrast to desktop computing, ubiquitous computing can occur using any device, in any location, and in any format. A user interacts with the computer, which can exist in many different forms, including laptop computers, tablets and terminals in everyday objects such as a fridge or a pair of glasses. The underlying technologies to support ubiquitous computing include Internet, advanced middleware, operating system, mobile code, sensors, microprocessors, new I/O and user interfaces, networks, mobile protocols, location and positioning and new materials.
This paradigm is also described as pervasive computing, ambient intelligence, ambient media or "everyware". Each term emphasizes slightly different aspects. When primarily concerning the objects involved, it is also known as physical computing, the Internet of Things, haptic computing, and "things that think". Rather than propose a single definition for ubiquitous computing and for these related terms, a taxonomy of properties for ubiquitous computing has been proposed, from which different kinds or flavors of ubiquitous systems and applications can be described.
Ubiquitous computing touches on a wide range of research topics, including distributed computing, mobile computing, location computing, mobile networking, context-aware computing, sensor networks, human-computer interaction, and artificial intelligence.""
""Mobile computing "",,,,""Mobile computing is human–computer interaction by which a computer is expected to be transported during normal usage, which includes allows transmission of data, voice and video. Mobile computing involves mobile communication, mobile hardware, and mobile software. Communication issues include ad hoc networks and infrastructure networks as well as communication properties, protocols, data formats and concrete technologies. Hardware includes mobile devices or device components. Mobile software deals with the characteristics and requirements of mobile applications.""
""Ambient intelligence "",,,,""In computing, ambient intelligence (AmI) refers to electronic environments that are sensitive and responsive to the presence of people. Ambient intelligence is a vision on the future of consumer electronics, telecommunications and computing that was originally developed in the late 1990s for the time frame 2010–2020. In an ambient intelligence world, devices work in concert to support people in carrying out their everyday life activities, tasks and rituals in an easy, natural way using information and intelligence that is hidden in the network connecting these devices (see Internet of Things). As these devices grow smaller, more connected and more integrated into our environment, the technology disappears into our surroundings until only the user interface remains perceivable by users.
The ambient intelligence paradigm builds upon pervasive computing, ubiquitous computing, profiling, context awareness, and human-centric computer interaction design and is characterized by systems and technologies that are (Zelkha et al. 1998; Aarts, Harwig & Schuurmans 2001):
embedded: many networked devices are integrated into the environment
context aware: these devices can recognize you and your situational context
personalized: they can be tailored to your needs
adaptive: they can change in response to you
anticipatory: they can anticipate your desires without conscious mediation.
Ambient intelligence is closely related to the long term vision of an intelligent service system in which technologies are able to automate a platform embedding the required devices for powering context aware, personalized, adaptive and anticipatory services. Where in other media environment the interface is clearly distinct, in an ubiquitous environment 'content' differs. Artur Lugmayr defined such a smart environment by describing it as ambient media. It is constituted of the communication of information in ubiquitous and pervasive environments. The concept of ambient media relates to ambient media form, ambient media content, and ambient media technology. Its principles have been established by Artur Lugmayr and are manifestation, morphing, intelligence, and experience.
A typical context of ambient intelligence environment is a Home environment (Bieliková & Krajcovic 2001).

""
""Smartphones "",,,,""A smartphone is a mobile phone with an advanced mobile operating system which combines features of a personal computer operating system with other features useful for mobile or handheld use. They typically combine the features of a cell phone with those of other popular mobile devices, such as personal digital assistant (PDA), media player and GPS navigation unit. Most smartphones can access the Internet, have a touchscreen user interface, with either an LCD, OLED, AMOLED, LED or similar screen, can run third-party apps, music players and are camera phones. Most smartphones produced from 2012 onwards also have high-speed mobile broadband 4G LTE internet, motion sensors, and mobile payment.
^ "Smartphone". Phone Scoop. Retrieved December 15, 2011. 
^ "Feature Phone". Phone Scoop. Retrieved December 15, 2011. 
^ Andrew Nusca (August 20, 2009). "Smartphone vs. feature phone arms race heats up; which did you buy?". ZDNet. Retrieved December 15, 2011.""
""Interactive whiteboards "",,,,""An interactive whiteboard (IWB) is a large interactive display that connects to a computer. A projector projects the computer's desktop onto the board's surface where users control the computer using a pen, finger, stylus, or other device. The board is typically mounted to a wall or floor stand.
They are used in a variety of settings, including classrooms at all levels of education, in corporate board rooms and work groups, in training rooms for professional sports coaching, in broadcasting studios, and others.
The first interactive whiteboards were designed and manufactured for use in the office. They were developed by Xerox Parc around 1990. This board was used in small group meetings and round-tables.
The interactive whiteboard industry was expected to reach sales of US$1 billion worldwide by 2008; one of every seven classrooms in the world was expected to feature an interactive whiteboard by 2011 according to market research by Futuresource Consulting. In 2004, 26% of British primary classrooms had interactive whiteboards. The Becta Harnessing Technology Schools Survey 2007 indicated that 98% of secondary and 100% of primary schools had IWBs. By 2008 the average numbers of interactive whiteboards rose in both primary schools (18 compared with just over six in 2005, and eight in the 2007 survey) and secondary schools (38, compared with 18 in 2005 and 22 in 2007).
Uses for interactive whiteboards may include:
Running software that is loaded onto the connected PC, such as a web browsers or other software used in the classroom.
Capturing and saving notes written on a whiteboard to the connected PC
Capturing notes written on a graphics tablet connected to the whiteboard
Controlling the PC from the white board using click and drag, markup which annotates a program or presentation
Using OCR software to translate cursive writing on a graphics tablet into text
Using an Audience Response System so that presenters can poll a classroom audience or conduct quizzes, capturing feedback onto the whiteboard""
""Mobile phones "",,,,""A mobile phone is a telephone that can make and receive calls over a radio frequency carrier while the user is moving within a telephone service area. The radio frequency link establishes a connection to the switching systems of a mobile phone operator, which provides access to the public switched telephone network (PSTN). Most modern mobile telephone services use a cellular network architecture, and therefore mobile telephones are often also called cellular telephones or cell phones. In addition to telephony, modern mobile phones support a variety of other services, such as text messaging, MMS, email, Internet access, short-range wireless communications (infrared, Bluetooth), business applications, gaming, and photography. Mobile phones which offer these and more general computing capabilities are referred to as smartphones.
The first handheld mobile phone was demonstrated by John F. Mitchell and Martin Cooper of Motorola in 1973, using a handset weighing c. 4.4 lbs (2 kg). In 1983, the DynaTAC 8000x was the first commercially available handheld mobile phone. From 1983 to 2014, worldwide mobile phone subscriptions grew to over seven billion, penetrating 100% of the global population and reaching the bottom of the economic pyramid. In 2014, the top mobile phone manufacturers were Samsung, Nokia, Apple, and LG.""
""Mobile devices "",,,,""A mobile device is a small computing device, typically small enough to be handheld (and hence also commonly known as a handheld computer or simply handheld) having a display screen with touch input and/or a miniature keyboard and weighing less than 2 pounds (0.91 kg). Samsung, Sony, HTC, LG, Motorola Mobility and Apple are just a few examples of the many manufacturers that produce these types of devices.
A handheld computing device has an operating system (OS), and can run various types of application software, known as apps. Most handheld devices can also be equipped with Wi-Fi, Bluetooth, NFC and GPS capabilities that can allow connections to the Internet and other devices, such as an automobile or a microphone headset or can be used to provide Location-based services. A camera or media player feature for video or music files can also be typically found on these devices along with a stable battery power source such as a lithium battery. Increasingly mobile devices also contain sensors like accelerometers, compasses, magnetometers, or gyroscopes, allowing detection of orientation and motion. Mobile devices may provide biometric user authentication, such as using the built-in camera for face recognition or using a fingerprint sensor for fingerprint recognition. Examples are Android’s "Face Unlock" or Apple’s Touch ID.
Early pocket-sized devices were joined in the late 2000s by larger but otherwise similar tablet computers. Input and output of modern mobile devices are often combined into a touch-screen interface.
Smartphones and PDAs are popular among those who wish to use some of the powers of a conventional computer in environments where carrying one would be impractical. Enterprise digital assistants can further extend the available functionality for the business user by offering integrated data capture devices like barcode, RFID and smart card readers.""
""Portable media players "",,,,""A portable media player (PMP) or digital audio player (DAP) is a portable consumer electronics device capable of storing and playing digital media such as audio, images, and video files. The data is typically stored on a CD, DVD, flash memory, microdrive, or hard drive. Most portable media players are equipped with a 1/8 " headphone jack, which users can plug headphones into, or connect to a boombox or hifi system. In contrast, analog portable audio players play music from non-digital media that use analog signal storage, such as cassette tapes or vinyl records.
Often mobile digital audio players are marketed and sold as "portable MP3 players", even if they also support other file formats and media types. Increasing sales of smartphones and tablet computers have led to a decline in sales of portable media players, leading to some devices being phased out, though flagship devices like the Apple iPod and Sony Walkman are still in production. Portable DVD players are still manufactured by brands across the world.
This article focuses on portable devices that have the main function of playing media.

""
""Personal digital assistants "",,,,""A personal digital assistant (PDA), also known as a handheld PC, or personal data assistant, is a mobile device that functions as a personal information manager. The term evolved from Personal Desktop Assistant, a software term for an application that prompts or prods the user of a computer with suggestions or provides quick reference to contacts and other lists. PDAs were largely discontinued in the early 2010s after the widespread adoption of highly capable smartphones, in particular those based on iOS and Android.
Nearly all PDAs have the ability to connect to the Internet. A PDA has an electronic visual display, enabling it to include a web browser, all models also have audio capabilities enabling use as a portable media player, and also enabling most of them to be used as mobile phones. Most PDAs can access the Internet, intranets or extranets via Wi-Fi or Wireless Wide Area Networks. Most PDAs employ touchscreen technology.
The first PDA was released in 1984 by Psion, the Organizer. Followed by Psion's Series 3, in 1991, which began to resemble the more familiar PDA style. It also had a full keyboard. The term PDA was first used on January 7, 1992 by Apple Computer CEO John Sculley at the Consumer Electronics Show in Las Vegas, Nevada, referring to the Apple Newton. In 1994, IBM introduced the first PDA with full mobile phone functionality, the IBM Simon, which can also be considered the first smartphone. Then in 1996, Nokia introduced a PDA with full mobile phone functionality, the 9000 Communicator, which became the world's best-selling PDA. The Communicator spawned a new category of PDAs: the "PDA phone", now called "smartphone". Another early entrant in this market was Palm, with a line of PDA products which began in March 1996. The terms "personal digital assistant" and "PDA" apply to smartphones but are not used in marketing, media, or general conversation to refer to devices such as the BlackBerry, iPad, iPhone or Android devices.

""
""Handheld game consoles "",,,,""A handheld game console is a lightweight, portable video game console with a built-in screen, game controls, and speakers. Handheld game consoles are smaller than home video game consoles and contain the console, screen, speakers, and controls in one unit, allowing people to carry them and play them at any time or place.
In 1976, Mattel introduced the first handheld electronic game with the release of Auto Race. Later, several companies—including Coleco and Milton Bradley—made their own single-game, lightweight table-top or handheld electronic game devices. The oldest true handheld game console with interchangeable cartridges is the Milton Bradley Microvision in 1979.
Nintendo is credited with popularizing the handheld console concept with the release of the Game Boy in 1989 and as of 2014 continues to dominate the handheld console market with their Nintendo 2DS and 3DS systems.""
""E-book readers "",,,,""An e-reader, also called an e-book reader or e-book device, is a mobile electronic device that is designed primarily for the purpose of reading digital e-books and periodicals.
Any device that can display text on a screen may act as an e-reader, but specialized e-reader devices may optimize portability, readability (especially in sunlight), and battery life for this purpose. A single e-reader is capable of holding the digital equivalent of hundreds of printed texts with no added bulk or measurable mass.

""
""Tablet computers "",,,,""A tablet computer, commonly shortened to tablet, is a mobile computer with a touchscreen display, circuitry, and battery in a single device. Tablets come equipped with sensors, including cameras, a microphone, and an accelerometer, and the touchscreen display uses the recognition of finger or stylus gestures replacing the usage of the mouse and keyboard. They usually feature on-screen, pop-up virtual keyboards for typing. Tablets may have physical buttons for basic features such as speaker volume and power, and ports for network communications and battery charging. Tablets are typically larger than smartphones or personal digital assistants with screens 7 inches (18 cm) or larger, measured diagonally.
Tablets can be classified according to the presence and physical appearance of keyboards. Slates and booklets do not have a physical keyboard and typically feature text input performed through the use of a virtual keyboard projected on a touchscreen-enabled display. Hybrids, convertibles and 2-in-1s do have physical keyboards (although concealable or detachable), yet they typically also make use of virtual keyboards.
The format was conceptualized in the mid-20th century and prototyped and developed in the last two decades of that century. In April 2010, the iPad was released, which was the first mass-market tablet with finger-friendly multi-touch and a dedicated operating system. Tablets experienced a rapid rise in popularity and ubiquity and became a large product category.""
""Ubiquitous and mobile computing design and evaluation methods "",,,,""In design, human–computer interaction, and software development, interaction design, often abbreviated IxD, is defined as "the practice of designing interactive digital products, environments, systems, and services." Like many other design fields interaction design also has an interest in form but its main focus is on behaviour. What clearly marks interaction design as a design field as opposed to a science or engineering field is that it involves synthesising and imagining things as they might be, more so than focusing on how things are.
Interaction design is heavily focused on satisfying the needs and desires of the majority of people who will use the product; other disciplines like software engineering have a heavy focus on designing for technical stakeholders of a project.""
""Empirical studies in ubiquitous and mobile computing "",,,,""The following outline is provided as an overview of and topical guide to academic disciplines:
An academic discipline or field of study is a branch of knowledge that is taught and researched as part of higher education. A scholar's discipline is commonly defined and recognized by the university faculties and learned societies to which he or she belongs and the academic journals in which he or she publishes research. However, no formal criteria exist for defining an academic discipline.
Disciplines vary between well-established ones that exist in almost all universities and have well-defined rosters of journals and conferences and nascent ones supported by only a few universities and publications. A discipline may have branches, and these are often called sub-disciplines.
There is no consensus on how some academic disciplines should be classified (e.g., whether anthropology and linguistics are disciplines of social sciences or fields within the humanities). More generally, the proper criteria for organizing knowledge into disciplines are also open to debate.

""
""Treemaps "",,,,""In information visualization and computing, treemapping is a method for displaying hierarchical data by using nested rectangles.

""
""Hyperbolic trees "",,,,""A hyperbolic tree (often shortened as hypertree) is an information visualization and graph drawing method inspired by hyperbolic geometry.

Displaying hierarchical data as a tree suffers from visual clutter as the number of nodes per level can grow exponentially. For a simple binary tree, the maximum number of nodes at a level n is 2n, while the number of nodes for larger trees grows much more quickly. Drawing the tree as a node-link diagram thus requires exponential amounts of space to be displayed.
One approach is to use a hyperbolic tree, first introduced by Lamping et al. Hyperbolic trees employ hyperbolic space, which intrinsically has "more room" than Euclidean space. For instance, linearly increasing the radius of a circle in Euclidean space increases its circumference linearly, while the same circle in hyperbolic space would have its circumference increase exponentially. Exploiting this property allows laying out the tree in hyperbolic space in an uncluttered manner: placing a node far enough from its parent gives the node almost the same amount of space as its parent for laying out its own children.
Displaying a hyperbolic tree commonly utilizes the Poincaré disk model of hyperbolic geometry, though the Klein-Beltrami model can also be used. Both display the entire hyperbolic plane within a unit disk, making the entire tree visible at once. The unit disk gives a fish-eye lens view of the plane, giving more emphasis to nodes which are in focus and displaying nodes further out of focus closer to the boundary of the disk. Traversing the hyperbolic tree requires Möbius transformations of the space, bringing new nodes into focus and moving higher levels of the hierarchy out of view.
Hyperbolic trees have been patented in the U.S. by Xerox.""
""Heat maps "",,,,""A heat map is a graphical representation of data where the individual values contained in a matrix are represented as colors. Fractal maps and tree maps both often use a similar system of color-coding to represent the values taken by a variable in a hierarchy. The term is also used to mean its thematic application as a choropleth map.

""
""Graph drawings "",,,,""The International Symposium on Graph Drawing (GD) is an annual academic conference in which researchers present peer reviewed papers on graph drawing, information visualization of network information, geometric graph theory, and related topics.""
""Dendrograms "",,,,""A dendrogram (from Greek dendro "tree" and gramma "drawing") is a tree diagram frequently used to illustrate the arrangement of the clusters produced by hierarchical clustering. Dendrograms are often used in computational biology to illustrate the clustering of genes or samples, sometimes on top of heatmaps.

""
""Cladograms "",,,,""A cladogram (from Greek clados "branch" and gramma "character") is a diagram used in cladistics which shows relations among organisms. A cladogram is not, however, an evolutionary tree because it does not show how ancestors are related to descendants or how much they have changed; many evolutionary trees can be inferred from a single cladogram. A cladogram uses lines that branch off in different directions ending at a clade, a groups of organisms with a last common ancestor. There are many shapes of cladograms but they all have lines that branch off from other lines. The lines can be traced back to where they branch off. These branching off points represent a hypothetical ancestor (not an actual entity) which is inferred to exhibit the traits shared among the terminal taxa above it. This hypothetical ancestor might then provide clues about the order of evolution of various features, adaptation, and other evolutionary narratives about ancestors. Although traditionally such cladograms were generated largely on the basis of morphological characters, DNA and RNA sequencing data and computational phylogenetics are now very commonly used in the generation of cladograms, either on their own or in combination with morphology.""
""Scientific visualization "",,,,""Scientific visualization (also spelled scientific visualisation) is an interdisciplinary branch of science. According to Friendly (2008), it is "primarily concerned with the visualization of three-dimensional phenomena (architectural, meteorological, medical, biological, etc.), where the emphasis is on realistic renderings of volumes, surfaces, illumination sources, and so forth, perhaps with a dynamic (time) component". It is also considered a branch of computer science that is a subset of computer graphics. The purpose of scientific visualization is to graphically illustrate scientific data to enable scientists to understand, illustrate, and glean insight from their data.
^ Visualizations that have been created with VisIt. at wci.llnl.gov. Updated: November 8, 2007
^ Michael Friendly (2008). "Milestones in the history of thematic cartography, statistical graphics, and data visualization".""
""Visual analytics "",,,,""Visual analytics is an outgrowth of the fields of information visualization and scientific visualization that focuses on analytical reasoning facilitated by interactive visual interfaces.""
""Geographic visualization "",,,,""Geovisualization or Geovisualisation, short for Geographic Visualization, refers to a set of tools and techniques supporting geospatial data analysis through the use of interactive visualization.
Like the related fields of scientific visualization and information visualization  geovisualization emphasizes knowledge construction over knowledge storage or information transmission. To do this, geovisualization communicates geospatial information in ways that, when combined with human understanding, allow for data exploration and decision-making processes.
Traditional, static maps have a limited exploratory capability; the graphical representations are inextricably linked to the geographical information beneath. GIS and geovisualization allow for more interactive maps; including the ability to explore different layers of the map, to zoom in or out, and to change the visual appearance of the map, usually on a computer display. Geovisualization represents a set of cartographic technologies and practices that take advantage of the ability of modern microprocessors to render changes to a map in real time, allowing users to adjust the mapped data on the fly.""
""Information visualization "",,,,""Information visualization or information visualisation is the study of (interactive) visual representations of abstract data to reinforce human cognition. The abstract data include both numerical and non-numerical data, such as text and geographic information. However, information visualization differs from scientific visualization: "it’s infovis [information visualization] when the spatial representation is chosen, and it’s scivis [scientific visualization] when the spatial representation is given".""
""Visualization theory, concepts and paradigms "",,,,""The concept of design paradigms derives from the rather ambiguous idea of paradigm originating in the sociology of science, which carries at least two main meanings:
As models, archetypes, or quintessential examples of solutions to problems. A 'paradigmatic design' in this sense, refers to a design solution that is considered by a community as being successful and influential. Usually success is associated to market share or some other measure of popularity, but this need not be the case. For instance, the eMate and other Apple Newton devices can be considered as paradigmatic because of their influence in subsequent designs, despite their commercial failure.
As sociological paradigms, a design paradigm is the constellation of beliefs, rules, knowledge, etc. that is valid for a particular design community. Here a paradigm is not a particular solution, but rather the underlying system of ideas that causes a range of solutions to be 'normal' or 'obvious'. A current example is the laptop: as of 2010 the design paradigm of laptops includes a portable computer unit consisting of a QWERTY keyboard, a hinged screen, etc. Moreover, such device is assumed to be helpful in tasks such as education as in the One Laptop per Child project.
While the first meaning of "design paradigm" refers to exemplary design solutions that create "design trends", the second meaning refers to what a group of people expects from a type of design solutions.
The term "design paradigm" is used within the design professions, including architecture, industrial design and engineering design, to indicate an archetypal solution. Thus a Swiss Army Knife is a design paradigm illustrating the concept of a single object that changes configuration to address a number of problems.
Design paradigms have been introduced in a number of books including Design Paradigms: A Sourcebook for Creative Visualization by Warren Wake, and discussed in Design Paradigms: Case Histories of Error and Judgment in Engineering but never defined by Henry Petroski. This concept is close to design pattern coined by Christopher Alexander in A Pattern Language.
Design paradigms can be used either to describe a design solution, or as an approach to design problem solving. Problem solving occurs through a process of abstraction and characterization of design solutions, with subsequent categorization into problem solving types. The approach is akin to the use of metaphor in language; metaphors are used to help explain concepts that are new or unfamiliar, and to bridge between a problem we understand and a problem we don't. Design paradigms then can be seen as higher order metaphors; as the often three-dimensional distillation of a working relationship between parts, between groups of things, between the known and the unknown. In this sense, a bridge is a paradigm of the connection between the known and the unknown, and the functional equivalent of a physical bridge is consequently used in many fields from computer hardware to musical composition.
The design paradigms concept has proven so powerful in traditional fields of design, that it has inspired a branch of computer science, where computational analogies to design paradigms are commonly called software design patterns. Importantly however, in design professions the term "design pattern" usually describes a 2-dimensional structure, whereas the term "design paradigm" (or model) usually implies a higher order, having 3 or more dimensions.""
""Empirical studies in visualization "",,,,""Scientific modelling is a scientific activity, the aim of which is to make a particular part or feature of the world easier to understand, define, quantify, visualize, or simulate by referencing it to existing and usually commonly accepted knowledge. It requires selecting and identifying relevant aspects of a situation in the real world and then using different types of models for different aims, such as conceptual models to better understand, operational models to operationalize, mathematical models to quantify, and graphical models to visualize the subject. Modelling is an essential and inseparable part of scientific activity, and many scientific disciplines have their own ideas about specific types of modelling.
There is also an increasing attention to scientific modelling in fields such as science education, philosophy of science, systems theory, and knowledge visualization. There is growing collection of methods, techniques and meta-theory about all kinds of specialized scientific modelling.""
""Visualization design and evaluation methods "",,,,""Design science was introduced in 1963 by R. Buckminster Fuller  who defined it as a systematic form of designing. The concept of design science was taken up in S. A. Gregory's 1966 book of the 1965 Design Methods Conference  where he drew the distinction between scientific method and design method. Gregory was clear in his view that design was not a science and that design science referred to the scientific study of design. Herbert Simon in his 1968 Karl Taylor Compton lectures  used and popularized these terms in his argument for the scientific study of the artificial (as opposed to the natural). Over the intervening period the two terms have co-mingled to the point where design science has come to have both meanings, with the meaning of scientific study of design now predominating.
^ 
^ 
^""
""Accessibility theory, concepts and paradigms "",,,,""Accessibility refers to the design of products, devices, services, or environments for people with disabilities. The concept of accessible design ensures both "direct access" (i.e. unassisted) and "indirect access" meaning compatibility with a person's assistive technology (for example, computer screen readers).
Accessibility can be viewed as the "ability to access" and benefit from some system or entity. The concept focuses on enabling access for people with disabilities, or special needs, or enabling access through the use of assistive technology; however, research and development in accessibility brings benefits to everyone.
Accessibility is not to be confused with usability, which is the extent to which a product (such as a device, service, or environment) can be used by specified users to achieve specified goals with effectiveness, efficiency and satisfaction in a specified context of use.
Accessibility is strongly related to universal design which is the process of creating products that are usable by people with the widest possible range of abilities, operating within the widest possible range of situations. This is about making things accessible to all people (whether they have a disability or not).""
""Empirical studies in accessibility "",,,,""Universal design (often inclusive design) refers to broad-spectrum ideas meant to produce buildings, products and environments that are inherently accessible to older people, people without disabilities, and people with disabilities.
The term "universal design" was coined by the architect Ronald L. Mace to describe the concept of designing all products and the built environment to be aesthetic and usable to the greatest extent possible by everyone, regardless of their age, ability, or status in life. However, it was the work of Selwyn Goldsmith, author of Designing for the Disabled (1963), who really pioneered the concept of free access for disabled people. His most significant achievement was the creation of the dropped curb - now a standard feature of the built environment.
Universal design emerged from slightly earlier barrier-free concepts, the broader accessibility movement, and adaptive and assistive technology and also seeks to blend aesthetics into these core considerations. As life expectancy rises and modern medicine increases the survival rate of those with significant injuries, illnesses, and birth defects, there is a growing interest in universal design. There are many industries in which universal design is having strong market penetration but there are many others in which it has not yet been adopted to any great extent. Universal design is also being applied to the design of technology, instruction, services, and other products and environments.
Curb cuts or sidewalk ramps, essential for people in wheelchairs but also used by all, are a common example. Color-contrast dishware with steep sides that assists those with visual or dexterity problems are another. There are also cabinets with pull-out shelves, kitchen counters at several heights to accommodate different tasks and postures, and, amidst many of the world's public transit systems, low-floor buses that "kneel" (bring their front end to ground level to eliminate gap) and/or are equipped with ramps rather than on-board lifts.
^ "Ronald L. Mace on NC State University, College of Design". Design.ncsu.edu. Retrieved 2013-07-26. 
^ Paul Harpur, 'From universal exclusion to universal equality: Regulating Ableism in a Digital Age' (2013) 40 Northern Kentucky Law Review 3, 529-565.""
""Accessibility design and evaluation methods "",,,,""Accessibility refers to the design of products, devices, services, or environments for people with disabilities. The concept of accessible design ensures both "direct access" (i.e. unassisted) and "indirect access" meaning compatibility with a person's assistive technology (for example, computer screen readers).
Accessibility can be viewed as the "ability to access" and benefit from some system or entity. The concept focuses on enabling access for people with disabilities, or special needs, or enabling access through the use of assistive technology; however, research and development in accessibility brings benefits to everyone.
Accessibility is not to be confused with usability, which is the extent to which a product (such as a device, service, or environment) can be used by specified users to achieve specified goals with effectiveness, efficiency and satisfaction in a specified context of use.
Accessibility is strongly related to universal design which is the process of creating products that are usable by people with the widest possible range of abilities, operating within the widest possible range of situations. This is about making things accessible to all people (whether they have a disability or not).""
""Accessibility technologies "",,,,""Predictive text is an input technology used where one key or button represents many letters, such as on the numeric keypads of mobile phones and in accessibility technologies. Each key press results in a prediction rather than repeatedly sequencing through the same group of "letters" it represents, in the same, invariable order. Predictive text could allow for an entire word to be input by single keypress. Predictive text makes efficient use of fewer device keys to input writing into a text message, an e-mail, an address book, a calendar, and the like.
The most widely used, general, predictive text systems are T9, iTap, eZiText, and LetterWise/WordWise. There are many unique ways to build a device that predicts text, but all predictive text systems have initial linguistic settings that offer predictions that are re-prioritized to adapt to each user. This learning adapts, by way of the device memory, to a user's disambiguating feedback that results in corrective key presses, such as pressing a "next" key to get to the intention. Most predictive text systems have a user database to facilitate this process.
Theoretically the number of keystrokes required per desired character in the finished writing is, on average, comparable to using a keyboard. This is approximately true providing that all words used are in its database, punctuation is ignored, and no input mistakes are made typing or spelling. In practice, these factors are found to cause tremendous variance in the efficiency gain. The theoretical keystrokes per character, KSPC, of a keyboard is KSPC=1.00, and of multi-tap is KSPC=2.03. Eatoni' LetterWise is a predictive multi-tap hybrid, which when operating on a standard telephone keypad achieves KSPC=1.15 for English.
The choice of which predictive text system is the best to use involves matching the user's preferred interface style, the user's level of learned ability to operate predictive text software, and the user's efficiency goal. There are various levels of risk in predictive text systems, versus multi-tap systems, because the predicted text that is automatically written that provide the speed and mechanical efficiency benefit, could, if the user is not careful to review, result in transmitting misinformation. Predictive text systems take time to learn to use well, and so generally, a device's system has user options to set up the choice of multi-tap or of any one of several schools of predictive text methods.""
""Accessibility systems and tools "",,,,""The Job Accommodation Network (JAN) is a service provided by the United States Department of Labor's Office of Disability Employment Policy (ODEP). JAN is one of several ODEP technical assistance centers. JAN facilitates the employment and retention of workers with disabilities by providing employers, employment providers, people with disabilities, their family members, and other interested parties with information on job accommodations, entrepreneurship, and related subjects. JAN's efforts are in support of the employment, including self-employment and small business ownership, of people with disabilities.""
""Algebraic algorithms "",,,,""In computational mathematics, computer algebra, also called symbolic computation or algebraic computation is a scientific area that refers to the study and development of algorithms and software for manipulating mathematical expressions and other mathematical objects. Although, properly speaking, computer algebra should be a subfield of scientific computing, they are generally considered as distinct fields because scientific computing is usually based on numerical computation with approximate floating point numbers, while symbolic computation emphasizes exact computation with expressions containing variables that have no given value and are manipulated as symbols, hence the name symbolic computation.
Software applications that perform symbolic calculations are called computer algebra systems, with the term system alluding to the complexity of the main applications that include, at least, a method to represent mathematical data in a computer, a user programming language (usually different from the language used for the implementation), a dedicated memory manager, a user interface for the input/output of mathematical expressions, a large set of routines to perform usual operations, like simplification of expressions, differentiation using chain rule, polynomial factorization, indefinite integration, etc.
At the beginning of computer algebra, circa 1970, when the long-known algorithms were first put on computers, they turned out to be highly inefficient. Therefore, a large part of the work of the researchers in the field consisted in revisiting classical algebra in order to make it effective and to discover efficient algorithms to implement this effectiveness. A typical example of this kind of work is the computation of polynomial greatest common divisors, which is required to simplify fractions. Surprisingly, the classical Euclid's algorithm turned out to be inefficient for polynomials over infinite fields, and thus new algorithms needed to be developed. The same was also true for the classical algorithms from linear algebra.
Computer algebra is widely used to experiment in mathematics and to design the formulas that are used in numerical programs. It is also used for complete scientific computations, when purely numerical methods fail, like in public key cryptography or for some non-linear problems.""
""Symbolic calculus algorithms "",,,,""Calculus (from Latin calculus, literally "small pebble used for counting") is the mathematical study of change, in the same way that geometry is the study of shape and algebra is the study of operations and their application to solving equations. It has two major branches, differential calculus (concerning rates of change and slopes of curves), and integral calculus (concerning accumulation of quantities and the areas under and between curves); these two branches are related to each other by the fundamental theorem of calculus. Both branches make use of the fundamental notions of convergence of infinite sequences and infinite series to a well-defined limit. Generally, modern calculus is considered to have been developed in the 17th century by Isaac Newton and Gottfried Leibniz. Today, calculus has widespread uses in science, engineering and economics and can solve many problems that elementary algebra alone cannot.
Calculus is a part of modern mathematics education. A course in calculus is a gateway to other, more advanced courses in mathematics devoted to the study of functions and limits, broadly called mathematical analysis. Calculus has historically been called "the calculus of infinitesimals", or "infinitesimal calculus". The word "calculus" comes from Latin (calculus) and refers to a small stone used for counting. Calculus (plural calculi) is also used for naming some methods of calculation or theories of computation, such as propositional calculus, calculus of variations, lambda calculus, and process calculus.""
""Hybrid symbolic-numeric methods "",,,,""In mathematics and computer science, symbolic-numeric computation is the use of software that combines symbolic and numeric methods to solve problems.""
""Number theory algorithms "",,,,""Victor Shoup is a computer scientist and mathematician. He obtained a PhD in computer science from the University of Wisconsin–Madison in 1989, and he did his undergraduate work at the University of Wisconsin-Eau Claire. He is a professor at the Courant Institute of Mathematical Sciences at New York University, focusing on algorithm and cryptography courses. He has held positions at AT&T Bell Labs, the University of Toronto, Saarland University, and the IBM Zurich Research Laboratory.
Shoup's main research interests and contributions are computer algorithms relating to number theory, algebra, and cryptography. His contributions to these fields include:
The Cramer–Shoup cryptosystem asymmetric encryption algorithm bears his name.
His freely available (under the terms of the GNU GPL) C++ library of number theory algorithms, NTL, is widely used and well regarded for its high performance.
He is the author of a widely used textbook, A Computational Introduction to Number Theory and Algebra, which is freely available online.
He has proved (while at IBM Zurich) a lower bound to the computational complexity of generic algorithms for solving the discrete logarithm problem, a problem in group theory which is of considerable importance to public-key cryptography.
He is closely involved in the development of an emerging ISO standard for public-key cryptography.""
""Equation and inequality solving algorithms "",,,,""Not to be confused with Inequation. "Less than" and "Greater than" redirect here. For the use of the "<" and ">" signs as punctuation, see Bracket. For the UK insurance brand "More Th>n", see More Than (company).

In mathematics, an inequality is a relation that holds between two values when they are different (see also: equality).
The notation a ≠ b means that a is not equal to b.
It does not say that one is greater than the other, or even that they can be compared in size.
If the values in question are elements of an ordered set, such as the integers or the real numbers, they can be compared in size.
The notation a < b means that a is less than b.
The notation a > b means that a is greater than b.
In either case, a is not equal to b. These relations are known as strict inequalities. The notation a < b may also be read as "a is strictly less than b".
In contrast to strict inequalities, there are two types of inequality relations that are not strict:
The notation a ≤ b means that a is less than or equal to b (or, equivalently, not greater than b, or at most b).
The notation a ≥ b means that a is greater than or equal to b (or, equivalently, not less than b, or at least b).
An additional use of the notation is to show that one quantity is much greater than another, normally by several orders of magnitude.
The notation a ≪ b means that a is much less than b. (In measure theory, however, this notation is used for absolute continuity, an unrelated concept.)
The notation a ≫ b means that a is much greater than b.""
""Optimization algorithms "",,,,""In numerical analysis, an iterative method is called locally convergent if the successive approximations produced by the method are guaranteed to converge to a solution when the initial approximation is already close enough to the solution. Iterative methods for nonlinear equations and their systems, such as Newton's method are usually only locally convergent.
An iterative method that converges for an arbitrary initial approximation is called globally convergent. Iterative methods for systems of linear equations are usually globally convergence.""
""Representation of mathematical functions "",,,,""In microeconomics, supply and demand is an economic model of price determination in a market. It concludes that in a competitive market, the unit price for a particular good, or other traded item such as labor or liquid financial assets, will vary until it settles at a point where the quantity demanded (at the current price) will equal the quantity supplied (at the current price), resulting in an economic equilibrium for price and quantity transacted.

""
""Representation of Boolean functions "",,,,""Oleg Borisovich Lupanov (Russian: Оле́г Бори́сович Лупа́нов, June 2, 1932 – May 3, 2006) was a Soviet and Russian mathematician, dean of the Moscow State University's Faculty of Mechanics and Mathematics (1980–2006), head of the Chair of Discrete Mathematics of the Faculty of Mechanics and Mathematics (1981–2006).
Together with his graduate school advisor, Sergey Vsevolodovich Yablonsky, he is considered one of the founders of the Soviet school of Mathematical Cybernetics. In particular he authored pioneering works on synthesis and complexity of Boolean circuits, and of control systems in general (Russian: Управляющие системы), the term used in the USSR and Russia for a generalization of finite state automata, Boolean circuits and multi-valued logic circuits.
Ingo Wegener, in his book The Complexity of Boolean Functions, credits O. B. Lupanov for coining the term Shannon effect in his 1970 paper, to refer to the fact that almost all Boolean functions have nearly the same circuit complexity as the hardest function.
O. B. Lupanov is best known for his (k, s)-Lupanov representation of Boolean functions that he used to devise an asymptotically optimal method of Boolean circuit synthesis, thus proving the asymptotically tight upper bound on Boolean circuit complexity:

^ Oleg Lupanov at the Mathematics Genealogy Project
^ Oleg Borisovich Lupanov, a Russian Wikipedia entry
^ I. Wegener, The Complexity of Boolean Functions [1]. John Wiley and Sons Ltd, and B. G. Teubner, Stuttgart, 1987. page 87.
^ O. B. Lupanov, On circuits of functional elements with delay. Problemy Kibernetiki, Vol. 23, 1970, pp. 43–81.
^ O. B. Lupanov, A method of circuit synthesis. Izvesitya VUZ, Radiofizika Vol. 1, 1958, pp. 120–140.""
""Self-organization "",,,,""Self-organization is a process where some form of overall order or coordination arises out of the local interactions between smaller component parts of an initially disordered system. The process of self-organization can be spontaneous, and it is not necessarily controlled by any auxiliary agent outside of the system. It is often triggered by random fluctuations that are amplified by positive feedback. The resulting organization is wholly decentralized or distributed over all the components of the system. As such, the organization is typically robust and able to survive and, even, self-repair substantial damage or perturbations. Chaos theory discusses self-organization in terms of islands of predictability in a sea of chaotic unpredictability. Self-organization occurs in a variety of physical, chemical, biological, robotic, social, and cognitive systems. Examples of its realization can be found in crystallization, thermal convection of fluids, chemical oscillation, animal swarming, and neural networks.
^ Betzler, S. B.; Wisnet, A.; Breitbach, B.; Mitterbauer, C.; Weickert, J.; Schmidt-Mende, L.; Scheu, C. (2014). "Template-free synthesis of novel, highly-ordered 3D hierarchical Nb3O7(OH) superstructures with semiconductive and photoactive properties". Journal of Materials Chemistry A 2 (30): 12005. doi:10.1039/C4TA02202E.""
""Shared memory algorithms "",,,,""Borůvka's algorithm is an algorithm for finding a minimum spanning tree in a graph for which all edge weights are distinct.
It was first published in 1926 by Otakar Borůvka as a method of constructing an efficient electricity network for Moravia. The algorithm was rediscovered by Choquet in 1938; again by Florek, Łukasiewicz, Perkal, Steinhaus, and Zubrzycki in 1951; and again by Sollin  in 1965. Because Sollin was the only computer scientist in this list living in an English speaking country, this algorithm is frequently called Sollin's algorithm, especially in the parallel computing literature.
The algorithm begins by first examining each vertex and adding the cheapest edge from that vertex to another in the graph, without regard to already added edges, and continues joining these groupings in a like manner until a tree spanning all vertices is completed.""
""Vector / streaming algorithms "",,,,""Scalable Vector Graphics (SVG) is an XML-based vector image format for two-dimensional graphics with support for interactivity and animation. The SVG specification is an open standard developed by the World Wide Web Consortium (W3C) since 1999.
SVG images and their behaviors are defined in XML text files. This means that they can be searched, indexed, scripted, and compressed. As XML files, SVG images can be created and edited with any text editor, but are more often created with drawing software.
All major modern web browsers—including Mozilla Firefox, Internet Explorer, Google Chrome, Opera, and Safari—have at least some degree of SVG rendering support.""
""Massively parallel algorithms "",,,,""Computer-supported collaboration (CSC) research focuses on technology that affects groups, organizations, communities and societies, e.g., voice mail and text chat. It grew from cooperative work study of supporting people's work activities and working relationships. As net technology increasingly supported a wide range of recreational and social activities, consumer markets expanded the user base, enabling more and more people to connect online to create what researchers have called a computer supported cooperative work, which includes "all contexts in which technology is used to mediate human activities such as communication, coordination, cooperation, competition, entertainment, games, art, and music" (from CSCW 2004).""
""Parallel programming languages "",,,,""Parallel computing is a type of computation in which many calculations are carried out simultaneously, operating on the principle that large problems can often be divided into smaller ones, which are then solved at the same time. There are several different forms of parallel computing: bit-level, instruction-level, data, and task parallelism. Parallelism has been employed for many years, mainly in high-performance computing, but interest in it has grown lately due to the physical constraints preventing frequency scaling. As power consumption (and consequently heat generation) by computers has become a concern in recent years, parallel computing has become the dominant paradigm in computer architecture, mainly in the form of multi-core processors.
Parallel computing is closely related to concurrent computing—they are frequently used together, and often conflated, though the two are distinct: it is possible to have parallelism without concurrency (such as bit-level parallelism), and concurrency without parallelism (such as multitasking by time-sharing on a single-core CPU). In parallel computing, a computational task is typically broken down in several, often many, very similar subtasks that can be processed independently and whose results are combined afterwards, upon completion. In contrast, in concurrent computing, the various processes often do not address related tasks; when they do, as is typical in distributed computing, the separate tasks may have a varied nature and often require some inter-process communication during execution.
Parallel computers can be roughly classified according to the level at which the hardware supports parallelism, with multi-core and multi-processor computers having multiple processing elements within a single machine, while clusters, MPPs, and grids use multiple computers to work on the same task. Specialized parallel computer architectures are sometimes used alongside traditional processors, for accelerating specific tasks.
In some cases parallelism is transparent to the programmer, such as in bit-level or instruction-level parallelism, but explicitly parallel algorithms, particularly those that use concurrency, are more difficult to write than sequential ones, because concurrency introduces several new classes of potential software bugs, of which race conditions are the most common. Communication and synchronization between the different subtasks are typically some of the greatest obstacles to getting good parallel program performance.
A theoretical upper bound on the speed-up of a single program as a result of parallelization is given by Amdahl's law.

""
""Information extraction "",,,,""Information extraction (IE) is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents. In most of the cases this activity concerns processing human language texts by means of natural language processing (NLP). Recent activities in multimedia document processing like automatic annotation and content extraction out of images/audio/video could be seen as information extraction.
Due to the difficulty of the problem, current approaches to IE focus on narrowly restricted domains. An example is the extraction from news wire reports of corporate mergers, such as denoted by the formal relation:
,
from an online news sentence such as:
"Yesterday, New York based Foo Inc. announced their acquisition of Bar Corp."
A broad goal of IE is to allow computation to be done on the previously unstructured data. A more specific goal is to allow logical reasoning to draw inferences based on the logical content of the input data. Structured data is semantically well-defined data from a chosen target domain, interpreted with respect to category and context.
Information Extraction is the part of a greater puzzle which deals with the problem of devising automatic methods for text management, beyond its transmission, storage and display. The discipline of information retrieval (IR) has developed automatic methods, typically of a statistical flavor, for indexing large document collections and classifying documents. Another complementary approach is that of natural language processing (NLP) which has solved the problem of modelling human language processing with considerable success when taking into account the magnitude of the task. In terms of both difficulty and emphasis, IE deals with tasks in between both IR and NLP. In terms of input, IE assumes the existence of a set of documents in which each document follows a template, i.e. describes one or more entities or events in a manner that is similar to those in other documents but differing in the details. An example, consider a group of newswire articles on Latin American terrorism with each article is presumed to be based upon one or more terroristic acts. We also define for any given IE task a template, which is a(or a set of) case frame(s) to hold the information contained in a single document. For the terrorism example, a template would have slots corresponding to the perpetrator, victim, and weapon of the terroristic act, and the date on which the event happened. An IE system for this problem is required to “understand” an attack article only enough to find data corresponding to the slots in this template.""
""Machine translation "",,,,""Machine translation, sometimes referred to by the abbreviation MT (not to be confused with computer-aided translation, machine-aided human translation (MAHT) or interactive translation) is a sub-field of computational linguistics that investigates the use of software to translate text or speech from one language to another.
On a basic level, MT performs simple substitution of words in one language for words in another, but that alone usually cannot produce a good translation of a text because recognition of whole phrases and their closest counterparts in the target language is needed. Solving this problem with corpus and statistical techniques is a rapidly growing field that is leading to better translations, handling differences in linguistic typology, translation of idioms, and the isolation of anomalies.
Current machine translation software often allows for customization by domain or profession (such as weather reports), improving output by limiting the scope of allowable substitutions. This technique is particularly effective in domains where formal or formulaic language is used. It follows that machine translation of government and legal documents more readily produces usable output than conversation or less standardised text.
Improved output quality can also be achieved by human intervention: for example, some systems are able to translate more accurately if the user has unambiguously identified which words in the text are proper names. With the assistance of these techniques, MT has proven useful as a tool to assist human translators and, in a very limited number of cases, can even produce output that can be used as is (e.g., weather reports).
The progress and potential of machine translation have been debated much through its history. Since the 1950s, a number of scholars have questioned the possibility of achieving fully automatic machine translation of high quality. Some critics claim that there are in-principle obstacles to automating the translation process.""
""Discourse, dialogue and pragmatics "",,,,""Discourse ethics refers to a type of argument that attempts to establish normative or ethical truths by examining the presuppositions of discourse. Variations of this argument have been used in the establishment of egalitarian ethics, as well as libertarian ethics.""
""Natural language generation "",,,,""Natural Language Generation (NLG) is the natural language processing task of generating natural language from a machine representation system such as a knowledge base or a logical form. Psycholinguists prefer the term language production when such formal representations are interpreted as models for mental representations.
It could be said an NLG system is like a translator that converts a computer based representation into a natural language representation. However, the methods to produce the final language are different from those of a compiler due to the inherent expressivity of natural languages. NLG has existed for a long time but it is only recently that commercial NLG technology had become widely available and self service.
NLG may be viewed as the opposite of natural language understanding: whereas in natural language understanding the system needs to disambiguate the input sentence to produce the machine representation language, in NLG the system needs to make decisions about how to put a concept into words.
Simple examples are systems that generate form letters. These do not typically involve grammar rules, but may generate a letter to a consumer, e.g. stating that a credit card spending limit was reached. More complex NLG systems dynamically create texts to meet a communicative goal. As in other areas of natural language processing, this can be done using either explicit models of language (e.g., grammars) and the domain, or using statistical models derived by analysing human-written texts.""
""Speech recognition "",,,,""Speech recognition (SR) is the inter-disciplinary sub-field of computational linguistics which incorporates knowledge and research in the linguistics, computer science, and electrical engineering fields to develop methodologies and technologies that enables the recognition and translation of spoken language into text by computers and computerized devices such as those categorized as Smart Technologies and robotics. It is also known as "automatic speech recognition" (ASR), "computer speech recognition", or just "speech to text" (STT).
Some SR systems use "training" (also called "enrollment") where an individual speaker reads text or isolated vocabulary into the system. The system analyzes the person's specific voice and uses it to fine-tune the recognition of that person's speech, resulting in increased accuracy. Systems that do not use training are called "speaker independent" systems. Systems that use training are called "speaker dependent".
Speech recognition applications include voice user interfaces such as voice dialing (e.g. "Call home"), call routing (e.g. "I would like to make a collect call"), domotic appliance control, search (e.g. find a podcast where particular words were spoken), simple data entry (e.g., entering a credit card number), preparation of structured documents (e.g. a radiology report), speech-to-text processing (e.g., word processors or emails), and aircraft (usually termed Direct Voice Input).
The term voice recognition or speaker identification refers to identifying the speaker, rather than what they are saying. Recognizing the speaker can simplify the task of translating speech in systems that have been trained on a specific person's voice or it can be used to authenticate or verify the identity of a speaker as part of a security process.
From the technology perspective, speech recognition has a long history with several waves of major innovations. Most recently, the field has benefited from advances in deep learning and big data. The advances are evidenced not only by the surge of academic papers published in the field, but more importantly by the world-wide industry adoption of a variety of deep learning methods in designing and deploying speech recognition systems. These speech industry players include Microsoft, Google, IBM, Baidu (China), Apple, Amazon, Nuance, IflyTek (China), many of which have publicized the core technology in their speech recognition systems as being based on deep learning.""
""Lexical semantics "",,,,""Lexical semantics (also known as lexicosemantics), is a subfield of linguistic semantics. The units of analysis in lexical semantics are lexical units which include not only words but also sub-words or sub-units such as affixes and even compound words and phrases. Lexical units make up the catalogue of words in a language, the lexicon. Lexical semantics looks at how the meaning of the lexical units correlates with the structure of the language or syntax. This is referred to as syntax-semantic interface.
The study of lexical semantics looks at:
the classification and decomposition of lexical items
the differences and similarities in lexical semantic structure cross-linguistically
the relationship of lexical meaning to sentence meaning and syntax.
Lexical units, also referred to as syntactic atoms, can stand alone such as in the case of root words or parts of compound words or they necessarily attach to other units such as prefixes and suffixes do. The former are called free morphemes and the latter bound morphemes. They fall into a narrow range of meanings (semantic fields) and can combine with each other to generate new meanings.
^ Pustejovsky, James (1995). The Generative Lexicon. MIT Press. 
^ Di Sciullo, Anne-Marie; Williams, Edwin (1987). On the definition of word. Cambridge, MA: MIT press.""
""Phonology / morphology "",,,,""Theoretical linguistics is the branch of linguistics that is most concerned with developing models of linguistic knowledge. The fields that are generally considered the core of theoretical linguistics are syntax, phonology, morphology, and semantics. Although phonetics often guides phonology, it is often excluded from the purview of theoretical linguistics, along with psycholinguistics and sociolinguistics. Theoretical linguistics also involves the search for an explanation of linguistic universals, that is, properties all, or many, languages have in common.""
""Language resources "",,,,""LREC, the International Conference on Language Resources and Evaluation is a biennial conference organised by the European Language Resources Association with the support of institutions and organisations involved in Natural language processing. The series of LREC conferences was launched in Granada in 1998. Since then, LREC has become the major event on Language Resources (LRs) and Evaluation for Human Language Technologies (HLT). The aim of LREC is to provide an overview of the state-of-the-art, explore new R&D directions and emerging trends, exchange information regarding LRs and their applications, evaluation methodologies and tools, ongoing and planned activities, industrial uses and needs, requirements coming from the e-society, both with respect to policy issues and to technological and organisational ones.""
""Description logics "",,,,""Description logics (DL) is a family of formal knowledge representation languages. Many DLs are more expressive than propositional logic but less expressive than first-order predicate logic. In contrast to the latter, the core reasoning problems for DLs are (usually) decidable, and efficient decision procedures have been designed and implemented for these problems.
DLs are used in artificial intelligence to describe and reason about the relevant concepts of an application domain (known as terminological knowledge). It is of particular importance in providing a logical formalism for ontologies and the Semantic Web: the Web Ontology Language [OWL] and its profile is based on DLs. The most notable application of DLs and OWL is in biomedical informatics where DL assists in the codification of biomedical knowledge.

""
""Semantic networks "",,,,""A semantic network, or frame network, is a network that represents semantic relations between concepts. This is often used as a form of knowledge representation. It is a directed or undirected graph consisting of vertices, which represent concepts, and edges.""
""Nonmonotonic, default reasoning and belief revision "",,,,""A non-monotonic logic is a formal logic whose consequence relation is not monotonic. In other words, non-monotonic logics are devised to capture and represent defeasible inferences (c.f. defeasible reasoning), i.e., a kind of inference in which reasoners draw tentative conclusions, enabling reasoners to retract their conclusion(s) based on further evidence. Most studied formal logics have a monotonic consequence relation, meaning that adding a formula to a theory never produces a reduction of its set of consequences. Intuitively, monotonicity indicates that learning a new piece of knowledge cannot reduce the set of what is known. A monotonic logic cannot handle various reasoning tasks such as reasoning by default (consequences may be derived only because of lack of evidence of the contrary), abductive reasoning (consequences are only deduced as most likely explanations), some important approaches to reasoning about knowledge (the ignorance of a consequence must be retracted when the consequence becomes known), and similarly, belief revision (new knowledge may contradict old beliefs).

""
""Probabilistic reasoning "",,,,""The aim of a probabilistic logic (also probability logic and probabilistic reasoning) is to combine the capacity of probability theory to handle uncertainty with the capacity of deductive logic to exploit structure of formal argument. The result is a richer and more expressive formalism with a broad range of possible application areas. Probabilistic logics attempt to find a natural extension of traditional logic truth tables: the results they define are derived through probabilistic expressions instead. A difficulty with probabilistic logics is that they tend to multiply the computational complexities of their probabilistic and logical components. Other difficulties include the possibility of counter-intuitive results, such as those of Dempster-Shafer theory. The need to deal with a broad variety of contexts and issues has led to many different proposals.""
""Vagueness and fuzzy logic "",,,,""Fuzzy logic is a form of many-valued logic in which the truth values of variables may be any real number between 0 and 1, considered to be "fuzzy". By contrast, in Boolean logic, the truth values of variables may only be 0 or 1, often called "crisp" values. Fuzzy logic has been extended to handle the concept of partial truth, where the truth value may range between completely true and completely false. Furthermore, when linguistic variables are used, these degrees may be managed by specific (membership) functions.
The term fuzzy logic was introduced with the 1965 proposal of fuzzy set theory by Lotfi Zadeh. Fuzzy logic has been applied to many fields, from control theory to artificial intelligence. Fuzzy logic had however been studied since the 1920s, as infinite-valued logic—notably by Łukasiewicz and Tarski.""
""Temporal reasoning "",,,,""Spatial–temporal reasoning is an area of Artificial Intelligence in computer science working on representing and reasoning spatial-temporal knowledge in mind, developing high-level control systems of robots for navigating and understanding time and space, either by leveraging results on spatial-temporal knowledge in mind of other research fields, i.e. cognitive psychology, linguistics, or based on commonsense understanding of space and time of researchers themselves. A convergent result in cognitive psychology is that the connection relation is the first spatial relation that human babies acquire, followed by understanding orientation relations and distance relations. Internal relations among the three kinds of spatial relations can be computationally and systematically explained within the theory of cognitive prism as follows: (1) the connection between Mozart and my relationships is primitive; (2) an orientation relation is a distance comparison relation: you being in front of me can be interpreted as you are nearer to my front side than my other sides; (3) a distance relation is connection relations using a third object: you being one meter away from me can be interpreted as an object with the maximum extension of one meter can be connected with you and me simultaneously.

Without addressing internal relations among spatial relations, AI researchers contributed many fragmentary representations. Examples of temporal calculi include Allen's interval algebra, and Vilain's & Kautz's point algebra. The most prominent spatial calculi are mereotopological calculi, Frank's cardinal direction calculus, Freksa's double cross calculus, Egenhofer and Franzosa's 4- and 9-intersection calculi, Ligozat's flip-flop calculus, various region connection calculi (RCC), and the Oriented Point Relation Algebra. Recently, spatio-temporal calculi have been designed that combine spatial and temporal information. For example, the spatiotemporal constraint calculus (STCC) by Gerevini and Nebel combines Allen's interval algebra with RCC-8. Moreover, the qualitative trajectory calculus (QTC) allows for reasoning about moving objects.
An emphasis in the literature has been on qualitative spatial-temporal reasoning which is based on qualitative abstractions of temporal and spatial aspects of the common-sense background knowledge on which our human perspective of physical reality is based. Methodologically, qualitative constraint calculi restrict the vocabulary of rich mathematical theories dealing with temporal or spatial entities such that specific aspects of these theories can be treated within decidable fragments with simple qualitative (non-metric) languages. Contrary to mathematical or physical theories about space and time, qualitative constraint calculi allow for rather inexpensive reasoning about entities located in space and time. For this reason, the limited expressiveness of qualitative representation formalism calculi is a benefit if such reasoning tasks need to be integrated in applications. For example, some of these calculi may be implemented for handling spatial GIS queries efficiently and some may be used for navigating, and communicating with, a mobile robot.

Most of these calculi can be formalized as abstract relation algebras, such that reasoning can be carried out at a symbolic level. For computing solutions of a constraint network, the path-consistency algorithm is an important tool.

""
""Cognitive robotics "",,,,""Cognitive robotics is concerned with endowing a robot with intelligent behavior by providing it with a processing architecture that will allow it to learn and reason about how to behave in response to complex goals in a complex world. Cognitive robotics may be considered the engineering branch of embodied cognitive science and embodied embedded cognition.

""
""Ontology engineering "",,,,""Ontology engineering in computer science and information science is a field which studies the methods and methodologies for building ontologies: formal representations of a set of concepts within a domain and the relationships between those concepts. A large-scale representation of abstract concepts such as actions, time, physical objects and beliefs would be an example of ontological engineering. Ontology engineering is one of the areas of applied ontology, and can be seen as an application of philosophical ontology. Core ideas and objectives of ontology engineering are also central in conceptual modeling.""
""Logic programming and answer set programming "",,,,""Negation as failure (NAF, for short) is a non-monotonic inference rule in logic programming, used to derive  (i.e. that  is assumed not to hold) from failure to derive . Note that  can be different from the statement  of the logical negation of , depending on the completeness of the inference algorithm and thus also on the formal logic system.
Negation as failure has been an important feature of logic programming since the earliest days of both Planner and Prolog. In Prolog, it is usually implemented using Prolog's extralogical constructs.""
""Spatial and physical reasoning "",,,,""Spatial–temporal reasoning is an area of Artificial Intelligence in computer science working on representing and reasoning spatial-temporal knowledge in mind, developing high-level control systems of robots for navigating and understanding time and space, either by leveraging results on spatial-temporal knowledge in mind of other research fields, i.e. cognitive psychology, linguistics, or based on commonsense understanding of space and time of researchers themselves. A convergent result in cognitive psychology is that the connection relation is the first spatial relation that human babies acquire, followed by understanding orientation relations and distance relations. Internal relations among the three kinds of spatial relations can be computationally and systematically explained within the theory of cognitive prism as follows: (1) the connection between Mozart and my relationships is primitive; (2) an orientation relation is a distance comparison relation: you being in front of me can be interpreted as you are nearer to my front side than my other sides; (3) a distance relation is connection relations using a third object: you being one meter away from me can be interpreted as an object with the maximum extension of one meter can be connected with you and me simultaneously.

Without addressing internal relations among spatial relations, AI researchers contributed many fragmentary representations. Examples of temporal calculi include Allen's interval algebra, and Vilain's & Kautz's point algebra. The most prominent spatial calculi are mereotopological calculi, Frank's cardinal direction calculus, Freksa's double cross calculus, Egenhofer and Franzosa's 4- and 9-intersection calculi, Ligozat's flip-flop calculus, various region connection calculi (RCC), and the Oriented Point Relation Algebra. Recently, spatio-temporal calculi have been designed that combine spatial and temporal information. For example, the spatiotemporal constraint calculus (STCC) by Gerevini and Nebel combines Allen's interval algebra with RCC-8. Moreover, the qualitative trajectory calculus (QTC) allows for reasoning about moving objects.
An emphasis in the literature has been on qualitative spatial-temporal reasoning which is based on qualitative abstractions of temporal and spatial aspects of the common-sense background knowledge on which our human perspective of physical reality is based. Methodologically, qualitative constraint calculi restrict the vocabulary of rich mathematical theories dealing with temporal or spatial entities such that specific aspects of these theories can be treated within decidable fragments with simple qualitative (non-metric) languages. Contrary to mathematical or physical theories about space and time, qualitative constraint calculi allow for rather inexpensive reasoning about entities located in space and time. For this reason, the limited expressiveness of qualitative representation formalism calculi is a benefit if such reasoning tasks need to be integrated in applications. For example, some of these calculi may be implemented for handling spatial GIS queries efficiently and some may be used for navigating, and communicating with, a mobile robot.

Most of these calculi can be formalized as abstract relation algebras, such that reasoning can be carried out at a symbolic level. For computing solutions of a constraint network, the path-consistency algorithm is an important tool.

""
""Reasoning about belief and knowledge "",,,,""Confirmation bias, also called confirmatory bias or myside bias, is the tendency to search for, interpret, favor, and recall information in a way that confirms one's beliefs or hypotheses, while giving disproportionately less consideration to alternative possibilities. It is a type of cognitive bias and a systematic error of inductive reasoning. People display this bias when they gather or remember information selectively, or when they interpret it in a biased way. The effect is stronger for emotionally charged issues and for deeply entrenched beliefs. People also tend to interpret ambiguous evidence as supporting their existing position. Biased search, interpretation and memory have been invoked to explain attitude polarization (when a disagreement becomes more extreme even though the different parties are exposed to the same evidence), belief perseverance (when beliefs persist after the evidence for them is shown to be false), the irrational primacy effect (a greater reliance on information encountered early in a series) and illusory correlation (when people falsely perceive an association between two events or situations).
A series of experiments in the 1960s suggested that people are biased toward confirming their existing beliefs. Later work re-interpreted these results as a tendency to test ideas in a one-sided way, focusing on one possibility and ignoring alternatives. In certain situations, this tendency can bias people's conclusions. Explanations for the observed biases include wishful thinking and the limited human capacity to process information. Another explanation is that people show confirmation bias because they are weighing up the costs of being wrong, rather than investigating in a neutral, scientific way.
Confirmation biases contribute to overconfidence in personal beliefs and can maintain or strengthen beliefs in the face of contrary evidence. Poor decisions due to these biases have been found in political and organizational contexts.""
""Planning under uncertainty "",,,,""A partially observable Markov decision process (POMDP) is a generalization of a Markov decision process (MDP). A POMDP models an agent decision process in which it is assumed that the system dynamics are determined by an MDP, but the agent cannot directly observe the underlying state. Instead, it must maintain a probability distribution over the set of possible states, based on a set of observations and observation probabilities, and the underlying MDP.
The POMDP framework is general enough to model a variety of real-world sequential decision processes. Applications include robot navigation problems, machine maintenance, and planning under uncertainty in general. The framework originated in the operations research community, and was later taken over by the artificial intelligence and automated planning communities.
An exact solution to a POMDP yields the optimal action for each possible belief over the world states. The optimal action maximizes (or minimizes) the expected reward (or cost) of the agent over a possibly infinite horizon. The sequence of optimal actions is known as the optimal policy of the agent for interacting with its environment.""
""Multi-agent planning "",,,,""In computer science multi-agent planning involves coordinating the resources and activities of multiple "agents".
NASA says, "multiagent planning is concerned with planning by (and for) multiple agents. It can involve agents planning for a common goal, an agent coordinating the plans (plan merging) or planning of others, or agents refining their own plans while negotiating over tasks or resources. The topic also involves how agents can do this in real time while executing plans (distributed continual planning). Multiagent scheduling differs from multiagent planning the same way planning and scheduling differ: in scheduling often the tasks that need to be performed are already decided, and in practice, scheduling tends to focus on algorithms for specific problem domains". [1]""
""Robotic planning "",,,,""A rapidly exploring random tree (RRT) is an algorithm designed to efficiently search nonconvex, high-dimensional spaces by randomly building a space-filling tree. The tree is constructed incrementally from samples drawn randomly from the search space and is inherently biased to grow towards large unsearched areas of the problem. RRTs were developed by Steven M. LaValle and James J. Kuffner Jr.  . They easily handle problems with obstacles and differential constraints (nonholonomic and kinodynamic) and have been widely used in autonomous robotic path planning.
RRTs can be viewed as a technique to generate open-loop trajectories for nonlinear systems with state constraints. An RRT can also be considered as a Monte-Carlo method to bias search into the largest Voronoi regions of a graph in a configuration space. Some variations can even be considered stochastic fractals.""
""Heuristic function construction "",,,,""In computer science, specifically in algorithms related to pathfinding, a heuristic function is said to be admissible if it never overestimates the cost of reaching the goal, i.e. the cost it estimates to reach the goal is not higher than the lowest possible cost from the current point in the path.""
""Discrete space search "",,,,""In computer science, a search algorithm is an algorithm for finding an item with specified properties among a collection of items which are coded into a computer program, that look for clues to return what is wanted. The items may be stored individually as records in a database; or may be elements of a search space defined by a mathematical formula or procedure, such as the roots of an equation with integer variables; or a combination of the two, such as the Hamiltonian circuits of a graph.""
""Continuous space search "",,,,""The National Aeronautics and Space Administration (NASA) is the agency of the United States Federal Government responsible for the civilian space program as well as aeronautics and aerospace research.
President Dwight D. Eisenhower established the National Aeronautics and Space Administration (NASA) in 1958 with a distinctly civilian (rather than military) orientation encouraging peaceful applications in space science. The National Aeronautics and Space Act was passed on July 29, 1958, disestablishing NASA's predecessor, the National Advisory Committee for Aeronautics (NACA). The new agency became operational on October 1, 1958.
Since that time, most US space exploration efforts have been led by NASA, including the Apollo moon-landing missions, the Skylab space station, and later the Space Shuttle. Currently, NASA is supporting the International Space Station and is overseeing the development of the Orion Multi-Purpose Crew Vehicle, the Space Launch System and Commercial Crew vehicles. The agency is also responsible for the Launch Services Program (LSP) which provides oversight of launch operations and countdown management for unmanned NASA launches.
NASA science is focused on better understanding Earth through the Earth Observing System, advancing heliophysics through the efforts of the Science Mission Directorate's Heliophysics Research Program, exploring bodies throughout the Solar System with advanced robotic spacecraft missions such as New Horizons, and researching astrophysics topics, such as the Big Bang, through the Great Observatories and associated programs. NASA shares data with various national and international organizations such as from the Greenhouse Gases Observing Satellite.""
""Randomized search "",,,,""In computer science, the treap and the randomized binary search tree are two closely related forms of binary search tree data structures that maintain a dynamic set of ordered keys and allow binary searches among the keys. After any sequence of insertions and deletions of keys, the shape of the tree is a random variable with the same probability distribution as a random binary tree; in particular, with high probability its height is proportional to the logarithm of the number of keys, so that each search, insertion, or deletion operation takes logarithmic time to perform.""
""Game tree search "",,,,""In game theory, a game tree is a directed graph whose nodes are positions in a game and whose edges are moves. The complete game tree for a game is the game tree starting at the initial position and containing all possible moves from each position; the complete tree is the same tree as that obtained from the extensive-form game representation.

The diagram shows the first two levels, or plies, in the game tree for tic-tac-toe. The rotations and reflections of positions are equivalent, so the first player has three choices of move: in the center, at the edge, or in the corner. The second player has two choices for the reply if the first player played in the center, otherwise five choices. And so on.
The number of leaf nodes in the complete game tree is the number of possible different ways the game can be played. For example, the game tree for tic-tac-toe has 255,168 leaf nodes.
Game trees are important in artificial intelligence because one way to pick the best move in a game is to search the game tree using the minimax algorithm or its variants. The game tree for tic-tac-toe is easily searchable, but the complete game trees for larger games like chess are much too large to search. Instead, a chess-playing program searches a partial game tree: typically as many plies from the current position as it can search in the time available. Except for the case of "pathological" game trees  (which seem to be quite rare in practice), increasing the search depth (i.e., the number of plies searched) generally improves the chance of picking the best move.
Two-person games can also be represented as and-or trees. For the first player to win a game, there must exist a winning move for all moves of the second player. This is represented in the and-or tree by using disjunction to represent the first player's alternative moves and using conjunction to represent all of the second player's moves.""
""Robotic planning "",,,,""A rapidly exploring random tree (RRT) is an algorithm designed to efficiently search nonconvex, high-dimensional spaces by randomly building a space-filling tree. The tree is constructed incrementally from samples drawn randomly from the search space and is inherently biased to grow towards large unsearched areas of the problem. RRTs were developed by Steven M. LaValle and James J. Kuffner Jr.  . They easily handle problems with obstacles and differential constraints (nonholonomic and kinodynamic) and have been widely used in autonomous robotic path planning.
RRTs can be viewed as a technique to generate open-loop trajectories for nonlinear systems with state constraints. An RRT can also be considered as a Monte-Carlo method to bias search into the largest Voronoi regions of a graph in a configuration space. Some variations can even be considered stochastic fractals.""
""Computational control theory "",,,,""Computational mathematics involves mathematical research in areas of science where computing plays a central and essential role, emphasizing algorithms, numerical methods, and symbolic methods. Computation in the research is prominent. Computational mathematics emerged as a distinct part of applied mathematics by the early 1950s. Currently, computational mathematics can refer to or include:
computational science, also known as scientific computation or computational engineering
solving mathematical problems by computer simulation as opposed to analytic methods of applied mathematics
numerical methods used in scientific computation, for example numerical linear algebra and numerical solution of partial differential equations
stochastic methods, such as Monte Carlo methods and other representations of uncertainty in scientific computation, for example stochastic finite elements
the mathematics of scientific computation (the theoretical side involving mathematical proofs), in particular numerical analysis, the theory of numerical methods (but theory of computation and complexity of algorithms belong to theoretical computer science)
symbolic computation and computer algebra systems
computer-assisted research in various areas of mathematics, such as logic (automated theorem proving), discrete mathematics (search for mathematical structures such as groups), number theory (primality testing and factorization), cryptography, and computational algebraic topology
computational linguistics, the use of mathematical and computer techniques in natural languages
computational algebraic geometry
computational group theory
computational geometry
computational number theory
computational topology
computational statistics
algorithmic information theory
algorithmic game theory""
""Motion path planning "",,,,""Kineo Computer Aided Motion ("Kineo CAM") was a computer software company based in Toulouse, France that was awarded the European ICT Prize in 2007 in Hannover, Germany for KineoWorks, its automatic motion planning, path planning and pathfinding technology. It was acquired by Siemens PLM Software in 2012.
KineoWorks is a core software component dedicated to motion planning that enables automatic motion of any mechanical system or virtual artifact in a 3D environment, ensuring collision avoidance and respecting kinematic constraints.
Kineo Collision Detector (KCD) is a state-of-the-art collision detection software library with an object-oriented API. It is included in KineoWorks and exists also as a standalone library. It works with a hierarchical architecture of heterogeneous data types based on composite design pattern and is especially suited for large 3D models.
The Kineo CAM main market is PLM, DMU and CAD/CAM systems, robotics and coordinate-measuring machines (CMM).""
""Cognitive science "",,,,""Cognitive science is the interdisciplinary scientific study of the mind and its processes. It examines what cognition is, what it does and how it works. It includes research on intelligence and behaviour, especially focusing on how information is represented, processed, and transformed (in faculties such as perception, language, memory, attention, reasoning, and emotion) within nervous systems (humans or other animals) and machines (e.g. computers). Cognitive science consists of multiple research disciplines, including psychology, artificial intelligence, philosophy, neuroscience, linguistics, and anthropology. It spans many levels of analysis, from low-level learning and decision mechanisms to high-level logic and planning; from neural circuitry to modular brain organization. The fundamental concept of cognitive science is that "thinking can best be understood in terms of representational structures in the mind and computational procedures that operate on those structures."""
""Theory of mind "",,,,""Theory of mind (often abbreviated ToM) is the ability to attribute mental states—beliefs, intents, desires, pretending, knowledge, etc.—to oneself and others and to understand that others have beliefs, desires, intentions, and perspectives that are different from one's own. Deficits can occur in people with autism spectrum disorders, schizophrenia, attention deficit hyperactivity disorder, as well as alcoholics who have suffered brain damage due to alcohol's neurotoxicity. Although philosophical approaches to this exist, the theory of mind as such is distinct from the philosophy of mind.""
""Multi-agent systems "",,,,""A multi-agent system (M.A.S.) is a computerized system composed of multiple interacting intelligent agents within an environment. Multi-agent systems can be used to solve problems that are difficult or impossible for an individual agent or a monolithic system to solve. Intelligence may include some methodic, functional, procedural approach, algorithmic search or reinforcement learning. Although there is considerable overlap, a multi-agent system is not always the same as an agent-based model (ABM). The goal of an ABM is to search for explanatory insight into the collective behavior of agents (which don't necessarily need to be "intelligent") obeying simple rules, typically in natural systems, rather than in solving specific practical or engineering problems. The terminology of ABM tends to be used more often in the sciences, and MAS in engineering and technology. Topics where multi-agent systems research may deliver an appropriate approach include online trading, disaster response, and modelling social structures.""
""Intelligent agents "",,,,""In artificial intelligence, an intelligent agent (IA) is an autonomous entity which observes through sensors and acts upon an environment using actuators (i.e. it is an agent) and directs its activity towards achieving goals (i.e. it is "rational", as defined in economics). Intelligent agents may also learn or use knowledge to achieve their goals. They may be very simple or very complex: a reflex machine such as a thermostat is an intelligent agent.

Intelligent agents are often described schematically as an abstract functional system similar to a computer program. For this reason, intelligent agents are sometimes called abstract intelligent agents (AIA) to distinguish them from their real world implementations as computer systems, biological systems, or organizations. Some definitions of intelligent agents emphasize their autonomy, and so prefer the term autonomous intelligent agents. Still others (notably Russell & Norvig (2003)) considered goal-directed behavior as the essence of intelligence and so prefer a term borrowed from economics, "rational agent".
Intelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations.
Intelligent agents are also closely related to software agents (an autonomous computer program that carries out tasks on behalf of users). In computer science, the term intelligent agent may be used to refer to a software agent that has some intelligence, regardless if it is not a rational agent by Russell and Norvig's definition. For example, autonomous programs used for operator assistance or data mining (sometimes referred to as bots) are also called "intelligent agents".""
""Mobile agents "",,,,""In computer science, a mobile agent is a composition of computer software and data which is able to migrate (move) from one computer to another autonomously and continue its execution on the destination computer.""
""Cooperation and coordination "",,,,""The United Nations-Oceans (UN-Oceans or UN-O) is an inter-agency coordination mechanism of the United Nations, set up to enhance cooperation and coordination of activities concerned with the world oceans and coasts. The UN-Oceans was formed in September 2003 by the United Nations System Chief Executive Board (CEB), to replace the Sub-Committee on Oceans and Coastal Areas (SOCA) of the Administrative Committee on coordination (ACC) that was formed in 1993.""
""Biometrics "",,,,""Biometrics refers to metrics related to human characteristics. Biometrics authentication (or realistic authentication) is used in computer science as a form of identification and access control. It is also used to identify individuals in groups that are under surveillance.
Biometric identifiers are the distinctive, measurable characteristics used to label and describe individuals. Biometric identifiers are often categorized as physiological versus behavioral characteristics. Physiological characteristics are related to the shape of the body. Examples include, but are not limited to fingerprint, palm veins, face recognition, DNA, palm print, hand geometry, iris recognition, retina and odour/scent. Behavioral characteristics are related to the pattern of behavior of a person, including but not limited to typing rhythm, gait, and voice. Some researchers have coined the term behaviometrics to describe the latter class of biometrics.
More traditional means of access control include token-based identification systems, such as a driver's license or passport, and knowledge-based identification systems, such as a password or personal identification number. Since biometric identifiers are unique to individuals, they are more reliable in verifying identity than token and knowledge-based methods; however, the collection of biometric identifiers raises privacy concerns about the ultimate use of this information.
According to a CSO article the biometrics market will be worth US$13.8 billion in 2015.""
""Scene understanding "",,,,""Aerial video is an emerging form of data acquisition for scene understanding and object tracking. The video is captured by low flying aerial platforms that integrate Global Positioning Systems (GPS) and automated image processing to improve the accuracy and cost-effectiveness of data collection and reduction. Recorders can incorporate in-flight voice records from the cockpit intercom system. The addition of audio narration is an extremely valuable tool for documentation and communication. GPS data is incorporated with a text-captioning device on each video frame. Helicopter platforms enable "low and slow" flights, acquiring a continuous visual record without motion blur.
Innovations in remote sensing cameras have allowed the identification of objects that could not have been previously identified. Pipeline and power corridors and their infrastructure can be documented with digital media recording. Video Mapping System is an example of how this technology is used today.
Since the 1980s, aerial videography has seen increased use in applications where its advantages over traditional photography (lower cost and immediate availability of data) outweigh its disadvantages (poorer spatial resolution and difficulty of analysis due to lack of stereo imaging) (Mausel et al. 1992; Meisner 1986). King (1995) provides a comprehensive review of the evolution of video sensors and their applications, many of which focused on:
The measurement of transient phenomena such as wildlife populations (Sidle and Ziewits 1990; Strong and Cowardin 1995) and pest infestations (Everitt et al. 1994);
Mapping of dynamic land features such as wetland plant communities (Jennings et al. 1992) and coastal
landforms (Eleveld et al. 2000);
Land cover mapping in remote areas with limited existing aerial photography and poor infrastructure
(Marshet al. 1994; Slaymaker and Hannah 1997).""
""Activity recognition and understanding "",,,,""The Royal Medal, also known as The Queen's Medal, is a silver-gilt medal awarded each year by the Royal Society, two for "the most important contributions to the advancement of natural knowledge" and one for "distinguished contributions in the applied sciences" made within the Commonwealth of Nations. The award was created by George IV and first awarded in 1826. Initially there were two medals awarded, both for the most important discovery within the last year, a time period which was lengthened to five years and then shortened to three. The format was supported by William IV and Victoria, who had the conditions changed in 1837 so that mathematics was a subject for which a Royal Medal could be awarded, albeit only every third year. The conditions were changed again in 1850 so that:

... the Royal Medals in each year should be awarded for the two most important contributions to the advancement of Natural Knowledge, published originally in Her Majesty's dominions within a period of not more than ten years and not less than one year of the date of the award, subject, of course, to Her Majesty's approval. ... in the award of the Royal Medals, one should be given in each of the two great divisions of Natural Knowledge.

In 1965, the system was changed to its current format, in which three Medals are awarded annually by the Monarch on the recommendation of the Royal Society Council. Because of its dual nature (covering both physical and biological science) the award winners are chosen by both the A- and B-side Award Committees. Since its establishment in 1826 the medal has been awarded 405 times.""
""Video summarization "",,,,""Automatic summarization is the process of reducing a text document with a computer program in order to create a summary that retains the most important points of the original document. Technologies that can make a coherent summary take into account variables such as length, writing style and syntax. Automatic data summarization is part of machine learning and data mining. The main idea of summarization is to find a representative subset of the data, which contains the information of the entire set. Summarization technologies are used in a large number of sectors in industry today. An example of the use of summarization technology is search engines such as Google. Other examples include document summarization, image collection summarization and video summarization. Document summarization, tries to automatically create a representative summary or abstract of the entire document, by finding the most informative sentences. Similarly, in image summarization the system finds the most representative and important (or salient) images. Similarly, in consumer videos one would want to remove the boring or repetitive scenes, and extract out a much shorter and concise version of the video. This is also important, say for surveillance videos, where one might want to extract only important events in the recorded video, since most part of the video may be uninteresting with nothing going on. As the problem of information overload grows, and as the amount of data increases, the interest in automatic summarization is also increasing.
Generally, there are two approaches to automatic summarization: extraction and abstraction. Extractive methods work by selecting a subset of existing words, phrases, or sentences in the original text to form the summary. In contrast, abstractive methods build an internal semantic representation and then use natural language generation techniques to create a summary that is closer to what a human might generate. Such a summary might contain words not explicitly present in the original. Research into abstractive methods is an increasingly important and active research area, however due to complexity constraints, research to date has focused primarily on extractive methods. In some application domains, extractive summarization makes more sense. Examples of these include image collection summarization and video summarization.""
""Visual inspection "",,,,""Visual inspection is a common method of quality control, data acquisition, and data analysis. Visual Inspection, used in maintenance of facilities, mean inspection of equipment and structures using either or all of raw human senses such as vision, hearing, touch and smell and/or any non-specialized inspection equipment. Inspections requiring Ultrasonic, X-Ray equipment, Infra-red, etc. are not typically regarded as Visual Inspection as these Inspection methodologies require specialized equipment, training and certification.""
""Vision for robotics "",,,,""Geoffrey Barrows is an American inventor and the founder of Centeye, a company that specializes in the development of insect vision for robotics. He holds a BS in applied mathematics from the University of Virginia, an MS in electrical engineering from Stanford University, and a Ph.D. in electrical engineering from the University of Maryland at College Park. In 2003 he was recognized as a "young innovator" by being included in the MIT Technology Review's TR100 list.""
""Scene anomaly detection "",,,,""Primeval: New World is a Canadian–British science fiction television program, set in Vancouver, British Columbia, created by Judith and Garfield Reeves-Stevens. A co-production between Impossible Pictures and Omni Film Productions for Space, the show is a spin-off of the British series Primeval. However, the two shows have little to do with each other. It was broadcast on Space in Canada, and on Watch in the United Kingdom. As with Primeval, the premise of Primeval: New World involves a team who has to deal with animals from the past and future that travel through time to the present day through anomalies. On 21 February 2013, it was announced that the series had been cancelled after a single season.
^ "SPACE orders new original one-hour drama series Primeval: New World". Bell Media. 15 September 2011. Archived from the original on 15 September 2011. Retrieved 15 September 2011. 
^ Vlessing, Etan (21 February 2013). "Canada's Space Channel Cancels 'Primeval: New World'". The Hollywood Reporter. Retrieved 22 February 2013.""
""Epipolar geometry "",,,,""Epipolar geometry is the geometry of stereo vision. When two cameras view a 3D scene from two distinct positions, there are a number of geometric relations between the 3D points and their projections onto the 2D images that lead to constraints between the image points. These relations are derived based on the assumption that the cameras can be approximated by the pinhole camera model.""
""Computational photography "",,,,""Computational photography or computational imaging refers to digital image capture and processing techniques that use digital computation instead of optical processes. Computational photography can improve the capabilities of a camera, or introduce features that were not possible at all with film based photography, or reduce the cost or reduce the size of camera elements. Examples of computational photography include in-camera computation of digital panoramas, high-dynamic-range images, and light field cameras. Light field cameras use novel optical elements to capture three dimensional scene information which can then be used to produce 3D images, enhanced of depth-of-field, and selective de-focusing (or "post focus"). Enhanced depth-of-field reduces the need for mechanical focusing systems. All of these features use computational imaging techniques.
The definition of computational photography has evolved to cover a number of subject areas in computer graphics, computer vision, and applied optics. These areas are given below, organized according to a taxonomy proposed by Shree K. Nayar. Within each area is a list of techniques, and for each technique one or two representative papers or books are cited. Deliberately omitted from the taxonomy are image processing (see also digital image processing) techniques applied to traditionally captured images in order to produce better images. Examples of such techniques are image scaling, dynamic range compression (i.e. tone mapping), color management, image completion (a.k.a. inpainting or hole filling), image compression, digital watermarking, and artistic image effects. Also omitted are techniques that produce range data, volume data, 3D models, 4D light fields, 4D, 6D, or 8D BRDFs, or other high-dimensional image-based representations. Epsilon Photography is a sub-field of computational photography.""
""Hyperspectral imaging "",,,,""Hyperspectral imaging, like other spectral imaging, collects and processes information from across the electromagnetic spectrum. The goal of hyperspectral imaging is to obtain the spectrum for each pixel in the image of a scene, with the purpose of finding objects, identifying materials, or detecting processes.
Whereas the human eye sees color of visible light in mostly three bands (red, green, and blue), spectral imaging divides the spectrum into many more bands. This technique of dividing images into bands can be extended beyond the visible. In hyperspectral imaging, the recorded spectra have fine wavelength resolution and cover a wide range of wavelengths.
Engineers build hyperspectral sensors and processing systems for applications in astronomy, agriculture, biomedical imaging, geosciences, physics, and surveillance. Hyperspectral sensors look at objects using a vast portion of the electromagnetic spectrum. Certain objects leave unique 'fingerprints' in the electromagnetic spectrum. Known as spectral signatures, these 'fingerprints' enable identification of the materials that make up a scanned object. For example, a spectral signature for oil helps geologists find new oil fields.""
""Motion capture "",,,,""Motion capture (Mo-cap for short) is the process of recording the movement of objects or people. It is used in military, entertainment, sports, medical applications, and for validation of computer vision and robotics. In filmmaking and video game development, it refers to recording actions of human actors, and using that information to animate digital character models in 2D or 3D computer animation. When it includes face and fingers or captures subtle expressions, it is often referred to as performance capture. In many fields, motion capture is sometimes called motion tracking, but in filmmaking and games, motion tracking usually refers more to match moving.
In motion capture sessions, movements of one or more actors are sampled many times per second. Whereas early techniques used images from multiple cameras to calculate 3D positions, often the purpose of motion capture is to record only the movements of the actor, not his or her visual appearance. This animation data is mapped to a 3D model so that the model performs the same actions as the actor. This process may be contrasted with the older technique of rotoscoping, as seen in Ralph Bakshi's The Lord of the Rings (1978) and American Pop (1981). The animated character movements were achieved in these films by tracing over a live-action actor, capturing the actor's motions and movements. To explain, an actor is filmed performing an action, and then the recorded film is projected onto an animation table frame-by-frame. Animators trace the live-action footage onto animation cels, capturing the actor's outline and motions frame-by-frame, and then they fill in the traced outlines with the animated character. The completed animation cels are then photographed frame-by-frame, exactly matching the movements and actions of the live-action footage. The end result of which is that the animated character replicates exactly the live-action movements of the actor. However, this process takes a considerable amount of time and effort.
Camera movements can also be motion captured so that a virtual camera in the scene will pan, tilt, or dolly around the stage driven by a camera operator while the actor is performing, and the motion capture system can capture the camera and props as well as the actor's performance. This allows the computer-generated characters, images and sets to have the same perspective as the video images from the camera. A computer processes the data and displays the movements of the actor, providing the desired camera positions in terms of objects in the set. Retroactively obtaining camera movement data from the captured footage is known as match moving or camera tracking.""
""3D imaging "",,,,""In computer vision and computer graphics, 3D reconstruction is the process of capturing the shape and appearance of real objects. This process can be accomplished either by active or passive methods. If the model is allowed to change its shape in time, this is referred to as non-rigid or spatio-temporal reconstruction.""
""Active vision "",,,,""An area of computer vision is active vision, sometimes also called active computer vision. An active vision system is one that can manipulate the viewpoint of the camera(s) in order to investigate the environment and get better information from it.""
""Image segmentation "",,,,""In computer vision, image segmentation is the process of partitioning a digital image into multiple segments (sets of pixels, also known as superpixels). The goal of segmentation is to simplify and/or change the representation of an image into something that is more meaningful and easier to analyze. Image segmentation is typically used to locate objects and boundaries (lines, curves, etc.) in images. More precisely, image segmentation is the process of assigning a label to every pixel in an image such that pixels with the same label share certain characteristics.
The result of image segmentation is a set of segments that collectively cover the entire image, or a set of contours extracted from the image (see edge detection). Each of the pixels in a region are similar with respect to some characteristic or computed property, such as color, intensity, or texture. Adjacent regions are significantly different with respect to the same characteristic(s). When applied to a stack of images, typical in medical imaging, the resulting contours after image segmentation can be used to create 3D reconstructions with the help of interpolation algorithms like Marching cubes.""
""Video segmentation "",,,,""The Informedia Digital Library is an ongoing research program at Carnegie Mellon University to build search engines and information visualization technology for many types of media.
The program has carried out research on Spoken Document Retrieval, Video Information Retrieval, Video Segmentation, face recognition, and Cross-language information retrieval.
The Lycos search engine was an early product of the Informedia Digital Library Project.
The project is led by Howard Wactlar. Researchers on the project have included: Michael Mauldin, Alex Hauptmann, Michael Christel, Michael Witbrock, Raj Reddy, Takeo Kanade and Scott Stevens.""
""Object detection "",,,,""Object detection is a computer technology related to computer vision and image processing that deals with detecting instances of semantic objects of a certain class (such as humans, buildings, or cars) in digital images and videos. Well-researched domains of object detection include face detection and pedestrian detection. Object detection has applications in many areas of computer vision, including image retrieval and video surveillance.""
""Object recognition "",,,,""In the field of computer vision object recognition describes the task of finding and identifying objects in an image or video sequence. Humans recognize a multitude of objects in images with little effort, despite the fact that the image of the objects may vary somewhat in different view points, in many different sizes and scales or even when they are translated or rotated. Objects can even be recognized when they are partially obstructed from view. This task is still a challenge for computer vision systems. Many approaches to the task have been implemented over multiple decades.

""
""Object identification "",,,,""Akonadi is a storage service for personal information management (PIM) data and metadata named after the oracle goddess of justice in Ghana. It is one of the “pillars” (core technologies) behind the KDE SC 4 project, although it is designed to be used in any desktop environment. It is extensible and provides concurrent read, write, and query access.
Akonadi provides unique desktop-wide object identification and retrieval. It functions as an extensible data storage for all PIM applications. In KDE 3 each PIM application had different data storage and handling methods, which led to several implementations of essentially the same features. Besides data storage, Akonadi has several other components including search, and a library (cache) for easy access and notification of data changes.
Akonadi communicates with servers to fetch and send data instead of applications through a specialized API. Data can then be retrieved from Akonadi by a model designed to collect specific data (mail, calendar, contacts, etc.). The application itself is made of viewers and editors to display data to the user and let them input data. Akonadi also supports metadata created by applications.
Because Akonadi takes care of data storage and retrieval, which are traditionally the difficult parts of creating a PIM application, development of PIM applications is made much easier. The Mailody developer Tom Albers demonstrated how a mail reader could be created in only 10 minutes using Akonadi.""
""Ranking "",,,,""A ranking is a relationship between a set of items such that, for any two items, the first is either 'ranked higher than', 'ranked lower than' or 'ranked equal to' the second. In mathematics, this is known as a weak order or total preorder of objects. It is not necessarily a total order of objects because two different objects can have the same ranking. The rankings themselves are totally ordered. For example, materials are totally preordered by hardness, while degrees of hardness are totally ordered.
By reducing detailed measures to a sequence of ordinal numbers, rankings make it possible to evaluate complex information according to certain criteria. Thus, for example, an Internet search engine may rank the pages it finds according to an estimation of their relevance, making it possible for the user quickly to select the pages they are likely to want to see.
Analysis of data obtained by ranking commonly requires non-parametric statistics.

""
""Learning to rank "",,,,""Learning to rank or machine-learned ranking (MLR) is the application of machine learning, typically supervised, semi-supervised or reinforcement learning, in the construction of ranking models for information retrieval systems. Training data consists of lists of items with some partial order specified between items in each list. This order is typically induced by giving a numerical or ordinal score or a binary judgment (e.g. "relevant" or "not relevant") for each item. The ranking model's purpose is to rank, i.e. produce a permutation of items in new, unseen lists in a way which is "similar" to rankings in the training data in some sense.
Learning to rank is a relatively new research area which has emerged in the past decade.""
""Supervised learning by classification "",,,,""Large margin nearest neighbor (LMNN) classification is a statistical machine learning algorithm for metric learning. It learns a pseudometric designed for k-nearest neighbor classification. The algorithm is based on semidefinite programming, a sub-class of convex optimization.
The goal of supervised learning (more specifically classification) is to learn a decision rule that can categorize data instances into pre-defined classes. The k-nearest neighbor rule assumes a training data set of labeled instances (i.e. the classes are known). It classifies a new data instance with the class obtained from the majority vote of the k closest (labeled) training instances. Closeness is measured with a pre-defined metric. Large margin nearest neighbors is an algorithm that learns this global (pseudo-)metric in a supervised fashion to improve the classification accuracy of the k-nearest neighbor rule.""
""Supervised learning by regression "",,,,""In statistics and machine learning, the bias–variance tradeoff (or dilemma) is the problem of simultaneously minimizing two sources of error that prevent supervised learning algorithms from generalizing beyond their training set:
The bias is error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).
The variance is error from sensitivity to small fluctuations in the training set. High variance can cause overfitting: modeling the random noise in the training data, rather than the intended outputs.
The bias–variance decomposition is a way of analyzing a learning algorithm's expected generalization error with respect to a particular problem as a sum of three terms, the bias, variance, and a quantity called the irreducible error, resulting from noise in the problem itself.
This tradeoff applies to all forms of supervised learning: classification, regression (function fitting), and structured output learning. It has also been invoked to explain the effectiveness of heuristics in human learning.""
""Cost-sensitive learning "",,,,""Proactive learning is a generalization of active learning designed to relax unrealistic assumptions and thereby reach practical applications.
"Active learning seeks to select the most informative unlabeled instances and ask an omniscient oracle for their labels, so as to retrain a learning algorithm maximizing accuracy. However, the oracle is assumed to be infallible (never wrong), indefatigable (always answers), individual (only one oracle), and insensitive to costs (always free or always charges the same)." 
"In real life, it is possible and more general to have multiple sources of information with differing reliabilities or areas of expertise. Active learning also assumes that the single oracle is perfect, always providing a correct answer when requested. In reality, though, an “oracle” (if we generalize the term to mean any source of expert information) may be incorrect (fallible) with a probability that should be a function of the difficulty of the question. Moreover, an oracle may be reluctant – it may refuse to answer if it is too uncertain or too busy. Finally, active learning presumes the oracle is either free or charges uniform cost in label elicitation. Such an assumption is naive since cost is likely to be regulated by difficulty (amount of work required to formulate an answer) or other factors." 
Proactive learning relaxes all four of these assumptions, relying on a decision-theoretic approach to jointly select the optimal oracle and instance, by casting the problem as a utility optimization problem subject to a budget constraint.""
""Cluster analysis "",,,,""Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters). It is a main task of exploratory data mining, and a common technique for statistical data analysis, used in many fields, including machine learning, pattern recognition, image analysis, information retrieval, bioinformatics and data compression.
Cluster analysis itself is not one specific algorithm, but the general task to be solved. It can be achieved by various algorithms that differ significantly in their notion of what constitutes a cluster and how to efficiently find them. Popular notions of clusters include groups with small distances among the cluster members, dense areas of the data space, intervals or particular statistical distributions. Clustering can therefore be formulated as a multi-objective optimization problem. The appropriate clustering algorithm and parameter settings (including values such as the distance function to use, a density threshold or the number of expected clusters) depend on the individual data set and intended use of the results. Cluster analysis as such is not an automatic task, but an iterative process of knowledge discovery or interactive multi-objective optimization that involves trial and failure. It will often be necessary to modify data preprocessing and model parameters until the result achieves the desired properties.
Besides the term clustering, there are a number of terms with similar meanings, including automatic classification, numerical taxonomy, botryology (from Greek βότρυς "grape") and typological analysis. The subtle differences are often in the usage of the results: while in data mining, the resulting groups are the matter of interest, in automatic classification the resulting discriminative power is of interest. This often leads to misunderstandings between researchers coming from the fields of data mining and machine learning, since they use the same terms and often the same algorithms, but have different goals.
Cluster analysis was originated in anthropology by Driver and Kroeber in 1932 and introduced to psychology by Zubin in 1938 and Robert Tryon in 1939 and famously used by Cattell beginning in 1943 for trait theory classification in personality psychology.""
""Anomaly detection "",,,,""In data mining, anomaly detection (or outlier detection) is the identification of items, events or observations which do not conform to an expected pattern or other items in a dataset. Typically the anomalous items will translate to some kind of problem such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are also referred to as outliers, novelties, noise, deviations and exceptions.
In particular in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts in activity. This pattern does not adhere to the common statistical definition of an outlier as a rare object, and many outlier detection methods (in particular unsupervised methods) will fail on such data, unless it has been aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro clusters formed by these patterns.
Three broad categories of anomaly detection techniques exist. Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal by looking for instances that seem to fit least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labeled as "normal" and "abnormal" and involves training a classifier (the key difference to many other statistical classification problems is the inherent unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behavior from a given normal training data set, and then testing the likelihood of a test instance to be generated by the learnt model.""
""Mixture modeling "",,,,""In statistics, a mixture model is a probabilistic model for representing the presence of subpopulations within an overall population, without requiring that an observed data set should identify the sub-population to which an individual observation belongs. Formally a mixture model corresponds to the mixture distribution that represents the probability distribution of observations in the overall population. However, while problems associated with "mixture distributions" relate to deriving the properties of the overall population from those of the sub-populations, "mixture models" are used to make statistical inferences about the properties of the sub-populations given only observations on the pooled population, without sub-population identity information.
Some ways of implementing mixture models involve steps that attribute postulated sub-population-identities to individual observations (or weights towards such sub-populations), in which case these can be regarded as types of unsupervised learning or clustering procedures. However, not all inference procedures involve such steps.
Mixture models should not be confused with models for compositional data, i.e., data whose components are constrained to sum to a constant value (1, 100%, etc.). However, compositional models can be thought of as mixture models, where members of the population are sampled at random. Conversely, mixture models can be thought of as compositional models, where the total size of the population has been normalized to 1.""
""Topic modeling "",,,,""In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract "topics" that occur in a collection of documents. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: "dog" and "bone" will appear more often in documents about dogs, "cat" and "meow" will appear in documents about cats, and "the" and "is" will appear equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document's balance of topics is.
Although topic models were first described and implemented in the context of natural language processing, they have applications in other fields such as bioinformatics.""
""Source separation "",,,,""Source separation problems in digital signal processing are those in which several signals have been mixed together into a combined signal and the objective is to recover the original component signals from the combined signal. The classical example of a source separation problem is the cocktail party problem, where a number of people are talking simultaneously in a room (for example, at a cocktail party), and a listener is trying to follow one of the discussions. The human brain can handle this sort of auditory source separation problem, but it is a difficult problem in digital signal processing. This was first analyzed by Colin Cherry.
Several approaches have been proposed for the solution of this problem but development is currently still very much in progress. Some of the more successful approaches are principal components analysis and independent components analysis, which work well when there are no delays or echoes present; that is, the problem is simplified a great deal. The field of computational auditory scene analysis attempts to achieve auditory source separation using an approach that is based on human hearing.
The human brain must also solve this problem in real time. In human perception this ability is commonly referred to as auditory scene analysis or the cocktail party effect.""
""Motif discovery "",,,,""In genetics, a sequence motif is a nucleotide or amino-acid sequence pattern that is widespread and has, or is conjectured to have, a biological significance. For proteins, a sequence motif is distinguished from a structural motif, a motif formed by the three-dimensional arrangement of amino acids which may not be adjacent.
An example is the N-glycosylation site motif:
Asn, followed by anything but Pro, followed by either Ser or Thr, followed by anything but Pro
where the three-letter abbreviations are the conventional designations for amino acids (see genetic code).""
""Dimensionality reduction and manifold learning "",,,,""In machine learning and statistics, dimensionality reduction or dimension reduction is the process of reducing the number of random variables under consideration, via obtaining a set "uncorrelated" principle variables. It can be divided into feature selection and feature extraction.

""
""Sequential decision making "",,,,""John Charles Gittins (born 1938) is a researcher in applied probability and operations research, who is a professor and Emeritus Fellow at Keble College, Oxford University.
He is renowned as the developer of the "Gittins index", which is used for sequential decision-making, especially in research and development in the pharmaceutical industry. He has research interests in applied probability, decision analysis and optimal decisions, including optimal stopping and stochastic optimization.
Gittins was an Assistant Director of Research at the Department of Engineering, Cambridge University from 1967 to 1974. Then he was a lecturer at Oxford University from 1975 to 2005 and head of the Department of Statistics there for 6 years. In 1992, Oxford University awarded him the degree Doctor of Science (D. Sci.). In 1996 he became a Professor of Statistics at Oxford University.
He has been awarded the Rollo Davidson Prize (1982) for early-career probabilists, and the Guy Medal in Silver (1984).
^ Whittle, Peter (1980). "Multi-armed bandits and the Gittins index". Journal of the Royal Statistical Society, Series B 42 (2): 143–149. JSTOR 2984953. 
^ Whittle, Peter (2002). "Applied probability in Great Britain (50th anniversary issue of Operations Research)". Oper. Res. 50 (1): 227–239. doi:10.1287/opre.50.1.227.17792. JSTOR 3088474.""
""Inverse reinforcement learning "",,,,""Reinforcement learning is an area of machine learning inspired by behaviorist psychology, concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. The problem, due to its generality, is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics, and genetic algorithms. In the operations research and control literature, the field where reinforcement learning methods are studied is called approximate dynamic programming. The problem has been studied in the theory of optimal control, though most studies are concerned with the existence of optimal solutions and their characterization, and not with the learning or approximation aspects. In economics and game theory, reinforcement learning may be used to explain how equilibrium may arise under bounded rationality.
In machine learning, the environment is typically formulated as a Markov decision process (MDP) as many reinforcement learning algorithms for this context utilize dynamic programming techniques. The main difference between the classical techniques and reinforcement learning algorithms is that the latter do not need knowledge about the MDP and they target large MDPs where exact methods become infeasible.
Reinforcement learning differs from standard supervised learning in that correct input/output pairs are never presented, nor sub-optimal actions explicitly corrected. Further, there is a focus on on-line performance, which involves finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge). The exploration vs. exploitation trade-off in reinforcement learning has been most thoroughly studied through the multi-armed bandit problem and in finite MDPs.""
""Apprenticeship learning "",,,,""Apprenticeship learning, or apprenticeship via inverse reinforcement learning (AIRP), is a concept in the field of artificial intelligence and machine learning, developed by Pieter Abbeel, Associate Professor in Berkeley's EECS department, and Andrew Ng, Associate Professor in Stanford University's Computer Science Department. It was incepted in 2004. AIRP deals with "Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform"
AIRP concept is closely related to reinforcement learning (RL) that is a sub-area of machine learning concerned with how an agent ought to take actions in an environment so as to maximize some notion of long-term reward. AIRP algorithms are used when the reward function is unknown. The algorithms use observations of the behavior of an expert to teach the agent the optimal actions in certain states of the environment.
AIRP is a special case of the general area of learning from demonstration (LfD), where the goal is to learn a complex task by observing a set of expert traces (demonstrations). AIRP is the intersection of LfD and RL.

""
""Multi-agent reinforcement learning "",,,,""Michael Lederman Littman (born August 30, 1966) is a computer scientist. He works mainly in reinforcement learning, but has done work in machine learning, game theory, computer networking, partially observable Markov decision process solving, computer solving of analogy problems and other areas. He is currently a professor of computer science at Brown University.
Before graduate school, Littman worked with Thomas Landauer at Bellcore and was granted a patent for one of the earliest systems for Cross-language information retrieval. Littman received his Ph.D. in computer science from Brown University in 1996. From 1996 to 1999, he was a professor at Duke University. During his time at Duke, he worked on an automated crossword solver PROVERB, which won an Outstanding Paper Award in 1999 from AAAI and competed in the American Crossword Puzzle Tournament. From 2000 to 2002, he worked at AT&T. From 2002 to 2012, he was a professor at Rutgers University; he chaired the department from 2009-12. In Summer 2012 he returned to Brown University as a full professor. He also appeared in a TurboTax commercial.""
""Adversarial learning "",,,,""Adversarial machine learning is a research field that lies at the intersection of machine learning and computer security. It aims to enable the safe adoption of machine learning techniques in adversarial settings like spam filtering, malware detection and biometric recognition.
The problem arises from the fact that machine learning techniques were originally designed for stationary environments in which the training and test data are assumed to be generated from the same (although possibly unknown) distribution. In the presence of intelligent and adaptive adversaries, however, this working hypothesis is likely to be violated to at least some degree (depending on the adversary). In fact, a malicious adversary can carefully manipulate the input data exploiting specific vulnerabilities of learning algorithms to compromise the whole system security.
Examples include: attacks in spam filtering, where spam messages are obfuscated through misspelling of bad words or insertion of good words; attacks in computer security, e.g., to obfuscate malware code within network packets  or mislead signature detection; attacks in biometric recognition, where fake biometric traits may be exploited to impersonate a legitimate user (biometric spoofing)  or to compromise users’ template galleries that are adaptively updated over time.""
""Transfer learning "",,,,""Inductive transfer, or transfer learning, is a research problem in machine learning that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. For example, the abilities acquired while learning to walk presumably apply when one learns to run, and knowledge gained while learning to recognize cars could apply when recognizing trucks. This area of research bears some relation to the long history of psychological literature on transfer of learning, although formal ties between the two fields are limited.
The earliest cited work on transfer in machine learning is attributed to Lorien Pratt [5] who formulated the discriminability-based transfer (DBT) algorithm in 1993. In 1997, the journal Machine Learning [6] published a special issue devoted to Inductive Transfer and by 1998, the field had advanced to include multi-task learning, along with a more formal analysis of its theoretical foundations. Learning to Learn, edited by Sebastian Thrun and Pratt, is a comprehensive overview of the state of the art of inductive transfer at the time of its publication.
Inductive transfer has also been applied in cognitive science, with the journal Connection Science publishing a special issue on Reuse of Neural Networks through Transfer in 1996.
Notably, scientists have developed algorithms for inductive transfer in Markov logic networks and Bayesian networks. Furthermore, researchers have applied techniques for transfer to problems in text classification, spam filtering, and urban combat simulation.  
There still exists much potential in this field while the "transfer" hasn't yet led to significant improvement in learning. Also, an intuitive understanding could be that "transfer means a learner can directly learn from other correlated learners". However, in this way, such a methodology in transfer learning, whose direction is illustrated by, is not a hot spot in the area yet.""
""Lifelong machine learning "",,,,""Kernel methods are a well-established tool to analyze the relationship between input data and the corresponding output of a function. Kernels encapsulate the properties of functions in a computationally efficient way and allow algorithms to easily swap functions of varying complexity.
In typical machine learning algorithms, these functions produce a scalar output. Recent development of kernel methods for functions with vector-valued output is due, at least in part, to interest in simultaneously solving related problems. Kernels which capture the relationship between the problems allow them to borrow strength from each other. Algorithms of this type include multi-task learning (also called multi-output learning or vector-valued learning), transfer learning, and co-kriging. Multi-label classification can be interpreted as mapping inputs to (binary) coding vectors with length equal to the number of classes.
In Gaussian processes, kernels are called covariance functions. Multiple-output functions correspond to considering multiple processes. See Bayesian interpretation of regularization for the connection between the two perspectives.""
""Learning under covariate shift "",,,,""In machine learning, the kernel embedding of distributions (also called the kernel mean or mean map) comprises a class of nonparametric methods in which a probability distribution is represented as an element of a reproducing kernel Hilbert space (RKHS). A generalization of the individual data-point feature mapping done in classical kernel methods, the embedding of distributions into infinite-dimensional feature spaces can preserve all of the statistical features of arbitrary distributions, while allowing one to compare and manipulate distributions using Hilbert space operations such as inner products, distances, projections, linear transformations, and spectral analysis. This learning framework is very general and can be applied to distributions over any space  on which a sensible kernel function (measuring similarity between elements of ) may be defined. For example, various kernels have been proposed for learning from data which are: vectors in , discrete classes/categories, strings, graphs/networks, images, time series, manifolds, dynamical systems, and other structured objects. The theory behind kernel embeddings of distributions has been primarily developed by Alex Smola, Le Song , Arthur Gretton, and Bernhard Schölkopf.
The analysis of distributions is fundamental in machine learning and statistics, and many algorithms in these fields rely on information theoretic approaches such as entropy, mutual information, or Kullback–Leibler divergence. However, to estimate these quantities, one must first either perform density estimation, or employ sophisticated space-partitioning/bias-correction strategies which are typically infeasible for high-dimensional data. Commonly, methods for modeling complex distributions rely on parametric assumptions that may be unfounded or computationally challenging (e.g. Gaussian mixture models), while nonparametric methods like kernel density estimation (Note: the smoothing kernels in this context have a different interpretation than the kernels discussed here) or characteristic function representation (via the Fourier transform of the distribution) break down in high-dimensional settings.
Methods based on the kernel embedding of distributions sidestep these problems and also possess the following advantages:
Data may be modeled without restrictive assumptions about the form of the distributions and relationships between variables
Intermediate density estimation is not needed
Practitioners may specify the properties of a distribution most relevant for their problem (incorporating prior knowledge via choice of the kernel)
If a characteristic kernel is used, then the embedding can uniquely preserve all information about a distribution, while thanks to the kernel trick, computations on the potentially infinite-dimensional RKHS can be implemented in practice as simple Gram matrix operations
Dimensionality-independent rates of convergence for the empirical kernel mean (estimated using samples from the distribution) to the kernel embedding of the true underlying distribution can be proven.
Learning algorithms based on this framework exhibit good generalization ability and finite sample convergence, while often being simpler and more effective than information theoretic methods
Thus, learning via the kernel embedding of distributions offers a principled drop-in replacement for information theoretic approaches and is a framework which not only subsumes many popular methods in machine learning and statistics as special cases, but also can lead to entirely new learning algorithms.""
""Batch learning "",,,,""In computer science, online machine learning is a method of machine learning in which data becomes available in a sequential order and is used to update our best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. Online learning is a common technique used in areas of machine learning where it is computationally infeasible to train over the entire dataset, requiring the need of out-of-core algorithms. It is also used in situations where it is necessary for the algorithm to dynamically adapt to new patterns in the data, or when the data itself is generated as a function of time, e.g. stock price prediction.
Two general modelling strategies exist for online learning models: statistical learning models and adversarial models. In statistical learning models (e.g. stochastic gradient descent, perceptrons), the data samples are assumed to be independent and identically distributed random variables (i.e they are not adapting with time), and our algorithm just has a limited access to the data. In adversarial models, we instead look at our learning problem as a game between two players (the learner vs the data generator), and we are trying to minimize our losses regardless of the move played by the other player. In this model, the opponent is allowed to dynamically adapt the data generated based on the output of the learning algorithm. Spam filtering falls in this category, as the adversary will dynamically generate new spam based on the current behavior of the spam detector. Examples of algorithms in this model include follow the leader, follow the regularized leader etc.""
""Learning from demonstrations "",,,,""Reinforcement learning is an area of machine learning inspired by behaviorist psychology, concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. The problem, due to its generality, is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics, and genetic algorithms. In the operations research and control literature, the field where reinforcement learning methods are studied is called approximate dynamic programming. The problem has been studied in the theory of optimal control, though most studies are concerned with the existence of optimal solutions and their characterization, and not with the learning or approximation aspects. In economics and game theory, reinforcement learning may be used to explain how equilibrium may arise under bounded rationality.
In machine learning, the environment is typically formulated as a Markov decision process (MDP) as many reinforcement learning algorithms for this context utilize dynamic programming techniques. The main difference between the classical techniques and reinforcement learning algorithms is that the latter do not need knowledge about the MDP and they target large MDPs where exact methods become infeasible.
Reinforcement learning differs from standard supervised learning in that correct input/output pairs are never presented, nor sub-optimal actions explicitly corrected. Further, there is a focus on on-line performance, which involves finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge). The exploration vs. exploitation trade-off in reinforcement learning has been most thoroughly studied through the multi-armed bandit problem and in finite MDPs.""
""Learning from critiques "",,,,""Learning disability is a classification that includes several areas of functioning in which a person has difficulty learning in a typical manner, usually caused by an unknown factor or factors. Given the "difficulty learning in a typical manner", this does not exclude the ability to learn in a different manner. Therefore, some people can be more accurately described as having a "Learning Difference", thus avoiding any misconception of being disabled with a lack of ability to learn and possible negative stereotyping.
While learning disability, learning disorder and learning difficulty are often used interchangeably, they differ in many ways. Disorder refers to significant learning problems in an academic area. These problems, however, are not enough to warrant an official diagnosis. Learning disability on the other hand, is an official clinical diagnosis, whereby the individual meets certain criteria, as determined by a professional (psychologist, pediatrician, etc.). The difference is in degree, frequency, and intensity of reported symptoms and problems, and thus the two should not be confused. When the term "learning disorder" is used, it describes a group of disorders characterized by inadequate development of specific academic, language, and speech skills. Types of learning disorders include reading (dyslexia), mathematics (dyscalculia) and writing (dysgraphia).
The unknown factor is the disorder that affects the brain's ability to receive and process information. This disorder can make it problematic for a person to learn as quickly or in the same way as someone who is not affected by a learning disability. People with a learning disability have trouble performing specific types of skills or completing tasks if left to figure things out by themselves or if taught in conventional ways.
Individuals with learning disabilities can face unique challenges that are often pervasive throughout the lifespan. Depending on the type and severity of the disability, interventions and current technologies may be used to help the individual learn strategies that will foster future success. Some interventions can be quite simplistic, while others are intricate and complex. Current technologies may require student training to be effective classroom supports. Teachers, parents and schools can create plans together that tailor intervention and accommodations to aid the individuals in successfully becoming independent learners. School psychologists and other qualified professionals quite often help design the intervention and coordinate the execution of the intervention with teachers and parents. Social support may improve the learning for students with learning disabilities.""
""Learning from implicit feedback "",,,,""Implicit cognition refers to unconscious influences such as knowledge, perception, or memory, that influence a person's behavior, even though they themselves have no conscious awareness whatsoever of those influences.""
""Active learning settings "",,,,""The HighScope early childhood education approach, used in preschool, kindergarten, childcare, or elementary school settings, was developed in Ypsilanti, Michigan in the 1960s. It is now common there and in some other countries.
The philosophy behind HighScope is based on child development theory and research, originally drawing on the work of Jean Piaget and John Dewey. Since then, the HighScope Curriculum has evolved to include the findings of ongoing cognitive-developmental and brain research. In its teaching practices, the HighScope Curriculum draws upon the work of developmental psychologist and educator Lev Vygotsky, especially the strategy of adult scaffolding — supporting children at their current developmental level and helping them build upon it — in a social setting where children have opportunities to choose materials, ideas, and people to interact within the projects they initiate. The adults working with the children see themselves more as facilitators or partners than as managers or supervisors.

""
""Semi-supervised learning settings "",,,,""Semi-supervised learning is a class of supervised learning tasks and techniques that also make use of unlabeled data for training - typically a small amount of labeled data with a large amount of unlabeled data. Semi-supervised learning falls between unsupervised learning (without any labeled training data) and supervised learning (with completely labeled training data). Many machine-learning researchers have found that unlabeled data, when used in conjunction with a small amount of labeled data, can produce considerable improvement in learning accuracy. The acquisition of labeled data for a learning problem often requires a skilled human agent (e.g. to transcribe an audio segment) or a physical experiment (e.g. determining the 3D structure of a protein or determining whether there is oil at a particular location). The cost associated with the labeling process thus may render a fully labeled training set infeasible, whereas acquisition of unlabeled data is relatively inexpensive. In such situations, semi-supervised learning can be of great practical value. Semi-supervised learning is also of theoretical interest in machine learning and as a model for human learning.
As in the supervised learning framework, we are given a set of  independently identically distributed examples  with corresponding labels . Additionally, we are given  unlabeled examples . Semi-supervised learning attempts to make use of this combined information to surpass the classification performance that could be obtained either by discarding the unlabeled data and doing supervised learning or by discarding the labels and doing unsupervised learning.
Semi-supervised learning may refer to either transductive learning or inductive learning. The goal of transductive learning is to infer the correct labels for the given unlabeled data  only. The goal of inductive learning is to infer the correct mapping from  to .
Intuitively, we can think of the learning problem as an exam and labeled data as the few example problems that the teacher solved in class. The teacher also provides a set of unsolved problems. In the transductive setting, these unsolved problems are a take-home exam and you want to do well on them in particular. In the inductive setting, these are practice problems of the sort you will encounter on the in-class exam.
It is unnecessary (and, according to Vapnik's principle, imprudent) to perform transductive learning by way of inferring a classification rule over the entire input space; however, in practice, algorithms formally designed for transduction or induction are often used interchangeably.""
""Classification and regression trees "",,,,""Decision tree learning uses a decision tree as a predictive model which maps observations about an item to conclusions about the item's target value. It is one of the predictive modelling approaches used in statistics, data mining and machine learning. Tree models where the target variable can take a finite set of values are called classification trees. In these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees.
In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data but not decisions; rather the resulting classification tree can be an input for decision making. This page deals with decision trees in data mining.""
""Support vector machines "",,,,""In machine learning, support vector machines (SVMs, also support vector networks) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training examples, each marked for belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples into one category or the other, making it a non-probabilistic binary linear classifier. An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall on.
In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.
When data are not labeled, a supervised learning is not possible, and an unsupervised learning is required, that would find natural clustering of the data to groups, and map new data to these formed groups. The clustering algorithm which provides an improvement to the support vector machines is called support vector clustering and is often used in industrial applications either when data is not labeled or when only some data is labeled as a preprocessing for a classification pass.""
""Gaussian processes "",,,,""In probability theory and statistics, a Gaussian process is a statistical distribution where observations occur in a continuous domain, e.g. time or space. In a Gaussian process, every point in some continuous input space is associated with a normally distributed random variable. Moreover, every finite collection of those random variables has a multivariate normal distribution. The distribution of a Gaussian process is the joint distribution of all those (infinitely many) random variables, and as such, it is a distribution over functions with a continuous domain, e.g. time or space.
The concept of Gaussian processes is named after Carl Friedrich Gauss because it is based on the notion of the Gaussian distribution (normal distribution). Gaussian processes can be seen as an infinite-dimensional generalization of multivariate normal distributions.
Gaussian processes are important in statistical modelling because of properties inherited from the normal. For example, if a random process is modeled as a Gaussian process, the distributions of various derived quantities can be obtained explicitly. Such quantities include the average value of the process over a range of times and the error in estimating the average using sample values at a small set of times.""
""Neural networks "",,,,""In machine learning and cognitive science, artificial neural networks (ANNs) are a family of models inspired by biological neural networks (the central nervous systems of animals, in particular the brain) and are used to estimate or approximate functions that can depend on a large number of inputs and are generally unknown. Artificial neural networks are generally presented as systems of interconnected "neurons" which exchange messages between each other. The connections have numeric weights that can be tuned based on experience, making neural nets adaptive to inputs and capable of learning.
For example, a neural network for handwriting recognition is defined by a set of input neurons which may be activated by the pixels of an input image. After being weighted and transformed by a function (determined by the network's designer), the activations of these neurons are then passed on to other neurons. This process is repeated until finally, an output neuron is activated. This determines which character was read.
Like other machine learning methods –  systems that learn from data –  neural networks have been used to solve a wide variety of tasks that are hard to solve using ordinary rule-based programming, including computer vision and speech recognition.""
""Inductive logic learning "",,,,""Inductive reasoning (as opposed to deductive reasoning or abductive reasoning) is reasoning in which the premises are viewed as supplying strong evidence for the truth of the conclusion. While the conclusion of a deductive argument is certain, the truth of the conclusion of an inductive argument is probable, based upon the evidence given.
Many dictionaries define inductive reasoning as reasoning that derives general principles from specific observations, though some sources disagree with this usage.
The philosophical definition of inductive reasoning is more nuanced than simple progression from particular/individual instances to broader generalizations. Rather, the premises of an inductive logical argument indicate some degree of support (inductive probability) for the conclusion but do not entail it; that is, they suggest truth but do not ensure it. In this manner, there is the possibility of moving from general statements to individual instances (for example, statistical syllogisms, discussed below).""
""Statistical relational learning "",,,,""Statistical relational learning (SRL) is a subdiscipline of artificial intelligence and machine learning that is concerned with domain models that exhibit both uncertainty (which can be dealt with using statistical methods) and complex, relational structure. Typically, the knowledge representation formalisms developed in SRL use (a subset of) first-order logic to describe relational properties of a domain in a general manner (universal quantification) and draw upon probabilistic graphical models (such as Bayesian networks or Markov networks) to model the uncertainty; some also build upon the methods of inductive logic programming. Significant contributions to the field have been made since the late 1990s.
As is evident from the characterization above, the field is not strictly limited to learning aspects; it is equally concerned with reasoning (specifically probabilistic inference) and knowledge representation. Therefore, alternative terms that reflect the main foci of the field include statistical relational learning and reasoning (emphasizing the importance of reasoning) and first-order probabilistic languages (emphasizing the key properties of the languages with which models are represented).

""
""Maximum likelihood modeling "",,,,""A statistical language model is a probability distribution over sequences of words. Given such a sequence, say of length m, it assigns a probability  to the whole sequence. Having a way to estimate the relative likelihood of different phrases is useful in many natural language processing applications. Language modeling is used in speech recognition, machine translation, part-of-speech tagging, parsing, handwriting recognition, information retrieval and other applications.
In speech recognition, the computer tries to match sounds with word sequences. The language model provides context to distinguish between words and phrases that sound similar. For example, in American English, the phrases "recognize speech" and "wreck a nice beach" are pronounced almost the same but mean very different things. These ambiguities are easier to resolve when evidence from the language model is incorporated with the pronunciation model and the acoustic model.
Language models are used in information retrieval in the query likelihood model. Here a separate language model is associated with each document in a collection. Documents are ranked based on the probability of the query Q in the document's language model . Commonly, the unigram language model is used for this purpose—otherwise known as the bag of words model.
Data sparsity is a major problem in building language models. Most possible word sequences will not be observed in training. One solution is to make the assumption that the probability of a word only depends on the previous n words. This is known as an n-gram model or unigram model when n = 1.""
""Maximum entropy modeling "",,,,""Maximum entropy spectral estimation is a method of spectral density estimation. The goal is to improve the spectral quality based on the principle of maximum entropy. The method is based on choosing the spectrum which corresponds to the most random or the most unpredictable time series whose autocorrelation function agrees with the known values. This assumption, which corresponds to the concept of maximum entropy as used in both statistical mechanics and information theory, is maximally non-committal with regard to the unknown values of the autocorrelation function of the time series. It is simply the application of maximum entropy modeling to any type of spectrum and is used in all fields where data is presented in spectral form. The usefulness of the technique varies based on the source of the spectral data since it is dependent on the amount of assumed knowledge about the spectrum that can be applied to the model.
In maximum entropy modeling, probability distributions are created on the basis of that which is known, leading to a type of statistical inference about the missing information which is called the maximum entropy estimate. For example, in spectral analysis the expected peak shape is often known, but in a noisy spectrum the center of the peak may not be clear. In such a case, inputting the known information allows the maximum entropy model to derive a better estimate of the center of the peak, thus improving spectral accuracy.""
""Maximum a posteriori modeling "",,,,""In Bayesian statistics, a maximum a posteriori probability (MAP) estimate is a mode of the posterior distribution. The MAP can be used to obtain a point estimate of an unobserved quantity on the basis of empirical data. It is closely related to Fisher's method of maximum likelihood (ML), but employs an augmented optimization objective which incorporates a prior distribution over the quantity one wants to estimate. MAP estimation can therefore be seen as a regularization of ML estimation.""
""Mixture models "",,,,""In statistics, a mixture model is a probabilistic model for representing the presence of subpopulations within an overall population, without requiring that an observed data set should identify the sub-population to which an individual observation belongs. Formally a mixture model corresponds to the mixture distribution that represents the probability distribution of observations in the overall population. However, while problems associated with "mixture distributions" relate to deriving the properties of the overall population from those of the sub-populations, "mixture models" are used to make statistical inferences about the properties of the sub-populations given only observations on the pooled population, without sub-population identity information.
Some ways of implementing mixture models involve steps that attribute postulated sub-population-identities to individual observations (or weights towards such sub-populations), in which case these can be regarded as types of unsupervised learning or clustering procedures. However, not all inference procedures involve such steps.
Mixture models should not be confused with models for compositional data, i.e., data whose components are constrained to sum to a constant value (1, 100%, etc.). However, compositional models can be thought of as mixture models, where members of the population are sampled at random. Conversely, mixture models can be thought of as compositional models, where the total size of the population has been normalized to 1.""
""Latent variable models "",,,,""In statistics, latent variables (from Latin: present participle of lateo (“lie hidden”), as opposed to observable variables), are variables that are not directly observed but are rather inferred (through a mathematical model) from other variables that are observed (directly measured). Mathematical models that aim to explain observed variables in terms of latent variables are called latent variable models. Latent variable models are used in many disciplines, including psychology, economics, medicine, physics, machine learning/artificial intelligence, bioinformatics, natural language processing, econometrics, management and the social sciences.
Sometimes latent variables correspond to aspects of physical reality, which could in principle be measured, but may not be for practical reasons. In this situation, the term hidden variables is commonly used (reflecting the fact that the variables are "really there", but hidden). Other times, latent variables correspond to abstract concepts, like categories, behavioral or mental states, or data structures. The terms hypothetical variables or hypothetical constructs may be used in these situations.
One advantage of using latent variables is that it reduces the dimensionality of data. A large number of observable variables can be aggregated in a model to represent an underlying concept, making it easier to understand the data. In this sense, they serve a function similar to that of scientific theories. At the same time, latent variables link observable ("sub-symbolic") data in the real world to symbolic data in the modeled world.
Latent variables, as created by factor analytic methods, generally represent "shared" variance, or the degree to which variables "move" together. Variables that have no correlation cannot result in a latent construct based on the common factor model.""
""Bayesian network models "",,,,""Variable-order Bayesian network (VOBN) models provide an important extension of both the Bayesian network models and the variable-order Markov models. VOBN models are used in machine learning in general and have shown great potential in bioinformatics applications. These models extend the widely used position weight matrix (PWM) models, Markov models, and Bayesian network (BN) models.
In contrast to the BN models, where each random variable depends on a fixed subset of random variables, in VOBN models these subsets may vary based on the specific realization of observed variables. The observed realizations are often called the context and, hence, VOBN models are also known as context-specific Bayesian networks. The flexibility in the definition of conditioning subsets of variables turns out to be a real advantage in classification and analysis applications, as the statistical dependencies between random variables in a sequence of variables (not necessarily adjacent) may be taken into account efficiently, and in a position-specific and context-specific manner.""
""Perceptron algorithm "",,,,""In machine learning, the perceptron is an algorithm for supervised learning of binary classifiers: functions that can decide whether an input (represented by a vector of numbers) belongs to one class or another. It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time.
The perceptron algorithm dates back to the late 1950s; its first implementation, in custom hardware, was one of the first artificial neural networks to be produced.""
""Non-negative matrix factorization "",,,,""Non-negative matrix factorization (NMF), also non-negative matrix approximation is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect. Also, in applications such as processing of audio spectrograms non-negativity is inherent to the data being considered. Since the problem is not exactly solvable in general, it is commonly approximated numerically.
NMF finds applications in such fields as computer vision, document clustering, chemometrics, audio signal processing and recommender systems.""
""Factor analysis "",,,,""Factor analysis is a statistical method used to describe variability among observed, correlated variables in terms of a potentially lower number of unobserved variables called factors. For example, it is possible that variations in say six observed variables mainly reflect the variations in two unobserved (underlying) variables. Factor analysis searches for such joint variations in response to unobserved latent variables. The observed variables are modelled as linear combinations of the potential factors, plus "error" terms. The information gained about the interdependencies between observed variables can be used later to reduce the set of variables in a dataset. Factor analysis originated in psychometrics and is used in behavioral sciences, social sciences, marketing, product management, operations research, and other fields that deal with data sets where there are large numbers of observed variables that are thought to reflect a smaller number of underlying/latent variables.
Factor analysis is related to principal component analysis (PCA), but the two are not identical. There has been significant controversy in the field over differences between the two techniques (see section on exploratory factor analysis versus principal components analysis) below. Clearly though, PCA is a more basic version of exploratory factor analysis (EFA) that was developed in the early days prior to the advent of high-speed computers. From the point of view of exploratory analysis, the eigenvalues of PCA are inflated component loadings, i.e., contaminated with error variance.""
""Principal component analysis "",,,,""Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. The number of principal components is less than or equal to the number of original variables. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. The resulting vectors are an uncorrelated orthogonal basis set. The principal components are orthogonal because they are the eigenvectors of the covariance matrix, which is symmetric. PCA is sensitive to the relative scaling of the original variables.
PCA was invented in 1901 by Karl Pearson, as an analogue of the principal axis theorem in mechanics; it was later independently developed (and named) by Harold Hotelling in the 1930s. Depending on the field of application, it is also named the discrete Kosambi-Karhunen–Loève transform (KLT) in signal processing, the Hotelling transform in multivariate quality control, proper orthogonal decomposition (POD) in mechanical engineering, singular value decomposition (SVD) of X (Golub and Van Loan, 1983), eigenvalue decomposition (EVD) of XTX in linear algebra, factor analysis (for a discussion of the differences between PCA and factor analysis see Ch. 7 of ), Eckart–Young theorem (Harman, 1960), or Schmidt–Mirsky theorem in psychometrics, empirical orthogonal functions (EOF) in meteorological science, empirical eigenfunction decomposition (Sirovich, 1987), empirical component analysis (Lorenz, 1956), quasiharmonic modes (Brooks et al., 1988), spectral decomposition in noise and vibration, and empirical modal analysis in structural dynamics.
PCA is mostly used as a tool in exploratory data analysis and for making predictive models. PCA can be done by eigenvalue decomposition of a data covariance (or correlation) matrix or singular value decomposition of a data matrix, usually after mean centering (and normalizing or using Z-scores) the data matrix for each attribute. The results of a PCA are usually discussed in terms of component scores, sometimes called factor scores (the transformed variable values corresponding to a particular data point), and loadings (the weight by which each standardized original variable should be multiplied to get the component score).
PCA is the simplest of the true eigenvector-based multivariate analyses. Often, its operation can be thought of as revealing the internal structure of the data in a way that best explains the variance in the data. If a multivariate dataset is visualised as a set of coordinates in a high-dimensional data space (1 axis per variable), PCA can supply the user with a lower-dimensional picture, a projection or "shadow" of this object when viewed from its (in some sense; see below) most informative viewpoint. This is done by using only the first few principal components so that the dimensionality of the transformed data is reduced.
PCA is closely related to factor analysis. Factor analysis typically incorporates more domain specific assumptions about the underlying structure and solves eigenvectors of a slightly different matrix.
PCA is also related to canonical correlation analysis (CCA). CCA defines coordinate systems that optimally describe the cross-covariance between two datasets while PCA defines a new orthogonal coordinate system that optimally describes variance in a single dataset.""
""Canonical correlation analysis "",,,,""In statistics, canonical-correlation analysis (CCA) is a way of making sense of cross-covariance matrices. If we have two vectors X = (X1, ..., Xn) and Y = (Y1, ..., Ym) of random variables, and there are correlations among the variables, then canonical-correlation analysis will find linear combinations of the Xi and Yj which have maximum correlation with each other. T. R. Knapp notes "virtually all of the commonly encountered parametric tests of significance can be treated as special cases of canonical-correlation analysis, which is the general procedure for investigating the relationships between two sets of variables." The method was first introduced by Harold Hotelling in 1936. It is important to be familiar with basic linear algebra, and transposition in order to use canonical-correlation analysis.""
""Latent Dirichlet allocation "",,,,""In natural language processing, Latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's creation is attributable to one of the document's topics. LDA is an example of a topic model and was first presented as a graphical model for topic discovery by David Blei, Andrew Ng, and Michael I. Jordan in 2003. Essentially the same model was also proposed independently by J. K. Pritchard, M. Stephens, and P. Donnelly in the study of population genetics. Both papers have been highly influential, with 13320 and 15857 citations respectively in January 2016.""
""Rule learning "",,,,""Rule induction is an area of machine learning in which formal rules are extracted from a set of observations. The rules extracted may represent a full scientific model of the data, or merely represent local patterns in the data.""
""Instance-based learning "",,,,""In machine learning, instance-based learning (sometimes called memory-based learning) is a family of learning algorithms that, instead of performing explicit generalization, compares new problem instances with instances seen in training, which have been stored in memory. Instance-based learning is a kind of lazy learning.
It is called instance-based because it constructs hypotheses directly from the training instances themselves. This means that the hypothesis complexity can grow with the data: in the worst case, a hypothesis is a list of n training items and the computational complexity of classifying a single new instance is O(n). One advantage that instance-based learning has over other methods of machine learning is its ability to adapt its model to previously unseen data: instance-based learners may simply store a new instance or throw an old instance away.
Examples of instance-based learning algorithm are the k-nearest neighbor algorithm, kernel machines and RBF networks. These store (a subset of) their training set; when predicting a value/class for a new instance, they compute distances or similarities between this instance and the training instances to make a decision.
To battle the memory complexity of storing all training instances, as well as the risk of overfitting to noise in the training set, instance reduction algorithms have been proposed.
Gagliardi applies this family of classifiers in medical field as second-opinion diagnostic tools and as tools for the knowledge extraction phase in the process of knowledge discovery in databases. One of these classifiers (called Prototype exemplar learning classifier (PEL-C) is able to extract a mixture of abstracted prototypical cases (that are syndromes) and selected atypical clinical cases.""
""Markov decision processes "",,,,""Markov decision processes (MDPs) provide a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. MDPs are useful for studying a wide range of optimization problems solved via dynamic programming and reinforcement learning. MDPs were known at least as early as the 1950s (cf. Bellman 1957). A core body of research on Markov decision processes resulted from Ronald A. Howard's book published in 1960, Dynamic Programming and Markov Processes. They are used in a wide area of disciplines, including robotics, automated control, economics, and manufacturing.
More precisely, a Markov Decision Process is a discrete time stochastic control process. At each time step, the process is in some state , and the decision maker may choose any action  that is available in state . The process responds at the next time step by randomly moving into a new state , and giving the decision maker a corresponding reward .
The probability that the process moves into its new state  is influenced by the chosen action. Specifically, it is given by the state transition function . Thus, the next state  depends on the current state  and the decision maker's action . But given  and , it is conditionally independent of all previous states and actions; in other words, the state transitions of an MDP process satisfies the Markov property.
Markov decision processes are an extension of Markov chains; the difference is the addition of actions (allowing choice) and rewards (giving motivation). Conversely, if only one action exists for each state and all rewards are the same (e.g., zero), a Markov decision process reduces to a Markov chain.""
""Partially-observable Markov decision processes "",,,,""In computer science, a predictive state representation (PSR) is a dynamical system representation that keeps track of the state of the system using predictions of future observations. A PSR's state is grounded directly to statistics over observable quantities. This is in contrast to other models of dynamical systems, such as partially observable Markov decision processes (POMDPs) where the state of the system is represented as a probability distribution over unobserved nominal states.""
""Stochastic games "",,,,""In game theory, a stochastic game, introduced by Lloyd Shapley in the early 1950s, is a dynamic game with probabilistic transitions played by one or more players. The game is played in a sequence of stages. At the beginning of each stage the game is in some state. The players select actions and each player receives a payoff that depends on the current state and the chosen actions. The game then moves to a new random state whose distribution depends on the previous state and the actions chosen by the players. The procedure is repeated at the new state and play continues for a finite or infinite number of stages. The total payoff to a player is often taken to be the discounted sum of the stage payoffs or the limit inferior of the averages of the stage payoffs.
Stochastic games generalize both Markov decision processes and repeated games.""
""Deep belief networks "",,,,""In machine learning, a deep belief network (DBN) is a generative graphical model, or alternatively a type of deep neural network, composed of multiple layers of latent variables ("hidden units"), with connections between the layers but not between units within each layer.
When trained on a set of examples in an unsupervised way, a DBN can learn to probabilistically reconstruct its inputs. The layers then act as feature detectors on inputs. After this learning step, a DBN can be further trained in a supervised way to perform classification.
DBNs can be viewed as a composition of simple, unsupervised networks such as restricted Boltzmann machines (RBMs) or autoencoders, where each sub-network's hidden layer serves as the visible layer for the next. This also leads to a fast, layer-by-layer unsupervised training procedure, where contrastive divergence is applied to each sub-network in turn, starting from the "lowest" pair of layers (the lowest visible layer being a training set).
The observation, due to Yee-Whye Teh, Geoffrey Hinton's student, that DBNs can be trained greedily, one layer at a time, led to one of the first effective deep learning algorithms.""
""Value iteration "",,,,""Markov decision processes (MDPs) provide a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. MDPs are useful for studying a wide range of optimization problems solved via dynamic programming and reinforcement learning. MDPs were known at least as early as the 1950s (cf. Bellman 1957). A core body of research on Markov decision processes resulted from Ronald A. Howard's book published in 1960, Dynamic Programming and Markov Processes. They are used in a wide area of disciplines, including robotics, automated control, economics, and manufacturing.
More precisely, a Markov Decision Process is a discrete time stochastic control process. At each time step, the process is in some state , and the decision maker may choose any action  that is available in state . The process responds at the next time step by randomly moving into a new state , and giving the decision maker a corresponding reward .
The probability that the process moves into its new state  is influenced by the chosen action. Specifically, it is given by the state transition function . Thus, the next state  depends on the current state  and the decision maker's action . But given  and , it is conditionally independent of all previous states and actions; in other words, the state transitions of an MDP process satisfies the Markov property.
Markov decision processes are an extension of Markov chains; the difference is the addition of actions (allowing choice) and rewards (giving motivation). Conversely, if only one action exists for each state and all rewards are the same (e.g., zero), a Markov decision process reduces to a Markov chain.""
""Q-learning "",,,,""Q-learning is a model-free reinforcement learning technique. Specifically, Q-learning can be used to find an optimal action-selection policy for any given (finite) Markov decision process (MDP). It works by learning an action-value function that ultimately gives the expected utility of taking a given action in a given state and following the optimal policy thereafter. A policy is a rule that the agent follows in selecting actions, given the state it is in. When such an action-value function is learned, the optimal policy can be constructed by simply selecting the action with the highest value in each state. One of the strengths of Q-learning is that it is able to compare the expected utility of the available actions without requiring a model of the environment. Additionally, Q-learning can handle problems with stochastic transitions and rewards, without requiring any adaptations. It has been proven that for any finite MDP, Q-learning eventually finds an optimal policy, in the sense that the expected value of the total reward return over all successive steps, starting from the current state, is the maximum achievable.""
""Policy iteration "",,,,""Markov decision processes (MDPs) provide a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. MDPs are useful for studying a wide range of optimization problems solved via dynamic programming and reinforcement learning. MDPs were known at least as early as the 1950s (cf. Bellman 1957). A core body of research on Markov decision processes resulted from Ronald A. Howard's book published in 1960, Dynamic Programming and Markov Processes. They are used in a wide area of disciplines, including robotics, automated control, economics, and manufacturing.
More precisely, a Markov Decision Process is a discrete time stochastic control process. At each time step, the process is in some state , and the decision maker may choose any action  that is available in state . The process responds at the next time step by randomly moving into a new state , and giving the decision maker a corresponding reward .
The probability that the process moves into its new state  is influenced by the chosen action. Specifically, it is given by the state transition function . Thus, the next state  depends on the current state  and the decision maker's action . But given  and , it is conditionally independent of all previous states and actions; in other words, the state transitions of an MDP process satisfies the Markov property.
Markov decision processes are an extension of Markov chains; the difference is the addition of actions (allowing choice) and rewards (giving motivation). Conversely, if only one action exists for each state and all rewards are the same (e.g., zero), a Markov decision process reduces to a Markov chain.""
""Temporal difference learning "",,,,""Temporal difference (TD) learning is a prediction-based machine learning method. It has primarily been used for the reinforcement learning problem, and is said to be "a combination of Monte Carlo ideas and dynamic programming (DP) ideas." TD resembles a Monte Carlo method because it learns by sampling the environment according to some policy, and is related to dynamic programming techniques as it approximates its current estimate based on previously learned estimates (a process known as bootstrapping). The TD learning algorithm is related to the temporal difference model of animal learning.
As a prediction method, TD learning considers that subsequent predictions are often correlated in some sense. In standard supervised predictive learning, one learns only from actually observed values: A prediction is made, and when the observation is available, the prediction is adjusted to better match the observation. As elucidated by Richard Sutton, the core idea of TD learning is that one adjusts predictions to match other, more accurate, predictions about the future. This procedure is a form of bootstrapping, as illustrated with the following example:
"Suppose you wish to predict the weather for Saturday, and you have some model that predicts Saturday's weather, given the weather of each day in the week. In the standard case, you would wait until Saturday and then adjust all your models. However, when it is, for example, Friday, you should have a pretty good idea of what the weather would be on Saturday - and thus be able to change, say, Monday's model before Saturday arrives."
Mathematically speaking, both in a standard and a TD approach, one would try to optimize some cost function, related to the error in our predictions of the expectation of some random variable, E[z]. However, while in the standard approach one in some sense assumes E[z] = z (the actual observed value), in the TD approach we use a model. For the particular case of reinforcement learning, which is the major application of TD methods, z is the total return and E[z] is given by the Bellman equation of the return.

""
""Spectral methods "",,,,""Spectral methods are a class of techniques used in applied mathematics and scientific computing to numerically solve certain differential equations, often involving the use of the Fast Fourier Transform. The idea is to write the solution of the differential equation as a sum of certain "basis functions" (for example, as a Fourier series which is a sum of sinusoids) and then to choose the coefficients in the sum in order to satisfy the differential equation as well as possible.
Spectral methods and finite element methods are closely related and built on the same ideas; the main difference between them is that spectral methods use basis functions that are nonzero over the whole domain, while finite element methods use basis functions that are nonzero only on small subdomains. In other words, spectral methods take on a global approach while finite element methods use a local approach. Partially for this reason, spectral methods have excellent error properties, with the so-called "exponential convergence" being the fastest possible, when the solution is smooth. However, there are no known three-dimensional single domain spectral shock capturing results (shock waves are not smooth). In the finite element community, a method where the degree of the elements is very high or increases as the grid parameter h decreases to zero is sometimes called a spectral element method.
Spectral methods can be used to solve ordinary differential equations (ODEs), partial differential equations (PDEs) and eigenvalue problems involving differential equations. When applying spectral methods to time-dependent PDEs, the solution is typically written as a sum of basis functions with time-dependent coefficients; substituting this in the PDE yields a system of ODEs in the coefficients which can be solved using any numerical method for ODEs. Eigenvalue problems for ODEs are similarly converted to matrix eigenvalue problems.
Spectral methods were developed in a long series of papers by Steven Orszag starting in 1969 including, but not limited to, Fourier series methods for periodic geometry problems, polynomial spectral methods for finite and unbounded geometry problems, pseudospectral methods for highly nonlinear problems, and spectral iteration methods for fast solution of steady state problems. The implementation of the spectral method is normally accomplished either with collocation or a Galerkin or a Tau approach.
Spectral methods are computationally less expensive than finite element methods, but become less accurate for problems with complex geometries and discontinuous coefficients. This increase in error is a consequence of the Gibbs phenomenon.""
""Feature selection "",,,,""In machine learning and statistics, feature selection, also known as variable selection, attribute selection or variable subset selection, is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. Feature selection techniques are used for three reasons:

simplification of models to make them easier to interpret by researchers/users,
shorter training times,
enhanced generalization by reducing overfitting(formally, reduction of variance)

The central premise when using a feature selection technique is that the data contains many features that are either redundant or irrelevant, and can thus be removed without incurring much loss of information. Redundant or irrelevant features are two distinct notions, since one relevant feature may be redundant in the presence of another relevant feature with which it is strongly correlated.
Feature selection techniques should be distinguished from feature extraction. Feature extraction creates new features from functions of the original features, whereas feature selection returns a subset of the features. Feature selection techniques are often used in domains where there are many features and comparatively few samples (or data points). Archetypal cases for the application of feature selection include the analysis of written texts and DNA microarray data, where there are many thousands of features, and a few tens to hundreds of samples.""
""Modeling methodologies "",,,,""Data modeling in software engineering is the process of creating a data model for an information system by applying formal data modeling techniques.
^""
""Uncertainty quantification "",,,,""Uncertainty quantification (UQ) is the science of quantitative characterization and reduction of uncertainties in both computational and real world applications. It tries to determine how likely certain outcomes are if some aspects of the system are not exactly known. An example would be to predict the acceleration of a human body in a head-on crash with another car: even if we exactly knew the speed, small differences in the manufacturing of individual cars, how tightly every bolt has been tightened, etc., will lead to different results that can only be predicted in a statistical sense.
Many problems in the natural sciences and engineering are also rife with sources of uncertainty. Computer experiments on computer simulations are the most common approach to study problems in uncertainty quantification.

""
""Systems theory "",,,,""Systems theory or systems science is the interdisciplinary study of systems in general, with the goal of discovering patterns and elucidating principles that can be discerned from, and applied to, all types of systems at all nesting levels in all fields of research. Systems theory can reasonably be considered a specialization of systems thinking or as the goal output of systems science and systems engineering, with an emphasis on generality useful across a broad range of systems (versus the particular models of individual fields).
A central topic of systems theory is self-regulating systems, i.e. systems self-correcting through feedback. Self-regulating systems are found in nature, including the physiological systems of our body, in local and global ecosystems, and in climate—and in human learning processes (from the individual on up through international organizations like the UN).
^ Biel, R. and Mu-Jeong Kho (2009)"The Issue of Energy within a Dialectical Approach to the Regulationist Problematique," Recherches & Régulation Working Papers, RR Série ID 2009-1, Association Recherche & Régulation: 1-21." (PDF). http://theorie-regulation.org. 2009-11-23. Retrieved 2012-08-09.""
""Network science "",,,,""Network science is an academic field which studies complex networks such as telecommunication networks, computer networks, biological networks, cognitive and semantic networks, and social networks, considering distinct elements or actors represented by nodes (or vertices) and the connections between the elements or actors as links (or edges). The field draws on theories and methods including graph theory from mathematics, statistical mechanics from physics, data mining and information visualization from computer science, inferential modeling from statistics, and social structure from sociology. The United States National Research Council defines network science as "the study of network representations of physical, biological, and social phenomena leading to predictive models of these phenomena."
^ Committee on Network Science for Future Army Applications (2006). Network Science. National Research Council. ISBN 0309653886.""
""Uncertainty quantification "",,,,""Uncertainty quantification (UQ) is the science of quantitative characterization and reduction of uncertainties in both computational and real world applications. It tries to determine how likely certain outcomes are if some aspects of the system are not exactly known. An example would be to predict the acceleration of a human body in a head-on crash with another car: even if we exactly knew the speed, small differences in the manufacturing of individual cars, how tightly every bolt has been tightened, etc., will lead to different results that can only be predicted in a statistical sense.
Many problems in the natural sciences and engineering are also rife with sources of uncertainty. Computer experiments on computer simulations are the most common approach to study problems in uncertainty quantification.

""
""Quantum mechanic simulation "",,,,""Quantum mechanics (QM; also known as quantum physics or quantum theory) including quantum field theory, is a fundamental branch of physics concerned with processes involving, for example, atoms and photons. In such processes, said to be quantized, the action has been observed to be only in integer multiples of the Planck constant, a physical quantity that is exceedingly, indeed perhaps ultimately, small. This is utterly inexplicable in classical physics.
Quantum mechanics gradually arose from Max Planck's solution in 1900 to the black-body radiation problem (reported 1859) and Albert Einstein's 1905 paper which offered a quantum-based theory to explain the photoelectric effect (reported 1887). Early quantum theory was profoundly reconceived in the mid-1920s.
The reconceived theory is formulated in various specially developed mathematical formalisms. In one of them, a mathematical function, the wave function, provides information about the probability amplitude of position, momentum, and other physical properties of a particle.
Important applications of quantum mechanical theory include superconducting magnets, light-emitting diodes and the laser, the transistor and semiconductors such as the microprocessor, medical and research imaging such as magnetic resonance imaging and electron microscopy, and explanations for many biological and physical phenomena.""
""Molecular simulation "",,,,""Molecular modelling encompasses all theoretical methods and computational techniques used to model or mimic the behaviour of molecules. The techniques are used in the fields of computational chemistry, drug design, computational biology and materials science for studying molecular systems ranging from small chemical systems to large biological molecules and material assemblies. The simplest calculations can be performed by hand, but inevitably computers are required to perform molecular modelling of any reasonably sized system. The common feature of molecular modelling techniques is the atomistic level description of the molecular systems. This may include treating atoms as the smallest individual unit (the Molecular mechanics approach), or explicitly modeling electrons of each atom (the quantum chemistry approach).""
""Rare-event simulation "",,,,""Rare event sampling is an umbrella term for a group of computer simulation methods intended to selectively sample 'special' regions of the dynamic space of systems which are unlikely to visit those special regions through brute-force simulation. A familiar example of a rare event in this context would be nucleation of a raindrop from over-saturated water vapour: although raindrops form every day, relative to the length and time scales defined by the motion of water molecules in the vapour phase the formation of a liquid droplet is extremely rare.
Due to the wide use of computer simulation across very different fields of endeavour, articles on the topic arise from quite disparate sources and it is difficult to make a coherent survey of rare event sampling techniques. Contemporary methods include Transition Path Sampling (TPS), Repetitive Simulation Trials After Reaching Thresholds (RESTART), Forward Flux Sampling (FFS), Generalized Splitting, Adaptive Multilevel Splitting (AMS), Stochastic Process Rare Event Sampling (SPRES) and Subset simulation. The first published rare event technique was by Herman Kahn and Theodore Edward Harris in 1951, who in turn referred to an unpublished technical report by John von Neumann and Stanislaw Ulam.""
""Discrete-event simulation "",,,,""A discrete-event simulation (DES) models the operation of a system as a discrete sequence of events in time. Each event occurs at a particular instant in time and marks a change of state in the system. Between consecutive events, no change in the system is assumed to occur; thus the simulation can directly jump in time from one event to the next.
This contrasts with continuous simulation in which the simulation continuously tracks the system dynamics over time. Instead of being event-based, this is called an activity-based simulation; time is broken up into small time slices and the system state is updated according to the set of activities happening in the time slice. Because discrete-event simulations do not have to simulate every time slice, they can typically run much faster than the corresponding continuous simulation.
A more recent method is the three-phased approach to discrete event simulation (Pidd, 1998). In this approach, the first phase is to jump to the next chronological event. The second phase is to execute all events that unconditionally occur at that time (these are called B-events). The third phase is to execute all events that conditionally occur at that time (these are called C-events). The three phase approach is a refinement of the event-based approach in which simultaneous events are ordered so as to make the most efficient use of computer resources. The three-phase approach is used by a number of commercial simulation software packages, but from the user's point of view, the specifics of the underlying simulation method are generally hidden.""
""Agent / discrete models "",,,,""An agent-based model (ABM) is one of a class of computational models for simulating the actions and interactions of autonomous agents (both individual or collective entities such as organizations or groups) with a view to assessing their effects on the system as a whole. It combines elements of game theory, complex systems, emergence, computational sociology, multi-agent systems, and evolutionary programming. Monte Carlo methods are used to introduce randomness. Particularly within ecology, ABMs are also called individual-based models (IBMs), and individuals within IBMs may be simpler than fully autonomous agents within ABMs. A review of recent literature on individual-based models, agent-based models, and multiagent systems shows that ABMs are used on non-computing related scientific domains including biology, ecology and social science. Agent-based modeling is related to, but distinct from, the concept of multi-agent systems or multi-agent simulation in that the goal of ABM is to search for explanatory insight into the collective behavior of agents obeying simple rules, typically in natural systems, rather than in designing agents or solving specific practical or engineering problems.
Agent-based models are a kind of microscale model that simulate the simultaneous operations and interactions of multiple agents in an attempt to re-create and predict the appearance of complex phenomena. The process is one of emergence from the lower (micro) level of systems to a higher (macro) level. As such, a key notion is that simple behavioral rules generate complex behavior. This principle, known as K.I.S.S. ("Keep it simple, stupid"), is extensively adopted in the modeling community. Another central tenet is that the whole is greater than the sum of the parts. Individual agents are typically characterized as boundedly rational, presumed to be acting in what they perceive as their own interests, such as reproduction, economic benefit, or social status, using heuristics or simple decision-making rules. ABM agents may experience "learning", adaptation, and reproduction.
Most agent-based models are composed of: (1) numerous agents specified at various scales (typically referred to as agent-granularity); (2) decision-making heuristics; (3) learning rules or adaptive processes; (4) an interaction topology; and (5) a non-agent environment. ABMs are typically implemented as computer simulations, either as custom software, or via ABM toolkits, and this software can be then used to test how changes in individual behaviors will affect the system's emerging overall behavior.""
""Distributed simulation "",,,,""Distributed Interactive Simulation (DIS) is an IEEE standard for conducting real-time platform-level wargaming across multiple host computers and is used worldwide, especially by military organizations but also by other agencies such as those involved in space exploration and medicine.

""
""Continuous simulation "",,,,""Continuous Simulation refers to a computer model of a physical system that continuously tracks system response according to a set of equations typically involving differential equations.""
""Continuous models "",,,,""Continuous modelling is the mathematical practice of applying a model to continuous data (data which has a potentially infinite number, and divisibility, of attributes). They often use differential equations and are converse to discrete modelling.
Modelling is generally broken down into several steps:
Making assumptions about the data: The modeller decides what is influencing the data and what can be safely ignored.
Making equations to fit the assumptions.
Solving the equations.
Verifying the results: Various statistical tests are applied to the data and the model and compared.
If the model passes the verification progress it is put into practice.""
""Real-time simulation "",,,,""Real-time simulation refers to a computer model of a physical system that can execute at the same rate as actual "wall clock" time. In other words, the computer model runs at the same rate as the actual physical system. For example if a tank takes 10 minutes to fill in the real-world, the simulation would take 10 minutes as well.
Real-time simulation occurs commonly in computer gaming, but also is important in the industrial market for operator training and off-line controller tuning. Computer languages like LabVIEW, VisSim and Simulink allow quick creation of such real-time simulations and have connections to industrial displays and Programmable Logic Controllers via OLE for process control or digital and analog I/O cards. Several real-time simulator are available on the market like xPC Target and RT-LAB for mechatronic systems and using Simulink, eFPGAsim and eDRIVEsim for power electronic simulation and eMEGAsim, HYPERSIM and RTDS for power grid real-time (RTS) simulation.""
""Interactive simulation "",,,,""Distributed Interactive Simulation (DIS) is an IEEE standard for conducting real-time platform-level wargaming across multiple host computers and is used worldwide, especially by military organizations but also by other agencies such as those involved in space exploration and medicine.

""
""Multiscale systems "",,,,""Ioannis George (Yannís) Kevrekidis is the Pomeroy and Betty Perry Smith Professor of Engineering and Professor of Chemical and Biological Engineering with the School of Engineering and Applied Science at Princeton University.
Kevrekidis earned a diploma from the National Technical University of Athens in 1981, and a doctorate in mathematics from the University of Minnesota in 1987, under the supervision of Lanny D. Schmidt. His research interests include scientific computation for complex/multiscale systems modeling; process dynamics, computer modeling, and applied mathematics; spatiotemporal pattern formation; and nonlinear system identification and control.
In 2003 he was awarded the J.D. Crawford Prize of the Society for Industrial and Applied Mathematics for outstanding research in nonlinear science. In 2010 the American Institute of Chemical Engineers gave him their Richard H. Wilhelm Award in Chemical Reaction Engineering.""
""Data assimilation "",,,,""A numerical model determines how a model state at a particular time changes into the model state at a later time. Even if the numerical model were a perfect representation of an actual system (which of course can rarely if ever be the case) in order to make a perfect forecast of the future state of the actual system the initial state of the numerical model would also have to be a perfect representation of the actual state of the system.
Data assimilation or, more-or-less synonymously, data analysis is the process by which observations of the actual system are incorporated into the model state of a numerical model of that system. Applications of data assimilation arise in many fields of geosciences, perhaps most importantly in weather forecasting and hydrology.
A frequently encountered problem is that the number of observations of the actual system available for analysis is orders of magnitude smaller than the number of values required to specify the model state. The initial state of the numerical model cannot therefore be determined from the available observations alone. Instead, the numerical model is used to propagate information from past observations to the current time. This is then combined with current observations of the actual system using a data assimilation method.
Most commonly this leads to the numerical modelling system alternately performing a numerical forecast and a data analysis. This is known as analysis/forecast cycling. The forecast from the previous analysis to the current one is frequently called the background.
The analysis combines the information in the background with that of the current observations, essentially by taking a weighted mean of the two; using estimates of the uncertainty of each to determine their weighting factors. The data assimilation procedure is invariably multivariate and includes approximate relationships between the variables. The observations are of the actual system, rather than of the model's incomplete representation of that system, and so may have different relationships between the variables from those in the model. To reduce the impact of these problems incremental analyses are often performed. That is the analysis procedure determines increments which when added to the background yield the analysis. As the increments are generally small compared to the background values this leaves the analysis less affected by 'balance' errors in the analysed increments. Even so some filtering, known as initialisation, may be required to avoid problems, such as the excitement of unphysical wave like activity or even numerical instability, when running the numerical model from the analysed initial state.
As an alternative to analysis/forecast cycles, data assimilation can proceed by some sort of continuous process such as nudging, where the model equations themselves are modified to add terms that continuously push the model towards the observations.""
""Scientific visualization "",,,,""Scientific visualization (also spelled scientific visualisation) is an interdisciplinary branch of science. According to Friendly (2008), it is "primarily concerned with the visualization of three-dimensional phenomena (architectural, meteorological, medical, biological, etc.), where the emphasis is on realistic renderings of volumes, surfaces, illumination sources, and so forth, perhaps with a dynamic (time) component". It is also considered a branch of computer science that is a subset of computer graphics. The purpose of scientific visualization is to graphically illustrate scientific data to enable scientists to understand, illustrate, and glean insight from their data.
^ Visualizations that have been created with VisIt. at wci.llnl.gov. Updated: November 8, 2007
^ Michael Friendly (2008). "Milestones in the history of thematic cartography, statistical graphics, and data visualization".""
""Visual analytics "",,,,""Visual analytics is an outgrowth of the fields of information visualization and scientific visualization that focuses on analytical reasoning facilitated by interactive visual interfaces.""
""Simulation by animation "",,,,""Entertainment technology is the discipline of using manufactured or created components to enhance or make possible any sort of entertainment experience. Because entertainment categories are so broad, and because entertainment models the world in many ways, the types of implemented technology are derived from a variety of sources. Thus, in theatre, for example, entertainment technology practitioners must be able to design and construct scenery, install electrical systems, build clothing, use motors if there is scenery automation, provide plumbing (if functioning kitchen fixtures are required, or if "singing in the rain"), etc. In this way, the entertainment technology field intersects with most other types of technology.
Traditionally, entertainment technology is derived from theatrical stagecraft, and stagecraft is an important subset of the discipline. However, the rise of new types and venues for entertainment, as well as rapidly advancing technological development, has increased the range and scope of its practice.
Entertainment Technology includes:
Scenery fabrication
Properties
Costume
Lighting
Sound
Video
Show control
Automation
Animatronics
Interactive environments
Computer simulation
In animation and game design the phrase entertainment technology refers to a very real world of entertainment experiences made possible by the advent of primarily computer-mediated digital technologies.""
""Simulation languages "",,,,""A computer simulation language describes the operation of a simulation on a computer. There are two major types of simulation: continuous and discrete event though more modern languages can handle more complex combinations most languages also have a graphical interface and at least a simple statistic gathering capability for the analysis of the results. An important part of discrete-event languages is the ability to generate pseudo-random numbers and variants from different probability distributions. Examples are:
Discrete event simulation languages, viewing the model as a sequence of random events each causing a change in state.
AutoMod - Simulating reality
Arena
ExtendSim - Simulation software for discrete event, continuous, discrete rate and agent-based simulation**GPSS
Simio software for discrete event, continuous, and agent-based simulation.
SimPy, an open-source package based on Python
SIMSCRIPT II.5, a well established commercial compiler
Simula
jEQN a Domain Specific Language for M&S of Queueing Networks

Continuous simulation languages, viewing the model essentially as a set of differential equations.
Advanced Continuous Simulation Language (ACSL), which supports textual or graphical model specification
DYNAMO
VisSim, a visually programmed block diagram language

Hybrid, and other.
LMS Imagine.Lab AMESim, simulation platform to model and analyze multi-domain systems and predict their performances
Flowmaster V7 Software for the analysis of fluid mechanics within pipe networks using 1D Computational Fluid Dynamics
AnyLogic multi-method simulation tool, which supports System dynamics, Discrete event simulation, Agent-based modeling
Modelica, open-standard object-oriented language for modeling of complex physical systems
EcosimPro Language (EL) - Continuous modeling with discrete events
VHDL-AMS - Continuous conservative/signal flow discreent event and Register transfer level capability. It simulates control, logic, and physical effects in different engineering domains (hydraulic, electronic, mechanical, thermal, etc.). It is derived from the VHDL language.
Verilog-AMS - Continuous conservative/signal flow discreent event and Register transfer level capability. It simulates control, logic, and physical effects in different engineering domains (hydraulic, electronic, mechanical, thermal, etc.). It is derived from the Verilog language.
SeSAm Multiagent simulator and graphical modelling environment. (Free Software)
SimulationX, standard simulation software for valuation of the interaction of all components in multiphysics systems
Simulink - Continuous and discrete event capability
Scicos - Continuous-time, discrete-time and event based simulation tool distributed with ScicosLab. It contains a block diagram editor, a compiler, simulator and code generation facilities. Free software.
SPICE - Analog circuit simulation
Scilab contains a simulation package called Xcos
XMLlab - simulations with XML
Flexsim - 3D process simulation software for continuous, discrete event, or agent-based systems.
EICASLAB - Continuous, discrete and discrete event capability specifically devoted to support the automatic control design.
TRUE (Temporal Reasoning Universal Elaboration) Discrete and continuous capability, + 3D Modeler (3D Rendering using OpenGL graphics library) + Procedural animation
EJS, an environment to automatically generate Java code for simulations from its own language (XML files)
Netlogo NetLogo is a programmable multi-agent modeling environment.
ExtendSim simulation environment for discrete event, continuous, discrete-rate and agent-based simulation.

""
""Simulation tools "",,,,""nanoHUB.org is a science and engineering gateway comprising community-contributed resources and geared toward educational applications, professional networking, and interactive simulation tools for nanotechnology. Funded by the United States National Science Foundation (NSF), it is a product of the Network for Computational Nanotechnology (NCN), a multi-university initiative of eight member institutions including Purdue University, the University of California at Berkeley, the University of Illinois at Urbana-Champaign, Massachusetts Institute of Technology, the Molecular Foundry at Lawrence Berkeley National Laboratory, Norfolk State University, Northwestern University, and the University of Texas at El Paso. NCN was established to create a resource for nanoscience and nanotechnology via online services for research, education, and professional collaboration. NCN supports research efforts in nanoelectronics; nanoelectromechanical systems (NEMS); nanofluidics; nanomedicine, biology; and nanophotonics.""
""Simulation evaluation "",,,,""Geodesign is a set of techniques and enabling technologies for planning built and natural environments in an integrated process, including project conceptualization, analysis, design specification, stakeholder participation and collaboration, design creation, simulation, and evaluation (among other stages). "Geodesign is a design and planning method which tightly couples the creation of design proposals with impact simulations informed by geographic contexts."
^ Flaxman, Michael. Geodesign: Fundamental Principles and Routes Forward. Talk at GeoDesign Summit 2010.""
""Motion capture "",,,,""Motion capture (Mo-cap for short) is the process of recording the movement of objects or people. It is used in military, entertainment, sports, medical applications, and for validation of computer vision and robotics. In filmmaking and video game development, it refers to recording actions of human actors, and using that information to animate digital character models in 2D or 3D computer animation. When it includes face and fingers or captures subtle expressions, it is often referred to as performance capture. In many fields, motion capture is sometimes called motion tracking, but in filmmaking and games, motion tracking usually refers more to match moving.
In motion capture sessions, movements of one or more actors are sampled many times per second. Whereas early techniques used images from multiple cameras to calculate 3D positions, often the purpose of motion capture is to record only the movements of the actor, not his or her visual appearance. This animation data is mapped to a 3D model so that the model performs the same actions as the actor. This process may be contrasted with the older technique of rotoscoping, as seen in Ralph Bakshi's The Lord of the Rings (1978) and American Pop (1981). The animated character movements were achieved in these films by tracing over a live-action actor, capturing the actor's motions and movements. To explain, an actor is filmed performing an action, and then the recorded film is projected onto an animation table frame-by-frame. Animators trace the live-action footage onto animation cels, capturing the actor's outline and motions frame-by-frame, and then they fill in the traced outlines with the animated character. The completed animation cels are then photographed frame-by-frame, exactly matching the movements and actions of the live-action footage. The end result of which is that the animated character replicates exactly the live-action movements of the actor. However, this process takes a considerable amount of time and effort.
Camera movements can also be motion captured so that a virtual camera in the scene will pan, tilt, or dolly around the stage driven by a camera operator while the actor is performing, and the motion capture system can capture the camera and props as well as the actor's performance. This allows the computer-generated characters, images and sets to have the same perspective as the video images from the camera. A computer processes the data and displays the movements of the actor, providing the desired camera positions in terms of objects in the set. Retroactively obtaining camera movement data from the captured footage is known as match moving or camera tracking.""
""Procedural animation "",,,,""A procedural animation is a type of computer animation, used to automatically generate animation in real-time to allow for a more diverse series of actions than could otherwise be created using predefined animations.
Procedural animation is used to simulate particle systems (smoke, fire, water   ), cloth and clothing, rigid body dynamics, and hair and fur dynamics, as well as character animation.
In video games it is often used for simple things like turning a character's head when a player looks around (as in Quake III Arena) and more complex things, like ragdoll physics, which is usually used for the death of a character in which the ragdoll will realistically fall to the floor. A ragdoll usually consists of a series of connected rigid bodies that are programmed to have Newtonian physics acting upon them; therefore, very realistic effects can be generated that would very hardly be possible with traditional animation. For example, a character can die slumped over a cliff and the weight of its upper-body can drag the rest of it over the edge.
Even more complex examples of procedural animation can be found in the game Spore wherein user-created creatures will automatically be animated to all actions needed in the game from walking, to driving, to picking things up. In the game Unreal Tournament 3 bodies who have gone into ragdoll mode to fake death can arise from any position into which they have fallen and get back on their feet. The canceled Indiana Jones game from LucasArts shown at E3 2006 featured character motions that were animated entirely in real-time, with characters dodging, punching, and reacting to the environment based on an engine called Euphoria by NaturalMotion which has since been used in games such as Grand Theft Auto IV and Backbreaker.""
""Physical simulation "",,,,""Dynamical simulation, in computational physics, is the simulation of systems of objects that are free to move, usually in three dimensions according to Newton's laws of dynamics, or approximations thereof. Dynamical simulation is used in computer animation to assist animators to produce realistic motion, in industrial design (for example to simulate crashes as an early step in crash testing), and in video games. Body movement is calculated using time integration methods.""
""Motion processing "",,,,""Sensors able to detect three-dimensional motion have been commercially available for several decades and have been used in automobiles, aircraft and ships. However, initial size, power consumption and price had prevented their mass adoption in consumer electronics. While there are other kinds of motion detector technologies available commercially, there are four principle types of motion sensors which are important for motion processing in the consumer electronics market.""
""Collision detection "",,,,""Collision detection typically refers to the computational problem of detecting the intersection of two or more objects. While the topic is most often associated with its use in video games and other physical simulations, it also has applications in robotics. In addition to determining whether two objects have collided, collision detection systems may also calculate time of impact (TOI), and report a contact manifold (the set of intersecting points). Collision response deals with simulating what happens when a collision is detected (see physics engine, ragdoll physics). Solving collision detection problems requires extensive use of concepts from linear algebra and computational geometry.

""
""Rasterization "",,,,""Rasterisation (or rasterization) is the task of taking an image described in a vector graphics format (shapes) and converting it into a raster image (pixels or dots) for output on a video display or printer, or for storage in a bitmap file format.
In normal usage, the term refers to the popular rendering algorithm for displaying three-dimensional shapes on a computer. Rasterisation is currently the most popular technique for producing real-time 3D computer graphics. Real-time applications need to respond immediately to user input, and generally need to produce frame rates of at least 30 frames per second to achieve smooth animation.
Compared with other rendering techniques such as ray tracing, rasterisation is extremely fast. However, rasterization is simply the process of computing the mapping from scene geometry to pixels and does not prescribe a particular way to compute the color of those pixels. Shading, including programmable shading, may be based on physical light transport, or artistic intent.""
""Ray tracing "",,,,""In computer graphics, ray tracing is a technique for generating an image by tracing the path of light through pixels in an image plane and simulating the effects of its encounters with virtual objects. The technique is capable of producing a very high degree of visual realism, usually higher than that of typical scanline rendering methods, but at a greater computational cost. This makes ray tracing best suited for applications where the image can be rendered slowly ahead of time, such as in still images and film and television visual effects, and more poorly suited for real-time applications like video games where speed is critical. Ray tracing is capable of simulating a wide variety of optical effects, such as reflection and refraction, scattering, and dispersion phenomena (such as chromatic aberration).""
""Non-photorealistic rendering "",,,,""Non-photorealistic rendering (NPR) is an area of computer graphics that focuses on enabling a wide variety of expressive styles for digital art. In contrast to traditional computer graphics, which has focused on photorealism, NPR is inspired by artistic styles such as painting, drawing, technical illustration, and animated cartoons. NPR has appeared in movies and video games in the form of "toon shading", as well as in scientific visualization, architectural illustration and experimental animation. An example of a modern use of this method is that of cel-shaded animation.""
""Reflectance modeling "",,,,""The Oren–Nayar reflectance model, developed by Michael Oren and Shree K. Nayar, is a reflectivity model for diffuse reflection from rough surfaces. It has been shown to accurately predict the appearance of a wide range of natural surfaces, such as concrete, plaster, sand, etc.

""
""Visibility "",,,,""In meteorology, visibility is a measure of the distance at which an object or light can be clearly discerned. It is reported within surface weather observations and METAR code either in meters or statute miles, depending upon the country. Visibility affects all forms of traffic: roads, sailing and aviation. Meteorological visibility refers to transparency of air: in dark, meteorological visibility is still the same as in daylight for the same air.""
""Computational photography "",,,,""Computational photography or computational imaging refers to digital image capture and processing techniques that use digital computation instead of optical processes. Computational photography can improve the capabilities of a camera, or introduce features that were not possible at all with film based photography, or reduce the cost or reduce the size of camera elements. Examples of computational photography include in-camera computation of digital panoramas, high-dynamic-range images, and light field cameras. Light field cameras use novel optical elements to capture three dimensional scene information which can then be used to produce 3D images, enhanced of depth-of-field, and selective de-focusing (or "post focus"). Enhanced depth-of-field reduces the need for mechanical focusing systems. All of these features use computational imaging techniques.
The definition of computational photography has evolved to cover a number of subject areas in computer graphics, computer vision, and applied optics. These areas are given below, organized according to a taxonomy proposed by Shree K. Nayar. Within each area is a list of techniques, and for each technique one or two representative papers or books are cited. Deliberately omitted from the taxonomy are image processing (see also digital image processing) techniques applied to traditionally captured images in order to produce better images. Examples of such techniques are image scaling, dynamic range compression (i.e. tone mapping), color management, image completion (a.k.a. inpainting or hole filling), image compression, digital watermarking, and artistic image effects. Also omitted are techniques that produce range data, volume data, 3D models, 4D light fields, 4D, 6D, or 8D BRDFs, or other high-dimensional image-based representations. Epsilon Photography is a sub-field of computational photography.""
""Image processing "",,,,""In imaging science, image processing is processing of images using mathematical operations by using any form of signal processing for which the input is an image, a series of images, or a video, such as a photograph or video frame; the output of image processing may be either an image or a set of characteristics or parameters related to the image. Most image-processing techniques involve treating the image as a two-dimensional signal and applying standard signal-processing techniques to it. Images are also processed as three-dimensional signals where the third-dimension being time or the z-axis.
Image processing usually refers to digital image processing, but optical and analog image processing also are possible. This article is about general techniques that apply to all of them. The acquisition of images (producing the input image in the first place) is referred to as imaging.
Closely related to image processing are computer graphics and computer vision. In computer graphics, images are manually made from physical models of objects, environments, and lighting, instead of being acquired (via imaging devices such as cameras) from natural scenes, as in most animated movies. Computer vision, on the other hand, is often considered high-level image processing out of which a machine/computer/software intends to decipher the physical contents of an image or a sequence of images (e.g., videos or 3D full-body magnetic resonance scans).
In modern sciences and technologies, images also gain much broader scopes due to the ever growing importance of scientific visualization (of often large-scale complex scientific/experimental data). Examples include microarray data in genetic research, or real-time multi-asset portfolio trading in finance.""
""Image-based rendering "",,,,""Heung-Yeung "Harry" Shum (born 1966 or 1967) is an American computer scientist. He is the Executive Vice President, Technology & Research at Microsoft. He is known for his research on computer vision and computer graphics, and for the development  of web search engine Bing.""
""Antialiasing "",,,,""Anti-aliasing may refer to any of a number of techniques to combat the problems of aliasing in a sampled signal such as a digital image or digital audio recording.
Specific topics in anti-aliasing include:
Anti-aliasing filter, a filter used before a signal sampler, to restrict the bandwidth of a signal
Spatial anti-aliasing, the technique of minimizing aliasing when representing a high-resolution image at a lower resolution
Supersample anti-aliasing, a method of smoothing images rendered in computer-generated imagery
Multisample anti-aliasing, a type of anti-aliasing, a technique used in computer graphics to improve image quality

Temporal anti-aliasing, techniques to reduce or remove the effects of temporal aliasing""
""Graphics processors "",,,,""A graphics processing unit (GPU), also occasionally called visual processing unit (VPU), is a specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display. GPUs are used in embedded systems, mobile phones, personal computers, workstations, and game consoles. Modern GPUs are very efficient at manipulating computer graphics and image processing, and their highly parallel structure makes them more effective than general-purpose CPUs for algorithms where the processing of large blocks of visual data is done in parallel. In a personal computer, a GPU can be present on a video card, or it can be embedded on the motherboard or—in certain CPUs—on the CPU die.
The term GPU was popularized by Nvidia in 1999, who marketed the GeForce 256 as "the world's first GPU", or Graphics Processing Unit. It was presented as a "single-chip processor with integrated transform, lighting, triangle setup/clipping, and rendering engines that are capable of processing a minimum of 10 million polygons per second". Rival ATI Technologies coined the term visual processing unit or VPU with the release of the Radeon 9700 in 2002.""
""Graphics input devices "",,,,""DirectFB (Direct Frame Buffer) is a software library with a small memory footprint that provides graphics acceleration, input device handling and abstraction layer, and integrated windowing system with support for translucent windows and multiple display layers on top of the Linux framebuffer without requiring any kernel modifications. DirectFB is free and open-source software subject to the terms of the GNU Lesser General Public License (LGPL).
The library allows developers an alternative to a full X Window System (X11) server used in Unix-like operating systems. DirectFB allows applications to talk directly to video hardware through a direct API, speeding up and simplifying graphic operations.
It is often used by games and embedded systems developers to circumvent the overhead of a full X Window System server implementation. In 2006, a version of DirectFB was included in the software stack for the CE Linux Forum's Audio/Video Graphics Specification V2.
DirectFB can host XDirectFB, a rootless X server implementation that uses DirectFB windows for X11 top-level windows. XDirectFB is an interface that mimics the X11 interface through the DirectFB API to simplify running applications written for X11 on DirectFB.
DirectFBGL is an OpenGL extension for DirectFB/XDirectFB that uses Direct Rendering Infrastructure (DRI) in the Mesa 3D library to support OpenGL hardware acceleration.
Among the products that use DirectFB are LinuxTV, Ben Nanonote, the unreleased Palm Foleo mobile companion, the webOS operating system, Panasonic Viera Connect TVs and the Philips TVs based on jointSPACE.
As of 4 October 2015, DirectFB's website is down; it is believed that it went down in August 2015, and the last recorded Internet Archive record is 1 Aug 2015.""
""Mixed / augmented reality "",,,,""The International Symposium on Mixed and Augmented Reality (ISMAR) is the leading international academic conference in the field of Augmented Reality and Mixed Reality. The symposium is organized and supported by IEEE Computer Society and IEEE VGTC [1]. The first ISMAR conference was held in 2002 in Darmstadt, Germany. The creation of the conference emerged from the fusion of two former academic events dedicated to this research field: the International Symposium on Augmented Reality (ISAR) and the International Symposium on Mixed Reality (ISMR) [2].""
""Perception "",,,,""Perception (from the Latin perceptio, percipio) is the organization, identification, and interpretation of sensory information in order to represent and understand the environment. All perception involves signals in the nervous system, which in turn result from physical or chemical stimulation of the sense organs. For example, vision involves light striking the retina of the eye, smell is mediated by odor molecules, and hearing involves pressure waves. Perception is not the passive receipt of these signals, but is shaped by learning, memory, expectation, and attention.
Perception can be split into two processes. Firstly, processing sensory input, which transforms these low-level information to higher-level information (e.g., extracts shapes for object recognition). Secondly, processing which is connected with a person's concepts and expectations (knowledge) and selective mechanisms (attention) that influence perception.
Perception depends on complex functions of the nervous system, but subjectively seems mostly effortless because this processing happens outside conscious awareness.
Since the rise of experimental psychology in the 19th Century, psychology's understanding of perception has progressed by combining a variety of techniques. Psychophysics quantitatively describes the relationships between the physical qualities of the sensory input and perception. Sensory neuroscience studies the brain mechanisms underlying perception. Perceptual systems can also be studied computationally, in terms of the information they process. Perceptual issues in philosophy include the extent to which sensory qualities such as sound, smell or color exist in objective reality rather than in the mind of the perceiver.
Although the senses were traditionally viewed as passive receptors, the study of illusions and ambiguous images has demonstrated that the brain's perceptual systems actively and pre-consciously attempt to make sense of their input. There is still active debate about the extent to which perception is an active process of hypothesis testing, analogous to science, or whether realistic sensory information is rich enough to make this process unnecessary.
The perceptual systems of the brain enable individuals to see the world around them as stable, even though the sensory information is typically incomplete and rapidly varying. Human and animal brains are structured in a modular way, with different areas processing different kinds of sensory information. Some of these modules take the form of sensory maps, mapping some aspect of the world across part of the brain's surface. These different modules are interconnected and influence each other. For instance, taste is strongly influenced by smell.
^ Schacter, Daniel (2011). Psychology. Worth Publishers. 
^ a b Goldstein (2009) pp. 5–7
^ a b c d e Gregory, Richard. "Perception" in Gregory, Zangwill (1987) pp. 598–601.
^ a b Bernstein, Douglas A. (5 March 2010). Essentials of Psychology. Cengage Learning. pp. 123–124. ISBN 978-0-495-90693-3. Retrieved 25 March 2011. 
^ Gustav Theodor Fechner. Elemente der Psychophysik. Leipzig 1860
^""
""Graphics file formats "",,,,""Image file formats are standardized means of organizing and storing digital images. Image files are composed of digital data in one of these formats that can be rasterized for use on a computer display or printer. An image file format may store data in uncompressed, compressed, or vector formats. Once rasterized, an image becomes a grid of pixels, each of which has a number of bits to designate its color equal to the color depth of the device displaying it.""
""Virtual reality "",,,,""Virtual reality or virtual realities (VR), also known as immersive multimedia or computer-simulated reality, is a computer technology that replicates an environment, real or imagined, and simulates a user's physical presence and environment in a way that allows the user to interact with it. Virtual realities artificially create sensory experience, which can include sight, touch, hearing, and smell.
Most up-to-date virtual realities are displayed either on a computer screen or with a special virtual reality headset, and some simulations include additional sensory information and focus on real sound through speakers or headphones targeted towards VR users. Some advanced haptic systems now include tactile information, generally known as force feedback in medical, gaming and military applications. Furthermore, virtual reality covers remote communication environments which provide virtual presence of users with the concepts of telepresence and telexistence or a virtual artifact (VA) either through the use of standard input devices such as a keyboard and mouse, or through multimodal devices such as a wired glove or omnidirectional treadmills. The immersive environment can be similar to the real world in order to create a lifelike experience—for example, in simulations for pilot or combat training—or it can differ significantly from reality, such as in VR games.""
""Image compression "",,,,""The objective of image compression is to reduce irrelevance and redundancy of the image data in order to be able to store or transmit data in an efficient form.""
""Mesh models "",,,,""In computing, D3DX (Direct3D Extension) is a deprecated high level API library which is written to supplement Microsoft's Direct3D graphics API. The D3DX library was introduced in Direct3D 7, and subsequently was improved in Direct3D 9. It provides classes for common calculations on vectors, matrices and colors, calculating look-at and projection matrices, spline interpolations, and several more complicated tasks, such as compiling or assembling shaders used for 3D graphic programming, compressed skeletal animation storage and matrix stacks. There are several functions that provide complex operations over 3D meshes like tangent-space computation, mesh simplification, precomputed radiance transfer, optimizing for vertex cache friendliness and strip reordering, and generators for 3D text meshes. 2D features include classes for drawing screen-space lines, text and sprite based particle systems. Spatial functions include various intersection routines, conversion from/to barycentric coordinates and bounding box/sphere generators.
The D3DX library contains pre-written routines for doing things common to most 2D/3D applications, such as games. Since the Direct3D API is relatively low-level, using the D3DX library is usually much simpler.
In 2012, Microsoft announced that D3DX would be deprecated in the Windows 8 SDK, along with other development frameworks such as XNA. Shader effects, texture management, geometry optimizations and mesh models are available as separate sources published through CodePlex. The mathematical constructs of D3DX, like vectors and matrices, would be consolidated with XNAMath into a DirectXMath and spherical harmonics math is provided as separate source.""
""Mesh geometry models "",,,,""Geometry processing, or mesh processing, is an area of research that uses concepts from applied mathematics, computer science and engineering to design efficient algorithms for the acquisition, reconstruction, analysis, manipulation, simulation and transmission of complex 3D models.
Applications of geometry processing algorithms already cover a wide range of areas from multimedia, entertainment and classical computer-aided design, to biomedical computing, reverse engineering and scientific computing.""
""Parametric curve and surface models "",,,,""Geometric design (GD), also known as geometric modelling, is a branch of computational geometry. It deals with the construction and representation of free-form curves, surfaces, or volumes. Core problems are curve and surface modelling and representation. GD studies especially the construction and manipulation of curves and surfaces given by a set of points using polynomial, rational, piecewise polynomial, or piecewise rational methods. The most important instruments here are parametric curves and parametric surfaces, such as Bézier curves, spline curves and surfaces. An important non-parametric approach is the level set method.
Application areas include shipbuilding, aircraft, and automotive industries, as well as architectural design. The modern ubiquity and power of computers means that even perfume bottles and shampoo dispensers are designed using techniques unheard of by shipbuilders of 1960s.
Geometric models can be built for objects of any dimension in any geometric space. Both 2D and 3D geometric models are extensively used in computer graphics. 2D models are important in computer typography and technical drawing. 3D models are central to computer-aided design and manufacturing, and many applied technical fields such as geology and medical image processing.
Geometric models are usually distinguished from procedural and object-oriented models, which define the shape implicitly by an algorithm. They are also contrasted with digital images and volumetric models; and with implicit mathematical models such as the zero set of an arbitrary polynomial. However, the distinction is often blurred: for instance, geometric shapes can be represented by objects; a digital image can be interpreted as a collection of colored squares; and geometric shapes such as circles are defined by implicit mathematical equations. Also, the modeling of fractal objects often requires a combination of geometric and procedural techniques.
Geometric problems originating in architecture can lead to interesting research and results in geometry processing, computer-aided geometric design, and discrete differential geometry.""
""Point-based models "",,,,""The Scientific Computing and Imaging Institute (SCI) is a research institute located on the University of Utah campus in Salt Lake City, Utah. Its objective is to create new scientific computing techniques, tools, and systems with applications to various fields, including high performance computing, scientific visualization, image analysis, computational biology, data science, and graphics.
^ "Scientific Computing and Imaging Institute – Home". Retrieved 2013-04-16.""
""Volumetric models "",,,,""Diffuse optical imaging (DOI) is a method of imaging using near-infrared spectroscopy (NIRS)  or fluorescence-based methods. When used to create 3D volumetric models of the imaged material DOI is referred to as diffuse optical tomography, whereas 2D imaging methods are classified as diffuse optical topography.
The technique has many applications to neuroscience, sports medicine, wound monitoring, and cancer detection. Typically DOI techniques monitor changes in concentrations of oxygenated and deoxygenated hemoglobin and may additionally measure redox states of cytochromes. The technique may also be referred to as diffuse optical tomography (DOT), near infrared optical tomography (NIROT) or fluorescence diffuse optical tomography (FDOT), depending on the usage.
In neuroscience, functional measurements made using NIR wavelengths, DOI techniques may classify as functional near infrared spectroscopy fNIRS.""
""Self-organization "",,,,""Self-organization is a process where some form of overall order or coordination arises out of the local interactions between smaller component parts of an initially disordered system. The process of self-organization can be spontaneous, and it is not necessarily controlled by any auxiliary agent outside of the system. It is often triggered by random fluctuations that are amplified by positive feedback. The resulting organization is wholly decentralized or distributed over all the components of the system. As such, the organization is typically robust and able to survive and, even, self-repair substantial damage or perturbations. Chaos theory discusses self-organization in terms of islands of predictability in a sea of chaotic unpredictability. Self-organization occurs in a variety of physical, chemical, biological, robotic, social, and cognitive systems. Examples of its realization can be found in crystallization, thermal convection of fluids, chemical oscillation, animal swarming, and neural networks.
^ Betzler, S. B.; Wisnet, A.; Breitbach, B.; Mitterbauer, C.; Weickert, J.; Schmidt-Mende, L.; Scheu, C. (2014). "Template-free synthesis of novel, highly-ordered 3D hierarchical Nb3O7(OH) superstructures with semiconductive and photoactive properties". Journal of Materials Chemistry A 2 (30): 12005. doi:10.1039/C4TA02202E.""
""Concurrent programming languages "",,,,""Concurrent computing is a form of computing in which several computations are executing during overlapping time periods—concurrently—instead of sequentially (one completing before the next starts). This is a property of a system—this may be an individual program, a computer, or a network—and there is a separate execution point or "thread of control" for each computation ("process"). A concurrent system is one where a computation can advance without waiting for all other computations to complete; where more than one computation can advance at the same time.
As a programming paradigm, concurrent computing is a form of modular programming, namely factoring an overall computation into subcomputations that may be executed concurrently. Pioneers in the field of concurrent computing include Edsger Dijkstra, Per Brinch Hansen, and C.A.R. Hoare.""
""Concurrent algorithms "",,,,""In computer science, a concurrent algorithm is one that can be executed concurrently. Most standard computer algorithms are sequential algorithms, and assume that the algorithm is run from start to finish without any other processes executing. These often do not behave correctly when run concurrently, as demonstrated at right, and are often nondeterministic, as the actual sequence of computations is determined by the external scheduler. Concurrency often adds significant complexity to an algorithm, requiring concurrency control such as mutual exclusion to avoid problems such as race conditions.
Many parallel algorithms are run concurrently, particularly distributed algorithms, though these are distinct concepts in general.""
""Digital cash "",,,,""Electronic money, or e-money, is the money balance recorded electronically on a stored-value card. These cards have microprocessors embedded which can be loaded with a monetary value. Another form of electronic money is network money, software that allows the transfer of value on computer networks, particularly the internet. Electronic money is a floating claim on a private bank or other financial institution that is not linked to any particular account. Examples of electronic money are bank deposits, electronic funds transfer, direct deposit, payment processors, and digital currencies.
Electronic money can either be centralized, where there is a central point of control over the money supply, or decentralized, where the control over the money supply can come from various sources. Electronic money that is decentralized is also known as digital currencies. The major difference between E-money and digital currencies is that E-money doesn't change the value of the fiat currency (USD, EUR) it represents, but digital currency isn't equivalent to any fiat currency. In other words, all digital currency is Electronic money, but Electronic money isn't necessarily digital currency. Many mobile sub-systems have been introduced in the past few years including Google Wallet and Apple Pay.""
""E-commerce infrastructure "",,,,""Founded in January 2002, the Center for E-Commerce Infrastructure Development (CECID) is a research and development center in the University of Hong Kong committed to promoting e-commerce infrastructure development and standardization. A member of OASIS, W3C, RosettaNet, and the ebXML Asia Committee, CECID actively takes part in the development and implementation of international standards, such as Universal Business Language, Web Services, and RosettaNet. Through participation in these international and regional standards bodies, CECID follows closely the latest developments in e-commerce technology standards and promotes Hong Kong's e-commerce technology to technical communities overseas.
CECID's operation is primarily financed by R&D grants from the Innovation and Technology Commission of the Hong Kong Government for its two flagship research projects, namely Project Phoenix and Project Pyxis. In its completed Project Phoenix, CECID has produced several software packages that implement major ebXML specifications. These software packages include Hermes Message Service Handler, ebMail, and ebXMLRR Registry/Repository and are currently released under open source licenses on the freebXML.org website that CECID established in 2002. Commenced in 2004, Project Pyxis targets to develop enabling technology for e-business interoperability between trading partners and within large enterprises using various complementary and competing Web Services standards.""
""Electronic data interchange "",,,,""Electronic Data Interchange (EDI) is an electronic communication method that provides standards for exchanging data via any electronic means. By adhering to the same standard, two different companies or organizations, even in two different countries, can electronically exchange documents (such as purchase orders, invoices, shipping notices, and many others). EDI has existed for more than 30 years, and there are many EDI standards (including X12, EDIFACT, ODETTE, etc.), some of which address the needs of specific industries or regions. It also refers specifically to a family of standards. In 1996, the National Institute of Standards and Technology defined electronic data interchange as "the computer-to-computer interchange of strictly formatted messages that represent documents other than monetary instruments. EDI implies a sequence of messages between two parties, either of whom may serve as originator or recipient. The formatted data representing the documents may be transmitted from originator to recipient via telecommunications or physically transported on electronic storage media." It distinguishes mere electronic communication or data exchange, specifying that "in EDI, the usual processing of received messages is by computer only. Human intervention in the processing of a received message is typically intended only for error conditions, for quality review, and for special situations. For example, the transmission of binary or textual data is not EDI as defined here unless the data are treated as one or more data elements of an EDI message and are not normally intended for human interpretation as part of online data processing."
EDI can be formally defined as the transfer of structured data, by agreed message standards, from one computer system to another without human intervention.""
""Electronic funds transfer "",,,,""Electronic funds transfer (EFT) is the electronic transfer of money from one bank account to another, either within a single financial institution or across multiple institutions, through computer-based systems and without the direct intervention of bank staff. EFTs are known by a number of names. In the United States, they may be referred to as electronic checks or e-checks.
The term covers a number of different payment systems, for example:
cardholder-initiated transactions, using a payment card such as a credit or debit card
direct deposit payment initiated by the payer
direct debit payments for which a business debits the consumer's bank accounts for payment for goods or services
wire transfer via an international banking network such as SWIFT
electronic bill payment in online banking, which may be delivered by EFT or paper check
transactions involving stored value of electronic money, possibly in a private currency.""
""Online shopping "",,,,""Online shopping (sometimes known as e-tail from "electronic retail" or e-shopping) is a form of electronic commerce which allows consumers to directly buy goods or services from a seller over the Internet using a web browser. Alternative names are: e-web-store, e-shop, e-store, Internet shop, web-shop, web-store, online store, online storefront and virtual store. Mobile commerce (or m-commerce) describes purchasing from an online retailer's mobile optimized online site or app.
An online shop evokes the physical analogy of buying products or services at a bricks-and-mortar retailer or shopping center; the process is called business-to-consumer (B2C) online shopping. In the case where a business buys from another business, the process is called business-to-business (B2B) online shopping. The largest of these online retailing corporations are Alibaba, Amazon.com, and eBay.
^ "The Alibaba phenomenon". The Economics. March 23, 2013.""
""Online banking "",,,,""Online banking also known as internet banking, e-banking, or virtual banking, is an electronic payment system that enables customers of a bank or other financial institution to conduct a range of financial transactions through the financial institution's website. The online banking system will typically connect to or be part of the core banking system operated by a bank and is in contrast to branch banking that was the traditional way customers access banking services. Basically Online Banking, Internet Banking & E-Banking is the same thing. There is no difference fundamentally and in mechanism.
To access a financial institution's online banking facility, a customer with internet access would need to register with the institution for the service, and set up a password and other credentials for customer verification. The credentials for online banking is normally not the same as for telephone or mobile banking. Financial institutions now routinely allocate customers numbers, whether or not customers have indicated an intention to access their online banking facility. Customers' numbers are normally not the same as account numbers, because a number of customer accounts can be linked to the one customer number. The customer number can be linked to any account that the customer controls, such as cheque, savings, loan, credit card and other accounts.
The customer visits the financial institution's secure website, and enters the online banking facility using the customer number and credentials previously set up. The types of financial transactions which a customer may transact through online banking usually includes obtaining account balances and list of latest transactions, electronic bill payments, and funds transfers between a customer's or another's accounts. Most also enable a customer to download copies of statements, which can be printed at the customer's premises (with some banks charging a fee for mailing hardcopies of bank statements). Some banks also enable customers to download transactions directly into the customer's accounting software. The facility may also enable the customer to order cheque-books, statements, report loss of credit cards, stop payment on a cheque, advise change of address, and other routine transactions.

""
""Secure online transactions "",,,,""Vincent Aron Cate (born 1963) is a cryptography software developer based in Anguilla. He graduated from the University of California, Berkeley and ran an Atari hardware business in the 1980s before beginning a Ph.D. programme at Carnegie Mellon University, but dropped out and moved to Anguilla to pursue business opportunities there. In his new home, he would go on to establish an internet service provider, a computer club for young students, and an annual cryptography conference. A former U.S. citizen, he gave up his citizenship in 1998 in protest of U.S. laws on the export of cryptography.""
""Online auctions "",,,,""An online auction is an auction which is held over the internet. Online auctions come in many different formats, but most popularly they are ascending English auctions, descending Dutch auctions, first-price sealed-bid, Vickrey auctions, or sometimes even a combination of multiple auctions, taking elements of one and forging them with another. The scope and reach of these auctions have been propelled by the Internet to a level beyond what the initial purveyors had anticipated. This is mainly because online auctions break down and remove the physical limitations of traditional auctions such as geography, presence, time, space, and a small target audience. This influx in reachability has also made it easier to commit unlawful actions within an auction. In 2002, online auctions were projected to account for 30% of all online e-commerce due to the rapid expansion of the popularity of the form of electronic commerce.
^ a b Bapna, R.; Goes, P.; Gupta, A. (2001). "Insights and analyses of online auctions". Communications of the ACM 44 (11): 42. doi:10.1145/384150.384160. 
^ Albert, M. R. (2002). "E-Buyer Beware: Why Online Auction Fraud Should Be Regulated". American Business Law Journal 39 (4): 575. doi:10.1111/j.1744-1714.2002.tb00306.x. 
^ Vakrat, Y.; Seidmann, A. (2000). "Implications of the bidders' arrival process on the design of online auctions". Proceedings of the 33rd Annual Hawaii International Conference on System Sciences. p. 7. doi:10.1109/HICSS.2000.926822. ISBN 0-7695-0493-0.""
""Intranets "",,,,""An intranet is a private network accessible only to an organization's staff. Generally a wide range of information and services from the organization's internal IT systems are available that would not be available to the public from the Internet. A company-wide intranet can constitute an important focal point of internal communication and collaboration, and provide a single starting point to access internal and external resources. In its simplest form an intranet is established with the technologies for local area networks (LANs) and wide area networks (WANs).
Intranets began to appear in a range of larger organizations from 1994. The launch of a free webserver from Microsoft in 1996 helped make the technology accessible to a wider market.

""
""Extranets "",,,,""An extranet is a website that allows controlled access to partners, vendors and suppliers or an authorized set of customers – normally to a subset of the information accessible from an organization's intranet. An extranet is similar to a DMZ in that it provides access to needed services for authorised parties, without granting access to an organization's entire network.
Historically the term was occasionally also used in the sense of two organisations sharing their internal networks over a VPN.""
""Enterprise resource planning "",,,,""Enterprise resource planning (ERP) is a category of business-management software—typically a suite of integrated applications—that an organization can use to collect, store, manage and interpret data from many business activities, including:
product planning, purchase
manufacturing or service delivery
marketing and sales
inventory management
shipping and payment
ERP provides an integrated view of core business processes, often in real-time, using common databases maintained by a database management system. ERP systems track business resources—cash, raw materials, production capacity—and the status of business commitments: orders, purchase orders, and payroll. The applications that make up the system share data across various departments (manufacturing, purchasing, sales, accounting, etc.) that provide the data. ERP facilitates information flow between all business functions, and manages connections to outside stakeholders.
Enterprise system software is a multibillion-dollar industry that produces components that support a variety of business functions. IT investments have become the largest category of capital expenditure in United States-based businesses over the past decade. Though early ERP systems focused on large enterprises, smaller enterprises increasingly use ERP systems.
The ERP system is considered a vital organizational tool because it integrates varied organizational systems and facilitates error-free transactions and production. However, developing an ERP system differs from traditional system development. ERP systems run on a variety of computer hardware and network configurations, typically using a database as an information repository.""
""Enterprise applications "",,,,""Enterprise software, also known as enterprise application software (EAS), is computer software used to satisfy the needs of an organization rather than individual users. Such organizations would include businesses, schools, interest-based user groups, clubs, charities, or governments. Enterprise software is an integral part of a (computer based) Information System.
Services provided by enterprise software are typically business-oriented tools such as online shopping and online payment processing, interactive product catalogue, automated billing systems, security, enterprise content management, IT service management, customer relationship management, enterprise resource planning, business intelligence, project management, collaboration, human resource management, manufacturing, occupational health and safety, enterprise application integration, and enterprise forms automation.
As enterprises have similar departments and systems in common, enterprise software is often available as a suite of customizable programs. Generally, the complexity of these tools requires specialist capabilities and specific knowledge.""
""Data centers "",,,,""A data center is a facility used to house computer systems and associated components, such as telecommunications and storage systems. It generally includes redundant or backup power supplies, redundant data communications connections, environmental controls (e.g., air conditioning, fire suppression) and various security devices. Large data centers are industrial scale operations using as much electricity as a small town.""
""Business process modeling "",,,,""Business process modeling (BPM) in systems engineering is the activity of representing processes of an enterprise, so that the current process may be analyzed or improved. BPM is typically performed by business analysts, who provide expertise in the modeling discipline; by subject matter experts, who have specialized knowledge of the processes being modeled; or more commonly by a team comprising both. Alternatively, the process model can be derived directly from events' logs using process mining tools.
The business objective is often to increase process speed or reduce cycle time; to increase quality; or to reduce costs, such as labor, materials, scrap, or capital costs. In practice, a management decision to invest in business process modeling is often motivated by the need to document requirements for an information technology project.
Change management programs are typically involved to put any improved business processes into practice. With advances in software design, the vision of BPM models becoming fully executable (and capable of simulations and round-trip engineering) is coming closer to reality.""
""Business process monitoring "",,,,""The purpose of this table is to provide reference information about the provenance and history of commercial open-source applications, adopting Business models for open-source software, alphabetized by the product/service name. It is not to be used or interpreted as an advertisement for the vendors.""
""Business intelligence "",,,,""Business intelligence (BI) can be described as "a set of techniques and tools for the acquisition and transformation of raw data into meaningful and useful information for business analysis purposes". The term "data surfacing" is also more often associated with BI functionality. BI technologies are capable of handling large amounts of unstructured data to help identify, develop and otherwise create new strategic business opportunities. The goal of BI is to allow for the easy interpretation of these large volumes of data. Identifying new opportunities and implementing an effective strategy based on insights can provide businesses with a competitive market advantage and long-term stability.
BI technologies provide historical, current and predictive views of business operations. Common functions of business intelligence technologies are reporting, online analytical processing, analytics, data mining, process mining, complex event processing, business performance management, benchmarking, text mining, predictive analytics and prescriptive analytics.
BI can be used to support a wide range of business decisions ranging from operational to strategic. Basic operating decisions include product positioning or pricing. Strategic business decisions include priorities, goals and directions at the broadest level. In all cases, BI is most effective when it combines data derived from the market in which a company operates (external data) with data from company sources internal to the business such as financial and operations data (internal data). When combined, external and internal data can provide a more complete picture which, in effect, creates an "intelligence" that cannot be derived by any singular set of data. Amongst myriad uses, BI tools empower organisations to gain insight into new markets, assess demand and suitability of products and services for different market segments and gauge the impact of marketing efforts.""
""Enterprise architecture management "",,,,""Enterprise Architecture Management (or EAM) is a "management practice that establishes, maintains and uses a coherent set of guidelines, architecture principles and governance regimes that provide direction and practical help in the design and development of an enterprise's architecture to achieve its vision and strategy."""
""Enterprise architecture frameworks "",,,,""The ISO/IEC/IEEE 42010 Conceptual Model of Architecture Description defines the term architecture framework as:
"An architecture framework establishes a common practice for creating, interpreting, analyzing and using architecture descriptions within a particular domain of application or stakeholder community. Examples of Architecture Frameworks: MODAF, TOGAF, Kruchten's 4+1 View Model, RM-ODP."
Especially the domain within a company or other organisation is covered by enterprise architecture frameworks.
The Survey of Architecture Frameworks lists some of the available architecture frameworks.""
""Enterprise architecture modeling "",,,,""ArchiMate (/ˈɑːr.kɪmeɪt/ AR-ki-mayt) is an open and independent enterprise architecture modeling language to support the description, analysis and visualization of architecture within and across business domains in an unambiguous way.
ArchiMate is a technical standard from The Open Group and is based on the concepts of the IEEE 1471 standard. It is supported by various tool vendors and consulting firms. ArchiMate is also a registered trademark of The Open Group. The Open Group has a certification program for ArchiMate users, software tools and courses.
ArchiMate distinguishes itself from other languages such as Unified Modeling Language (UML) and Business Process Modeling Notation (BPMN) by its enterprise modelling scope.""
""Service-oriented architectures "",,,,""A service-oriented architecture (SOA) is an architectural pattern in computer software design in which application components provide services to other components via a communications protocol, typically over a network. The principles of service-orientation are independent of any vendor, product or technology.
A service is a self-contained unit of functionality, such as retrieving an online bank statement. By that definition, a service is an operation that may be discretely invoked. However, in the Web Services Description Language (WSDL), a service is an interface definition that may list several discrete services/operations. And elsewhere, the term service is used for a component that is encapsulated behind an interface. This widespread ambiguity is reflected in what follows.
Services can be combined to provide the functionality of a large software application. SOA makes it easier for software components on computers connected over a network to cooperate. Every computer can run any number of services, and each service is built in a way that ensures that the service can exchange information with any other service in the network without human interaction and without the need to make changes to the underlying program itself.""
""Event-driven architectures "",,,,""Event-driven architecture (EDA), also known as Message-driven architecture, is a software architecture pattern promoting the production, detection, consumption of, and reaction to events.
An event can be defined as "a significant change in state". For example, when a consumer purchases a car, the car's state changes from "for sale" to "sold". A car dealer's system architecture may treat this state change as an event whose occurrence can be made known to other applications within the architecture. From a formal perspective, what is produced, published, propagated, detected or consumed is a (typically asynchronous) message called the event notification, and not the event itself, which is the state change that triggered the message emission. Events do not travel, they just occur. However, the term event is often used metonymically to denote the notification message itself, which may lead to some confusion.
This architectural pattern may be applied by the design and implementation of applications and systems which transmit events among loosely coupled software components and services. An event-driven system typically consists of event emitters (or agents), event consumers (or sinks), and event channels. Emitters have the responsibility to detect, gather, and transfer events. An Event Emitter does not know the consumers of the event, it does not even know if a consumer exists, and in case it exists, it does not know how the event is used or further processed. Sinks have the responsibility of applying a reaction as soon as an event is presented. The reaction might or might not be completely provided by the sink itself. For instance, the sink might just have the responsibility to filter, transform and forward the event to another component or it might provide a self-contained reaction to such event. Event channels are conduits in which events are transmitted from event emitters to event consumers. The knowledge of the correct distribution of events is exclusively present within the event channel. The physical implementation of event channels can be based on traditional components such as message-oriented middleware or point-to-point communication which might require a more appropriate transactional executive framework.
Building applications and systems around an event-driven architecture allows these applications and systems to be constructed in a manner that facilitates more responsiveness, because event-driven systems are, by design, more normalized to unpredictable and asynchronous environments.
Event-driven architecture can complement service-oriented architecture (SOA) because services can be activated by triggers fired on incoming events. This paradigm is particularly useful whenever the sink does not provide any self-contained executive.
SOA 2.0 evolves the implications SOA and EDA architectures provide to a richer, more robust level by leveraging previously unknown causal relationships to form a new event pattern. This new business intelligence pattern triggers further autonomous human or automated processing that adds exponential value to the enterprise by injecting value-added information into the recognized pattern which could not have been achieved previously.
Computing machinery and sensing devices (like sensors, actuators, controllers) can detect state changes of objects or conditions and create events which can then be processed by a service or system. Event triggers are conditions that result in the creation of an event.

""
""Business rules "",,,,""Commercial law, also known as business law, is the body of law that applies to the rights, relations, and conduct of persons and businesses engaged in commerce, merchandising, trade, and sales. It is often considered to be a branch of civil law and deals with issues of both private law and public law.
Commercial law includes within its compass such titles as principal and agent; carriage by land and sea; merchant shipping; guarantee; marine, fire, life, and accident insurance; bills of exchange and partnership. It can also be understood to regulate corporate contracts, hiring practices, and the manufacture and sales of consumer goods. Many countries have adopted civil codes that contain comprehensive statements of their commercial law.
In the United States, commercial law is the province of both the United States Congress, under its power to regulate interstate commerce, and the states, under their police power. Efforts have been made to create a unified body of commercial law in the United States; the most successful of these attempts has resulted in the general adoption of the Uniform Commercial Code, which has been adopted in all 50 states (with some modification by state legislatures), the District of Columbia, and the U.S. territories.
Various regulatory schemes control how commerce is conducted, particularly vis-a-vis employees and customers. Privacy laws, safety laws (e.g., the Occupational Safety and Health Act in the United States), and food and drug laws are some examples.""
""Enterprise modeling "",,,,""Enterprise modelling is the abstract representation, description and definition of the structure, processes, information and resources of an identifiable business, government body, or other large organization.
It deals with the process of understanding an enterprise business and improving its performance through creation of enterprise models. This includes the modelling of the relevant business domain (usually relatively stable), business processes (usually more volatile), and Information technology.""
""Enterprise data management "",,,,""Enterprise Data Management (EDM) is:
A concept – referring to the ability of an organization to precisely define, easily integrate and effectively retrieve data for both internal applications and external communication.
A business objective – focused on the creation of accurate, consistent and transparent content. EDM emphasizes data precision, granularity and meaning and is concerned with how the content is integrated into business applications as well as how it is passed along from one business process to another.
EDM arose to address circumstances where users within organizations independently source, model, manage and store data. Uncoordinated approaches by various segments of the organization can result in data conflicts and quality inconsistencies – lowering the trustworthiness of the data as it is used for operations and reporting.
The goal of EDM is trust and confidence in data assets. Its components are:""
""Reference models "",,,,""The Data Reference Model (DRM) is one of the five reference models of the Federal Enterprise Architecture (FEA).""
""Business-IT alignment "",,,,""Business-IT alignment is a dynamic state in which a business organization is able to use information technology (IT) effectively to achieve business objectives - typically improved financial performance or marketplace competitiveness. Some definitions focus more on outcomes (the ability of IT to produce business value) than means (the harmony between IT and business decision-makers within the organizations); for example,

alignment is the capacity to demonstrate a positive relationship between information technologies and the accepted financial measures of performance.

This alignment is in contrast to what is often experienced in organizations: IT and business professionals unable to bridge the gap between themselves because of differences in objectives, culture, and incentives and a mutual ignorance for the other group's body of knowledge. This rift generally results in expensive IT systems that do not provide adequate return on investment. For this reason, the search for Business / IT Alignment is closely associated with attempts to improve the business value of IT investments.
Business-information technology alignment, the "holy grail" of organizations, integrates the information technology to the strategy, mission, and goals of the organization. There are six key characteristics in order to achieve this alignment. First, the organization must view information technology as an instrument to transform the business. This includes exploring other revenue streams and integrating other facets of their business into each other. For example, using one central data warehouse to combine two separate, but partnering businesses. Next, an organization must hold customer service, both externally and internally, at the utmost importance. This communication between the organization and their customers must not be lost. Thirdly, an organization must rotate both IT and business professionals across different departments and job functions. They must have the knowledge and experience of both sides of the business so that understanding and communication is achieved. Once those three characteristics are achieved, then an organization must provide clear and specific goals to both the IT and business employees. This will create and integration of both entities to achieve a common goal. The fifth characteristic ensures that IT and business employees understand how the company makes or loses money. This is important so that money is not carelessly poured into the IT department and there is no return on that investment. Lastly, organizations must create a vibrant and inclusive company culture. There must not only be informational unity, but a company as whole.
It is not unusual for business and IT professionals within an organization to experience conflict and in-fighting as lack of mutual understanding and the failure to produce desired results leads to blaming and mistrust. The search for B/I alignment often includes efforts to establish trust between these two groups and a mechanism for consensus decision-making.""
""IT architectures "",,,,""In the United States, the National Register of Historic Places classifies its listings by various types of architecture. Listed properties often are given one or more of 40 standard architectural style classifications that appear in the National Register Information System (NRIS) database. Other properties are given a custom architectural description with "vernacular" or other qualifiers, and others have no style classification. Many National Register-listed properties do not fit into the several categories listed here, or they fit into more specialized subcategories.

""
""IT governance "",,,,""Information and technology (IT) governance is a subset discipline of corporate governance, focused on information and technology (IT) and its performance and risk management. The interest in IT governance is due to the ongoing need within organizations to focus value creation efforts on an organization's strategic objectives and to better manage the performance of those responsible for creating this value in the best interest of all stakeholders. It has evolved from The Principles of Scientific Management, Total Quality Management and ISO 9001 Quality management system.
Historically, board-level executives deferred key IT decisions to the company's IT management and business leaders. Short-term goals of those responsible for managing IT can be in conflict with the best interests of other stakeholders unless proper oversight is established. IT governance systematically involves everyone: board members, executive management, staff, customers, communities, investors and regulators. An IT Governance framework is used to identify, establish and link the mechanisms to oversee the use of information and related technology to create value and manage the risks associated with using information and technology.
Various definitions of IT governance exist. While in the business world the focus has been on managing performance and creating value, in the academic world the focus has been on "specifying the decision rights and an accountability framework to encourage desirable behavior in the use of IT."
The IT Governance Institute's definition is: "... leadership, organizational structures and processes to ensure that the organisation's IT sustains and extends the organisation's strategies and objectives."
AS8015, the Australian Standard for Corporate Governance of Information and Communication Technology (ICT), defines Corporate Governance of ICT as "The system by which the current and future use of ICT is directed and controlled. It involves evaluating and directing the plans for the use of ICT to support the organisation and monitoring this use to achieve plans. It includes the strategy and policies for using ICT within an organisation."""
""Enterprise computing infrastructures "",,,,""Redundant Array of Inexpensive Servers (RAIS) is the use of multiple servers to provide the same service in such a way that service will still be available if the servers fails. The term may imply some kind of load balancing between the servers.
Ia a simple, high performance, mainframe-grade alternative to today’s Enterprise computing infrastructure solutions. Te RAIS, turn an array of ordinary servers into a single virtual machine like RAID turns a cluster of ordinary disks into a single block device. Every RAIS node is a stateless computing unit.
RAIS stripes and mirrors application code and memory across an array of ordinary servers using the standard RAID schemata of level 0, level 1, level 5, level 1+0. This is possible through the invention of a radical new memory management system called Versioned Memory.
RAIS provide the benefits of an SMP at the entry cost of clusters.""
""Enterprise application integration "",,,,""Enterprise application integration (EAI) is the use of software and computer systems' architectural principles to integrate a set of enterprise computer applications.""
""Information integration and interoperability "",,,,""The Joint Deployable Analysis Team (JDAT) is part of the J6 Directorate of the JSC Staff, and was transitioned out of the United States Joint Forces Command (USJFCOM) Joint Fires Integration and Interoperability Team (JFIIT) in June 2011.""
""Avionics "",,,,""Avionics are the electronic systems used on aircraft, artificial satellites, and spacecraft. Avionic systems include communications, navigation, the display and management of multiple systems, and the hundreds of systems that are fitted to aircraft to perform individual functions. These can be as simple as a searchlight for a police helicopter or as complicated as the tactical system for an airborne early warning platform. The term avionics is a portmanteau of the words aviation and electronics.""
""Archaeology "",,,,""Archaeology, or archeology, is the study of human activity through the recovery and analysis of material culture. The archaeological record consists of artifacts, architecture, biofacts or ecofacts, and cultural landscapes. Archaeology can be considered both a social science and a branch of the humanities. In North America, archaeology is considered a sub-field of anthropology, while in Europe archaeology is often viewed as either a discipline in its own right or a sub-field of other disciplines.
Archaeologists study human prehistory and history, from the development of the first stone tools at Lomekwi, eastern Africa, 3.3 million years ago up until recent decades. Archaeology as a field is distinct from the discipline of paleontology, the study of fossil remains. Archaeology is particularly important for learning about prehistoric societies, for whom there may be no written records to study. Prehistory includes over 99% of the human past, from the Paleolithic until the advent of literacy in societies across the world. Archaeology has various goals, which range from understanding culture history to reconstructing past lifeways to documenting and explaining changes in human societies through time.
The discipline involves surveying, excavation and eventually analysis of data collected to learn more about the past. In broad scope, archaeology relies on cross-disciplinary research. It draws upon anthropology, history, art history, classics, ethnology, geography, geology, linguistics, semiology, physics, information sciences, chemistry, statistics, paleoecology, paleontology, paleozoology, paleoethnobotany, and paleobotany.
Archaeology developed out of antiquarianism in Europe during the 19th century, and has since become a discipline practiced across the world. Since its early development, various specific sub-disciplines of archaeology have developed, including maritime archaeology, feminist archaeology and archaeoastronomy, and numerous different scientific techniques have been developed to aid archaeological investigation. Nonetheless, today, archaeologists face many problems, such as dealing with pseudoarchaeology, the looting of artifacts, a lack of public interest, and opposition to the excavation of human remains.""
""Astronomy "",,,,""Astronomy, a natural science, is the study of celestial objects (such as stars, galaxies, planets, moons, asteroids, comets and nebulae) and processes (such as supernovae explosions, gamma ray bursts, and cosmic microwave background radiation), the physics, chemistry, and evolution of such objects and processes, and more generally all phenomena that originate outside the atmosphere of Earth. A related but distinct subject, physical cosmology, is concerned with studying the Universe as a whole.
Astronomy is one of the oldest sciences. The early civilizations in recorded history, such as the Babylonians, Greeks, Indians, Egyptians, Nubians, Iranians, Chinese, and Maya performed methodical observations of the night sky. However, the invention of the telescope was required before astronomy was able to develop into a modern science. Historically, astronomy has included disciplines as diverse as astrometry, celestial navigation, observational astronomy and the making of calendars, but professional astronomy is nowadays often considered to be synonymous with astrophysics.
During the 20th century, the field of professional astronomy split into observational and theoretical branches. Observational astronomy is focused on acquiring data from observations of astronomical objects, which is then analyzed using basic principles of physics. Theoretical astronomy is oriented toward the development of computer or analytical models to describe astronomical objects and phenomena. The two fields complement each other, with theoretical astronomy seeking to explain the observational results and observations being used to confirm theoretical results.
Astronomy is one of the few sciences where amateurs can still play an active role, especially in the discovery and observation of transient phenomena. Amateur astronomers have made and contributed to many important astronomical discoveries.""
""Chemistry "",,,,""Chemistry is a branch of physical science that studies the composition, structure, properties and change of matter. Chemistry includes topics such as the properties of individual atoms, how atoms form chemical bonds to create chemical compounds, the interactions of substances through intermolecular forces that give matter its general properties, and the interactions between substances through chemical reactions to form different substances.
Chemistry is sometimes called the central science because it bridges other natural sciences, including physics, geology and biology. For the differences between chemistry and physics see Comparison of chemistry and physics.
Scholars disagree about the etymology of the word chemistry. The history of chemistry can be traced to alchemy, which had been practiced for several millennia in various parts of the world.""
""Environmental sciences "",,,,""Environmental science is an interdisciplinary academic field that integrates physical, biological and information sciences (including ecology, biology, physics, chemistry, zoology, mineralogy, oceanology, limnology, soil science, geology, atmospheric science, and geodesy) to the study of the environment, and the solution of environmental problems. Environmental science emerged from the fields of natural history and medicine during the Enlightenment. Today it provides an integrated, quantitative, and interdisciplinary approach to the study of environmental systems.
Related areas of study include environmental studies and environmental engineering. Environmental studies incorporates more of the social sciences for understanding human relationships, perceptions and policies towards the environment. Environmental engineering focuses on design and technology for improving environmental quality in every aspect.
Environmental scientists work on subjects like the understanding of earth processes, evaluating alternative energy systems, pollution control and mitigation, natural resource management, and the effects of global climate change. Environmental issues almost always include an interaction of physical, chemical, and biological processes. Environmental scientists bring a systems approach to the analysis of environmental problems. Key elements of an effective environmental scientist include the ability to relate space, and time relationships as well as quantitative analysis.
Environmental science came alive as a substantive, active field of scientific investigation in the 1960s and 1970s driven by (a) the need for a multi-disciplinary approach to analyze complex environmental problems, (b) the arrival of substantive environmental laws requiring specific environmental protocols of investigation and (c) the growing public awareness of a need for action in addressing environmental problems. Events that spurred this development included the publication of Rachel Carson's landmark environmental book Silent Spring along with major environmental issues becoming very public, such as the 1969 Santa Barbara oil spill, and the Cuyahoga River of Cleveland, Ohio, "catching fire" (also in 1969), and helped increase the visibility of environmental issues and create this new field of study.""
""Computer-aided design "",,,,""Computer-aided design (CAD) is the use of computer systems to aid in the creation, modification, analysis, or optimization of a design. CAD software is used to increase the productivity of the designer, improve the quality of design, improve communications through documentation, and to create a database for manufacturing. CAD output is often in the form of electronic files for print, machining, or other manufacturing operations.
Computer-aided design is used in many fields. Its use in designing electronic systems is known as electronic design automation, or EDA. In mechanical design it is known as mechanical design automation (MDA) or computer-aided design (CAD), which includes the process of creating a technical drawing with the use of computer software.
CAD software for mechanical design uses either vector-based graphics to depict the objects of traditional drafting, or may also produce raster graphics showing the overall appearance of designed objects. However, it involves more than just shapes. As in the manual drafting of technical and engineering drawings, the output of CAD must convey information, such as materials, processes, dimensions, and tolerances, according to application-specific conventions.
CAD may be used to design curves and figures in two-dimensional (2D) space; or curves, surfaces, and solids in three-dimensional (3D) space.
CAD is an important industrial art extensively used in many applications, including automotive, shipbuilding, and aerospace industries, industrial and architectural design, prosthetics, and many more. CAD is also widely used to produce computer animation for special effects in movies, advertising and technical manuals, often called DCC digital content creation. The modern ubiquity and power of computers means that even perfume bottles and shampoo dispensers are designed using techniques unheard of by engineers of the 1960s. Because of its enormous economic importance, CAD has been a major driving force for research in computational geometry, computer graphics (both hardware and software), and discrete differential geometry.
The design of geometric models for object shapes, in particular, is occasionally called computer-aided geometric design (CAGD).""
""Physics "",,,,""Physics (from Ancient Greek: φυσική (ἐπιστήμη) phusikḗ (epistḗmē) "knowledge of nature", from φύσις phúsis "nature") is the natural science that involves the study of matter and its motion through space and time, along with related concepts such as energy and force. One of the most fundamental scientific disciplines, the main goal of physics is to understand how the universe behaves.
Physics is one of the oldest academic disciplines, perhaps the oldest through its inclusion of astronomy. Over the last two millennia, physics was a part of natural philosophy along with chemistry, biology, and certain branches of mathematics, but during the scientific revolution in the 17th century, the natural sciences emerged as unique research programs in their own right. Physics intersects with many interdisciplinary areas of research, such as biophysics and quantum chemistry, and the boundaries of physics are not rigidly defined. New ideas in physics often explain the fundamental mechanisms of other sciences while opening new avenues of research in areas such as mathematics and philosophy.
Physics also makes significant contributions through advances in new technologies that arise from theoretical breakthroughs. For example, advances in the understanding of electromagnetism or nuclear physics led directly to the development of new products that have dramatically transformed modern-day society, such as television, computers, domestic appliances, and nuclear weapons; advances in thermodynamics led to the development of industrialization, and advances in mechanics inspired the development of calculus.""
""Mathematics and statistics "",,,,""Mathematics (from Greek μάθημα máthēma, “knowledge, study, learning”) is the study of topics such as quantity (numbers), structure, space, and change. There is a range of views among mathematicians and philosophers as to the exact scope and definition of mathematics.
Mathematicians seek out patterns and use them to formulate new conjectures. Mathematicians resolve the truth or falsity of conjectures by mathematical proof. When mathematical structures are good models of real phenomena, then mathematical reasoning can provide insight or predictions about nature. Through the use of abstraction and logic, mathematics developed from counting, calculation, measurement, and the systematic study of the shapes and motions of physical objects. Practical mathematics has been a human activity for as far back as written records exist. The research required to solve mathematical problems can take years or even centuries of sustained inquiry.
Rigorous arguments first appeared in Greek mathematics, most notably in Euclid's Elements. Since the pioneering work of Giuseppe Peano (1858–1932), David Hilbert (1862–1943), and others on axiomatic systems in the late 19th century, it has become customary to view mathematical research as establishing truth by rigorous deduction from appropriately chosen axioms and definitions. Mathematics developed at a relatively slow pace until the Renaissance, when mathematical innovations interacting with new scientific discoveries led to a rapid increase in the rate of mathematical discovery that has continued to the present day.
Galileo Galilei (1564–1642) said, "The universe cannot be read until we have learned the language and become familiar with the characters in which it is written. It is written in mathematical language, and the letters are triangles, circles and other geometrical figures, without which means it is humanly impossible to comprehend a single word. Without these, one is wandering about in a dark labyrinth." Carl Friedrich Gauss (1777–1855) referred to mathematics as "the Queen of the Sciences". Benjamin Peirce (1809–1880) called mathematics "the science that draws necessary conclusions". David Hilbert said of mathematics: "We are not speaking here of arbitrariness in any sense. Mathematics is not like a game whose tasks are determined by arbitrarily stipulated rules. Rather, it is a conceptual system possessing internal necessity that can only be so and by no means otherwise." Albert Einstein (1879–1955) stated that "as far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality."
Mathematics is essential in many fields, including natural science, engineering, medicine, finance and the social sciences. Applied mathematics has led to entirely new mathematical disciplines, such as statistics and game theory. Mathematicians also engage in pure mathematics, or mathematics for its own sake, without having any application in mind. There is no clear line separating pure and applied mathematics, and practical applications for what began as pure mathematics are often discovered.""
""Avionics "",,,,""Avionics are the electronic systems used on aircraft, artificial satellites, and spacecraft. Avionic systems include communications, navigation, the display and management of multiple systems, and the hundreds of systems that are fitted to aircraft to perform individual functions. These can be as simple as a searchlight for a police helicopter or as complicated as the tactical system for an airborne early warning platform. The term avionics is a portmanteau of the words aviation and electronics.""
""Internet telephony "",,,,""Voice over IP (VoIP) is a methodology and group of technologies for the delivery of voice communications and multimedia sessions over Internet Protocol (IP) networks, such as the Internet. Other terms commonly associated with VoIP are IP telephony, Internet telephony, broadband telephony, and broadband phone service.
The term Internet telephony specifically refers to the provisioning of communications services (voice, fax, SMS, voice-messaging) over the public Internet, rather than via the public switched telephone network (PSTN). The steps and principles involved in originating VoIP telephone calls are similar to traditional digital telephony and involve signaling, channel setup, digitization of the analog voice signals, and encoding. Instead of being transmitted over a circuit-switched network, however, the digital information is packetized, and transmission occurs as IP packets over a packet-switched network. Such transmission entails careful considerations about resource management different from time-division multiplexing (TDM) networks.
Early providers of voice-over-IP services offered business models and technical solutions that mirrored the architecture of the legacy telephone network. Second-generation providers, such as Skype, have built closed networks for private user bases, offering the benefit of free calls and convenience while potentially charging for access to other communication networks, such as the PSTN. This has limited the freedom of users to mix-and-match third-party hardware and software. Third-generation providers, such as Google Talk, have adopted the concept of federated VoIP—which is a departure from the architecture of the legacy networks. These solutions typically allow dynamic interconnection between users on any two domains on the Internet when a user wishes to place a call.
VoIP systems employ session control and signaling protocols to control the signaling, set-up, and tear-down of calls. They transport audio streams over IP networks using special media delivery protocols that encode voice, audio, video with audio codecs, and video codecs as Digital audio by streaming media. Various codecs exist that optimize the media stream based on application requirements and network bandwidth; some implementations rely on narrowband and compressed speech, while others support high fidelity stereo codecs. Some popular codecs include μ-law and a-law versions of G.711, G.722, a popular open source voice codec known as iLBC, a codec that only uses 8 kbit/s each way called G.729, and many others.
VoIP is available on many smartphones, personal computers, and on Internet access devices. Calls and SMS text messages may be sent over 3G/4G or Wi-Fi.

""
""Molecular sequence analysis "",,,,""6-phosphofructo-2-kinase/fructose-2,6-biphosphatase 1 is an enzyme that in humans is encoded by the PFKFB1 gene.
This gene encodes a member of the family of bifunctional 6-phosphofructo-2-kinase:fructose-2,6-biphosphatase enzymes. The enzyme forms a homodimer that catalyzes both the synthesis and degradation of fructose-2,6-biphosphate using independent catalytic domains. Fructose-2,6-biphosphate is an activator of the glycolysis pathway and an inhibitor of the gluconeogenesis pathway. Consequently, regulating fructose-2,6-biphosphate levels through the activity of this enzyme is thought to regulate glucose homeostasis.""
""Molecular evolution "",,,,""Molecular evolution is the process of change in the sequence composition of cellular molecules such as DNA, RNA, and proteins across generations. The field of molecular evolution uses principles of evolutionary biology and population genetics to explain patterns in these changes. Major topics in molecular evolution concern the rates and impacts of single nucleotide changes, neutral evolution vs. natural selection, origins of new genes, the genetic nature of complex traits, the genetic basis of speciation, evolution of development, and ways that evolutionary forces influence genomic and phenotypic changes.""
""Computational transcriptomics "",,,,""Systems biology is the computational and mathematical modeling of complex biological systems. An emerging engineering approach applied to biological scientific research, systems biology is a biology-based inter-disciplinary field of study that focuses on complex interactions within biological systems, using a holistic approach (holism instead of the more traditional reductionism) to biological research. Particularly from year 2000 onwards, the concept has been used widely in the biosciences in a variety of contexts. For example, the Human Genome Project is an example of applied systems thinking in biology which has led to new, collaborative ways of working on problems in the biological field of genetics. One of the outreaching aims of systems biology is to model and discover emergent properties, properties of cells, tissues and organisms functioning as a system whose theoretical description is only possible using techniques which fall under the remit of systems biology. These typically involve metabolic networks or cell signaling networks.""
""Biological networks "",,,,""A biological network is any network that applies to biological systems. A network is any system with sub-units that are linked into a whole, such as species units linked into a whole food web. Biological networks provide a mathematical analysis of connections found in ecological, evolutionary, and physiological studies, such as neural networks. The analysis of biological networks with respect to human diseases has led to the field of network medicine.""
""Sequencing and genotyping technologies "",,,,""The interdisciplinary research field of Computational and Statistical Genetics uses the latest approaches in genomics, quantitative genetics, computational sciences, bioinformatics and statistics to develop and apply computationally efficient and statistically robust methods to sort through increasingly rich and massive genome wide data sets to identify complex genetic patterns, gene functionalities and interactions, disease and phenotype associations involving the genomes of various organisms. This field is also often referred to as computational genomics. This is an important discipline within the umbrella field computational biology.""
""Computational proteomics "",,,,""LabKey Server is free, open source software available for scientists to integrate, analyze, and share biomedical research data. The platform provides a secure data repository that allows web-based querying, reporting, and collaborating across a range of data sources. Specific scientific applications and workflows can be added on top of the basic platform and leverage a data processing pipeline.""
""Molecular structural biology "",,,,""Molecular biology /məˈlɛkjʊlər/ concerns the molecular basis of biological activity between biomolecules in the various systems of a cell, including the interactions between DNA, RNA and proteins and their biosynthesis, as well as the regulation of these interactions. Writing in Nature in 1961, William Astbury described molecular biology as:

"...not so much a technique as an approach, an approach from the viewpoint of the so-called basic sciences with the leading idea of searching below the large-scale manifestations of classical biology for the corresponding molecular plan. It is concerned particularly with the forms of biological molecules and [...] is predominantly three-dimensional and structural—which does not mean, however, that it is merely a refinement of morphology. It must at the same time inquire into genesis and function."""
""Computational genomics "",,,,""Computational genomics (often referred to as Computational Genetics) refers to the use of computational and statistical analysis to decipher biology from genome sequences and related data, including both DNA and RNA sequence as well as other "post-genomic" data (i.e., experimental data obtained with technologies that require the genome sequence, such as genomic DNA microarrays). These, in combination with computational and statistical approaches to understanding the function of the genes and statistical association analysis, this field is also often referred to as Computational and Statistical Genetics/genomics. As such, computational genomics may be regarded as a subset of bioinformatics and computational biology, but with a focus on using whole genomes (rather than individual genes) to understand the principles of how the DNA of a species controls its biology at the molecular level and beyond. With the current abundance of massive biological datasets, computational studies have become one of the most important means to biological discovery.

""
""Computational genomics "",,,,""Computational genomics (often referred to as Computational Genetics) refers to the use of computational and statistical analysis to decipher biology from genome sequences and related data, including both DNA and RNA sequence as well as other "post-genomic" data (i.e., experimental data obtained with technologies that require the genome sequence, such as genomic DNA microarrays). These, in combination with computational and statistical approaches to understanding the function of the genes and statistical association analysis, this field is also often referred to as Computational and Statistical Genetics/genomics. As such, computational genomics may be regarded as a subset of bioinformatics and computational biology, but with a focus on using whole genomes (rather than individual genes) to understand the principles of how the DNA of a species controls its biology at the molecular level and beyond. With the current abundance of massive biological datasets, computational studies have become one of the most important means to biological discovery.

""
""Systems biology "",,,,""Systems biology is the computational and mathematical modeling of complex biological systems. An emerging engineering approach applied to biological scientific research, systems biology is a biology-based inter-disciplinary field of study that focuses on complex interactions within biological systems, using a holistic approach (holism instead of the more traditional reductionism) to biological research. Particularly from year 2000 onwards, the concept has been used widely in the biosciences in a variety of contexts. For example, the Human Genome Project is an example of applied systems thinking in biology which has led to new, collaborative ways of working on problems in the biological field of genetics. One of the outreaching aims of systems biology is to model and discover emergent properties, properties of cells, tissues and organisms functioning as a system whose theoretical description is only possible using techniques which fall under the remit of systems biology. These typically involve metabolic networks or cell signaling networks.""
""Consumer health "",,,,""Consumer Health Informatics (CHI) is a sub-branch of Health Informatics helps bridge the gap between patients and health resources. It is defined by the American Medical Informatics Association as "the field devoted to informatics from multiple consumer or patient views". The Consumer Health Informatics Working Group (CHIWG) of the International Medical Informatics Association (IMIA) define it as “the use of modern computers and telecommunications to support consumers in obtaining information, analyzing unique health care needs and helping them make decisions about their own health"
CHI includes patient-focused informatics, health literacy, and consumer education. The focus of this field is to allow consumers to manage their own health, through the use of internet-based strategies and resources with consumer-friendly language. Currently, CHI stands at a crossroads between various healthcare related fields such as nursing, public health, health promotion, and health education
The Kaiser model is an example of allowing patients to remotely communicate with their physicians or other healthcare professionals.
Consumer Health Informatics include technologies focused on patients as the primary users to health information.
Consumer Health Informatics includes: Information Resources, Communications, Remote Monitoring, Videoconferencing, and Telepresence.
Medical informatics has expanded rapidly over the past couple of years. After decades of development of information systems designed primarily for physicians and other healthcare managers and professionals, there is an increasing interest in reaching consumers and patients directly through computers and telecommunications systems. Consumer health informatics is the branch of medical informatics that analyses consumers' needs for information; studies and implements methods of making information accessible to consumers; and models and integrates consumers' preferences into medical information systems. Consumer informatics stands at the crossroads of other disciplines, such as nursing informatics, public health, health promotion, health education, library science, and communication science, and is perhaps the most challenging and rapidly expanding field in medical informatics; it is paving the way for health care in the information age.""
""Health informatics "",,,,""Health informatics (also called health care informatics, healthcare informatics, medical informatics, nursing informatics, clinical informatics, or biomedical informatics) is informatics in health care. It is a multidisciplinary field that uses health information technology (HIT) to improve health care via any combination of higher quality, higher efficiency (spurring lower cost and thus greater availability), and new opportunities. The disciplines involved include information science, computer science, social science, behavioral science, management science, and others. The NLM defines health informatics as "the interdisciplinary study of the design, development, adoption and application of IT-based innovations in healthcare services delivery, management and planning." It deals with the resources, devices, and methods required to optimize the acquisition, storage, retrieval, and use of information in health and biomedicine. Health informatics tools include amongst others computers, clinical guidelines, formal medical terminologies, and information and communication systems. It is applied to the areas of nursing, clinical care, dentistry, pharmacy, public health, occupational therapy, physical therapy and (bio)medical research, and alternative medicine. All of which are designed to improve the overall of effectiveness of patient care delivery by ensuring that the data generated is of a high quality e.g. an mHealth based early warning scorecard.
The international standards on the subject are covered by ICS 35.240.80 in which ISO 27799:2008 is one of the core components.
Molecular bioinformatics and clinical informatics have converged into the field of translational bioinformatics.

^ "NLM Definition". NLM. 
^ O’donoghue, John, and John Herbert. "Data management within mHealth environments: Patient sensors, mobile devices, and databases." Journal of Data and Information Quality (JDIQ) 4.1 (2012): 5.
^ Mettler T, Raptis DA (2012). "What constitutes the field of health information systems? Fostering a systematic framework and research agenda". Health Informatics Journal 18 (2): 147–56. doi:10.1177/1460458212452496. PMID 22733682. 
^ Popularity of usage of software in homeopathy is shown in example video of homeopathic repertorisation: Shilpa Bhouraskar, Working quick acute cases on Homeopathic Software (YouTube)
^ O’Donoghue, John, et al. "Modified early warning scorecard: the role of data/information quality within the decision making process." Electronic Journal Information Systems Evaluation Volume 14.1 (2011).
^ "35.240.80: IT applications in health care technology". ISO. Retrieved 2008-06-15. 
^ Fraser, Ross. "ISO 27799: Security management in health using ISO/IEC 17799" (PDF). Retrieved 2008-06-15.""
""Bioinformatics "",,,,""Bioinformatics /ˌbaɪ.oʊˌɪnfərˈmætɪks/ is an interdisciplinary field that develops methods and software tools for understanding biological data. As an interdisciplinary field of science, bioinformatics combines computer science, statistics, mathematics, and engineering to analyze and interpret biological data. Bioinformatics has been used for in silico analyses of biological queries using mathematical and statistical techniques.
Bioinformatics is both an umbrella term for the body of biological studies that use computer programming as part of their methodology, as well as a reference to specific analysis "pipelines" that are repeatedly used, particularly in the field of genomics. Common uses of bioinformatics include the identification of candidate genes and nucleotides (SNPs). Often, such identification is made with the aim of better understanding the genetic basis of disease, unique adaptations, desirable properties (esp. in agricultural species), or differences between populations. In a less formal way, bioinformatics also tries to understand the organisational principles within nucleic acid and protein sequences.""
""Metabolomics / metabonomics "",,,,""Metabolomics is the scientific study of chemical processes involving metabolites. Specifically, metabolomics is the "systematic study of the unique chemical fingerprints that specific cellular processes leave behind", the study of their small-molecule metabolite profiles. The metabolome represents the collection of all metabolites in a biological cell, tissue, organ or organism, which are the end products of cellular processes. mRNA gene expression data and proteomic analyses reveal the set of gene products being produced in the cell, data that represents one aspect of cellular function. Conversely, metabolic profiling can give an instantaneous snapshot of the physiology of that cell. One of the challenges of systems biology and functional genomics is to integrate proteomic, transcriptomic, and metabolomic information to provide a better understanding of cellular biology.""
""Population genetics "",,,,""Population genetics is the study of the distribution and change in frequency of alleles within populations, and as such it sits firmly within the field of evolutionary biology. The main processes of evolution are natural selection, genetic drift, gene flow, mutation, and genetic recombination and they form an integral part of the theory that underpins population genetics. Studies in this branch of biology examine such phenomena as adaptation, speciation, population subdivision, and population structure.
Population genetics was a vital ingredient in the emergence of the modern evolutionary synthesis. Its primary founders were Sewall Wright, J. B. S. Haldane and Ronald Fisher, who also laid the foundations for the related discipline of quantitative genetics.
Traditionally a highly mathematical discipline, modern population genetics encompasses theoretical, lab and field work. Computational approaches, often utilising coalescent theory, have played a central role since the 1980s.""
""Computational proteomics "",,,,""LabKey Server is free, open source software available for scientists to integrate, analyze, and share biomedical research data. The platform provides a secure data repository that allows web-based querying, reporting, and collaborating across a range of data sources. Specific scientific applications and workflows can be added on top of the basic platform and leverage a data processing pipeline.""
""Transcriptomics "",,,,""The transcriptome is the set of all messenger RNA molecules in one cell or a population of cells. It differs from the exome in that it includes only those RNA molecules found in a specified cell population, and usually includes the amount or concentration of each RNA molecule in addition to the molecular identities.""
""Ethnography "",,,,""Ethnography (from Greek ἔθνος ethnos "folk, people, nation" and γράφω grapho "I write") is the systematic study of people and cultures. It is designed to explore cultural phenomena where the researcher observes society from the point of view of the subject of the study. An ethnography is a means to represent graphically and in writing the culture of a group. The word can thus be said to have a "double meaning," which partly depends on whether it is used as a count noun or uncountably. The resulting field study or a case report reflects the knowledge and the system of meanings in the lives of a cultural group.
Ethnography, as the presentation of empirical data on human societies and cultures, was pioneered in the biological, social, and cultural branches of anthropology, but it has also become popular in the social sciences in general—sociology, communication studies, history—wherever people study ethnic groups, formations, compositions, resettlements, social welfare characteristics, materiality, spirituality, and a people's ethnogenesis. The typical ethnography is a holistic study and so includes a brief history, and an analysis of the terrain, the climate, and the habitat. In all cases it should be reflexive, make a substantial contribution toward the understanding of the social life of humans, have an aesthetic impact on the reader, and express a credible reality. An ethnography records all observed behavior and describes all symbol-meaning relations, using concepts that avoid causal explanations.""
""Law "",,,,""Law is a system of rules that are enforced through social institutions to govern behavior. Laws can be made by a collective legislature or by a single legislator, resulting in statutes, by the executive through decrees and regulations, or by judges through binding precedent, normally in common law jurisdictions. Private individuals can create legally binding contracts, including arbitration agreements that may elect to accept alternative arbitration to the normal court process. The formation of laws themselves may be influenced by a constitution, written or tacit, and the rights encoded therein. The law shapes politics, economics, history and society in various ways and serves as a mediator of relations between people.
A general distinction can be made between (a) civil law jurisdictions (including Catholic canon law and socialist law), in which the legislature or other central body codifies and consolidates their laws, and (b) common law systems, where judge-made precedent is accepted as binding law. Historically, religious laws played a significant role even in settling of secular matters, which is still the case in some religious communities, particularly Jewish, and some countries, particularly Islamic. Islamic Sharia law is the world's most widely used religious law.
The adjudication of the law is generally divided into two main areas referred to as (i) Criminal law and (ii) Civil law. Criminal law deals with conduct that is considered harmful to social order and in which the guilty party may be imprisoned or fined. Civil law (not to be confused with civil law jurisdictions above) deals with the resolution of lawsuits (disputes) between individuals or organizations. These resolutions seek to provide a legal remedy (often monetary damages) to the winning litigant. Under civil law, the following specialties, among others, exist: Contract law regulates everything from buying a bus ticket to trading on derivatives markets. Property law regulates the transfer and title of personal property and real property. Trust law applies to assets held for investment and financial security. Tort law allows claims for compensation if a person's property is harmed. Constitutional law provides a framework for the creation of law, the protection of human rights and the election of political representatives. Administrative law governs what executive branch agencies may and may not do, procedures that they must follow to do it, and judicial review when a member of the public is harmed by an agency action. International law governs affairs between sovereign states in activities ranging from trade to military action. To implement and enforce the law and provide services to the public by public servants, a government's bureaucracy, military, and police are vital. While all these organs of the state are creatures created and bound by law, an independent legal profession and a vibrant civil society inform and support their progress.
Law provides a rich source of scholarly inquiry into legal history, philosophy, economic analysis and sociology. Law also raises important and complex issues concerning equality, fairness, and justice. There is an old saying that 'all are equal before the law', although Jonathan Swift argued that 'Laws are like cobwebs, which may catch small flies, but let wasps and hornets break through.' In 1894, the author Anatole France said sarcastically, "In its majestic equality, the law forbids rich and poor alike to sleep under bridges, beg in the streets, and steal loaves of bread." Writing in 350 BC, the Greek philosopher Aristotle declared, "The rule of law is better than the rule of any individual." Mikhail Bakunin said: "All law has for its object to confirm and exalt into a system the exploitation of the workers by a ruling class". Cicero said "more law, less justice". Marxist doctrine asserts that law will not be required once the state has withered away.
^ Luban, Law's Blindfold, 23.
^ Robertson, Crimes against humanity, 90.
^ "What is sharia law?". brightknowledge.org. 
^ (France, The Red Lily, Chapter VII). The original French is: "La loi, dans un grand souci d'égalité, interdit aux riches comme aux pauvres de coucher sous les ponts, de mendier dans les rues et de voler du pain.".
^ Aristotle. Politics, Book 3#3:16. n.b. This translation reads, "it is more proper that law should govern than any one of the citizens"
^ Stewart and Burgess. Collins Dictionary of Law. HarperCollins Publishers. 1996. ISBN 0-00-470009-0. Page 229.
^ Cicero, De Officiis, I, 10, 33. Latin: "summum ius, summa iniuria". For the translation given above, see, for example, Adler and Doren, Great Treasury of Western Thought, Bowker, 1977, p 851.
^ "Withering away of the state". Palgrave McMillan Dictionary of Political Thought. 2007.""
""Psychology "",,,,""Psychology is the study of behavior and mind, embracing all aspects of human experience. It is an academic discipline and an applied science which seeks to understand individuals and groups by establishing general principles and researching specific cases. In this field, a professional practitioner or researcher is called a psychologist and can be classified as a social, behavioral, or cognitive scientist. Psychologists attempt to understand the role of mental functions in individual and social behavior, while also exploring the physiological and biological processes that underlie cognitive functions and behaviors.
Psychologists explore concepts such as perception, cognition, attention, emotion, intelligence, phenomenology, motivation, brain functioning, personality, behavior, and interpersonal relationships, including psychological resilience, family resilience, and other areas. Psychologists of diverse orientations also consider the unconscious mind. Psychologists employ empirical methods to infer causal and correlational relationships between psychosocial variables. In addition, or in opposition, to employing empirical and deductive methods, some—especially clinical and counseling psychologists—at times rely upon symbolic interpretation and other inductive techniques. Psychology has been described as a "hub science", with psychological findings linking to research and perspectives from the social sciences, natural sciences, medicine, humanities, and philosophy.
While psychological knowledge is often applied to the assessment and treatment of mental health problems, it is also directed towards understanding and solving problems in several spheres of human activity. By many accounts psychology ultimately aims to benefit society. The majority of psychologists are involved in some kind of therapeutic role, practicing in clinical, counseling, or school settings. Many do scientific research on a wide range of topics related to mental processes and behavior, and typically work in university psychology departments or teach in other academic settings (e.g., medical schools, hospitals). Some are employed in industrial and organizational settings, or in other areas such as human development and aging, sports, health, and the media, as well as in forensic investigation and other aspects of law.""
""Economics "",,,,""Economics is the social science that describes the factors that determine the production, distribution and consumption of goods and services.
The term economics comes from the Ancient Greek οἰκονομία from οἶκος (oikos, "house") and νόμος (nomos, "custom" or "law"), hence "rules of the house (hold for good management)". 'Political economy' was the earlier name for the subject, but economists in the late 19th century suggested "economics" as a shorter term for "economic science" to establish itself as a separate discipline outside of political science and other social sciences.
Economics focuses on the behavior and interactions of economic agents and how economies work. Consistent with this focus, primary textbooks often distinguish between microeconomics and macroeconomics. Microeconomics examines the behavior of basic elements in the economy, including individual agents and markets, their interactions, and the outcomes of interactions. Individual agents may include, for example, households, firms, buyers, and sellers. Macroeconomics analyzes the entire economy (meaning aggregated production, consumption, savings, and investment) and issues affecting it, including unemployment of resources (labor, capital, and land), inflation, economic growth, and the public policies that address these issues (monetary, fiscal, and other policies).
Other broad distinctions within economics include those between positive economics, describing "what is," and normative economics, advocating "what ought to be"; between economic theory and applied economics; between rational and behavioral economics; and between mainstream economics (more "orthodox" and dealing with the "rationality-individualism-equilibrium nexus") and heterodox economics (more "radical" and dealing with the "institutions-history-social structure nexus").
Besides the traditional concern in production, distribution, and consumption in an economy, economic analysis may be applied throughout society, as in business, finance, health care, and government. Economic analyses may also be applied to such diverse subjects as crime, education, the family, law, politics, religion, social institutions, war, science, and the environment. Education, for example, requires time, effort, and expenses, plus the foregone income and experience, yet these losses can be weighted against future benefits education may bring to the agent or the economy. At the turn of the 21st century, the expanding domain of economics in the social sciences has been described as economic imperialism.

The ultimate goal of economics is to improve the living conditions of people in their everyday life.""
""Sociology "",,,,""Sociology is the study of social behavior or society, including its origins, development, organization, networks, and institutions. It is a social science that uses various methods of empirical investigation and critical analysis to develop a body of knowledge about social order, disorder, and change. Many sociologists aim to conduct research that may be applied directly to social policy and welfare, while others focus primarily on refining the theoretical understanding of social processes. Subject matter ranges from the micro level of individual agency and interaction to the macro level of systems and the social structure.
The traditional focuses of sociology include social stratification, social class, social mobility, religion, secularization, law, sexuality and deviance. As all spheres of human activity are affected by the interplay between social structure and individual agency, sociology has gradually expanded its focus to further subjects, such as health, medical, military and penal institutions, the Internet, education, and the role of social activity in the development of scientific knowledge.
The range of social scientific methods has also expanded. Social researchers draw upon a variety of qualitative and quantitative techniques. The linguistic and cultural turns of the mid-twentieth century led to increasingly interpretative, hermeneutic, and philosophic approaches towards the analysis of society. Conversely, the end of the 1990s and the beginning of 2000s have seen the rise of new analytically, mathematically and computationally rigorous techniques, such as agent-based modelling and social network analysis.
Social research informs politicians and policy makers, educators, planners, lawmakers, administrators, developers, business magnates, managers, social workers, non-governmental organizations, non-profit organizations, and people interested in resolving social issues in general. There is often a great deal of crossover between social research, market research, and other statistical fields.
^ sociology. (n.d.). The American Heritage Science Dictionary the name was created by dewight D Eisenhower . Retrieved 13 July 2013, from Dictionary.com website: http://dictionary.reference.com/browse/sociology
^ http://www.dummies.com/how-to/content/sociology-for-dummies-cheat-sheet.html
^ http://www.pasadena.edu/studentservices/counseling/graduation/documents/aa-t_sociology.pdf
^ https://www.colgate.edu/docs/default-source/default-document-library/sociology-a-21st-century-major.pdf?sfvrsn=0
^ http://www.asanet.org/introtosociology/Documents/Field%20of%20sociology033108.htm#whatissociology
^ 
^ 
^ 
^ 
^ 
^ Lynn R. Kahle, Pierre Valette-Florence (2012). Marketplace Lifestyles in an Age of Social Media. New York: M.E. Sharpe, Inc. ISBN 978-0-7656-2561-8.""
""Evidence collection, storage and analysis "",,,,""Digital forensics (sometimes known as digital forensic science) is a branch of forensic science encompassing the recovery and investigation of material found in digital devices, often in relation to computer crime. The term digital forensics was originally used as a synonym for computer forensics but has expanded to cover investigation of all devices capable of storing digital data. With roots in the personal computing revolution of the late 1970s and early '80s, the discipline evolved in a haphazard manner during the 1990s, and it was not until the early 21st century that national policies emerged.
Digital forensics investigations have a variety of applications. The most common is to support or refute a hypothesis before criminal or civil (as part of the electronic discovery process) courts. Forensics may also feature in the private sector; such as during internal corporate investigations or intrusion investigation (a specialist probe into the nature and extent of an unauthorized network intrusion).
The technical aspect of an investigation is divided into several sub-branches, relating to the type of digital devices involved; computer forensics, network forensics, forensic data analysis and mobile device forensics. The typical forensic process encompasses the seizure, forensic imaging (acquisition) and analysis of digital media and the production of a report into collected evidence.
As well as identifying direct evidence of a crime, digital forensics can be used to attribute evidence to specific suspects, confirm alibis or statements, determine intent, identify sources (for example, in copyright cases), or authenticate documents. Investigations are much broader in scope than other areas of forensic analysis (where the usual aim is to provide answers to a series of simpler questions) often involving complex time-lines or hypotheses.""
""Network forensics "",,,,""Network forensics is a sub-branch of digital forensics relating to the monitoring and analysis of computer network traffic for the purposes of information gathering, legal evidence, or intrusion detection. Unlike other areas of digital forensics, network investigations deal with volatile and dynamic information. Network traffic is transmitted and then lost, so network forensics is often a pro-active investigation.
Network forensics generally has two uses. The first, relating to security, involves monitoring a network for anomalous traffic and identifying intrusions. An attacker might be able to erase all log files on a compromised host; network-based evidence might therefore be the only evidence available for forensic analysis. The second form relates to law enforcement. In this case analysis of captured network traffic can include tasks such as reassembling transferred files, searching for keywords and parsing human communication such as emails or chat sessions.
Two systems are commonly used to collect network data; a brute force "catch it as you can" and a more intelligent "stop look listen" method.""
""System forensics "",,,,""Computer forensics (sometimes known as computer forensic science) is a branch of digital forensic science pertaining to evidence found in computers and digital storage media. The goal of computer forensics is to examine digital media in a forensically sound manner with the aim of identifying, preserving, recovering, analyzing and presenting facts and opinions about the digital information.
Although it is most often associated with the investigation of a wide variety of computer crime, computer forensics may also be used in civil proceedings. The discipline involves similar techniques and principles to data recovery, but with additional guidelines and practices designed to create a legal audit trail.
Evidence from computer forensics investigations is usually subjected to the same guidelines and practices of other digital evidence. It has been used in a number of high-profile cases and is becoming widely accepted as reliable within U.S. and European court systems.""
""Data recovery "",,,,""In computing, data recovery is a process of salvaging inaccessible data from corrupted or damaged secondary storage, removable media or files, when the data they store cannot be accessed in a normal way. The data is most often salvaged from storage media such as internal or external hard disk drives (HDDs), solid-state drives (SSDs), USB flash drives, magnetic tapes, CDs, DVDs, RAID subsystems, and other electronic devices. Recovery may be required due to physical damage to the storage device or logical damage to the file system that prevents it from being mounted by the host operating system (OS).
The most common data recovery scenario involves an operating system failure, malfunction of a storage device, accidental damage or deletion, etc. (typically, on a single-drive, single-partition, single-OS system), in which case the goal is simply to copy all wanted files to another drive. This can be easily accomplished using a Live CD, many of which provide a means to mount the system drive and backup drives or removable media, and to move the files from the system drive to the backup media with a file manager or optical disc authoring software. Such cases can often be mitigated by disk partitioning and consistently storing valuable data files (or copies of them) on a different partition from the replaceable OS system files.
Another scenario involves a drive-level failure, such as a compromised file system or drive partition, or a hard disk drive failure. In any of these cases, the data cannot be easily read. Depending on the situation, solutions involve repairing the file system, partition table or master boot record, or drive recovery techniques ranging from software-based recovery of corrupted data, hardware- and software-based recovery of damaged service areas (also known as the hard disk drive's "firmware"), to hardware replacement on a physically damaged drive. If a drive recovery is necessary, the drive itself has typically failed permanently, and the focus is rather on a one-time recovery, salvaging whatever data can be read.
In a third scenario, files have been "deleted" from a storage medium. Typically, the contents of deleted files are not removed immediately from the drive; instead, references to them in the directory structure are removed, and the space they occupy is made available for later overwriting. For the end users, deleted files are not discoverable through a standard file manager, but that data still technically exists on the drive. In the meantime, the original file contents remain, often in a number of disconnected fragments, and may be recoverable.
The term "data recovery" is also used in the context of forensic applications or espionage, where data which have been encrypted or hidden, rather than damaged, are recovered.""
""Fine arts "",,,,""In Western European academic traditions, fine art is art developed primarily for aesthetics or beauty, distinguishing it from applied art that also has to serve some practical function.
Historically, the five main fine arts were painting, sculpture, architecture, music and poetry, with performing arts including theatre and dance. Today, the fine arts commonly include additional forms, such as film, photography, conceptual art, and printmaking. However, in some institutes of learning or in museums, fine art and frequently the term fine arts (pl.) as well, are associated exclusively with visual art forms.
One definition of fine art is "a visual art considered to have been created primarily for aesthetic and intellectual purposes and judged for its beauty and meaningfulness, specifically, painting, sculpture, drawing, watercolor, graphics, and architecture." In that sense, there are conceptual differences between the fine arts and the applied arts. As originally conceived, and as understood for much of the modern era, the perception of aesthetic qualities required a refined judgment usually referred to as having good taste, which differentiated fine art from popular art and entertainment. However, in the Postmodern era, the value of good taste is disappearing, to the point that having bad taste has become synonymous with being avant-garde. The term "fine art" is now rarely found in art history, but remains common in the art trade and as a title for university departments and degrees, even if rarely used in teaching.
The word "fine" does not so much denote the quality of the artwork in question, but the purity of the discipline according to traditional Western European canons. This definition originally excluded the applied or decorative arts, and the products of what were regarded as crafts. In contemporary practice these distinctions and restrictions have become essentially meaningless, as the concept or intention of the artist is given primacy, regardless of the means through which this is expressed.

""
""Performing arts "",,,,""Performing arts are art forms in which artists use their voices and/or the movements of their bodies, often in relation to other objects, to convey artistic expression—as opposed to, for example, purely visual arts, in which artists use paint/canvas or various materials to create physical or static art objects. Performing arts include a variety of disciplines but all are intended to be performed in front of a live audience.""
""Computer-aided design "",,,,""Computer-aided design (CAD) is the use of computer systems to aid in the creation, modification, analysis, or optimization of a design. CAD software is used to increase the productivity of the designer, improve the quality of design, improve communications through documentation, and to create a database for manufacturing. CAD output is often in the form of electronic files for print, machining, or other manufacturing operations.
Computer-aided design is used in many fields. Its use in designing electronic systems is known as electronic design automation, or EDA. In mechanical design it is known as mechanical design automation (MDA) or computer-aided design (CAD), which includes the process of creating a technical drawing with the use of computer software.
CAD software for mechanical design uses either vector-based graphics to depict the objects of traditional drafting, or may also produce raster graphics showing the overall appearance of designed objects. However, it involves more than just shapes. As in the manual drafting of technical and engineering drawings, the output of CAD must convey information, such as materials, processes, dimensions, and tolerances, according to application-specific conventions.
CAD may be used to design curves and figures in two-dimensional (2D) space; or curves, surfaces, and solids in three-dimensional (3D) space.
CAD is an important industrial art extensively used in many applications, including automotive, shipbuilding, and aerospace industries, industrial and architectural design, prosthetics, and many more. CAD is also widely used to produce computer animation for special effects in movies, advertising and technical manuals, often called DCC digital content creation. The modern ubiquity and power of computers means that even perfume bottles and shampoo dispensers are designed using techniques unheard of by engineers of the 1960s. Because of its enormous economic importance, CAD has been a major driving force for research in computational geometry, computer graphics (both hardware and software), and discrete differential geometry.
The design of geometric models for object shapes, in particular, is occasionally called computer-aided geometric design (CAGD).""
""Language translation "",,,,""Translation is the communication of the meaning of a source-language text by means of an equivalent target-language text. While interpreting—the facilitating of oral or sign-language communication between users of different languages—antedates writing, translation began only after the appearance of written literature. There exist partial translations of the Sumerian Epic of Gilgamesh (ca. 2000 BCE) into Southwest Asian languages of the second millennium BCE.
Translators always risk inappropriate spill-over of source-language idiom and usage into the target-language translation. On the other hand, spill-overs have imported useful source-language calques and loanwords that have enriched the target languages. Indeed, translators have helped substantially to shape the languages into which they have translated.
Due to the demands of business documentation consequent to the Industrial Revolution that began in the mid-18th century, some translation specialties have become formalized, with dedicated schools and professional associations.
Because of the laboriousness of translation, since the 1940s engineers have sought to automate translation (machine translation) or to mechanically aid the human translator (computer-assisted translation). The rise of the Internet has fostered a world-wide market for translation services and has facilitated language localization.
Translation studies systematically study the theory and practice of translation.
^ The Oxford Companion to the English Language, Namit Bhatia, ed., 1992, pp. 1,051–54.
^ J.M. Cohen, "Translation", Encyclopedia Americana, 1986, vol. 27, p. 12.
^ Christopher Kasparek, "The Translator's Endless Toil", The Polish Review, vol. XXVIII, no. 2, 1983, pp. 84-87.
^ Andrew Wilson, Translators on Translating: Inside the Invisible Art, Vancouver, CCSP Press, 2009.
^ W.J. Hutchins, Early Years in Machine Translation: Memoirs and Biographies of Pioneers, Amsterdam, John Benjamins, 2000.
^ M. Snell-Hornby, The Turns of Translation Studies: New Paradigms or Shifting Viewpoints?, Philadelphia, John Benjamins, 2006, p. 133.
^ Susan Bassnett, Translation studies, pp. 13-37.""
""Media arts "",,,,""New media art is a genre that encompasses artworks created with new media technologies, including digital art, computer graphics, computer animation, virtual art, Internet art, interactive art, video games, computer robotics, 3D printing, cyborg art and art as biotechnology. The term differentiates itself by its resulting cultural objects and social events, which can be seen in opposition to those deriving from old visual arts (i.e. traditional painting, sculpture, etc.). This concern with medium is a key feature of much contemporary art and indeed many art schools and major universities now offer majors in "New Genres" or "New Media" and a growing number of graduate programs have emerged internationally. New Media Art often involves interaction between artist and observer or between observers and the artwork, which responds to them. Yet, as several theorists and curators have noted, such forms of interaction, social exchange, participation, and transformation do not distinguish new media art but rather serve as a common ground that has parallels in other strands of contemporary art practice. Such insights emphasize the forms of cultural practice that arise concurrently with emerging technological platforms, and question the focus on technological media, per se.
New Media concerns are often derived from the telecommunications, mass media and digital electronic modes of delivering the artworks involve, with practices ranging from conceptual to virtual art, performance to installation.""
""Sound and music computing "",,,,""Sound and Music Computing (SMC) is a research field that studies the whole sound and music communication chain from a multidisciplinary point of view. By combining scientific, technological and artistic methodologies it aims at understanding, modeling and generating sound and music through computational approaches.""
""Digital libraries and archives "",,,,""Bangalore (/ˈbæŋɡəlɔːr/; Kannada: ಬೆಂಗಳೂರು, IPA: [beŋɡaɭuːru]) is the capital and IT city of the Indian state of Karnataka.
Bangalore, or Bengaluru, as it is known today, was founded by Kempe Gowda, who built a mud fort at the site in 1537. It has developed over the years into an industrial and technological hub in India. An unofficial website of the heritage of the city, including the built, natural and cultural history of the city and the region is at www.bangaloreheritage.in.""
""Publishing "",,,,""Publishing is the process of production and dissemination of literature, music, or information — the activity of making information available to the general public. In some cases, authors may be their own publishers, meaning originators and developers of content also provide media to deliver and display the content for the same. Also, the word publisher can refer to the individual who leads a publishing company or an imprint or to a person who owns/heads a magazine.
Traditionally, the term refers to the distribution of printed works such as books (the "book trade") and newspapers. With the advent of digital information systems and the Internet, the scope of publishing has expanded to include electronic resources such as the electronic versions of books and periodicals, as well as micropublishing, websites, blogs, video game publishers, and the like.
Publishing includes the following stages of development: acquisition, copy editing, production, printing (and its electronic equivalents), and marketing and distribution.
Publication is also important as a legal concept:
As the process of giving formal notice to the world of a significant intention, for example, to marry or enter bankruptcy;
As the essential precondition of being able to claim defamation; that is, the alleged libel must have been published, and
For copyright purposes, where there is a difference in the protection of published and unpublished works.
There are two categories of book publisher:
Non-Paid Publishers: The term non-paid publisher refers to those publication houses that do not charge authors at all to publish the book.
Paid Publishers: The author has to meet with the total expense to get the book published, and the author has full right to set up marketing policies. This is also known as vanity publishing.

""
""Cyberwarfare "",,,,""Cyberwarfare has been defined as "actions by a nation-state to penetrate another nation's computers or networks for the purposes of causing damage or disruption," but other definitions also include non-state actors, such as terrorist groups, companies, political or ideological extremist groups, hacktivists, and transnational criminal organizations.
Some governments have made it an integral part of their overall military strategy, with some having invested heavily in cyberwarfare capability.""
""Cartography "",,,,""Cartography (from Greek χάρτης khartēs, "map"; and γράφειν graphein, "write") is the study and practice of making maps. Combining science, aesthetics, and technique, cartography builds on the premise that reality can be modeled in ways that communicate spatial information effectively.
The fundamental problems of traditional cartography are to:
Set the map's agenda and select traits of the object to be mapped. This is the concern of map editing. Traits may be physical, such as roads or land masses, or may be abstract, such as toponyms or political boundaries.
Represent the terrain of the mapped object on flat media. This is the concern of map projections.
Eliminate characteristics of the mapped object that are not relevant to the map's purpose. This is the concern of generalization.
Reduce the complexity of the characteristics that will be mapped. This is also the concern of generalization.
Orchestrate the elements of the map to best convey its message to its audience. This is the concern of map design.
Modern cartography constitutes many theoretical and practical foundations of geographic information systems.""
""Agriculture "",,,,""Agriculture is the cultivation of animals, plants, fungi, and other life forms for food, fiber, biofuel, medicinal and other products used to sustain and enhance human life. Agriculture was the key development in the rise of sedentary human civilization, whereby farming of domesticated species created food surpluses that nurtured the development of civilization. The study of agriculture is known as agricultural science. The history of agriculture dates back thousands of years, and its development has been driven and defined by greatly different climates, cultures, and technologies. In the civilized world, industrial agriculture based on large-scale monoculture farming has become the dominant agricultural methodology.
Modern agronomy, plant breeding, agrochemicals such as pesticides and fertilizers, and technological developments have in many cases sharply increased yields from cultivation, but at the same time have caused widespread ecological damage and negative human health effects. Selective breeding and modern practices in animal husbandry have similarly increased the output of meat, but have raised concerns about animal welfare and the health effects of the antibiotics, growth hormones, and other chemicals commonly used in industrial meat production. Genetically modified organisms are an increasing component of agriculture, although they are banned in several countries. Agricultural food production and water management are increasingly becoming global issues that are fostering debate on a number of fronts. Significant degradation of land and water resources, including the depletion of aquifers, has been observed in recent decades, and the effects of global warming on agriculture and of agriculture on global warming are still not fully understood.
The major agricultural products can be broadly grouped into foods, fibers, fuels, and raw materials. Specific foods include cereals (grains), vegetables, fruits, oils, meats and spices. Fibers include cotton, wool, hemp, silk and flax. Raw materials include lumber and bamboo. Other useful materials are produced by plants, such as resins, dyes, drugs, perfumes, biofuels and ornamental products such as cut flowers and nursery plants. Over one third of the world's workers are employed in agriculture, second only to the services' sector, although the percentages of agricultural workers in developed countries has decreased significantly over the past several centuries.""
""Voting / election technologies "",,,,""Voting machines are the total combination of mechanical, electromechanical, or electronic equipment (including software, firmware, and documentation required to program control, and support equipment), that is used to define ballots; to cast and count votes; to report or display election results; and to maintain and produce any audit trail information. The first voting machines were mechanical but it is increasingly more common to use electronic voting machines.
A voting system includes the practices and associated documentation used to identify system components and versions of such components; to test the system during its development and maintenance; to maintain records of system errors or defects; to determine specific changes made after initial certification; and to make available any materials to the voter (such as notices, instructions, forms, or paper ballots).
Traditionally, a voting machine has been defined by the mechanism the system uses to cast votes and further categorized by the location where the system tabulates the votes.
Voting machines have different levels of usability, security, efficiency and accuracy. Certain systems may be more or less accessible to all voters, or not accessible to those voters with certain types of disabilities. They can also have an effect on the public's ability to oversee elections.""
""E-government "",,,,""E-government (short for electronic government - 'EG', also known as e-gov, Internet government, digital government, online government, connected government) consists of the digital interactions between a citizen and their government (C2G), between governments and government agencies (G2G), between government and citizens (G2C), between government and employees (G2E), and between government and businesses/commerce (G2B). Essentially, e-government delivery models can be briefly summed up as (Jeong, 2007):
G2G (government to governments)
G2C (government to citizens)
G2E (government to employees)
G2B (government to businesses)
This digital interaction consists of e-citizen at all levels of government (city, state/province, national, and international), governance, information and communication technology (ICT), and business process re-engineering (BPR).""
""Word processors "",,,,""A word processor is an electronic device or computer software application, that performs the task of composition, editing, formatting, printing of documents.
The word processor was a stand-alone office machine in the 1960s, combining the keyboard text-entry and printing functions of an electric typewriter, with a recording unit, either tape or floppy disk (as used by the Wang machine) with a simple dedicated computer processor for the editing of text. Although features and designs varied among manufacturers and models, and new features were added as technology advanced, word processors typically featured a monochrome display and the ability to save documents on memory cards or diskettes. Later models introduced innovations such as spell-checking programs, and improved formatting options.
As the more versatile combination of personal computers and printers became commonplace, and computer software applications for word processing became popular, most business machine companies stopped manufacturing dedicated word processor machines. As of 2009 there were only two U.S. companies, Classic and AlphaSmart, which still made them. Many older machines, however, remain in use. Since 2009, Sentinel has offered a machine described as a "word processor", but it is more accurately a highly specialised microcomputer used for accounting and publishing.
Word processing was one of the earliest applications for the personal computer in office productivity and was the most popular application on home and personal computers until the World Wide Web rose to prominence in the mid-1990s.
Although the early word processors evolved to use tag-based markup for document formatting, most modern word processors take advantage of a graphical user interface providing some form of what-you-see-is-what-you-get ("WYSIWYG") editing. Most are powerful systems consisting of one or more programs that can produce any arbitrary combination of images, graphics and text, the latter handled with type-setting capability. Typical features of a modern word processor include font application, spell checking, grammar checking, a built-in thesaurus, automatic text correction, Web integration, and HTML exporting, among others. In its simplest form, a word processor is little more than a large Expensive Typewriter-like machine that makes correcting mistakes possible before printing.
Microsoft Word is the most widely used word processing software according to a user tracking system built into the software. Microsoft estimates that roughly half a billion people use the Microsoft Office suite, which includes Word. Many other word processing applications exist, including WordPerfect (which dominated the market from the mid-1980s to early-1990s on computers running Microsoft's MS-DOS operating system, and still (2014) is favored for legal applications) and open source applications OpenOffice.org Writer, LibreOffice Writer, AbiWord, KWord, and LyX. Web-based word processors, such as Office Web Apps or Google Docs, are a relatively new category.""
""Spreadsheets "",,,,""A spreadsheet is an interactive computer application for organization, analysis and storage of data in tabular form. Spreadsheets are developed as computerized simulations of paper accounting worksheets. The program operates on data entered in cells of an array, organized in rows and columns. Each cell of the array may contain either numeric or text data, or the results of formulas that automatically calculate and display a value based on the contents of other cells.
Spreadsheet users may adjust any stored value and observe the effects on calculated values. This makes the spreadsheet useful for "what-if" analysis since many cases can be rapidly investigated without manual recalculation. Modern spreadsheet software can have multiple interacting sheets, and can display data either as text and numerals, or in graphical form.
Besides performing basic arithmetic and mathematical functions, modern spreadsheets provide built-in functions for common financial and statistical operations. Such calculations as net present value or standard deviation can be applied to tabular data with a pre-programmed function in a formula. Spreadsheet programs also provide conditional expressions, functions to convert between text and numbers, and functions that operate on strings of text.
Spreadsheets have replaced paper-based systems throughout the business world. Although they were first developed for accounting or bookkeeping tasks, they now are used extensively in any context where tabular lists are built, sorted, and shared.
LANPAR was the first electronic spreadsheet on mainframe and time sharing computers. VisiCalc was the first electronic spreadsheet on a microcomputer, and it helped turn the Apple II computer into a popular and widely used system. Lotus 1-2-3 was the leading spreadsheet when DOS was the dominant operating system. Excel now has the largest market share on the Windows and Macintosh platforms. A spreadsheet program is a standard feature of an office productivity suite; since the advent of web apps, office suites now also exist in web app form.

""
""Computer games "",,,,""PC games, also known as computer games or personal computer games, are video games played on a personal computer rather than a dedicated video game console or arcade machine. Their defining characteristics include a lack of any centralized controlling authority, a greater degree of user control over the video-gaming hardware and software used and a generally greater capacity in input, processing, and output.
Home computer games became popular following the video game crash of 1983, particularly in Europe, leading to the era of the "bedroom coder". From the mid-90s onwards, PC games lost mass-market traction to console games before enjoying a resurgence in the mid-2000s through digital distribution. The uncoordinated nature of the PC game market and its lack of physical media make precisely assessing its size difficult.

""
""Microcomputers "",,,,""A microcomputer is a small, relatively inexpensive computer with a microprocessor as its central processing unit (CPU). It includes a microprocessor, memory, and input/output (I/O) facilities. Microcomputers became popular in the 1970s and 1980s with the advent of increasingly powerful microprocessors. The predecessors to these computers, mainframes and minicomputers, were comparatively much larger and more expensive (though indeed present-day mainframes such as the IBM System z machines use one or more custom microprocessors as their CPUs). Many microcomputers (when equipped with a keyboard and screen for input and output) are also personal computers (in the generic sense).
The abbreviation micro was common during the 1970s and 1980s, but has now fallen out of common usage.""
""Consumer products "",,,,""In economics, any commodity which is produced and subsequently consumed by the consumer, to satisfy its current wants or needs, is a consumer good or final good. Consumer goods are goods that are ultimately consumed rather than used in the production of another good. For example, a microwave oven or a bicycle which is sold to a consumer is a final good or consumer good, whereas the components which are sold to be used in those goods are called intermediate goods. For example, textiles or transistors which can be used to make some further goods.
When used in measures of national income and output, the term "final goods" only includes new goods. For instance, the GDP excludes items counted in an earlier year to prevent double counting of production based on resales of the same item second and third hand. In this context the economic definition of goods includes what are commonly known as services.
Manufactured goods are goods that have been processed in any way. As such, they are the opposite of raw materials, but include intermediate goods as well as final goods.""
""Supply chain management "",,,,""Supply chain management (SCM) is the management of the flow of goods and services. It includes the movement and storage of raw materials, work-in-process inventory, and finished goods from point of origin to point of consumption. Interconnected or interlinked networks, channels and node businesses are involved in the provision of products and services required by end customers in a supply chain. Supply chain management has been defined as the "design, planning, execution, control, and monitoring of supply chain activities with the objective of creating net value, building a competitive infrastructure, leveraging worldwide logistics, synchronizing supply with demand and measuring performance globally."
SCM draws heavily from the areas of industrial engineering, systems engineering, operations management, logistics, procurement, and information technology, and strives for an integrated approach.

""
""Command and control "",,,,""There are several definitions of command and control (C2). According to older versions of U.S. Army FM 3-0, C2 in a military organization is the exercise of authority and direction by a properly designated commanding officer over assigned and attached forces in the accomplishment of the mission. The term may also refer to command and control systems within a military system.
The 1988 NATO definition reads: Command and control is the exercise of authority and direction by a properly designated [individual] over assigned [resources] in the accomplishment of a [common goal].
The Australian Defence Force definition is similar: C2 is the system empowering designated personnel to exercise lawful authority and direction over assigned forces for the accomplishment of missions and tasks. (The Australian doctrine goes on to state: The use of agreed terminology and definitions is fundamental to any C2 system and the development of joint doctrine and procedures. The definitions in the following paragraphs have some agreement internationally, although not every potential ally will use the terms with exactly the same meaning.)
Canadian defence scientists Ross Pigeau and Carol McCann discuss the issues and uncertainties related to the definition of command & control in their article in the Canadian Military Journal. The book by Vassiliou et al.  contains an appendix comparing and contrasting various definitions, and also provides a useful overarching definition that subsumes most of the official ones: "'Command and Control' (C2) denotes the set of organizational and technical attributes and processes by which an enterprise marshals and employs human, physical, and information resources to solve problems and accomplish missions."""
""Computer-aided manufacturing "",,,,""Computer-aided manufacturing (CAM) is the use of software to control machine tools and relate in the manufacturing of workpieces. This is not the only definition for CAM, but it is the most common; CAM may also refer to the use of a computer to assist in all operations of a manufacturing plant, including planning, management, transportation and storage. Its primary purpose is to create a faster production process and components and tooling with more precise dimensions and material consistency, which in some cases, uses only the required amount of raw material (thus minimizing waste), while simultaneously reducing energy consumption. CAM is now a system used in schools and lower educational purposes. CAM is a subsequent computer-aided process after computer-aided design (CAD) and sometimes computer-aided engineering (CAE), as the model generated in CAD and verified in CAE can be input into CAM software, which then controls the machine tool. CAM is used in many schools alongside Computer Aided Design (CAD) to create objects.""
""Decision analysis "",,,,""Decision analysis (DA) is the discipline comprising the philosophy, theory, methodology, and professional practice necessary to address important decisions in a formal manner. Decision analysis includes many procedures, methods, and tools for identifying, clearly representing, and formally assessing important aspects of a decision, for prescribing a recommended course of action by applying the maximum expected utility action axiom to a well-formed representation of the decision, and for translating the formal representation of a decision and its corresponding recommendation into insight for the decision maker and other stakeholders.""
""Transportation "",,,,""Transport or transportation is the movement of people, animals and goods from one location to another. Modes of transport include air, rail, road, water, cable, pipeline and space. The field can be divided into infrastructure, vehicles and operations. Transport is important because it enables trade between persons, which is essential for the development of civilizations.
Transport infrastructure consists of the fixed installations including roads, railways, airways, waterways, canals and pipelines and terminals such as airports, railway stations, bus stations, warehouses, trucking terminals, refueling depots (including fueling docks and fuel stations) and seaports. Terminals may be used both for interchange of passengers and cargo and for maintenance.
Vehicles traveling on these networks may include automobiles, bicycles, buses, trains, trucks, people, helicopters, watercraft, spacecraft and aircraft. Operations deal with the way the vehicles are operated, and the procedures set for this purpose including financing, legalities and policies. In the transport industry, operations and ownership of infrastructure can be either public or private, depending on the country and mode.
Passenger transport may be public, where operators provide scheduled services, or private. Freight transport has become focused on containerization, although bulk transport is used for large volumes of durable items. Transport plays an important part in economic growth and globalization, but most types cause air pollution and use large amounts of land. While it is heavily subsidized by governments, good planning of transport is essential to make traffic flow and restrain urban sprawl.""
""Forecasting "",,,,""Forecasting is the process of making predictions of the future based on past and present data and analysis of trends. A commonplace example might be estimation of some variable of interest at some specified future date. Prediction is a similar, but more general term. Both might refer to formal statistical methods employing time series, cross-sectional or longitudinal data, or alternatively to less formal judgmental methods. Usage can differ between areas of application: for example, in hydrology, the terms "forecast" and "forecasting" are sometimes reserved for estimates of values at certain specific future times, while the term "prediction" is used for more general estimates, such as the number of times floods will occur over a long period.
Risk and uncertainty are central to forecasting and prediction; it is generally considered good practice to indicate the degree of uncertainty attaching to forecasts. In any case, the data must be up to date in order for the forecast to be as accurate as possible.""
""Marketing "",,,,""Marketing is a widely used term to describe the means of communication between the company and the consumer audience. Marketing is the adaptation of the commercial activities and use of institutions by the organizations with a purpose to induce behavioral change on a short-term or permanent basis. The American Marketing Association most recently defined Marketing as "the activity, set of institutions, and processes for creating, communicating, delivering, and exchanging offerings that have value for customers, clients, partners, and society at large." 
The techniques used in marketing include choosing target markets through market analysis and market segmentation, as well as understanding methods of influence on the consumer behavior. The marketing planning creates strategies for the company to place advertising to the dedicated consumer.
From a societal point of view, marketing provides the link between a society's material requirements and its economic patterns of response. This way marketing satisfies these needs and wants through the development of exchange processes and the building of long-term relationships.
In the case of nonprofit organization marketing, the aim is to increase the deliver an ethos message about the organization's services to the applicable audience. Governments often employ marketing to communicate messages with a social purpose, such as a public health or safety message, to citizens.

""
""Digital libraries and archives "",,,,""Bangalore (/ˈbæŋɡəlɔːr/; Kannada: ಬೆಂಗಳೂರು, IPA: [beŋɡaɭuːru]) is the capital and IT city of the Indian state of Karnataka.
Bangalore, or Bengaluru, as it is known today, was founded by Kempe Gowda, who built a mud fort at the site in 1537. It has developed over the years into an industrial and technological hub in India. An unofficial website of the heritage of the city, including the built, natural and cultural history of the city and the region is at www.bangaloreheritage.in.""
""Computer-assisted instruction "",,,,""Educational technology is defined by the Association for Educational Communications and Technology as "the study and ethical practice of facilitating learning and improving performance by creating, using, and managing appropriate technological processes and resources." 
Educational technology refers to the use of both physical hardware and educational theoretics. It encompasses several domains, including learning theory, computer-based training, online learning, and, where mobile technologies are used, m-learning. Accordingly, there are several discrete aspects to describing the intellectual and technical development of educational technology:
educational technology as the theory and practice of educational approaches to learning
educational technology as technological tools and media that assist in the communication of knowledge, and its development and exchange
educational technology for learning management systems (LMS), such as tools for student and curriculum management, and education management information systems (EMIS)
educational technology itself as an educational subject; such courses may be called "Computer Studies" or "Information and Communication Technology (ICT)"""
""Interactive learning environments "",,,,""Donald Charles "Don" Lavoie (April 4, 1951 – November 6, 2001) was an Austrian school economist. He worked at the Cato Institute. He wrote two books on the problem of economic calculation. His first book on this subject was Rivalry and Central Planning (Cambridge University Press 1985). This book stressed the importance of the process of competitive rivalry in markets. His second book was National Economic Planning: What Is Left? (Cambridge, Massachusetts: Ballinger Publishing Company, 1985). This book dealt with the problem of non-comprehensive planning. He was influenced by Friedrich Hayek, Michael Polanyi and Ludwig Lachmann.
Among his students, there are a number of "contemporary Austrian" economists: Peter Boettke, David Prychitko, Steven Horwitz, Thomas Rustici, Mark Gilbert, Ralph Rector, Emily Chamlee-Wright, Howie Baetjer and Virgil Storr.
Don Lavoie was co-founder of the interdisciplinary unit known as the Program on Social & Organizational Learning at George Mason University which offers a Master's degree in Organizational Learning.
Lavoie was awarded a Ph. D. in economics from New York University in 1981 for thesis entitled Rivalry and central planning : a re-examination of the debate over economic calculation under socialism.
As a scholar, he studied the philosophy of the social sciences (especially the application of hermeneutics to economics) and Comparative Economic Systems (especially Marxian theories of socialism). Along with Richard Ebeling, Lavoie pioneered the attempt to merge Austrian Economics with philosophical hermeneutics in the late 1980s, and in particular with the hermeneutics of Hans Georg Gadamer. His influence here extended to many of his students mentioned above. His effort drew criticism from several members of the Austrian School associated with the Mises Institute, especially Murray Rothbard and Hans-Hermann Hoppe.
As a young professor, he worked on the philosophy and practice of electronically mediated discourse. He knew the importance for organizations of new ways of cultivating interactive learning environments (groupware and hypertext software environments) in order to enhance communicative processes. He showed the fundamental nature of social learning processes, whether in market exchanges, in verbal conversations, or in hypertext-based dialogue.
In the book Culture and Enterprise: The Development, Representation and Morality of Business (New York: Routledge, 2000) written with Emily Chamlee-Wright, they take into account the important role of culture in a nation's economic development.
Lavoie was diagnosed with pancreatic cancer in the spring of 2001. He died of a stroke later that year.""
""Collaborative learning "",,,,""Collaborative learning is a situation in which two or more people learn or attempt to learn something together. Unlike individual learning, people engaged in collaborative learning capitalize on one another’s resources and skills (asking one another for information, evaluating one another’s ideas, monitoring one another’s work, etc.). More specifically, collaborative learning is based on the model that knowledge can be created within a population where members actively interact by sharing experiences and take on asymmetry roles. Put differently, collaborative learning refers to methodologies and environments in which learners engage in a common task where each individual depends on and is accountable to each other. These include both face-to-face conversations and computer discussions (online forums, chat rooms, etc.). Methods for examining collaborative learning processes include conversation analysis and statistical discourse analysis.
Thus, collaborative learning is commonly illustrated when groups of students work together to search for understanding, meaning, or solutions or to create an artifact or product of their learning. Further, collaborative learning redefines traditional student-teacher relationship in the classroom which results in controversy over whether this paradigm is more beneficial than harmful. Collaborative learning activities can include collaborative writing, group projects, joint problem solving, debates, study teams, and other activities. The approach is closely related to cooperative learning.""
""Learning management systems "",,,,""A learning management system (LMS) is a software application for the administration, documentation, tracking, reporting and delivery of electronic educational technology (also called e-learning) courses or training programs.
Learning management systems range from systems for managing training and educational records to software for distributing online or blended/hybrid college courses over the Internet with features for online collaboration. Colleges, universities, school districts, and schools use LMSs to deliver online courses and augment on-campus courses. LMSs also act to augment the lessons the teacher is giving in a brick and mortar environment, not just replace them. Corporate training departments use LMSs to deliver online training, as well as to automate record-keeping and employee registration.""
""Distance learning "",,,,""Distance education or distance learning is the education of students who are not physically present at a school. Courses that are conducted partly through distance education and partly on-site are referred to as hybrid or blended education. Massive open online courses (MOOCs), offering large-scale interactive participation and open access through the World Wide Web or other network technologies, are recent developments in distance education. A number of other terms (distributed learning, e-learning, online learning, etc.) are used roughly synonymously with distance education.

""
""E-learning "",,,,""Educational technology is defined by the Association for Educational Communications and Technology as "the study and ethical practice of facilitating learning and improving performance by creating, using, and managing appropriate technological processes and resources." 
Educational technology refers to the use of both physical hardware and educational theoretics. It encompasses several domains, including learning theory, computer-based training, online learning, and, where mobile technologies are used, m-learning. Accordingly, there are several discrete aspects to describing the intellectual and technical development of educational technology:
educational technology as the theory and practice of educational approaches to learning
educational technology as technological tools and media that assist in the communication of knowledge, and its development and exchange
educational technology for learning management systems (LMS), such as tools for student and curriculum management, and education management information systems (EMIS)
educational technology itself as an educational subject; such courses may be called "Computer Studies" or "Information and Communication Technology (ICT)"""
""Computer-managed instruction "",,,,""Educational technology is defined by the Association for Educational Communications and Technology as "the study and ethical practice of facilitating learning and improving performance by creating, using, and managing appropriate technological processes and resources." 
Educational technology refers to the use of both physical hardware and educational theoretics. It encompasses several domains, including learning theory, computer-based training, online learning, and, where mobile technologies are used, m-learning. Accordingly, there are several discrete aspects to describing the intellectual and technical development of educational technology:
educational technology as the theory and practice of educational approaches to learning
educational technology as technological tools and media that assist in the communication of knowledge, and its development and exchange
educational technology for learning management systems (LMS), such as tools for student and curriculum management, and education management information systems (EMIS)
educational technology itself as an educational subject; such courses may be called "Computer Studies" or "Information and Communication Technology (ICT)"""
""Document searching "",,,,""WinFS (short for Windows Future Storage) is the code name for a canceled data storage and management system project based on relational databases, developed by Microsoft and first demonstrated in 2003 as an advanced storage subsystem for the Microsoft Windows operating system, designed for persistence and management of structured, semi-structured as well as unstructured data.
WinFS includes a relational database for storage of information, and allows any type of information to be stored in it, provided there is a well defined schema for the type. Individual data items could then be related together by relationships, which are either inferred by the system based on certain attributes or explicitly stated by the user. As the data has a well defined schema, any application can reuse the data; and using the relationships, related data can be effectively organized as well as retrieved. Because the system knows the structure and intent of the information, it can be used to make complex queries that enable advanced searching through the data and aggregating various data items by exploiting the relationships between them.

While WinFS and its shared type schema make it possible for an application to recognize the different data types, the application still has to be coded to render the different data types. Consequently, it would not allow development of a single application that can view or edit all data types; rather what WinFS enables applications to understand is the structure of all data and extract the information that it can use further. When WinFS was introduced at the 2003 Professional Developers Conference, Microsoft also released a video presentation, named IWish, showing mockup interfaces that showed how applications would expose interfaces that take advantage of a unified type system. The concepts shown in the video ranged from applications using the relationships of items to dynamically offer filtering options to applications grouping multiple related data types and rendering them in a unified presentation.
WinFS was billed as one of the pillars of the "Longhorn" wave of technologies, and would ship as part of the next version of Windows. It was subsequently decided that WinFS would ship after the release of Windows Vista, but those plans were shelved in June 2006, with some of its component technologies being integrated into upcoming releases of ADO.NET and Microsoft SQL Server. While it was then assumed by observers that WinFS was finished as a project, in November 2006 Steve Ballmer announced that WinFS was still in development, though it was not clear how the technology was to be delivered. Several components of the last Integrated Storage Initiative project, Microsoft Semantic Engine, presented at Microsoft PDC 2009, have been integrated back into the SQL Server "Denali". At the 2010 SQL Server PASS Community Summit, the forthcoming version of SQL Server ("Denali") was shown, which seems to incorporate many of the WinFS ideas.""
""Text editing "",,,,""A text editor is a type of program used for editing plain text files.
Text editors are provided with operating systems and software development packages, and can be used to change configuration files, documentation files and programming language source code.""
""Version control "",,,,""A component of software configuration management, version control, also known as revision control or source control, is the management of changes to documents, computer programs, large web sites, and other collections of information. Changes are usually identified by a number or letter code, termed the "revision number", "revision level", or simply "revision". For example, an initial set of files is "revision 1". When the first change is made, the resulting set is "revision 2", and so on. Each revision is associated with a timestamp and the person making the change. Revisions can be compared, restored, and with some types of files, merged.
The need for a logical way to organize and control revisions has existed for almost as long as writing has existed, but revision control became much more important, and complicated, when the era of computing began. The numbering of book editions and of specification revisions are examples that date back to the print-only era. Today, the most capable (as well as complex) revision control systems are those used in software development, where a team of people may change the same files.
Version control systems (VCS) most commonly run as stand-alone applications, but revision control is also embedded in various types of software such as word processors and spreadsheets, e.g., Google Docs and Sheets and in various content management systems, e.g., Wikipedia's Page history. Revision control allows for the ability to revert a document to a previous revision, which is critical for allowing editors to track each other's edits, correct mistakes, and defend against vandalism and spamming.
Software tools for revision control are essential for the organization of multi-developer projects.""
""Document metadata "",,,,""MultiMarkdown is a lightweight markup language created by Fletcher T. Penney and based on Markdown, which supports more export-formats and implements some added features currently not available with plain Markdown syntax.
It adds the following features to Markdown:
footnotes
tables
citations and bibliography (works best in LaTeX using BibTeX)
math support
automatic cross-referencing ability
smart typography, with support for multiple languages
image attributes
table and image captions
definition lists
glossary entries (LaTeX only)
document metadata (e.g. title, author, date, etc.)""
""Document analysis "",,,,""Content analysis is "a wide and heterogeneous set of manual or computer-assisted techniques for contextualized interpretations of documents produced by communication processes in the strict sense of that phrase (any kind of text, written, iconic, multimedia, etc.) or signification processes (traces and artifacts), having as ultimate goal the production of valid and trustworthy inferences."
Though the locution "content analysis" has come to be a sort of 'umbrella term' referring to an almost boundless set of quite diverse research approaches and techniques, it is still today in use in the Social and Computer Science Domains and in the Humanities to identify methods for studying and/or retrieving meaningful information from documents. In a more focused way, "content analysis" refers to a family of techniques oriented to the study of "mute evidence" of texts and artifacts. Texts come from communication processes in a narrow sense of that phrase (i.e. types of communication intentionally activated by a sender, using a code sufficiently shared with the receiver). There are 5 types of texts in content analysis:
written t. (books, papers, etc.),
oral t. (speech, theatre plays, etc.),
iconic t. (drawings, paintings, icons, etc.),
audio-visual t. (TV programs, movies, videos, etc.),
hypertexts (can be one or more of the texts above, on the Internet).
On the other side, content analysis can also study traces (documents from past times) and artifacts (non-linguistic documents), which come from communication processes in a broad sense of that phrase - commonly referred to as "signification" in Semiotics (in the absence of an intentional sender, semiosis is developed by abduction).
Despite the wide variety of options, generally speaking every "content analysis" method implies «a series of transformation procedures, equipped with a different degree of formalisation depending on the type of technique used, but which share the scientific re-elaboration of the object examined. This means, in short, guaranteeing the repeatability of the method, i.e.: that pre-set itinerary which, following pre-established procedures (techniques), has led to those results. This path changes consistently depending on the direction imprinted by the interpretative key of the researcher who, at the end of the day, is responsible for the operational decisions made».
Over the years, content analysis has been applied to a variety of scopes. Hermeneutics and Philology have been using content analysis since the dawn of time to interpret sacred and profane texts and, in not a few cases, to attribute texts' authorship and authenticity.
In recent times, particularly with the advent of mass communication, content analysis has known an increasing use to deeply analyse and understand media content and media logic. The political scientist Harold Lasswell formulated the core questions of content analysis in its early-mid 20th-century mainstream version: "Who says what, to whom, why, to what extent and with what effect?". The strong emphasis for a quantitative approach started up by Lasswell was finally carried out by another "father" of content analysis, Bernard Berelson, who proposed a definition of content analysis which, from this point of view, is emblematic: «a research technique for the objective, systematic and quantitative description of the manifest content of communication». This was the product of a positivist epistemological context which is quite close the naïve realism that has long since become obsolete. Approaches of this type are rising again due to the tremendous fertility of the most recent technologies and application within mass and personal communications. Content analysis has indeed come across huge amount of textual big data as a consequence of the recent spread of new media, particularly social media and mobile devices. Threats are represented by the fact that the complexity of the process of semiosis is not rarely underestimated and made banal whenever statistics is uncritically applied to large amount of analogic-native data. In such a case, the main problem stems from a naive use of measures and numbers as an always valid certificate of "objectivity" and "systematicity", though moving from the sharable principle to contain bad, offhand evidence-detached analyses spoiled by the «human tendency to read textual material selectively, in support of expectations rather than against them».""
""Document scanning "",,,,""""
""Graphics recognition and interpretation "",,,,""Speech recognition (SR) is the inter-disciplinary sub-field of computational linguistics which incorporates knowledge and research in the linguistics, computer science, and electrical engineering fields to develop methodologies and technologies that enables the recognition and translation of spoken language into text by computers and computerized devices such as those categorized as Smart Technologies and robotics. It is also known as "automatic speech recognition" (ASR), "computer speech recognition", or just "speech to text" (STT).
Some SR systems use "training" (also called "enrollment") where an individual speaker reads text or isolated vocabulary into the system. The system analyzes the person's specific voice and uses it to fine-tune the recognition of that person's speech, resulting in increased accuracy. Systems that do not use training are called "speaker independent" systems. Systems that use training are called "speaker dependent".
Speech recognition applications include voice user interfaces such as voice dialing (e.g. "Call home"), call routing (e.g. "I would like to make a collect call"), domotic appliance control, search (e.g. find a podcast where particular words were spoken), simple data entry (e.g., entering a credit card number), preparation of structured documents (e.g. a radiology report), speech-to-text processing (e.g., word processors or emails), and aircraft (usually termed Direct Voice Input).
The term voice recognition or speaker identification refers to identifying the speaker, rather than what they are saying. Recognizing the speaker can simplify the task of translating speech in systems that have been trained on a specific person's voice or it can be used to authenticate or verify the identity of a speaker as part of a security process.
From the technology perspective, speech recognition has a long history with several waves of major innovations. Most recently, the field has benefited from advances in deep learning and big data. The advances are evidenced not only by the surge of academic papers published in the field, but more importantly by the world-wide industry adoption of a variety of deep learning methods in designing and deploying speech recognition systems. These speech industry players include Microsoft, Google, IBM, Baidu (China), Apple, Amazon, Nuance, IflyTek (China), many of which have publicized the core technology in their speech recognition systems as being based on deep learning.""
""Optical character recognition "",,,,""Optical character recognition (optical character reader) (OCR) is the mechanical or electronic conversion of images of typed, handwritten or printed text into machine-encoded text. It is widely used as a form of data entry from printed paper data records, whether passport documents, invoices, bank statements, computerised receipts, business cards, mail, printouts of static-data, or any suitable documentation. It is a common method of digitising printed texts so that it can be electronically edited, searched, stored more compactly, displayed on-line, and used in machine processes such as machine translation, text-to-speech, key data and text mining. OCR is a field of research in pattern recognition, artificial intelligence and computer vision.
Early versions needed to be trained with images of each character, and worked on one font at a time. Advanced systems capable of producing a high degree of recognition accuracy for most fonts are now common. Some systems are capable of reproducing formatted output that closely approximates the original page including images, columns, and other non-textual components.""
""Online handwriting recognition "",,,,""Thomas Oriel Binford has been a leading researcher in image analysis and computer vision since 1967. He is known for pioneering a model-based approach to computer vision in which complex objects are represented as collections of generalized cylinders. His results reflect seminal work in numerous other areas of research including the interpretation of complex scenes using invariants and quasi-invariants, inference rules and evidential reasoning in extended Bayes networks of symbolic geometric constraints, the SUCCESSOR system, a portable, intelligent vision system, stereo and visual robot navigation, segmentation and feature estimation in complex images, color image analysis, surface material analysis, and image compression. He has led the development of numerous computer vision systems, including systems successfully employed in brain surgery on humans, high-precision automated machining, and helicopter navigation.
Binford received a Ph.D. in particle physics in 1965 from the University of Wisconsin–Madison, under the supervision of Myron L. Good; his thesis was entitled "Angular Distribution and Polarization of Neutral Hyperons Produced in Association with Neutral Kaons". He was a Fulbright Scholar at the Tata Institute of Fundamental Research in Mumbai, India from 1965 to 1966, and a research scientist at the MIT Artificial Intelligence Laboratory from 1966 to 1970. From 1970 to 2000 he was a professor of computer science at Stanford University; in 2000 he retired to become an emeritus professor. While at Stanford, Professor Binford supervised more than 40 PhD theses while leading research in computer vision, artificial intelligence, medical image processing, radar image understanding, robotics, industrial inspection, and manufacturing; notable students of Binford's include Rodney Brooks and Jitendra Malik.
Since retiring from active research at Stanford, Dr. Binford has founded and is chairman and chief technology officer of Read-Ink Technologies Pvt. Ltd., a company in Bangalore, India specializing in online handwriting recognition software. In 1994 Thomas Binford was elected a Fellow of the Association for the Advancement of Artificial Intelligence "for his role as a founding father in the field of computer vision and model-based perception in robotics; and for his many contributions to the fields".""
""Extensible Markup Language (XML) "",,,,""In computing, a news aggregator, also termed a feed aggregator, feed reader, news reader, RSS reader or simply aggregator, is client software or a web application which aggregates syndicated web content such as online newspapers, blogs, podcasts, and video blogs (vlogs) in one location for easy viewing. RSS is synchronized subscription system. Basically, RSS uses extensible markup language (XML) to structure pieces of information to be aggregated in a feed reader that displays the information in a user-friendly interface. The updates distributed include, for example, journal tables of contents, podcasts, videos,and news items.""
""Annotation "",,,,""An annotation is a metadata (e.g. a comment, explanation, presentational markup) attached to text, image, or other data. Often, annotations make reference to a specific part of the original data.""
""Format and notation "",,,,""A phonological rule is a formal way of expressing a systematic phonological or morphophonological process or diachronic sound change in language. Phonological rules are commonly used in generative phonology as a notation to capture sound-related operations and computations the human brain performs when producing or comprehending spoken language. They may use phonetic notation or distinctive features or both.
John Goldsmith (1995) defines phonological rules as mappings between two different levels of sound representation—in this case, the abstract or underlying level and the surface level—and Bruce Hayes (2009) describes them as "generalizations" about the different ways a sound can be pronounced in different environments. That is to say, phonological rules describe how a speaker goes from the abstract representation stored in their brain, to the actual sound they articulate when they speak. In general, phonological rules start with the underlying representation of a sound (the phoneme that is stored in the speaker's mind) and yield the final surface form, or what the speaker actually pronounces. When an underlying for has multiple surface forms, this is often referred to as allophony. For example, the English plural -s may be pronounced as [s] (in "cats"), [z] (in "cabs"), or as [əz] (in "buses"); these forms are all theorized to be stored mentally as the same -s, but the surface pronunciations are derived through a phonological rule.

""
""Multi / mixed media creation "",,,,""Multimedia is content that uses a combination of different content forms such as text, audio, images, animation, video and interactive content. Multimedia contrasts with media that use only rudimentary computer displays such as text-only or traditional forms of printed or hand-produced material.
Multimedia can be recorded and played, displayed, dynamic, interacted with or accessed by information content processing devices, such as computerized and electronic devices, but can also be part of a live performance. Multimedia devices are electronic media devices used to store and experience multimedia content. Multimedia is distinguished from mixed media in fine art; by including audio, for example, it has a broader scope. The term "rich media" is synonymous for interactive multimedia. Hypermedia scales up the amount of media content in multimedia application.""
""Image composition "",,,,""In photography, headroom or head room is a concept of aesthetic composition that addresses the relative vertical position of the subject within the frame of the image. Headroom refers specifically to the distance between the top of the subject's head and the top of the frame, but the term is sometimes used instead of lead room, nose room or 'looking room' to include the sense of space on both sides of the image. The amount of headroom that is considered aesthetically pleasing is a dynamic quantity; it changes relative to how much of the frame is filled by the subject.
One rule of thumb taken from classic portrait painting techniques, called the "rule of thirds", suggests that the subject's eyes, as a center of interest, are ideally positioned one-third of the way down from the top of the frame.
Moving images such as movie cameras and video cameras have the same headroom issues as still photography, but with the added factors of the movement of the subject, the movement of the camera, and the possibility of zooming in or out.
Perceptual psychological studies have been carried out with experimenters using a white dot placed in various positions within a frame to demonstrate that observers attribute potential motion to a static object within a frame, relative to its position. The unmoving object is described as 'pulling' toward the center or toward an edge or corner. Proper headroom is achieved when the object is no longer seen to be slipping out of the frame—when its potential for motion is seen to be neutral in all directions.
Headroom changes as the camera zooms in or out, and the camera must simultaneously tilt up or down to keep the center of interest approximately one-third of the way down from the top of the frame. The closer the subject, the less headroom needed. In extreme close-ups, the top of the head is out of the frame, but the concept of headroom still applies via the rule of thirds.
In television broadcast camera work, the amount of headroom seen by the production crew is slightly greater than the amount seen by home viewers, whose frames are reduced in area by about 5%. To adjust for this, broadcast camera headroom is slightly expanded so that home viewers will see the correct amount of headroom. Professional video camera viewfinders and professional video monitors often include an overscan setting to compare between full screen resolution and "domestic cut-off" as an aid to achieving good headroom and lead room.
One of the most common mistakes that casual camera users make is to have too much headroom: too much space above the subject's head.

""
""Hypertext / hypermedia creation "",,,,""Hypertext is text displayed on a computer display or other electronic devices with references (hyperlinks) to other text which the reader can immediately access, or where text can be revealed progressively at multiple levels of detail (also called StretchText). The hypertext pages are interconnected by hyperlinks, typically activated by a mouse click, keypress sequence or by touching the screen. Apart from text, hypertext is sometimes used to describe tables, images and other presentational content forms with hyperlinks. Hypertext is the underlying concept defining the structure of the World Wide Web, with pages often written in the Hypertext Markup Language (HTML). It enables an easy-to-use and flexible connection and sharing of information over the Internet.""
""Document scripting languages "",,,,""Call to Power II is a PC turn-based strategy game released by Activision', which itself was a successor to the Civilization series by Sid Meier; this game could not have "Civilization" in its title because the license to the Civilization name was lost.
In October 2003, Activision released the source code, enabling the Apolyton gaming community to debug, improve, and add new features.""
""Industry statistics "",,,,""The Bureau of Economic Analysis (BEA) is a U.S. government agency that provides official macroeconomic and industry statistics including the gross domestic product of the United States.
BEA is part of the United States Department of Commerce and is one of the principal agencies of the U.S. Federal Statistical System. Its stated mission is to "promote a better understanding of the U.S. economy by providing the most timely, relevant, and accurate economic data in an objective and cost-effective manner".
BEA has about 500 employees and an annual budget of approximately $96.5 million.""
""Computer manufacturing "",,,,""Computer-aided manufacturing (CAM) is the use of software to control machine tools and relate in the manufacturing of workpieces. This is not the only definition for CAM, but it is the most common; CAM may also refer to the use of a computer to assist in all operations of a manufacturing plant, including planning, management, transportation and storage. Its primary purpose is to create a faster production process and components and tooling with more precise dimensions and material consistency, which in some cases, uses only the required amount of raw material (thus minimizing waste), while simultaneously reducing energy consumption. CAM is now a system used in schools and lower educational purposes. CAM is a subsequent computer-aided process after computer-aided design (CAD) and sometimes computer-aided engineering (CAE), as the model generated in CAD and verified in CAE can be input into CAM software, which then controls the machine tool. CAM is used in many schools alongside Computer Aided Design (CAD) to create objects.""
""Sustainability "",,,,""In ecology, sustainability is the capacity to endure; it is how biological systems remain diverse and productive indefinitely. Long-lived and healthy wetlands and forests are examples of sustainable biological systems. In more general terms, sustainability is the endurance of systems and processes. The organizing principle for sustainability is sustainable development, which includes the four interconnected domains: ecology, economics, politics and culture. Sustainability science is the study of sustainable development and environmental science.
Sustainability can also be defined as a socio-ecological process characterized by the pursuit of a common ideal. An ideal is by definition unattainable in a given time/space but endlessly approachable and it is this endless pursuit what builds in sustainability in the process (ibid). Healthy ecosystems and environments are necessary to the survival of humans and other organisms. Ways of reducing negative human impact are environmentally-friendly chemical engineering, environmental resources management and environmental protection. Information is gained from green chemistry, earth science, environmental science and conservation biology. Ecological economics studies the fields of academic research that aim to address human economies and natural ecosystems.

Moving towards sustainability is also a social challenge that entails international and national law, urban planning and transport, local and individual lifestyles and ethical consumerism. Ways of living more sustainably can take many forms from reorganizing living conditions (e.g., ecovillages, eco-municipalities and sustainable cities), reappraising economic sectors (permaculture, green building, sustainable agriculture), or work practices (sustainable architecture), using science to develop new technologies (green technologies, renewable energy and sustainable fission and fusion power), or designing systems in a flexible and reversible manner, and adjusting individual lifestyles that conserve natural resources.
Despite the increased popularity of the use of the term "sustainability", the possibility that human societies will achieve environmental sustainability has been, and continues to be, questioned—in light of environmental degradation, climate change, overconsumption, population growth and societies' pursuit of indefinite economic growth in a closed system.""
""Project management techniques "",,,,""In organizational studies, resource management is the efficient and effective development of an organization's resources when they are needed. Such resources may include financial resources, inventory, human skills, production resources, or information technology (IT).
In the realm of project management, processes, techniques and philosophies as to the best approach for allocating resources have been developed. These include discussions on functional vs. cross-functional resource allocation as well as processes espoused by organizations like the Project Management Institute (PMI) through their Project Management Body of Knowledge (PMBOK) methodology of project management. Resource management is a key element to activity resource estimating and project human resource management. Both are essential components of a comprehensive project management plan to execute and monitor a project successfully. As is the case with the larger discipline of project management, there are resource management software tools available that automate and assist the process of resource allocation to projects and portfolio resource transparency including supply and demand of resources. The goal of these tools typically is to ensure that: (i) there are employees within our organization with required specific skill set and desired profile required for a project, (ii) decide the number and skill sets of new employees to hire, and (iii) allocate the workforce to various projects.""
""Systems planning "",,,,""Business system planning (BSP) is a method of analyzing, defining and designing the information architecture of organizations. It was introduced by IBM for internal use only in 1981, although initial work on BSP began during the early 1970s. BSP was later sold to organizations. It is a complex method dealing with interconnected data, processes, strategies, aims and organizational departments.
BSP was a new approach to IA; its goals are to:
Understand issues and opportunities with current applications
Develop future technology supporting the enterprise
Provide executives with direction and a decision-making framework for IT expenditures
Provide information systems (IS) with a developmental blueprint
The result of a BSP project is a technology roadmap aligning investments and business strategy. BSP comprises 15 steps, which are classified into three sections by function.""
""Systems development "",,,,""In software engineering, a software development methodology (also known as a system development methodology, software development life cycle, software development process, software process) is a splitting of software development work into distinct phases (or stages) containing activities with the intent of better planning and management. It is often considered a subset of the systems development life cycle. The methodology may include the pre-definition of specific deliverables and artifacts that are created and completed by a project team to develop or maintain an application.
Common methodologies include waterfall, prototyping, iterative and incremental development, spiral development, rapid application development, extreme programming and various types of agile methodology. Some people consider a life-cycle "model" a more general term for a category of methodologies and a software development "process" a more specific term to refer to a specific process chosen by a specific organization. For example, there are many specific software development processes that fit the spiral life-cycle model.""
""Computer and information systems training "",,,,""Computer engineering is a discipline that integrates several fields of electrical engineering and computer science required to develop computer hardware and software. Computer engineers usually have training in electronic engineering (or electrical engineering), software design, and hardware-software integration instead of only software engineering or electronic engineering. Computer engineers are involved in many hardware and software aspects of computing, from the design of individual microcontrollers, microprocessors, personal computers, and supercomputers, to circuit design. This field of engineering not only focuses on how computer systems themselves work, but also how they integrate into the larger picture.
Usual tasks involving computer engineers include writing software and firmware for embedded microcontrollers, designing VLSI chips, designing analog sensors, designing mixed signal circuit boards, and designing operating systems. Computer engineers are also suited for robotics research, which relies heavily on using digital systems to control and monitor electrical systems like motors, communications, and sensors.
In many institutions, computer engineering students are allowed to choose areas of in-depth study in their junior and senior year, because the full breadth of knowledge used in the design and application of computers is beyond the scope of an undergraduate degree. Other institutions may require engineering students to complete one year of General Engineering before declaring computer engineering as their primary focus.
^ IEEE Computer Society; ACM (December 12, 2004). Computer Engineering 2004: Curriculum Guidelines for Undergraduate Degree Programs in Computer Engineering (PDF). p. iii. Retrieved December 17, 2012. Computer System engineering has traditionally been viewed as a combination of both electronic engineering (EE) and computer science (CS). 
^ Trinity College Dublin. "What is Computer System Engineering". Retrieved April 21, 2006. , "Computer engineers need not only to understand how computer systems themselves work, but also how they integrate into the larger picture. Consider the car. A modern car contains many separate computer systems for controlling such things as the engine timing, the brakes and the air bags. To be able to design and implement such a car, the computer engineer needs a broad theoretical understanding of all these various subsystems & how they interact.
^ "Changing Majors @ Clemson". Clemson University. Retrieved September 20, 2011. 
^ "Declaring a College of Engineering Major". University of Arkansas. Retrieved September 20, 2011. 
^ "Degree Requirements". Carnegie Mellon University. Retrieved September 20, 2011.""
""Hardware selection "",,,,""Castlevania Chronicles (キャッスルヴァニアクロニクル, Kyassuruvania Kuronikuru) is a platform video game developed by Konami and originally released for the Sharp X68000 home computer in 1993 as Akumajō Dracula (悪魔城ドラキュラ, Akumajō Dorakyura, translated Devil's Castle Dracula). In 2001, the game was released in all regions on the PlayStation as Castlevania Chronicles with added features, including an Arranged Mode for new players. The original release is sometimes unofficially dubbed "Akumajō Dracula X68000" to differentiate it from other games in the series. It was made available for download via the PlayStation Network as a PSone Classic on December 18, 2008.
The game follows the same premise as Castlevania for the Nintendo Entertainment System, where the vampire hunter Simon Belmont must defeat Dracula and save Transylvania.""
""Computing equipment management "",,,,""Computing is any goal-oriented activity requiring, benefiting from, or creating a mathematical sequence of steps known as an algorithm — e.g. through computers. Computing includes designing, developing and building hardware and software systems; processing, structuring, and managing various kinds of information; doing scientific research on and with computers; making computer systems behave intelligently; and creating and using communications and entertainment media. The field of computing includes computer engineering, software engineering, computer science, information systems, and information technology.""
""Software maintenance "",,,,""Software maintenance in software engineering is the modification of a software product after delivery to correct faults, to improve performance or other attributes.
A common perception of maintenance is that it merely involves fixing defects. However, one study indicated that over 80% of maintenance effort is used for non-corrective actions. This perception is perpetuated by users submitting problem reports that in reality are functionality enhancements to the system. More recent studies put the bug-fixing proportion closer to 21%.""
""Centralization / decentralization "",,,,""Organizing is a systematic process of structuring, integrating, co-ordinating task goals, and activities to resources in order to attain objectives.""
""Quality assurance "",,,,""Quality assurance (QA) is a way of preventing mistakes or defects in manufactured products and avoiding problems when delivering solutions or services to customers; which ISO 9000 defines as "part of quality management focused on providing confidence that quality requirements will be fulfilled". This defect prevention in quality assurance differs subtly from defect detection and rejection in quality control, and has been referred to as a shift left as it focuses on quality earlier in the process.
Quality assurance comprises administrative and procedural activities implemented in a quality system so that requirements and goals for a product, service or activity will be fulfilled. It is the systematic measurement, comparison with a standard, monitoring of processes and an associated feedback loop that confers error prevention. This can be contrasted with quality control, which is focused on process output.
Two principles included in quality assurance are: "Fit for purpose" (the product should be suitable for the intended purpose); and "right first time" (mistakes should be eliminated). QA includes management of the quality of raw materials, assemblies, products and components, services related to production, and management, production and inspection processes.
Suitable quality is determined by product users, clients or customers, not by society in general. It is not related to cost, and adjectives or descriptors such as "high" and "poor" are not applicable. For example, a low priced product may be viewed as having high quality because it is disposable, whereas another may be viewed as having poor quality because it is not disposable.""
""Network operations "",,,,""The United States Air Force's 26th Network Operations Group (26 NOG) is a network operations unit located at Lackland AFB, Texas. The 26 NOG is commanded by Col Woolley.""
""File systems management "",,,,""TACTIC is a web-based, open source smart process application and digital asset management system supported by Southpaw Technology in Toronto, ON. Designed to optimize busy production environments with high volumes of content traffic, TACTIC applies business or workflow logic to combined database and file system management. Using elements of digital asset management, production asset management and workflow management, TACTIC tracks the creation and development of digital assets through production pipelines. TACTIC is available under both commercial and open-source licenses, and also as a hosted cloud service through Amazon Web Services Marketplace.""
""Information system economics "",,,,""International University Of Management & Administration Benin (French: Institut Universitaire des Métiers d'Avenir, IUMA) is university in Cotonou, Benin. IUMA is recognized by NUC.
The vision of the International University of Management & Administration is to promote young managers by giving them quality training based on official curricula of the leading careers. The IUMA gives aims at giving a high level training to young high school diploma holders.
List of Accredited Courses in the IUMA Cotonou International University Of Management & Administration Benin:
Pharmacy
Accounting
Business Administration
Hotel and Tourism Management
Architecture
Human Biology/Nutrition/Nursing
Hospital Management
Computer Engineering
Computer Science
Management Information System
Economics
Banking & Finance
Marketing
Political Science
Law
International Relation & European Studie
Finance Management
Mass Communication
French Language and Linguistic
Psychology

""
""History of hardware "",,,,""The history of computing hardware in the former Soviet Bloc is somewhat different from that of the Western world. As a result of the CoCom embargo, computers could not be imported on a large scale from capitalist countries. All computer hardware produced in Socialist countries were designed locally. This redevelopment led to some incompatibilities with International Electrotechnical Commission (IEC) and IEEE standards, such as spacing integrated circuit pins at 1⁄10 of a 25 mm length (colloquially a "metric inch") instead of a standard inch of 25.4 mm. This made Soviet chips unsellable on the world market outside the Comecon, and made test machinery more expensive.
^ Weiss, Gus W. (1996), "The Farewell Dossier: Duping the Soviets", Studies in Intelligence (Central Intelligence Agency) 
^ Deza, Michel, Encyclopedia of Distances, Google eBooks, p. 497 
^ Fred Langa, "An Editor's View" sidebar to "Computing in the U.S.S.R.", April 1991 BYTE Magazine, page 129""
""History of software "",,,,""Software can be defined as programmed instructions stored in the memory of stored-program digital computers for execution by the processor. The design for what would have been the first piece of software was written by Ada Lovelace in the 19th century but was never implemented.
Alan Turing is credited with being the first person to come up with a theory for software, which led to the two academic fields of computer science and software engineering. The first generation of software for early stored program digital computers in the late 1940s had its instructions written directly in binary code. Early on, it was very expensive when it was in low quantities, but as it became more popular in the 1980s, prices dropped significantly. It went from being an item that only belonged to the elite to the majority of the population owning one. Software would not be where it is today without Bill Gates and Steve Jobs, two pioneers in the industry that had monumental impacts on the history of software.""
""History of programming languages "",,,,""This article discusses the major developments in the history of programming languages. For a detailed timeline of events, see: Timeline of programming languages.""
""History of computing theory "",,,,""Hypercomputation or super-Turing computation refers to models of computation that can provide outputs that are not Turing computable. For example, a machine that could solve the halting problem would be a hypercomputer; so too would one that can correctly evaluate every statement in Peano arithmetic.
The Church–Turing thesis states that any "effectively computable" function that can be computed by a mathematician with a pen and paper using a finite set of simple algorithms, can be computed by a Turing machine. Hypercomputers compute functions that a Turing machine cannot and which are, hence, not effectively computable in the Church-Turing sense.
Technically the output of a random Turing machine is uncomputable; however, most hypercomputing literature focuses instead on the computation of useful, rather than random, uncomputable functions.""
""Computational thinking "",,,,""Computational Thinking (CT) is a process that generalizes a solution to open-ended problems. Open-ended problems encourage full, meaningful answers based on multiple variables, which require using decomposition, data representation, generalization, modeling, and algorithms found in Computational Thinking. Computational Thinking requires the decomposition of the entire decision making process, the variables involved, and all possible solutions, ensuring that the right decision is made based on the corresponding parameters and limitations of the problem. The term computational thinking was first used by Seymour Papert in 1980 and again in 1996. Computational thinking can be used to algorithmically solve complicated problems of scale, and is often used to realize large improvements in efficiency.""
""Accreditation "",,,,""Accreditation is the process in which certification of competency, authority, or credibility is presented.
Organizations that issue credentials or certify third parties against official standards are themselves formally accredited by accreditation bodies (such as UKAS); hence they are sometimes known as "accredited certification bodies". The accreditation process ensures that their certification practices are acceptable, typically meaning that they are competent to test and certify third parties, behave ethically and employ suitable quality assurance.""
""Model curricula "",,,,""Allyson Robinson is an American human rights activist, specializing in LGBT rights in the United States. She attended West Point before gender reassignment, graduated in 1994 majoring in her undergraduate degree in physics, and was then commissioned as an officer serving in the U.S. Army until 1999. She held the rank of Captain. Also prior to transition, she became an ordained Baptist minister, earning from the Baylor University's George W. Truett Theological Seminary, a Master of Divinity (M.Div.) with an emphasis on social justice.
In 2008 she joined the Human Rights Campaign (HRC), oversaw HRC's program to create model curricula for LGBT diversity training in the workplace and advanced to the executive director in 2012.
Later that year she began a short controversial tenure as the first executive director of OutServe-SLDN, a network of LGBT actively serving military personnel, following the merger of OutServe and Servicemembers Legal Defense Network in October 2012. She was the first transgender person to lead a national LGBT rights organization that did not have an explicit transgender focus.
^ a b "The HRC Story, "Allyson Robinson"". Human Rights Campaign. Retrieved 2012-10-25. 
^ Ed Pilkington. "Hagel open to review of US military ban on transgender individuals". the Guardian. 
^ a b Brydum, Sunnivie (2013-07-30). "What Really Happened When OutServe-SLDN Came Undone". Advocate.com. Retrieved 2014-06-29. 
^ a b "OutServe-SLDN co-chair steps down". Americablog.com. Retrieved 2014-06-29. 
^ Brydum, Sunnivie (2013-07-22). "Transgender Group Leaves OutServe-SLDN, Joins Startup Group SPART*A". Advocate.com. Retrieved 2014-06-29. 
^ "OutServe-SLDN to Restructure, Josh Seefried to Resign". Advocate.com. 
^ Aravosis, John (July 11, 2013). "Allyson Robinson announces resignation as ED of OutServe-SLDN". Americablog. Retrieved July 17, 2013. 
^ Aravosis, John (July 12, 2013). "OutServe-SLDN closes headquarters, reveals organization is bankrupt". Americablog. Retrieved July 17, 2013. 
^ Londoño, Ernesto (February 5, 2013). "Pentagon to extend certain benefits to same-sex spouses". The Washington Post. Retrieved February 7, 2013. 
^ Geidner, Chris (December 11, 2012). "OutServe-SLDN's Allyson Robinson First Recipient of Arcus Grants Supporting New Leadership". Metro Weekly. Retrieved February 7, 2013. 
^ "SLDN & Outserve Tap Army Veteran To Lead Newly Combined Organization". Servicemembers Legal Defense Network. Retrieved 2012-10-25. 
^ "Military Group Picks Trans Woman As Leader". Buzzfeed. Retrieved 2012-10-25.""
""Information systems education "",,,,""Bennett High School is located in the University Heights section of Buffalo, New York. The current principal is Bert Stevenson. It is named for Lewis J. Bennett, who donated the land for the school and for All High Stadium. Portions of the movie The Natural were filmed in All High Stadium, although it was filmed as Wrigley Field. Bennett High School is now fully a Exam/Magnet school with three College Prep programs, they are the Academy of International Law (Similar to Pre-law) Business and Computers (similar to Information Systems), and Education and the Arts.""
""Computer science education "",,,,""Computer science is the scientific and practical approach to computation and its applications. It is the systematic study of the feasibility, structure, expression, and mechanization of the methodical procedures (or algorithms) that underlie the acquisition, representation, processing, storage, communication of, and access to information. An alternate, more succinct definition of computer science is the study of automating algorithmic processes that scale. A computer scientist specializes in the theory of computation and the design of computational systems.
Its fields can be divided into a variety of theoretical and practical disciplines. Some fields, such as computational complexity theory (which explores the fundamental properties of computational and intractable problems), are highly abstract, while fields such as computer graphics emphasize real-world visual applications. Still other fields focus on challenges in implementing computation. For example, programming language theory considers various approaches to the description of computation, while the study of computer programming itself investigates various aspects of the use of programming language and complex systems. Human–computer interaction considers the challenges in making computers and computations useful, usable, and universally accessible to humans.""
""CS1 "",,,,""CS1 is a series of programmable logic controllers from Omron, and is their current top-tier offering. They come in two flavors - CS1G and CS1H, where CS1H is the more powerful one.""
""Computer engineering education "",,,,""The Federal University of Paraíba (Portuguese: Universidade Federal da Paraíba, UFPB) is a public university whose main campus is located in the city of João Pessoa, Paraíba, Brazil. Together with the Federal University of Campina Grande, it is the main university of the state of Paraiba, Brazil.
UFPB offers degrees (Bachelor's, Master's and Doctoral) in areas such as the liberal arts (including Law), health sciences (including Medicine and Dentistry), engineering and technology (Computer Science and Computer Engineering), business, education and the fine arts (Music, Theater, Art).
There are also campuses in Areia, Bananeiras and North Coast (Rio Tinto and Mamanguape).

""
""Information technology education "",,,,""Information technology (IT) is the application of computers to store, retrieve, transmit and manipulate data, often in the context of a business or other enterprise. IT is considered a subset of information and communications technology (ICT). In 2012, Zuppo proposed an ICT hierarchy where each hierarchy level "contain some degree of commonality in that they are related to technologies that facilitate the transfer of information and various types of electronically mediated communications.". Business/IT was one level of the ICT hierarchy.
The term is commonly used as a synonym for computers and computer networks, but it also encompasses other information distribution technologies such as television and telephones. Several industries are associated with information technology, including computer hardware, software, electronics, semiconductors, internet, telecom equipment, engineering, healthcare, e-commerce and computer services.
Humans have been storing, retrieving, manipulating and communicating information since the Sumerians in Mesopotamia developed writing in about 3000 BC, but the term information technology in its modern sense first appeared in a 1958 article published in the Harvard Business Review; authors Harold J. Leavitt and Thomas L. Whisler commented that "the new technology does not yet have a single established name. We shall call it information technology (IT)." Their definition consists of three categories: techniques for processing, the application of statistical and mathematical methods to decision-making, and the simulation of higher-order thinking through computer programs.
Based on the storage and processing technologies employed, it is possible to distinguish four distinct phases of IT development: pre-mechanical (3000 BC – 1450 AD), mechanical (1450–1840), electromechanical (1840–1940) electronic (1940–present), and moreover, IT as a service. This article focuses on the most recent period (electronic), which began in about 1940.""
""Information science education "",,,,""The Association for Library and Information Science Education (ALISE) promotes innovation and excellence in research, teaching, and service for educators and scholars in Library and Information Science and cognate disciplines internationally through leadership, collaboration, advocacy, and dissemination of research. 
ALISE is the successor organization to the Association of American Library Schools (AALS) which was founded in 1915. AALS replaced the American Library Association (ALA) Roundtable of Library School Instructors (1911-1915), but was not affiliated with the American Library Association until 1953. Organizationally, AALS and ALA had many connections, especially in the first 30 years of AALS’s existence. Donald G. Davis traced the efforts within ALA for the improvement of library education.""
""Computational science and engineering education "",,,,""Osman Yaşar (pronounced [jɑʃɑɾ]) is Empire Innovation Professor at the Computational Science (CPS) department at State University of New York (SUNY) College at Brockport. He holds 3 master’s degrees (physics, nuclear engineering, computer sciences) and a Ph.D. degree (engineering physics). His area of interest is supercomputing applications, computational fluid and particle dynamics, engine combustion modeling, parallel computing, plasma and radiation hydrodynamics, and adaptive mesh refinement. He established the first undergraduate program in computational science in the United States. He also established computational approach to math, science, and technology (C-MST) as a pedagogy at K-12 level. Dr. Yaşar testified before U.S. Congress about his efforts in improving math and science education.
He works closely with the industry, including General Motors, Chrysler, Cummins Engine, Intel, and Lockheed Martin. He served as the President of the Intel Supercomputer Users Group. He made important contributions in the field of science, engineering, and education. As a plasma physicist, he tackled the field of ignition in the combustion (mechanical engineering) community with more accurate models and as a computer scientist he developed algorithms to run record-breaking simulations on particle and fluid systems on supercomputers. Dr. Yaşar has more than 70 publications, developed more than 12 industrial codes, and served as Guest Editor for a number of Special Issues in his field.""
""Software engineering education "",,,,""The Software Engineering 2004 (SE2004) —formerly known as Computing Curriculum Software Engineering (CCSE)— is a document that provides recommendations for undergraduate education in software engineering. SE2004 was initially developed by a steering committee between 2001 and 2004. Its development was sponsored by the Association for Computing Machinery and the IEEE Computer Society. Important components of SE2004 include the Software Engineering Education Knowledge, a list of topics that all graduates should know, as well as a set of guidelines for implementing curricula and a set of proposed courses.""
""Informal education "",,,,""Informal Education is a general term for education outside of a standard school setting. Informal Education is the wise, respectful and spontaneous process of cultivating learning. It works through conversation, and the exploration and enlargement of experience. For Example, Informal education has activities with children, young people and adults. Sometimes there is a clear objective link to some broader plan - example around the development of reading. It can refer to various forms of alternative education, such as:
Unschooling or homeschooling
Autodidacticism (Self-teaching)
Youth work
Informal education consists of accidental, unclear, quantitative information. It usually has a quantitative aspect then a qualitative one. Informal education exceeds formal education in content and knowledge.""
""Computing literacy "",,,,""The following is a timeline of the history of the city of Novosibirsk, Russia.""
""Student assessment "",,,,""For the school board in Nova Scotia, see Conseil Scolaire Acadien Provincial
For the institution based in Cambridge, England, see Centre for Science and Policy
The Colorado Student Assessment Program (CSAP) was an assessment required by the No Child Left Behind Act administered by the Unit of Student Assessment in the Colorado Department of Education (CDE). The CSAP was designed to measure how well students are learning material from the Colorado Model Content Standards, the established content standards that all Colorado public school students should learn. The CSAP only tested four (mathematics, reading and writing, and science) of the thirteen subject areas in the Colorado Model Content Standards.""
""K-12 education "",,,,""K–12 (spoken as "k twelve", "k through twelve", or "k to twelve") is a term for the sum of primary and secondary education. It is used in the United States, Canada, South Korea, Turkey, the Philippines, Egypt, Australia and Iran. P–12 is also occasionally used in Australia. The expression is a shortening of kindergarten (K) for 4- to 6-year-olds through twelfth grade (12) for 7- to 19-year-olds, the first and last grades of free education in these countries, respectively. Also called ELHI, as a short for elementary to high school.""
""Adult education "",,,,""Adult education is a practice in which adults engage in systematic and sustained self–educating activities in order to gain new forms of knowledge, skills, attitudes, or values. It can mean any form of learning adults engage in beyond traditional schooling, encompassing basic literacy to personal fulfillment as a lifelong learner. In particular, adult education reflects a specific philosophy about learning and teaching based on the assumption that adults can and want to learn, that they are able and willing to take responsibility for that learning, and that the learning itself should respond to their needs. Driven by what one needs or wants to learn, the available opportunities, and the manner in which one learns, adult learning is affected by demographics, globalization and technology. The learning happens in many ways and in many contexts just as all adults' lives differ. Adult learning can be in any of the three contexts, i.e.
Formal – Structured learning that typically takes place in an education or training institution, usually with a set curriculum and carries credentials;
Non-formal – Learning that is organized by educational institutions but non credential. Non-formal learning opportunities may be provided in the workplace and through the activities of civil society organizations and groups;
Informal education – Learning that goes on all the time, resulting from daily life activities related to work, family, community or leisure (e.g. community baking class).""
""Employment issues "",,,,""Vote 1 Local Jobs is a minor political party in the state of Victoria, Australia. It was registered as a political party by the Victorian Electoral Commission in 2014.
The party was formed in 2014 by James Purcell, a councillor and mayor of the Shire of Moyne, and is mainly concerned with employment issues in Victoria's Western District. At the 2014 Victorian state election, the party ran two candidates each in the Western Victoria Region and the Northern Metropolitan Region of the Victorian Legislative Council (upper house). Purcell, standing in Western Victoria, was elected to parliament, sitting as a crossbencher from 29 November 2014.
^ "Registration of Vote 1 Local Jobs". Victorian Electoral Commission. 3 November 2014. Retrieved 19 November 2014. 
^ Sinnott, Alex (18 November 2014). "Upper house chance for Vote1 Local Jobs party". The Standard. Retrieved 19 November 2014. 
^ Moss, Dan (14 November 2014). "Shifting alliances and bitter distrust as Vic preference deals go down to the wire". Crikey. Retrieved 19 November 2014.""
""Automation "",,,,""Automation or automatic control, is the use of various control systems for operating equipment such as machinery, processes in factories, boilers and heat treating ovens, switching on telephone networks, steering and stabilization of ships, aircraft and other applications with minimal or reduced human intervention. Some processes have been completely automated.
The biggest benefit of automation is that it saves labor; however, it is also used to save energy and materials and to improve quality, accuracy and precision.
The term automation, inspired by the earlier word automatic (coming from automaton), was not widely used before 1947, when General Motors established an automation department. It was during this time that industry was rapidly adopting feedback controllers, which were introduced in the 1930s.
Automation has been achieved by various means including mechanical, hydraulic, pneumatic, electrical, electronic devices and computers, usually in combination. Complicated systems, such as modern factories, airplanes and ships typically use all these combined techniques.
^ Rifkin, Jeremy (1995). The End of Work: The Decline of the Global Labor Force and the Dawn of the Post-Market Era. Putnam Publishing Group. pp. 66, 75. ISBN 0-87477-779-8. 
^ Bennett, S. (1993). A History of Control Engineering 1930-1955. London: Peter Peregrinus Ltd. On behalf of the Institution of Electrical Engineers. ISBN 0-86341-280-7.""
""Computer supported cooperative work "",,,,""The term computer-supported cooperative work (CSCW) was first coined by Irene Greif and Paul M. Cashman in 1984, at a workshop attended by individuals interested in using technology to support people in their work. At about this same time, in 1987 Dr. Charles Findley presented the concept of Collaborative Learning-Work. According to Carstensen and Schmidt, CSCW addresses "how collaborative activities and their coordination can be supported by means of computer systems." On the one hand, many authors consider that CSCW and groupware are synonyms. On the other hand, different authors claim that while groupware refers to real computer-based systems, CSCW focuses on the study of tools and techniques of groupware as well as their psychological, social, and organizational effects. The definition of Wilson (1991) expresses the difference between these two concepts:

CSCW [is] a generic term, which combines the understanding of the way people work in groups with the enabling technologies of computer networking, and associated hardware, software, services and techniques.""
""Economic impact "",,,,""An economic impact analysis (EIA) examines the effect of an event on the economy in a specified area, ranging from a single neighborhood to the entire globe. It usually measures changes in business revenue, business profits, personal wages, and/or jobs. The economic event analyzed can include implementation of a new policy or project, or may simply be the presence of a business or organization. An economic impact analysis is commonly conducted when there is public concern about the potential impacts of a proposed project or policy.
An economic impact analysis typically measures or estimates the change in economic activity between two scenarios, one assuming the economic event occurs, and one assuming it does not occur (which is referred to as the counterfactual case). This can be accomplished either before or after the event (ex ante or ex post).""
""Offshoring "",,,,""Offshoring is the relocation of a business process from one country to another—typically an operational process, such as manufacturing, or supporting processes, such as accounting. Typically this refers to a company business, although state governments may also employ offshoring. More recently, offshoring has been associated primarily with the outsourcing of technical and administrative services supporting domestic and global operations from outside the home country ("offshore outsourcing"), by means of internal (captive) or external (outsourcing) delivery models.
The term is in use in several distinct but closely related ways. It is sometimes used broadly to include substitution of a service from any foreign source for a service formerly produced internally to the firm. In other cases, only imported services from subsidiaries or other closely related suppliers are included. A further complication is that intermediate goods, such as partially completed computers, are not consistently included in the scope of the term.
Offshoring can be seen in the context of either production offshoring or services offshoring. After its accession to the World Trade Organization (WTO) in 2001, the People's Republic of China emerged as a prominent destination for production offshoring. Another focus area has been the software industry as part of global software development and developing global information systems. After technical progress in telecommunications improved the possibilities of trade in services, India became a country leading in this domain, though many parts of the world are now emerging as offshore destinations.
The economic logic is to reduce costs, sometimes called labor arbitrage, to improve corporate profitability. Jobs are added in the destination country providing the goods or services (generally a lower-cost labor country), but are subtracted in the higher-cost labor country. The increased safety net costs of the unemployed may be absorbed by the government (taxpayers) in the high-cost country or by the company doing the offshoring. Europe experienced less offshoring than the United States due to policies that applied more costs to corporations and cultural barriers.
^ "The Offshoring of American Government", Cornell Law Review, Nov. 2008, available: http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1143044
^ Stephan Manning, Silvia Massini, Arie Lewin (October 20, 2008). "SSRN-A Dynamic Perspective on Next-Generation Offshoring: The Global Sourcing of Science and Engineering Talent". Academy of Management Perspectives (Social Science Electronic Publishing) 22 (3): 35–54. doi:10.5465/amp.2008.34587994. Retrieved 8 April 2014. 
^ See "Appendix II: Definitions of Offshoring" in General Accounting Office: "International Trade: Current Government Data Provide Limited Insight into Offshoring of Services", September 2004. Imported intermediate goods are included in offshoring in "Swenson, D: "International Outsourcing", in The New Palgrave Dictionary of Economics, 2008.
^ "Staying put. European jobs are not coming back because few of them went in the first place". The Economist. January 2013. Retrieved 8 April 2014.""
""Socio-technical systems "",,,,""Sociotechnical systems (STS) in organizational development is an approach to complex organizational work design that recognizes the interaction between people and technology in workplaces. The term also refers to the interaction between society's complex infrastructures and human behaviour. In this sense, society itself, and most of its substructures, are complex sociotechnical systems. The term sociotechnical systems was coined by Eric Trist, Ken Bamforth and Fred Emery, World War II era, based on their work with workers in English coal mines at the Tavistock Institute in London.
Sociotechnical systems pertains to theory regarding the social aspects of people and society and technical aspects of organizational structure and processes. Here, technical does not necessarily imply material technology. The focus is on procedures and related knowledge, i.e. it refers to the ancient Greek term logos. "Technical" is a term used to refer to structure and a broader sense of technicalities. Sociotechnical refers to the interrelatedness of social and technical aspects of an organization or the society as a whole. Sociotechnical theory therefore is about joint optimization, with a shared emphasis on achievement of both excellence in technical performance and quality in people's work lives. Sociotechnical theory, as distinct from sociotechnical systems, proposes a number of different ways of achieving joint optimisation. They are usually based on designing different kinds of organisation, ones in which the relationships between socio and technical elements lead to the emergence of productivity and wellbeing.""
""Codes of ethics "",,,,""Ethical codes are adopted by organizations to assist members in understanding the difference between 'right' and 'wrong' and in applying that understanding to their decisions. An ethical code generally implies documents at three levels: codes of business ethics, codes of conduct for employees, and codes of professional practice.""
""Employment issues "",,,,""Vote 1 Local Jobs is a minor political party in the state of Victoria, Australia. It was registered as a political party by the Victorian Electoral Commission in 2014.
The party was formed in 2014 by James Purcell, a councillor and mayor of the Shire of Moyne, and is mainly concerned with employment issues in Victoria's Western District. At the 2014 Victorian state election, the party ran two candidates each in the Western Victoria Region and the Northern Metropolitan Region of the Victorian Legislative Council (upper house). Purcell, standing in Western Victoria, was elected to parliament, sitting as a crossbencher from 29 November 2014.
^ "Registration of Vote 1 Local Jobs". Victorian Electoral Commission. 3 November 2014. Retrieved 19 November 2014. 
^ Sinnott, Alex (18 November 2014). "Upper house chance for Vote1 Local Jobs party". The Standard. Retrieved 19 November 2014. 
^ Moss, Dan (14 November 2014). "Shifting alliances and bitter distrust as Vic preference deals go down to the wire". Crikey. Retrieved 19 November 2014.""
""Funding "",,,,""Funding is the act of providing financial resources, usually in the form of money, or other values such as effort or time, to finance a need, program, and project, usually by an organisation or government. Generally, this word is used when a firm uses its internal reserves to satisfy its necessity for cash, while the term ‘financing‘ is used when the firms acquires capital from external sources.
Sources of funding include credit, venture capital, donations, grants, savings, subsidies, and taxes. Fundings such as donations, subsidies, and grants that have no direct requirement for return of investment are described as "soft funding" or "crowdfunding". Funding that facilitates the exchange of equity ownership in a company for capital investment via an online funding portal as per the Jumpstart Our Business Startups Act (alternately, the "JOBS Act of 2012") (U.S.) is known as equity crowdfunding.
Funds can be allocated for either short-term or long-term purposes.

""
""Testing, certification and licensing "",,,,""Board certification is the process by which a physician or other professional in the United States demonstrates a mastery of basic knowledge and skills through written, practical, or simulator-based testing.""
""Assistive technologies "",,,,""In computing, Accessibility Toolkit (ATK) refers in particular to the GNOME ATK. ATK is an application programming interface (API) for developing free/open source accessible applications for free and open platforms such as Linux or OpenBSD, led by the GNOME Project.
One common nomenclature to explain an accessibility framework is a usual client-server architecture. In that way, Assistive Technologies (ATs) such as screen readers, would be the clients of that framework, and computer applications would be the server. In this architecture the client and server need to communicate with each other, usually using the IPC technology of the platform. Ideally the accessibility framework expose this to the client and server in a transparent way.
Usually the API for both client-side and server-side applications are the same, and the accessibility framework provides a client-side and a server-side implementation of that API. In the case of GNOME, there are two different APIs, one for the client-side (Assistive Technology Service Provider Interface (AT-SPI)) and a different one for the server-side (ATK) due to historical reasons related to the underlying technologies.
^ "ATK git source code repository". Retrieved 2015-03-23. 
^ "ATK git source code repository, COPYING file". Retrieved 2014-03-30. 
^ Sánchez Prada, Mario (February 3, 2013). "Accessibility in [WebKit]GTK+". Retrieved 2014-03-30.""
""Digital rights management "",,,,""Digital rights management (DRM) schemes are various access control technologies that are used to restrict usage of proprietary hardware and copyrighted works. DRM technologies try to control the use, modification, and distribution of copyrighted works (such as software and multimedia content), as well as systems within devices that enforce these policies.
The use of digital rights management is not universally accepted. Proponents of DRM argue that it is necessary to prevent intellectual property from being copied freely, just as physical locks are needed to prevent personal property from being stolen, that it can help the copyright holder maintain artistic control, and that it can ensure continued revenue streams. Those opposed to DRM contend there is no evidence that DRM helps prevent copyright infringement, arguing instead that it serves only to inconvenience legitimate customers, and that DRM helps big business stifle innovation and competition. Furthermore, works can become permanently inaccessible if the DRM scheme changes or if the service is discontinued. DRM can also restrict users from exercising their legal rights under the copyright law, such as backing up copies of CDs or DVDs, lending materials out through a library, accessing works in the public domain, or using copyrighted materials for research and education under the fair use doctrine, and under French law. The Electronic Frontier Foundation (EFF) and the Free Software Foundation (FSF) consider the use of DRM systems to be an anti-competitive practice.
Worldwide, many laws have been created which criminalize the circumvention of DRM, communication about such circumvention, and the creation and distribution of tools used for such circumvention. Such laws are part of the Copyright Directive, the Digital Millennium Copyright Act, and DADVSI.
The term DRM is also sometimes referred to as "copy protection", "technical protection measures", "copy prevention", or "copy control", although the correctness of doing so is disputed.""
""Copyrights "",,,,""Copyright is a legal right created by the law of a country that grants the creator of an original work exclusive rights for its use and distribution. This is usually only for a limited time. The exclusive rights are not absolute but limited by limitations and exceptions to copyright law, including fair use. A major limitation on copyright is that copyright protects only the original expression of ideas, and not the underlying ideas themselves.
Copyright is a form of intellectual property, applicable to certain forms of creative work. Under US copyright law, legal protection attaches only to fixed representations in a tangible medium. The Berne Convention allows member countries to decide whether creative works must be "fixed" to enjoy copyright. Article 2, Section 2 of the Berne Convention states: "It shall be a matter for legislation in the countries of the Union to prescribe that works in general or any specified categories of works shall not be protected unless they have been fixed in some material form." Some countries do not require that a work be produced in a particular form to obtain copyright protection. For instance, Spain, France, and Australia do not require fixation for copyright protection. The United States and Canada, on the other hand, require that most works must be "fixed in a tangible medium of expression" to obtain copyright protection. U.S. law requires that the fixation be stable and permanent enough to be "perceived, reproduced or communicated for a period of more than transitory duration." Similarly, Canadian courts consider fixation to require that the work be "expressed to some extent at least in some material form, capable of identification and having a more or less permanent endurance." It is often shared among multiple authors, each of whom holds a set of rights to use or license the work, and who are commonly referred to as rightsholders. These rights frequently include reproduction, control over derivative works, distribution, public performance, and "moral rights" such as attribution.
Copyrights are considered territorial rights, which means that they do not extend beyond the territory of a specific jurisdiction. While many aspects of national copyright laws have been standardized through international copyright agreements, copyright laws vary by country.
Typically, the duration of a copyright spans the author's life plus 50 to 100 years (that is, copyright typically expires 50 to 100 years after the author dies, depending on the jurisdiction). Some countries require certain copyright formalities to establishing copyright, but most recognize copyright in any completed work, without formal registration. Generally, copyright is enforced as a civil matter, though some jurisdictions do apply criminal sanctions.
Most jurisdictions recognize copyright limitations, allowing "fair" exceptions to the creator's exclusivity of copyright and giving users certain rights. The development of digital media and computer network technologies have prompted reinterpretation of these exceptions, introduced new difficulties in enforcing copyright, and inspired additional challenges to copyright law's philosophic basis. Simultaneously, businesses with great economic dependence upon copyright, such as those in the music business, have advocated the extension and expansion of copyright and sought additional legal and technological enforcement.
^ See Ideas in Bitlaw, Works Unprotected by Copyright Law; Chapter 1: An Overview of Copyright, Section II.E. Ideas Versus Expression.
^ See Harvard Law School, Module 3: The Scope of Copyright Law. See also Tyler T. Ochoa, Copyright, Derivative Works and Fixation: Is Galoob a Mirage, or Does the Form(GEN) of the Alleged Derivative Work Matter?, 20 Santa Clara High Tech. L.J. 991, 999–1002 (2003) ("Thus, both the text of the Act and its legislative history demonstrate that Congress intended that a derivative work does not need to be fixed in order to infringe."). The legislative history of the 1976 Copyright Act says this difference was intended to address transitory works such as ballets, pantomimes, improvised performances, dumb shows, mime performances, and dancing.
^ See Harvard Law School, Module 3: The Scope of Copyright Law. See also Tyler T. Ochoa, Copyright, Derivative Works and Fixation: Is Galoob a Mirage, or Does the Form(GEN) of the Alleged Derivative Work Matter?, 20 Santa Clara High Tech. L.J. 991, 999–1002 (2003) ("Thus, both the text of the Act and its legislative history demonstrate that Congress intended that a derivative work does not need to be fixed in order to infringe."). The legislative history of the 1976 Copyright Act says this difference was intended to address transitory works such as ballets, pantomimes, improvised performances, dumb shows, mime performances, and dancing.
^ Copyright, University of California, 2014, retrieved 2014-12-15 
^ 17 U.S.C. § 106, United States of America, 2011, retrieved 2014-12-15 
^ "International Copyright Law Survey". Mincov Law Corporation.""
""Software reverse engineering "",,,,""Rigi is an interactive graph editor tool for software reverse engineering using the white box method, i.e. necessitating source code, thus it is mainly aimed at program comprehension. Rigi is distributed by its main author, Hausi A. Müller and the Rigi research group at the University of Victoria.
Rigi provides interactive links from the graphs it produces to the source code, but not vice versa. Rigi renders trees and grid-layout graphs using its own internal engine, but relies on University of Passau's GraphEd for more advanced layouts.
The public version of Rigi has built-in parsers ("fact extractors") for C and Cobol, and can leverage the C++ parser of IBM Visual Age. It can also accept external data in an RSF format (it introduced), so external parses can also feed it data, for example SHriMP tool's Java parser. Some efforts were made to integrate Rigi in Microsoft Visual Studio .NET. Early versions of Bauhaus were also built on top of Rigi; the author of this latter tool notes that the combination was rather slow for graphs having more than 500 nodes. Rigi was reportedly used to analyze some (undisclosed) embedded software at Nokia, in the range of hundreds of thousands of lines of code, and was met with positive feedback from the Nokia engineers.
Active development of Rigi has ceased in 1999, with the last official version released in 2003. A 2008 paper noted that "Rigi is a mature tool that is still used in research and popular in teaching, but it is currently no longer actively evolved and is in bug-fix mode."""
""Patents "",,,,""A patent (/ˈpætənt/ or /ˈpeɪtənt/) is a set of exclusive rights granted by a sovereign state to an inventor or assignee for a limited period of time in exchange for detailed public disclosure of an invention. An invention is a solution to a specific technological problem and is a product or a process. Patents are a form of intellectual property.
The procedure for granting patents, requirements placed on the patentee, and the extent of the exclusive rights vary widely between countries according to national laws and international agreements. Typically, however, a granted patent application must include one or more claims that define the invention. A patent may include many claims, each of which defines a specific property right. These claims must meet relevant patentability requirements, such as novelty, usefulness, and non-obviousness. The exclusive right granted to a patentee in most countries is the right to prevent others, or at least to try to prevent others, from commercially making, using, selling, importing, or distributing a patented invention without permission.
Under the World Trade Organization's (WTO) Agreement on Trade-Related Aspects of Intellectual Property Rights, patents should be available in WTO member states for any invention, in all fields of technology, and the term of protection available should be a minimum of twenty years. Nevertheless, there are variations on what is patentable subject matter from country to country.""
""Trademarks "",,,,""A trademark, trade mark, or trade-mark is a recognizable sign, design, or unique expression related to products or services of a particular source from those of others, although trademarks used to identify services are usually called service marks. The trademark owner can be an individual, business organization, or any legal entity. A trademark may be located on a package, a label, a voucher, or on the product itself. For the sake of corporate identity, trademarks are being displayed on company buildings.""
""Internet governance / domain names "",,,,""The following outline is provided as an overview of and topical guide to the Internet.
Internet – worldwide, publicly accessible network of interconnected computer networks that transmit data by packet switching using the standard Internet Protocol (IP). It is a "network of networks" that consists of millions of interconnected smaller domestic, academic, business, and government networks, which together carry various information and services, such as electronic mail, online chat, file transfer, and the interlinked Web pages and other documents of the World Wide Web.""
""Licensing "",,,,""The verb license or grant license means to give permission. The noun licence (British, Indian, Canadian, Australian, New Zealand, Irish, or South African English) or license (American English) refers to that permission as well as to the document recording that permission.
A license may be granted by a party ("licensor") to another party ("licensee") as an element of an agreement between those parties. A shorthand definition of a license is "an authorization (by the licensor) to use the licensed material (by the licensee)."
In particular, a license may be issued by authorities, to allow an activity that would otherwise be forbidden. It may require paying a fee and/or proving a capability. The requirement may also serve to keep the authorities informed on a type of activity, and to give them the opportunity to set conditions and limitations.
A licensor may grant a license under intellectual property laws to authorize a use (such as copying software or using a (patented) invention) to a licensee, sparing the licensee from a claim of infringement brought by the licensor. A license under intellectual property commonly has several components beyond the grant itself, including a term, territory, renewal provisions, and other limitations deemed vital to the licensor.
Term: many licenses are valid for a particular length of time. This protects the licensor should the value of the license increase, or market conditions change. It also preserves enforceability by ensuring that no license extends beyond the term of the agreement.
Territory: a license may stipulate what territory the rights pertain to. For example, a license with a territory limited to "North America" (Mexico/United States/Canada) would not permit a licensee any protection from actions for use in Japan.
A shorthand definition of license is "a promise by the licensor not to sue the licensee." That means without a license any use or exploitation of intellectual property by a third party would amount to copying or infringement. Such copying would be improper and could, by using the legal system, be stopped if the intellectual property owner wanted to do so.
Intellectual property licensing plays a major role in business, academia and broadcasting. Business practices such as franchising, technology transfer, publication and character merchandising entirely depend on the licensing of intellectual property. Land licensing (proprietary licensing) and IP licensing form sub-branches of law born out of the interplay of general laws of contract and specific principles and statutory laws relating to these respective assets.""
""Treaties "",,,,""A treaty is an agreement under international law entered into by actors in international law, namely sovereign states and international organizations. A treaty may also be known as an (international) agreement, protocol, covenant, convention, pact, or exchange of letters, among other terms. Regardless of terminology, all of these forms of agreements are, under international law, equally considered treaties and the rules are the same.
Treaties can be loosely compared to contracts: both are means of willing parties assuming obligations among themselves, and a party to either that fails to live up to their obligations can be held liable under international law.""
""Database protection laws "",,,,""A virtual human or digital clone is the creation or re-creation of a human being in image and voice using computer-generated imagery and sound, that is often indistinguishable from the real actor. This idea was first portrayed in the 1981 film Looker, wherein models had their bodies scanned digitally to create 3D computer generated images of the models, and then animating said images for use in TV commercials. Two 1992 books used this concept: "Fools" by Pat Cadigan, and Et Tu, Babe by Mark Leyner.
In general, virtual humans employed in movies are known as synthespians, virtual actors, vactors, cyberstars, or "silicentric" actors. There are several legal ramifications for the digital cloning of human actors, relating to copyright and personality rights. People who have already been digitally cloned as simulations include Bill Clinton, Marilyn Monroe, Fred Astaire, Ed Sullivan, Elvis Presley, Bruce Lee, Audrey Hepburn, Anna Marie Goddard, and George Burns. Ironically, data sets of Arnold Schwarzenegger for the creation of a virtual Arnold (head, at least) have already been made.
The name "Schwarzeneggerization" comes from the 1992 book Et Tu, Babe by Mark Leyner. In one scene, on pages 50–51, a character asks the shop assistant at a video store to have Arnold Schwarzenegger digitally substituted for existing actors into various works, including (amongst others) Rain Man (to replace both Tom Cruise and Dustin Hoffman), My Fair Lady (to replace Rex Harrison), Amadeus (to replace F. Murray Abraham), The Diary of Anne Frank (as Anne Frank), Gandhi (to replace Ben Kingsley), and It's a Wonderful Life (to replace James Stewart). Schwarzeneggerization is the name that Leyner gives to this process. Only 10 years later, Schwarzeneggerization was close to being reality.
By 2002, Schwarzenegger, Jim Carrey, Kate Mulgrew, Michelle Pfeiffer, Denzel Washington, Gillian Anderson, and David Duchovny had all had their heads laser scanned to create digital computer models thereof.
^ a b c Brooks Landon (2002). "Synthespians, Virtual Humans, and Hypermedia". In Veronica Hollinger and Joan Gordon. Edging Into the Future: Science Fiction and Contemporary Cultural Transformation. University of Pennsylvania Press. pp. 57–59. ISBN 0-8122-1804-3. 
^ Barbara Creed (2002). "The Cyberstar". In Graeme Turner. The Film Cultures Reader. Routledge. ISBN 0-415-25281-4.""
""Secondary liability "",,,,""Secondary liability, or indirect infringement, arises when a party materially contributes to, facilitates, induces, or is otherwise responsible for directly infringing acts carried out by another party. The US has statutorily codified secondary liability rules for trademarks and patents, but for matters relating to copyright, this has solely been a product of case law developments. In other words, courts, rather than Congress, have been the primary developers of theories and policies concerning secondary liability.""
""Hardware reverse engineering "",,,,""Engineering is the application of mathematics, empirical evidence and scientific, economic, social, and practical knowledge in order to invent, innovate, design, build, maintain, research, and improve structures, machines, tools, systems, components, materials, and processes.
The discipline of engineering is extremely broad, and encompasses a range of more specialized fields of engineering, each with a more specific emphasis on particular areas of applied science, technology and types of application.
The term Engineering is derived from the Latin ingenium, meaning "cleverness" and ingeniare, meaning "to contrive, devise".""
""Privacy policies "",,,,""A privacy policy is a statement or a legal document (in privacy law) that discloses some or all of the ways a party gathers, uses, discloses, and manages a customer or client's data. It fulfills a legal requirement to protect a customer or client's privacy. Personal information can be anything that can be used to identify an individual, not limited to but including name, address, date of birth, marital status, contact information, ID issue and expiry date, financial records, credit information, medical history, where one travels, and intentions to acquire goods and services. In the case of a business it is often a statement that declares a party's policy on how it collects, stores, and releases personal information it collects. It informs the client what specific information is collected, and whether it is kept confidential, shared with partners, or sold to other firms or enterprises.
The exact contents of a privacy policy will depend upon the applicable law and may need to address requirements across geographical boundaries and legal jurisdictions. Most countries have their own legislation and guidelines of who is covered, what information can be collected, and what it can be used for. In general, data protection laws in Europe cover the private sector as well as the public sector. Their privacy laws apply not only to government operations but also to private enterprises and commercial transactions.""
""Pornography "",,,,""Pornography (often abbreviated as "porn" or "porno" in informal usage) is the portrayal of sexual subject matter for the purpose of sexual arousal. Pornography may be presented in a variety of media, including books, magazines, postcards, photographs, sculpture, drawing, painting, animation, sound recording, film, video, and video games. The term applies to the depiction of the act rather than the act itself, and so does not include live exhibitions like sex shows and striptease. The primary subjects of pornographic depictions are pornographic models, who pose for still photographs, and pornographic actors or porn stars, who perform in pornographic films. If dramatic skills are not involved, a performer in a porn film may also be called a model.
Various groups within society have considered depictions of a sexual nature immoral, addictive and noxious, labeling them pornographic, and attempting to have them suppressed under obscenity and other laws, with varying degrees of success. Such works have also often been subject to censorship and other legal restraints to publication, display or possession. Such grounds and even the definition of pornography have differed in various historical, cultural, and national contexts.
Social attitudes towards the discussion and presentation of sexuality have become more tolerant and legal definitions of obscenity have become more limited, notably beginning in 1969 with Blue Movie by Andy Warhol, the first adult erotic film depicting explicit sex to receive wide theatrical release in the United States, and the subsequent Golden Age of Porn, leading to an industry for the production and consumption of pornography in the latter half of the 20th century. The introduction of home video and the Internet saw a boom in the worldwide porn industry that generates billions of dollars annually. Commercialized pornography accounts for over US$2.5 billion in the United States alone, including the production of various media and associated products and services. This industry employs thousands of performers along with support and production staff. It is also followed by dedicated industry publications and trade groups as well as the mainstream press, private organizations (watchdog groups), government agencies, and political organizations. More recently, sites such as Pornhub, RedTube and YouPorn, have served as repositories for home-made or semi-professional pornography, made available free by its creators (who could be called exhibitionists). It has presented a significant challenge to the commercial pornographic film industry.
Irrespective of the legal or social view of pornography, it has been used in a number of contexts. It is used, for example, at fertility clinics to stimulate sperm donors. Some couples use pornography at times for variety and to create a sexual interest or as part of foreplay. There is also some evidence that pornography can be used to treat voyeurism.
^ H. Mongomery Hyde (1964) A History of Pornography: 1–26.
^ 
^ 
^ 
^ 
^ Staff. "The Truth About California's Adult Entertainment Industry White Paper 1999". Adult Video News. Retrieved 28 April 2014. 
^ Rincover, Arnold (1990). "Can Pornography Be Used as Treatment for Voyeurism?". Toronto Star. 
^ Jackson, B (1969). "A case of voyeurism treated by counterconditioning". Behaviour Research and Therapy 7 (1): 133–4. doi:10.1016/0005-7967(69)90058-8. PMID 5767619.""
""Hate speech "",,,,""Hate speech, outside the law, is speech that attacks a person or group on the basis of attributes such as gender, ethnic origin, religion, race, disability, or sexual orientation.
In law, hate speech is any speech, gesture or conduct, writing, or display which is forbidden because it may incite violence or prejudicial action against or by a protected individual or group, or because it disparages or intimidates a protected individual or group. The law may identify a protected group by certain characteristics. In some countries, a victim of hate speech may seek redress under civil law, criminal law, or both. A website that uses hate speech is called a hate site. Most of these sites contain Internet forums and news briefs that emphasize a particular viewpoint. There has been debate over how freedom of speech applies to the Internet as well as hate speech in general.
Critics have argued that the term "hate speech" is a contemporary example of Newspeak, used to silence critics of social policies that have been poorly implemented in a rush to appear politically correct.""
""Political speech "",,,,""The First Amendment (Amendment I) to the United States Constitution prohibits the making of any law respecting an establishment of religion, impeding the free exercise of religion, abridging the freedom of speech, infringing on the freedom of the press, interfering with the right to peaceably assemble or prohibiting the petitioning for a governmental redress of grievances. It was adopted on December 15, 1791, as one of the ten amendments that constitute the Bill of Rights.
The Bill of Rights was originally proposed as a measure to assuage Anti-Federalist opposition to Constitutional ratification. Initially, the First Amendment applied only to laws enacted by the Congress, and many of its provisions were interpreted more narrowly than they are today. Beginning with Gitlow v. New York (1925), the Supreme Court applied the First Amendment to states—a process known as incorporation—through the Due Process Clause of the Fourteenth Amendment.
In Everson v. Board of Education (1947), the Court drew on Thomas Jefferson's correspondence to call for "a wall of separation between church and State", though the precise boundary of this separation remains in dispute. Speech rights were expanded significantly in a series of 20th and 21st-century court decisions which protected various forms of political speech, anonymous speech, campaign financing, pornography, and school speech; these rulings also defined a series of exceptions to First Amendment protections. The Supreme Court overturned English common law precedent to increase the burden of proof for defamation and libel suits, most notably in New York Times Co. v. Sullivan (1964). Commercial speech, however, is less protected by the First Amendment than political speech, and is therefore subject to greater regulation.
The Free Press Clause protects publication of information and opinions, and applies to a wide variety of media. In Near v. Minnesota (1931) and New York Times v. United States (1971), the Supreme Court ruled that the First Amendment protected against prior restraint—pre-publication censorship—in almost all cases. The Petition Clause protects the right to petition all branches and agencies of government for action. In addition to the right of assembly guaranteed by this clause, the Court has also ruled that the amendment implicitly protects freedom of association.""
""Technology and censorship "",,,,""Steven James Murdoch is a security researcher in the Computer Science Department, University College London. His research covers privacy-enhancing technology, Internet censorship, and anonymous communication, in particular Tor. He is also known for discovering several vulnerabilities in the EMV bank chipcard payment system (Chip and PIN).""
""Censoring filters "",,,,""Use of the Internet in Qatar has grown rapidly and is now widespread, but Internet access is also heavily filtered.

""
""Governmental surveillance "",,,,""Surveillance (/sərˈveɪ.əns/ or /sərˈveɪləns/) is the monitoring of the behavior, activities, or other changing information, usually of people for the purpose of influencing, managing, directing, or protecting them. This can include observation from a distance by means of electronic equipment (such as CCTV cameras), or interception of electronically transmitted information (such as Internet traffic or phone calls); and it can include simple, relatively no- or low-technology methods such as human intelligence agents and postal interception. The word surveillance comes from a French phrase for "watching over" ("sur" means "from above" and "veiller" means "to watch"), and is in contrast to more recent developments such as sousveillance.
Surveillance is used by governments for intelligence gathering, the prevention of crime, the protection of a process, person, group or object, or for the investigation of crime. It is also used by criminal organizations to plan and commit crimes such as robbery and kidnapping, by businesses to gather intelligence, and by private investigators.
Surveillance is often a violation of privacy, and is opposed by various civil liberties groups and activists. Liberal democracies have laws which restrict domestic government and private use of surveillance, usually limiting it to circumstances where public safety is at risk. Authoritarian government seldom have any domestic restrictions; and international espionage is common among all types of countries.""
""Corporate surveillance "",,,,""Surveillance (/sərˈveɪ.əns/ or /sərˈveɪləns/) is the monitoring of the behavior, activities, or other changing information, usually of people for the purpose of influencing, managing, directing, or protecting them. This can include observation from a distance by means of electronic equipment (such as CCTV cameras), or interception of electronically transmitted information (such as Internet traffic or phone calls); and it can include simple, relatively no- or low-technology methods such as human intelligence agents and postal interception. The word surveillance comes from a French phrase for "watching over" ("sur" means "from above" and "veiller" means "to watch"), and is in contrast to more recent developments such as sousveillance.
Surveillance is used by governments for intelligence gathering, the prevention of crime, the protection of a process, person, group or object, or for the investigation of crime. It is also used by criminal organizations to plan and commit crimes such as robbery and kidnapping, by businesses to gather intelligence, and by private investigators.
Surveillance is often a violation of privacy, and is opposed by various civil liberties groups and activists. Liberal democracies have laws which restrict domestic government and private use of surveillance, usually limiting it to circumstances where public safety is at risk. Authoritarian government seldom have any domestic restrictions; and international espionage is common among all types of countries.""
""Taxation "",,,,""A tax (from the Latin taxo) is a financial charge or other levy imposed upon a taxpayer (an individual or legal entity) by a state or the functional equivalent of a state to fund various public expenditures. A failure to pay, or evasion of or resistance to taxation, is usually punishable by law. Taxes are also imposed by many administrative divisions. Taxes consist of direct or indirect taxes and may be paid in money or as its labour equivalent. A few countries impose almost no taxation at all, such as the United Arab Emirates and Saudi Arabia.""
""Antitrust and competition "",,,,""The Senate Judiciary Subcommittee on Antitrust, Competition Policy and Consumer Rights is one of six subcommittees within the Senate Judiciary Committee.""
""Governmental regulations "",,,,""Regulation is an abstract concept of management of complex systems according to a set of rules and trends. In systems theory, these types of rules exist in various fields of biology and society, but the term has slightly different meanings according to context. For example:
in biology, gene regulation allows living organisms to adapt to their environment and maintain homeostasis
in government, typically a regulation specifically means a piece of delegated legislation drafted by subject matter experts to enforce a statutory instrument (primary legislation)
in business, industry self-regulation occurs through self-regulatory organizations and trade associations which allow industries to set rules with less government involvement
in psychology, self-regulation theory is the study of how individuals regulate their thoughts and behaviors to reach goals

""
""Online auctions policy "",,,,""An online auction is an auction which is held over the internet. Online auctions come in many different formats, but most popularly they are ascending English auctions, descending Dutch auctions, first-price sealed-bid, Vickrey auctions, or sometimes even a combination of multiple auctions, taking elements of one and forging them with another. The scope and reach of these auctions have been propelled by the Internet to a level beyond what the initial purveyors had anticipated. This is mainly because online auctions break down and remove the physical limitations of traditional auctions such as geography, presence, time, space, and a small target audience. This influx in reachability has also made it easier to commit unlawful actions within an auction. In 2002, online auctions were projected to account for 30% of all online e-commerce due to the rapid expansion of the popularity of the form of electronic commerce.
^ a b Bapna, R.; Goes, P.; Gupta, A. (2001). "Insights and analyses of online auctions". Communications of the ACM 44 (11): 42. doi:10.1145/384150.384160. 
^ Albert, M. R. (2002). "E-Buyer Beware: Why Online Auction Fraud Should Be Regulated". American Business Law Journal 39 (4): 575. doi:10.1111/j.1744-1714.2002.tb00306.x. 
^ Vakrat, Y.; Seidmann, A. (2000). "Implications of the bidders' arrival process on the design of online auctions". Proceedings of the 33rd Annual Hawaii International Conference on System Sciences. p. 7. doi:10.1109/HICSS.2000.926822. ISBN 0-7695-0493-0.""
""Consumer products policy "",,,,""Consumer protection is a group of laws and organizations designed to ensure the rights of consumers as well as fair trade, competition and accurate information in the marketplace. The laws are designed to prevent businesses that engage in fraud or specified unfair practices from gaining an advantage over competitors. They may also provide additional protection for those most vulnerable in society. Consumer protection laws are a form of government regulation, which aim to protect the rights of consumers. For example, a government may require businesses to disclose detailed information about products—particularly in areas where safety or public health is an issue, such as food. Consumer protection is linked to the idea of consumer rights, and to the formation of consumer organizations, which help consumers make better choices in the marketplace and get help with consumer complaints.
Other organizations that promote consumer protection include government organizations and self-regulating business organizations such as consumer protection agencies and organizations, the Federal Trade Commission, ombudsmen, Better Business Bureaus, etc.
A consumer is defined as someone who acquires goods or services for direct use or ownership rather than for resale or use in production and manufacturing.
Consumer interests can also be protected by promoting competition in the markets which directly and indirectly serve consumers, consistent with economic efficiency, but this topic is treated in competition law.
Consumer protection can also be asserted via non-government organizations and individuals as consumer activism""
""Censoring filters "",,,,""Use of the Internet in Qatar has grown rapidly and is now widespread, but Internet access is also heavily filtered.

""
""Broadband access "",,,,""Internet access is the process that enables individuals and organisations to connect to the Internet using computer terminals, computers, mobile devices, sometimes via computer networks. Once connected to the Internet, users can access Internet services, such as email and the World Wide Web. Internet service providers (ISPs) offer Internet access through various technologies that offer a wide range of data signaling rates (speeds).
Consumer use of the Internet first became popular through dial-up Internet access in the 1990s. By the first decade of the 21st century, many consumers in developed nations used faster, broadband Internet access technologies. As of 2014, broadband was ubiquitous around the world, with a global average connection speed exceeding 4 Mbit/s.
^ "Akamai Releases Second Quarter 2014 ‘State of the Internet' Report". Akamai. 30 September 2014. Retrieved 11 October 2014.""
""Net neutrality "",,,,""Net neutrality (also network neutrality, Internet neutrality, or net equality) is the principle that Internet service providers and governments should treat all data on the Internet the same, not discriminating or charging differentially by user, content, site, platform, application, type of attached equipment, or mode of communication. The term was coined by Columbia University media law professor Tim Wu in 2003, as an extension of the longstanding concept of a common carrier.
A widely cited example of a violation of net neutrality principles was the Internet service provider Comcast surreptitiously slowing uploads from peer-to-peer file sharing applications using forged packets. Research suggests that a combination of policy instruments will help realize the range of valued political and economic objectives central to the network neutrality debate. Combined with strong public opinion, this has led some governments to regulate broadband internet services as a public utility, similar to electricity, gas and water supply.

""
""Age-based restrictions "",,,,""The age of majority is the threshold of adulthood as it is conceptualized (and recognized or declared) in law. It is the chronological moment when minors cease to legally be considered children and assume control over their persons, actions, and decisions, thereby terminating the legal control and legal responsibilities of their parents or guardian over and for them. Most countries set majority at 18. The word majority here refers to having greater years and being of full age; it is opposed to minority, the state of being a minor. The law in a given jurisdiction may never actually use the term "age of majority" and the term thereby refers to a collection of laws bestowing the status of adulthood. The age of majority is a legally fixed age, concept, or statutory principle, which may differ depending on the jurisdiction, and may not necessarily correspond to actual mental or physical maturity of an individual.
Age of majority should not be confused with the age of sexual consent, marriageable age, school leaving age, drinking age, driving age, voting age, smoking age, etc., which all may be independent of, and sometimes set at a different age from, the age of majority.
Although a person may attain the age of majority in a particular jurisdiction, they may still be subject to age-based restrictions regarding matters such as the right to vote or stand for elective office, act as a judge, and many others.""
""Acceptable use policy restrictions "",,,,""The visa policy of Nepal is relatively liberal, allowing citizens of almost all nations to obtain a tourist visa on arrival.
In January 2014 Nepal introduced online visa application system.
All tourists are allowed to stay in Nepal for a maximum of 150 days in one calendar year.""
""Universal access "",,,,""Apple Universal Access is a component of the Mac OS X operating system that provides computing abilities to people with visual impairment, hearing impairment, or physical disability.""
""Spoofing attacks "",,,,""In the context of network security, a spoofing attack is a situation in which one person or program successfully masquerades as another by falsifying data, thereby gaining an illegitimate advantage.""
""Phishing "",,,,""Phishing is the attempt to acquire sensitive information such as usernames, passwords, and credit card details (and sometimes, indirectly, money), often for malicious reasons, by masquerading as a trustworthy entity in an electronic communication. The word is a neologism created as a homophone of fishing due to the similarity of using a bait in an attempt to catch a victim. Communications purporting to be from popular social web sites, auction sites, banks, online payment processors or IT administrators are commonly used to lure unsuspecting victims. Phishing emails may contain links to websites that are infected with malware. Phishing is typically carried out by email spoofing or instant messaging, and it often directs users to enter details at a fake website whose look and feel are almost identical to the legitimate one. Phishing is an example of social engineering techniques used to deceive users, and exploits the poor usability of current web security technologies. Attempts to deal with the growing number of reported phishing incidents include legislation, user training, public awareness, and technical security measures. Many websites have now created secondary tools for applications, like maps for games, but they should be clearly marked as to who wrote them, and users should not use the same passwords anywhere on the internet.
Phishing is a continual threat, and the risk is even larger in social media such as Facebook, Twitter, and Google+. Hackers could create a clone of a website and tell you to enter personal information, which is then emailed to them. Hackers commonly take advantage of these sites to attack people using them at their workplace, homes, or in public in order to take personal and security information that can affect the user or company (if in a workplace environment). Phishing takes advantage of the trust that the user may have since the user may not be able to tell that the site being visited, or program being used, is not real; therefore, when this occurs, the hacker has the chance to gain the personal information of the targeted user, such as passwords, usernames, security codes, and credit card numbers, among other things.""
""Identity theft "",,,,""While use of a pseudonym or alias is not necessarily unlawful, identity theft is the deliberate use of someone else's identity, usually as a method to gain a financial advantage or obtain credit and other benefits in the other person's name, and perhaps to the other person's disadvantage or loss. The person whose identity has been assumed may suffer adverse consequences if they are held responsible for the perpetrator's actions. Identity theft occurs when someone uses another's personally identifying information, like their name, identifying number, or credit card number, without their permission, to commit fraud or other crimes.
The term identity theft was coined in 1964.
"Determining the link between data breaches and identity theft is challenging, primarily because identity theft victims often do not know how their personal information was obtained," and identity theft is not always detectable by the individual victims, according to a report done for the FTC. Identity fraud is often but not necessarily the consequence of identity theft. Someone can steal or misappropriate personal information without then committing identity theft using the information about every person, such as when a major data breach occurs. A US Government Accountability Office study determined that "most breaches have not resulted in detected incidents of identity theft". The report also warned that "the full extent is unknown". A later unpublished study by Carnegie Mellon University noted that "Most often, the causes of identity theft is not known," but reported that someone else concluded that "the probability of becoming a victim to identity theft as a result of a data breach is ... around only 2%". More recently, an association of consumer data companies noted that one of the largest data breaches ever, accounting for over four million records, resulted in only about 1,800 instances of identity theft, according to the company whose systems were breached.
An October 2010 article entitled “Cyber Crime Made Easy" explained the level to which hackers are using malicious software. As one security specialist named Gunter Ollmann said, “Interested in credit card theft? There’s an app for that.” This statement summed up the ease with which these hackers are accessing all kinds of information online. The new program for infecting users’ computers is called Zeus; and the program is so hacker friendly that even an inexperienced hacker can operate it. Although the hacking program is easy to use, that fact does not diminish the devastating effects that Zeus (or other software like Zeus) can do to a computer and the user. For example, the article stated that programs like Zeus can steal credit card information, important documents, and even documents necessary for homeland security. If the hacker were to gain this information, it would mean identity theft or even a possible terrorist attack.
^ Synthetic ID Theft Cyber Space Times
^ Hoofnagle, Chris Jay, Identity Theft: Making the Known Unknowns Known. Harvard Journal of Law and Technology, Vol. 21, Fall 2007
^ "Oxford English Dictionary online". Oxford University Press. September 2007. Archived from the original on 2012-07-08. Retrieved 27 September 2010. 
^ Federal Trade Commission – 2006 Identity Theft Survey Report, p.4
^ "Data Breaches Are Frequent, but Evidence of Resulting Identity Theft Is Limited; However, the Full Extent Is Unknown" (PDF). Highlights of GAO-07-737, a report to congressional requesters. gao.gov. Retrieved 22 September 2010. 
^ Sasha Romanosky. "Do Data Breach Disclosure Laws Reduce Identity Theft?" (PDF). Heinz First Research Paper. heinz.cmu.edu. 
^ "Judge tosses all but one Hannaford data breach claim". Secure Thoughts. Retrieved 23 December 2014. 
^ Giles, Jim. "Cyber Crime Made Easy." New Scientist 205.2752 (2010): 20-21. Academic Search Premier. EBSCO. Web. 3 Oct. 2010.""
""Financial crime "",,,,""Financial crimes are crimes against property, involving the unlawful conversion of the ownership of property (belonging to one person) to one's own personal use and benefit. Financial crimes may involve fraud (cheque fraud, credit card fraud, mortgage fraud, medical fraud, corporate fraud, securities fraud (including insider trading), bank fraud, payment (point of sale) fraud, health care fraud); theft; scams or confidence tricks; tax evasion; bribery; embezzlement; identity theft; money laundering; and forgery and counterfeiting, including the production of Counterfeit money and consumer goods.
Financial crimes may involve additional criminal acts, such as computer crime, elder abuse, burglary, armed robbery, and even violent crime such as robbery or murder. Financial crimes may be carried out by individuals, corporations, or by organized crime groups. Victims may include individuals, corporations, governments, and entire economies.""
""Malware / spyware crime "",,,,""Spyware is software that aims to gather information about a person or organization without their knowledge and that may send such information to another entity without the consumer's consent, or that asserts control over a computer without the consumer's knowledge.
"Spyware" is mostly classified into four types: system monitors, trojans, adware, and tracking cookies. Spyware is mostly used for the purposes of tracking and storing Internet users' movements on the Web and serving up pop-up ads to Internet users.
Whenever spyware is used for malicious purposes, its presence is typically hidden from the user and can be difficult to detect. Some spyware, such as keyloggers, may be installed by the owner of a shared, corporate, or public computer intentionally in order to monitor users.
While the term spyware suggests software that monitors a user's computing, the functions of spyware can extend beyond simple monitoring. Spyware can collect almost any type of data, including personal information like internet surfing habits, user logins, and bank or credit account information. Spyware can also interfere with user control of a computer by installing additional software or redirecting web browsers. Some spyware can change computer settings, which can result in slow Internet connection speeds, un-authorized changes in browser settings, or changes to software settings.
Sometimes, spyware is included along with genuine software, and may come from a malicious website or may have been added to the intentional functionality of genuine software (see the paragraph about Facebook below). In response to the emergence of spyware, a small industry has sprung up dealing in anti-spyware software. Running anti-spyware software has become a widely recognized element of computer security practices, especially for computers running Microsoft Windows. A number of jurisdictions have passed anti-spyware laws, which usually target any software that is surreptitiously installed to control a user's computer.
In German-speaking countries, spyware used or made by the government is called govware by computer experts (in common parlance: Regierungstrojaner, literally 'Government Trojan'). Govware is typically a trojan horse software used to intercept communications from the target computer. Some countries like Switzerland and Germany have a legal framework governing the use of such software. In the US, the term policeware has been used for similar purposes.
Use of the term "spyware" has eventually declined as the practice of tracking users has been pushed ever further into the mainstream by major websites and data mining companies; these generally break no known laws and compel users to be tracked, not by fraudulent practices per se, but by the default settings created for users and the language of terms-of-service agreements. As one documented example, on March 7, 2011, CBS/Cnet News reported on a Wall Street Journal analysis revealing the practice of Facebook and other websites of tracking users' browsing activity, linked to their identity, far beyond users' visit and activity within the Facebook site itself. The report stated "Here's how it works. You go to Facebook, you log in, you spend some time there, and then ... you move on without logging out. Let's say the next site you go to is New York Times. Those buttons, without you clicking on them, have just reported back to Facebook and Twitter that you went there and also your identity within those accounts. Let's say you moved on to something like a site about depression. This one also has a tweet button, a Google widget, and those, too, can report back who you are and that you went there." The WSJ analysis was researched by Brian Kennish, founder of Disconnect, Inc. 

^ FTC Report (2005). "[1]"
^ SPYWARE "[2]"
^ Basil Cupa, Trojan Horse Resurrected: On the Legality of the Use of Government Spyware (Govware), LISS 2013, pp. 419-428
^ FAQ – Häufig gestellte Fragen
^ Jeremy Reimer (20 July 2007). "The tricky issue of spyware with a badge: meet 'policeware'". Ars Technica. 
^ Cooley, Brian (March 7, 2011). "'Like,' 'tweet' buttons divulge sites you visit: CNET News Video". CNet News. Retrieved March 7, 2011.""
""Governmental regulations "",,,,""Regulation is an abstract concept of management of complex systems according to a set of rules and trends. In systems theory, these types of rules exist in various fields of biology and society, but the term has slightly different meanings according to context. For example:
in biology, gene regulation allows living organisms to adapt to their environment and maintain homeostasis
in government, typically a regulation specifically means a piece of delegated legislation drafted by subject matter experts to enforce a statutory instrument (primary legislation)
in business, industry self-regulation occurs through self-regulatory organizations and trade associations which allow industries to set rules with less government involvement
in psychology, self-regulation theory is the study of how individuals regulate their thoughts and behaviors to reach goals

""
""Medical records "",,,,""The terms medical record, health record, and medical chart are used somewhat interchangeably to describe the systematic documentation of a single patient's medical history and care across time within one particular health care provider's jurisdiction. The medical record includes a variety of types of "notes" entered over time by health care professionals, recording observations and administration of drugs and therapies, orders for the administration of drugs and therapies, test results, x-rays, reports, etc. The maintenance of complete and accurate medical records is a requirement of health care providers and is generally enforced as a licensing or certification prerequisite.
The terms are used for both the physical folder that exists for each individual patient and for the body of information found therein.
Medical records have traditionally been compiled and maintained by health care providers, but advances in online data storage have led to the development of personal health records (PHR) that are maintained by patients themselves, often on third-party websites. This concept is supported by US national health administration entities and by AHIMA, the American Health Information Management Association.

Because many consider the information in medical records to be sensitive personal information covered by expectations of privacy, many ethical and legal issues are implicated in their maintenance, such as third-party access and appropriate storage and disposal. Although the storage equipment for medical records generally is the property of the health care provider, the actual record is considered in most jurisdictions to be the property of the patient, who may obtain copies upon request.""
""Personal health records "",,,,""A personal health record, or PHR, is a health record where health data and information related to the care of a patient is maintained by the patient. This stands in contrast to the more widely used electronic medical record, which is operated by institutions (such as hospitals) and contains data entered by clinicians or billing data to support insurance claims. The intention of a PHR is to provide a complete and accurate summary of an individual's medical history which is accessible online. The health data on a PHR might include patient-reported outcome data, lab results, data from devices such as wireless electronic weighing scales or collected passively from a smartphone.

""
""Genetic information "",,,,""A nucleic acid sequence is a succession of letters that indicate the order of nucleotides within a DNA (using GACT) or RNA (GACU) molecule. By convention, sequences are usually presented from the 5' end to the 3' end. For DNA, the sense strand is used. Because nucleic acids are normally linear (unbranched) polymers, specifying the sequence is equivalent to defining the covalent structure of the entire molecule. For this reason, the nucleic acid sequence is also termed the primary structure.
The sequence has capacity to represent information. Biological deoxyribonucleic acid represents the information which directs the functions of a living thing. In that context, the term genetic sequence is often used. Sequences can be read from the biological raw material through DNA sequencing methods.
Nucleic acids also have a secondary structure and tertiary structure. Primary structure is sometimes mistakenly referred to as primary sequence. Conversely, there is no parallel concept of secondary or tertiary sequence.""
""Patient privacy "",,,,""Medical privacy or health privacy is the practice of keeping information about a patient confidential. This involves both conversational discretion on the part of health care providers, and the security of medical records. The terms can also refer to the physical privacy of patients from other patients and providers while in a medical facility. Modern concerns include the degree of disclosure to insurance companies, employers, and other third parties. The advent of electronic medical records has raised new concerns about privacy, balanced with efforts to reduce duplication of services and medical errors.""
""Health information exchanges "",,,,""Health information exchange (HIE) is the mobilization of healthcare information electronically across organizations within a region, community or hospital system. In practice the term HIE may also refer to the organization that facilitates the exchange.
HIE provides the capability to electronically move clinical information among different health care information systems. The goal of HIE is to facilitate access to and retrieval of clinical data to provide safer and more timely, efficient, effective, and equitable patient-centered care. HIE is also useful to public health authorities to assist in analyses of the health of the population.
HIE systems facilitate the efforts of physicians and clinicians to meet high standards of patient care through electronic participation in a patient's continuity of care with multiple providers. Secondary health care provider benefits include reduced expenses associated with:
the manual printing, scanning and faxing of documents, including paper and ink costs, as well as the maintenance of associated office machinery
the physical mailing of patient charts and records, and phone communication to verify delivery of traditional communications, referrals, and test results
the time and effort involved in recovering missing patient information, including any duplicate tests required to recover such information
According to an internal study at Sushoo Health Information Exchange, the current method of exchanging patients' health information accounts for approximately $17,160 of expenses annually for a single-clinician practice.
Formal organizations are now emerging to provide both form and function for health information exchange efforts, both on independent and governmental/regional levels. These organizations are, in many cases, enabled and supported financially by statewide health information exchange grants from the Office of the National Coordinator for Health Information Technology. These grants were legislated into the HITECH components of the American Reinvestment and Recovery Act in 2009. The latter organizations (often called Regional Health Information Organizations, or RHIOs) are ordinarily geographically defined entities which develop and manage a set of contractual conventions and terms, arrange for the means of electronic exchange of information, and develop and maintain HIE standards.
In the United States, federal and state regulations regarding HIEs and HIT (health information technology) are still being defined. Federal regulations and incentive programs such as "Meaningful Use", which is formally known as the EHR Incentive Program,  are rapidly changing the face of this relatively new industry. In addition to changes driven by federal activities the lessons learned in the ongoing implementation of some state-sponsored HIEs (such as the North Carolina HIE) and the fluctuating nature of health care regulations at the level of the state governments themselves is leading to additional refinement. However, HIEs and RHIOs continue to struggle to achieve self-sustainability and the vast majority remain tied to Federal, State, or Independent grant funding in order to remain operational. Some exceptions exist, such as the Indiana HIE.""
""Remote medicine "",,,,""Remote laboratory (also known as online laboratory, remote workbench) is the use of telecommunications to remotely conduct real (as opposed to virtual) experiments, at the physical location of the operating technology, whilst the scientist is utilizing technology from a separate geographical location. Remote laboratory comprehends one or more remote experiments.

""
""Race and ethnicity "",,,,""An ethnic group or ethnicity is a category of people who identify with each other based on common ancestral, social, cultural, or national experiences. Unlike most other social groups, ethnicity is primarily an inherited status. Membership of an ethnic group tends to be defined by a shared cultural heritage, ancestry, origin myth, history, homeland, language and/or dialect, symbolic systems such as religion, mythology and ritual, cuisine, dressing style, art, and physical appearance.
Ethnic groups, derived from the same historical founder population, often continue to speak related languages and share a similar gene pool. By way of language shift, acculturation, adoption and religious conversion, it is possible for some individuals or groups to leave one ethnic group and become part of another (except for ethnic groups emphasizing racial purity as a key membership criterion).
Ethnicity is often used synonymously with ambiguous terms such as nation, people, and minority and migrant groups.
Depending on which source of group identity is emphasized to define membership, the following types of (often mutually overlapping) groups can be identified:
Ethno-linguistic, emphasizing shared language, dialect and/or script — example: Basques
Ethno-national, emphasizing a shared polity and/or sense of national identity — example: Soviet people
Ethno-racial, emphasizing shared physical appearance based on origins in a rather isolated geographical region such as a continent — example: Sub-Saharan Africans. However, well-defined criteria for physical appearance of races cannot be formulated, and every individual cannot unambiguously be identified with a human race only based on physical appearance, and not even based on genetic haplogroups, since completely isolated geographical areas do not exist. Therefore it is debatable if human races exist from biological point of view. In population statistics, race is typically a self-reported identity, based on genealogy rather than physical appearance.
Ethno-regional, emphasizing a distinct local sense of belonging stemming from relative geographic isolation — example: South Islanders
Ethno-religious, emphasizing shared affiliation with a particular religion, denomination and/or sect — example: Sikhs
In many cases – for instance, the sense of Jewish peoplehood – more than one aspect determines membership.
The largest ethnic groups in modern times comprise hundreds of millions of individuals (Han Chinese being the largest), while the smallest are limited to a few dozen individuals (numerous indigenous peoples worldwide). Larger ethnic groups may be subdivided into smaller sub-groups known variously as tribes or clans, which over time may become separate ethnic groups themselves due to endogamy and/or physical isolation from the parent group. Conversely, formerly separate ethnicities can merge to form a pan-ethnicity, and may eventually merge into one single ethnicity. Whether through division or amalgamation, the formation of a separate ethnic identity is referred to as ethnogenesis.""
""Religious orientation "",,,,""An individual's or community's religious orientation involves presumptions about the existence and nature of God or gods, religious prescriptions about morality and communal and personal spirituality. Such presumptions involve the study of psychology, ethics, sociology and anthropology.""
""Men "",,,,""A man is a male human. The term man is usually reserved for an adult male, with the term boy being the usual term for a male child or adolescent. However, the term man is also sometimes used to identify a male human, regardless of age, as in phrases such as "men's basketball".
Like most other male mammals, a man's genome typically inherits an X chromosome from his mother and a Y chromosome from his father. The male fetus produces larger amounts of androgens and smaller amounts of estrogens than a female fetus. This difference in the relative amounts of these sex steroids is largely responsible for the physiological differences that distinguish men from women. During puberty, hormones which stimulate androgen production result in the development of secondary sexual characteristics, thus exhibiting greater differences between the sexes. However, there are exceptions to the above for some intersex and transgender men.""
""Women "",,,,""A woman is a female human. The term woman is usually reserved for an adult, with the term girl being the usual term for a female child or adolescent. The term woman is also sometimes used to identify a female human, regardless of age, as in phrases such as "women's rights". "Woman" may also refer to a person's gender identity. Women with typical genetic development are usually capable of giving birth from puberty until menopause. In the context of gender identity, transgender people who are biologically determined to be male and identify as women cannot give birth. Some intersex people who identify as women cannot give birth because of either sterility or inheriting one or more Y chromosomes. In extremely rare cases, people who have Swyer syndrome can give birth with medical assistance. Throughout history women have assumed or been assigned various social roles.
^ Sexual Orientation and Gender Expression in Social Work Practice, edited by Deana F. Morrow and Lori Messinger (2006, ISBN 0231501862), page 8: "Gender identity refers to an individual's personal sense of identity as [man] or [woman], or some combination thereof."""
""Sexual orientation "",,,,""Sexual orientation is an enduring pattern of romantic or sexual attraction (or a combination of these) to persons of the opposite sex or gender, the same sex or gender, or to both sexes or more than one gender. These attractions are generally subsumed under heterosexuality, homosexuality, and bisexuality, while asexuality (the lack of sexual attraction to others) is sometimes identified as the fourth category.
These categories are aspects of the more nuanced nature of sexual identity and terminology. For example, people may use other labels, such as pansexual or polysexual, or none at all. According to the American Psychological Association, sexual orientation "also refers to a person's sense of identity based on those attractions, related behaviors, and membership in a community of others who share those attractions". Androphilia and gynephilia are terms used in behavioral science to describe sexual orientation as an alternative to a gender binary conceptualization. Androphilia describes sexual attraction to masculinity; gynephilia describes the sexual attraction to femininity. The term sexual preference largely overlaps with sexual orientation, but is generally distinguished in psychological research. A person who identifies as bisexual, for example, may sexually prefer one sex over the other. Sexual preference may also suggest a degree of voluntary choice, whereas the scientific consensus is that sexual orientation is not a choice.
Scientists do not know the exact cause of sexual orientation, but they believe that it is caused by a complex interplay of genetic, hormonal, and environmental influences. They favor biologically-based theories, which point to genetic factors, the early uterine environment, both, or the inclusion of genetic and social factors. There is no substantive evidence which suggests parenting or early childhood experiences play a role when it comes to sexual orientation. Research over several decades has demonstrated that sexual orientation ranges along a continuum, from exclusive attraction to the opposite sex to exclusive attraction to the same sex.
Sexual orientation is reported primarily within biology and psychology (including sexology), but it is also a subject area in anthropology, history (including social constructionism), and law, and there are other explanations that relate to sexual orientation and culture.""
""People with disabilities "",,,,""Disability is the consequence of an impairment that may be physical, cognitive, mental, sensory, emotional, developmental, or some combination of these that result in restrictions on an individual's ability to participate in what is considered "normal" in their everyday society. A disability may be present from birth, or occur during a person's lifetime.

Disabilities is an umbrella term, covering impairments, activity limitations, and participation restrictions. An impairment is a problem in body function or structure; an activity limitation is a difficulty encountered by an individual in executing a task or action; while a participation restriction is a problem experienced by an individual in involvement in life situations. Thus, disability is a complex phenomenon, reflecting an interaction between features of a person’s body and features of the society in which he or she lives.

Disability is a contested concept, with different meanings for different communities. On the one hand, it may be used to refer to physical or mental attributes that some institutions, particularly medicine, view as needing to be fixed (the medical model); it may refer to limitations on participation in social life imposed on people by the constraints of an ableist society (the social model); or the term may serve to name a social identity claimed by people with disabilities in order to mark their shared goals and politics.
The contest over disability's definition arose out of disability activism in the U.S. and U.K. in the 1970s, which challenged how medical conceptions of human variation dominated popular discourse about disabilities and how these were reflected in common terminology (e.g., "handicapped," "cripple"). Debates about proper terminology as well as over appropriate models and their implied politics continue in disability communities and the academic field of disability studies. In many countries the law requires that disabilities be clearly categorized and defined in order to assess which citizens qualify for disability benefits.""
""Geographic characteristics "",,,,""The Bug River (Polish: Bug [buk] or Western Bug; ; Ukrainian: Західний Буг, Zakhidnyy Buh, Belarusian: Захо́дні Буг, Zakhodni Buh; Russian: Западный Буг, Zapadnyy Bug) is a major European river which flows through three countries with a total length of 830 kilometres (520 mi).
A tributary of the Narew River, the Bug forms part of the border between Ukraine and Poland for 185 kilometres (115 mi), and between Belarus and Poland for 178 kilometres (111 mi), and is the fourth longest Polish river.""
""Cultural characteristics "",,,,""The Oksywie culture (ger. Oxhöft-Kultur) was an archaeological culture that existed in the area of modern-day Eastern Pomerania around the lower Vistula river from the 2nd century BC to the early 1st century AD. It is named after the village of Oksywie, now part of the city of Gdynia in northern Poland, where the first archaeological finds typical of this culture were discovered.
Archaeological research during the last few decades near Pomerania in Poland suggests that the transition of the local component of the Pomeranian culture into the Oksywie culture occurred in the 2nd century BC. A connection with the Rugii has been suggested.
Like other cultures of this period, the Oksywie culture related to La Tène cultural characteristics, and possessed traits typically shown from the Baltic cultures. Oksywie culture's ceramics and burial customs indicate strong ties with the Przeworsk culture. Men only had their ashes placed in well made black urns with fine finish and a decorative band around. Their graves were supplied with practical items for the afterlife such as utensils and weapons. Typically buried with the man, this culture would also place swords with one-sided edge, and the graves were often covered or marked by stones. Women's ashes were buried in hollows and supplied with feminine items.

""
""Children "",,,,""Biologically, a child (plural: children) is a human being between the stages of birth and puberty. The legal definition of child generally refers to a minor, otherwise known as a person younger than the age of majority.
Child may also describe a relationship with a parent (such as sons and daughters of any age) or, metaphorically, an authority figure, or signify group membership in a clan, tribe, or religion; it can also signify being strongly affected by a specific time, place, or circumstance, as in "a child of nature" or "a child of the Sixties".
There are many social issues that affect children, such as childhood education, bullying, child poverty, dysfunctional families, child labor, and hunger. Children can be raised by parents, by fosterers, guardians or partially raised in a day care center.""
""Adolescents "",,,,""Adolescence (from Latin adolescere, meaning "to grow up") is a transitional stage of physical and psychological human development that generally occurs during the period from puberty to legal adulthood (age of majority). The period of adolescence is most closely associated with the teenage years, though its physical, psychological and cultural expressions may begin earlier and end later. For example, although puberty has been historically associated with the onset of adolescent development, it now typically begins prior to the teenage years and there has been a normative shift of it occurring in preadolescence, particularly in females (see precocious puberty). Physical growth, as distinct from puberty (particularly in males), and cognitive development generally seen in adolescence, can also extend into the early twenties. Thus chronological age provides only a rough marker of adolescence, and scholars have found it difficult to agree upon a precise definition of adolescence.
A thorough understanding of adolescence in society depends on information from various perspectives, including psychology, biology, history, sociology, education, and anthropology. Within all of these perspectives, adolescence is viewed as a transitional period between childhood and adulthood, whose cultural purpose is the preparation of children for adult roles. It is a period of multiple transitions involving education, training, employment and unemployment, as well as transitions from one living circumstance to another.
The end of adolescence and the beginning of adulthood varies by country and by function. Furthermore, even within a single nation state or culture there can be different ages at which an individual is considered (chronologically and legally) mature enough for society to entrust them with certain privileges and responsibilities. Such milestones include driving a vehicle, having legal sexual relations, serving in the armed forces or on a jury, purchasing and drinking alcohol, voting, entering into contracts, finishing certain levels of education, and marriage. Adolescence is usually accompanied by an increased independence allowed by the parents or legal guardians, including less supervision as compared to preadolescence.
In studying adolescent development, adolescence can be defined biologically, as the physical transition marked by the onset of puberty and the termination of physical growth; cognitively, as changes in the ability to think abstractly and multi-dimensionally; or socially, as a period of preparation for adult roles. Major pubertal and biological changes include changes to the sex organs, height, weight, and muscle mass, as well as major changes in brain structure and organization. Cognitive advances encompass both increases in knowledge and in the ability to think abstractly and to reason more effectively. The study of adolescent development often involves interdisciplinary collaborations. For example, researchers in neuroscience or bio-behavioral health might focus on pubertal changes in brain structure and its effects on cognition or social relations. Sociologists interested in adolescence might focus on the acquisition of social roles (e.g., worker or romantic partner) and how this varies across cultures or social conditions. Developmental psychologists might focus on changes in relations with parents and peers as a function of school structure and pubertal status.""
""Accenture "",,,,""Accenture PLC is a multinational management consulting services company. Its incorporated headquarters have been in Dublin, Ireland since 1 September 2009. It is the world's largest consulting firm as measured by revenues and is a Fortune Global 100 company. As of 2015, the company reported net revenues of $31.0 billion, with more than 373,000 employees serving clients in more than 200 cities in 120 countries. In 2015 Accenture had about 130,000 employees in India, more than in any other country, about 60,000 in the US, and about 50,000 in the Philippines. Accenture's current clients include 94 of the Fortune Global 100 and more than 80 percent of the Fortune Global 500.
Accenture common equity is listed on the New York Stock Exchange, under the symbol ACN, and was added to the S&P 500 index on 5 July 2011. In 2015, Fortune magazine named it as the world's most admired Information Technology Services company.""
""Adobe Systems Incorporated "",,,,""Adobe Systems Incorporated is an American transnational computer software company. The company is headquartered in San Jose, California, United States. Adobe has historically focused upon the creation of multimedia and creativity software products, with a more-recent foray towards rich Internet application software development. It is best known for Photoshop, an image editing software. Adobe Reader, the Portable Document Format (PDF) and Adobe Creative Suite, as well as its successor Adobe Creative Cloud.
Adobe was founded in February 1982 by John Warnock and Charles Geschke, who established the company after leaving Xerox PARC in order to develop and sell the PostScript page description language. In 1985, Apple Computer licensed PostScript for use in its LaserWriter printers, which helped spark the desktop publishing revolution.
As of 2015, Adobe Systems has about 13,500 employees, about 40% of whom work in San Jose. Adobe also has major development operations in Newton, Massachusetts; New York City, New York; Orlando, Florida; Minneapolis, Minnesota; Lehi, Utah; Seattle, Washington; San Francisco and San Luis Obispo, California in the United States.
^ a b c "ADOBE SYSTEMS INC 2014 Annual Report Form (10-K)" (XBRL). United States Securities and Exchange Commission. January 21, 2014. 
^ a b "ADOBE SYSTEMS INC 2014 Q1 Quarterly Report Form (10-Q)" (XBRL). United States Securities and Exchange Commission. March 28, 2014. 
^ a b "2012 Form 10-K, Adobe Systems Incorporated" (PDF). Adobe Systems Incorporated. 
^ "Adobe Fast Facts" (PDF). March 9, 2009. Archived (PDF) from the original on 26 March 2009. Retrieved April 4, 2009. 
^ Edwards, Benj (April 27, 2010). "Four reasons the LaserWriter mattered". MacWorld. Retrieved September 24, 2015.""
""Advanced Information Systems "",,,,""General Dynamics Mission Systems is a business unit of American defense and aerospace company General Dynamics. General Dynamics Mission Systems integrates secure communication and information systems and technology. General Dynamics Mission Systems has core manufacturing in secure communications networks; radios and satellite technology for the defense, cyber, public safety, and intelligence communities.""
""Agere Systems, Inc. "",,,,""Agere Systems Inc. was an integrated circuit components company based in Allentown, Pennsylvania, USA. Spun out of Lucent Technologies in 2002, Agere was merged into LSI Corporation in 2007. LSI was in turn acquired by Avago Technologies in 2014.
Agere was incorporated on August 1, 2000 as a subsidiary of Lucent Technologies and then spun off on June 1, 2002. The name Agere was that of a Texas-based electronics company that Lucent had acquired in 2000, although the pronunciations of the company names are different. The Texas company was pronounced with three syllables and a hard "g": A-gear-uh. The company name was pronounced with two syllables and a hard "g": A-gear.
Apart from the main office in Allentown, the company also maintained offices and facilities in:
Reading, Pennsylvania, USA: The "Reading Works" facility, formerly Lucent/AT&T and Bell Labs. Closed in 2002.
Orlando, Florida, USA: The "Orlando Plant" was Agere's newest wholly owned wafer fabrication facility in the world. Opened in 1984 by AT&T, it was known for a time in the late 1990s as "Cirent Semiconductor" as it was operated as a joint venture with Cirrus Logic Corporation. The Orlando Plant was also home to Bell Labs' "Advanced Research and Development Facility (ADRF). Closed in 2005.
Dallas, Texas, USA: Agere Optoelectronics South (OES), formerly Hermann Technologies. Acquired 2000, closed 2002.
Whitefield, India: located in the city of Bangalore, which is involved in ASIC design and software development.
Raanana, Israel: This office was based on Modem-Art, a developer of advanced processor technology for 3G/UMTS mobile devices, which Agere has acquired in 2005.
China: Shanghai and Shenzhen.
Nieuwegein, Netherlands: This former NCR / AT&T / Lucent Technologies division known under the name WCND (Wireless Communication Network Division) was active in the development of Wi-Fi-technology and closed December 2004.

^ "LSI Logic and Agere Systems to Combine in All-Stock Transaction Valued at Approximately $4.0 Billion" (Press release). LSI Logic Corporation. 2006-12-04. Retrieved 2011-05-01. 
^ Merritt, Rick (2006-12-04). "LSI grabs Agere in $4 billion stock merger". EE Times. Retrieved 2011-05-01. 
^ "LSI Shareholders Approve Agere Merger" (Press release). LSI Logic Corporation. 2007-03-29. Retrieved 2011-05-01. 
^ "LSI Completes Merger With Agere Systems" (Press release). LSI Logic Corporation. 2007-04-02. Retrieved 2011-05-01.""
""Agilent Technologies "",,,,""Agilent Technologies is an American public research, development and manufacturing company established in 1999 as a spin-off from Hewlett-Packard. The resulting IPO of Agilent stock was the largest in the history of Silicon Valley at the time.
The company provides analytical instruments, software, services and consumables for the entire laboratory workflow. Agilent focuses its products and services on six markets: food, environmental and forensics, pharmaceutical, diagnostics, chemical and energy, and research.
^ 
^ Arensman, Russ (1 October 2002). "Unfinished business: managing one of the biggest spin-offs in corporate history would be a challenge even in the best of times. But what Agilent's Ned Barnholt got was the worst of times.". Electronic Business (Reed Business Information) (28.10) – via HighBeam Research. 
^ "Company Information". About Agilent. Agilent Technologies. Retrieved 2015-07-28.""
""Alcatel-Lucent "",,,,""Alcatel-Lucent S.A. (French pronunciation: ​[alkatɛl lysɛnt]) was a French global telecommunications equipment company, headquartered in Boulogne-Billancourt, France. As of January 2016, the company is controlled by Nokia.
The company focused on fixed, mobile and converged networking hardware, IP technologies, software and services, with operations in more than 130 countries. It had been named Industry Group Leader for Technology Hardware & Equipment sector in the 2014 Dow Jones Sustainability Indices review and listed in the 2014 Thomson Reuters Top 100 Global Innovators for the 4th consecutive year. Alcatel-Lucent also owns Bell Laboratories, one of the largest research and development facilities in the communications industry, whose employees have been awarded eight Nobel Prizes and the company holds in excess of 29,000 patents.
Alcatel-Lucent's chief executive officer is Michel Combes and the non-executive chairman of the board is Philippe Camus. Camus joined the company in the third quarter of 2008, alongside Ben Verwaayen as CEO, after Alcatel-Lucent's first CEO Patricia Russo and first chairman Serge Tchuruk resigned. For 2010, the company had revenues of €16 billion and a reported net loss of €334 million. For 2011, revenues were €15 billion, net loss of €1.1 billion. For 2012, revenues were €14.4 billion and net loss of €1.4 billion. After seven consecutive years of negative cash flows, in October 2013 the company announced plans to slash 10,000 employees, or 14% of the total current 72,000 workforce, as a part of a €1 billion cost reduction effort.
In June 2013, Michel Combes announced "The Shift Plan", a three-year plan including portfolio refocusing on IP networking, ultra-broadband access and cloud; 1 billion Euro in cost savings; selective asset sales intended to generate at least 1 billion Euro over the period of the plan and the restructuring of the Group's debt. On October 1, 2014, it announced that it had closed the sale of its subsidiary Alcatel-Lucent Enterprise to China Huaxin Post & Telecommunication Economy Development Center.
On April 15, 2015, Nokia announced that it would acquire Alcatel-Lucent for €15.6 billion and since January 2016, the Finnish company owns 79% of its French-American competitor's capital.[1]
On January 14, 2016, Alcatel-Lucent started operating as a part of the Nokia Group.
^ a b c d e http://www3.alcatel-lucent.com/wps/DocumentStreamerServlet?LMSG_CABINET=Docs_and_Resource_Ctr&LMSG_CONTENT_FILE=Financial_Info/Income_Statements/IR-2013-ALU-20-F.pdf
^ a b c Tonner, Andrew (6 January 2016). "Nokia and Alcatel-Lucent Finally Seal the Deal". The Motley Fool. Retrieved 9 January 2016. 
^ "Overview". 
^ http://www.sustainability-indices.com/images/140911-djsi-review-2014-en-vdef.pdf
^ "Thomson Reuters Names the 2014 Top 100 Global Innovators". Retrieved 18 April 2015. 
^ "Alcatel-Lucent announces Chairman Serge Tchuruk and CEO Pat Russo to step down" (Press release). Alcatel-Lucent. 2008-07-29. Retrieved 2009-04-28. 
^ "Alcatel-Lucent fourth quarter 2010 earnings" (Press release). Alcatel-Lucent. 2011-02-10. 
^ 2012 Annual Report on Form 20-F
^ "Alcatel-Lucent reportedly to reduce workforce by 10,000 jobs". Retrieved October 9, 2013. 
^ "Alcatel-Lucent Builds Future Around IP". Light Reading. Retrieved 18 April 2015. 
^""
""Bell Labs "",,,,""Bell Laboratories (also termed Bell Labs and formerly named AT&T Bell Laboratories and Bell Telephone Laboratories) is a research and scientific development company that now belongs to Nokia. Its headquarters are located in Murray Hill, New Jersey, in addition to other laboratories around the rest of the United States and in other countries.
The historic laboratory originated in the late 19th century as the Volta Laboratory and Bureau created by Alexander Graham Bell. Bell Labs was also at one time a division of the American Telephone & Telegraph Company (AT&T Corporation), half-owned through its Western Electric manufacturing subsidiary.
Researchers working at Bell Labs are credited with the development of radio astronomy, the transistor, the laser, the charge-coupled device (CCD), information theory, the operating systems Unix, Plan 9, Inferno, and the programming languages C, C++, and S. Eight Nobel Prizes have been awarded for work completed at Bell Laboratories.""
""AOL, Inc. "",,,,""AOL Inc. (previously known as America Online, written as AOL and styled as Aol.) is an American global mass media corporation based in New Jersey that develops, grows, and invests in brands and web sites such as The Huffington Post, TechCrunch and Engadget. The company's business spans digital distribution of content, products, and services, which it offers to consumers, publishers, and advertisers.
AOL originally provided dial up service to millions of Americans. At the height of its success it purchased the media conglomerate Time Warner. As dial up rapidly lost ground to broadband in the early 2000s, AOL's fortunes significantly retracted and it lost the vast majority of its value, laying off thousands of employees. AOL was eventually spun off, and Time Warner is worth fourteen times the value of AOL, as of late 2015.
On May 12, 2015, Verizon Communications announced plans to buy AOL for $50 per share in a deal valued at $4.4 billion. The acquisition was completed on June 23, 2015. In the following months, AOL also made deals with Microsoft and acquired several tech properties including Millennial Media and Kanvas to bolster their mobile ad-tech capabilities.""
""Apple, Inc. "",,,,""Apple Inc. is an American multinational technology company headquartered in Cupertino, California, that designs, develops, and sells consumer electronics, computer software, and online services. Its hardware products include the iPhone smartphone, the iPad tablet computer, the Mac personal computer, the iPod portable media player, and the Apple Watch smartwatch. Apple's consumer software includes the OS X and iOS operating systems, the iTunes media player, the Safari web browser, and the iLife and iWork creativity and productivity suites. Its online services include the iTunes Store, the iOS App Store and Mac App Store, and iCloud.
Apple was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne on April 1, 1976, to develop and sell personal computers. It was incorporated as Apple Computer, Inc. on January 3, 1977, and was renamed as Apple Inc. on January 9, 2007, to reflect its shifted focus toward consumer electronics. Apple (NASDAQ: AAPL) joined the Dow Jones Industrial Average on March 19, 2015.
Apple is the world's largest information technology company by revenue, the world's largest technology company by total assets, and the world's second-largest mobile phone manufacturer. In November 2014, in addition to being the largest publicly traded corporation in the world by market capitalization, Apple became the first U.S. company to be valued at over US$700 billion. The company employs 115,000 permanent full-time employees as of July 2015 and maintains 475 retail stores in seventeen countries as of March 2016. It operates the online Apple Store and iTunes Store, the latter of which is the world's largest music retailer. There are over one billion actively used Apple products worldwide as of March 2016.
Apple's worldwide annual revenue totaled $233 billion for the fiscal year ending in September 2015. The company enjoys a high level of brand loyalty and, according to the 2014 edition of the Interbrand Best Global Brands report, is the world's most valuable brand with a valuation of $118.9 billion. By the end of 2014, the corporation continued to receive significant criticism regarding the labor practices of its contractors and its environmental and business practices, including the origins of source materials.

""
""AT&T "",,,,""AT&T Inc. is an American multinational telecommunications corporation, headquartered at Whitacre Tower in downtown Dallas, Texas. AT&T is the second largest provider of mobile telephone and the largest provider of fixed telephone in the United States, and also provides broadband subscription television services. AT&T is the third-largest company in Texas (the largest non-oil company, behind only ExxonMobil and ConocoPhillips, and also the largest Dallas company). As of May 2014, AT&T is the 23rd-largest company in the world as measured by a composite of revenues, profits, assets and market value, and the 16th-largest non-oil company. As of 2016, it is also the 18th-largest mobile telecom operator in the world, with over 128.6 million mobile customers.
AT&T was ranked #6 on the 2015 rankings of the world's most valuable brands published by Millward Brown Optimor.
AT&T Inc. began its existence as Southwestern Bell Corporation, one of seven Regional Bell Operating Companies (RBOC's) created in 1983 in the divestiture of the American Telephone and Telegraph Company (founded 1885, later AT&T Corp.) following the 1982 United States v. AT&T antitrust lawsuit. Southwestern Bell changed its name to SBC Communications Inc. in 1995. In 2005, SBC purchased former parent AT&T Corp. and took on its branding, with the merged entity naming itself AT&T Inc. and using the iconic AT&T Corp. logo and stock-trading symbol.
The current AT&T reconstitutes much of the former Bell System and includes ten of the original 22 Bell Operating Companies, along with the original long distance division.

""
""BAE Systems "",,,,""BAE Systems plc is a British multinational defence, security and aerospace company headquartered in London in the United Kingdom and with operations worldwide. It is among the world's largest defence contractors; it ranked as the second-largest based on applicable 2012 revenues. Its largest operations are in the United Kingdom and United States, where its BAE Systems Inc. subsidiary is one of the six largest suppliers to the US Department of Defense. Other major markets include Australia, India and Saudi Arabia. The company was formed on 30 November 1999 by the £7.7 billion merger of two British companies; Marconi Electronic Systems (MES) – the defence electronics and naval shipbuilding subsidiary of the General Electric Company plc (GEC) – and British Aerospace (BAe) – an aircraft, munitions and naval systems manufacturer.
BAE Systems is the successor to various aircraft, shipbuilding, armoured vehicle, armaments and defence electronics companies, including The Marconi Company, the first commercial company devoted to the development and use of radio; A.V. Roe and Company, one of the world's first aircraft companies; de Havilland, manufacturer of the world's first commercial jet airliner; British Aircraft Corporation, co-manufacturer of the Concorde supersonic transport; Supermarine, manufacturer of the Spitfire; Yarrow Shipbuilders, builder of the Royal Navy's first destroyers; Fairfield Shipbuilding and Engineering Company, pioneer of the triple-expansion engine and builder of the world's first battlecruiser; and Vickers Shipbuilding and Engineering, builder of the Royal Navy's first submarines. Since its formation it has made a number of acquisitions, most notably of United Defense and Armor Holdings of the United States, and sold its shares in Airbus, Astrium, AMS and Atlas Elektronik.
BAE Systems is involved in several major defence projects, including the Lockheed Martin F-35 Lightning II, the Eurofighter Typhoon, the Astute-class submarine and the Queen Elizabeth-class aircraft carriers. BAE Systems is listed on the London Stock Exchange and is a constituent of the FTSE 100 Index.""
""BEA Systems, Inc. "",,,,""BEA Systems, Inc. was a company specialized in enterprise infrastructure software products which was wholly acquired by Oracle Corporation on April 29, 2008.""
""Blizzard Entertainment "",,,,""Blizzard Entertainment, Inc. is an American video game developer and publisher founded February 8, 1991, under the name Silicon & Synapse by three graduates of the University of California, Los Angeles, Michael Morhaime, Frank Pearce, and Allen Adham and is currently a subsidiary of American company Activision Blizzard. Based in Irvine, California, the company originally concentrated primarily on the creation of game ports for other studios before beginning development of their own software in 1993 with the development of games like Rock n' Roll Racing and The Lost Vikings. In 1994 the company became Chaos Studios then Blizzard Entertainment, Inc. after being acquired by distributor Davidson & Associates.
Shortly thereafter, Blizzard shipped out what would become their first breakthrough smash hit Warcraft: Orcs & Humans. Blizzard then went on to create several other successful video games, including other Warcraft sequels, the Diablo series, the StarCraft series, and also in 2004 the popular MMORPG World of Warcraft. Their most recent projects include: the first expansion for Diablo III: Reaper of Souls, the online collectible card game Hearthstone, the fifth expansion for World of Warcraft: Warlords of Draenor, the "Hero Brawler" Heroes of the Storm and the third and final expansion for StarCraft II: Legacy of the Void. There are two titles that are currently in development: Overwatch, an upcoming multiplayer first-person shooter first announced at BlizzCon on November 7, 2014 that entered beta testing in the third quarter of 2015 and the sixth expansion for World of Warcraft: Legion. It was announced on August 6, 2015 at Gamescom 2015 and its beta testing phase is upcoming in 2016.
On July 9, 2008, Activision officially merged with Vivendi Games, culminating in the inclusion of the Blizzard brand name in the title of the resulting holding company. On July 25, 2013, Activision Blizzard announced the purchase of 429 million shares from majority owner Vivendi. As a result, Activision Blizzard became a completely independent company. Blizzard Entertainment offers events to meet players and to announce games: the BlizzCon in California, United States, and the Blizzard Worldwide Invitational in other countries, including France, and South Korea.""
""Blue Sky Studios "",,,,""Blue Sky Studios is an American computer animation film studio based in Greenwich, Connecticut. The studio was founded in 1987 by Michael Ferraro, Carl Ludwig, Alison Brown, David Brown, Chris Wedge and Eugene Troubetzkoy after the company they worked in, MAGI, one of the visual effects studios behind Tron, shut down. Blue Sky Studios has been owned by 20th Century Fox since 1997. Using its in-house rendering software, the studio had worked on visual effects for commercials and films before completely dedicating itself to animated film production in 2002 with the release of Ice Age. Of the studio's ten animated films to date, Ice Age and Rio are its most successful film franchises, and The Peanuts Movie its most critically acclaimed film.

""
""Borland Software Corporation "",,,,""Borland Software Corporation is a software company that facilitates software deployment projects. Borland was first headquartered in Scotts Valley, California, then in Cupertino, California, and now in Austin, Texas. It is now a Micro Focus International subsidiary. It was founded in 1983 by Niels Jensen, Ole Henriksen, Mogens Glad and Philippe Kahn.""
""CA Technologies "",,,,""CA Technologies, formerly known as Computer Associates International, Inc. and CA, Inc., is one of the largest independent software corporations in the world. CA for short, is an American, multinational, publicly held company headquartered in New York, New York. The company creates systems software (and previously applications software) that runs in mainframe, distributed computing, virtual machine and cloud computing environments.
Although the company once sold anti-virus and Internet security commercial software programs for personal computers during its venture into the business-to-consumer ("B2C") market, it remains primarily known for its business-to-business ("B2B") mainframe and distributed (client/server, etc.) information technology ("IT") infrastructure applications since the spin off of their security products into Total Defense. CA Technologies claims that its computer software products are used by "a majority of the Fortune Global 500 companies, government organizations, educational institutions, and thousands of other companies in diverse industries worldwide."
CA Technologies posted $4.4 billion in revenue for fiscal year 2014 (ending March 31, 2014) and maintains offices in more than 40 countries. The company employs approximately 12,700 people (March 31, 2014). CA holds more than 950 patents worldwide, and has more than 900 patent applications pending.
In 2010 the company acquired eight companies to support its Cloud strategy: 3Tera, Nimsoft, NetQoS, Oblicore, Cassatt, 4Base Technology, Arcot Systems, and Hyperformix.""
""Cable & Wireless Worldwide "",,,,""Cable & Wireless Worldwide PLC (informally Cable & Wireless) was a British multinational telecommunications services company headquartered in Bracknell, United Kingdom. It was formed in 2010 by the split of Cable & Wireless plc into two companies, the other being Cable & Wireless Communications.
Cable & Wireless Worldwide specialised in servicing large corporates, governments, carriers and resellers and its services included managed voice, data and IP based services. It had operations in Asia Pacific, Europe, India, the Middle East & Africa and North America. The company was bought by Vodafone in July 2012 and integrated into the business on 1 April 2013.""
""Cadence Design Systems "",,,,""Cadence Design Systems, Inc (NASDAQ: CDNS) is an American electronic design automation (EDA) software and engineering services company, founded in 1988 by the merger of SDA Systems and ECAD, Inc. The company produces software and hardware for designing integrated circuits, systems on chips (SoCs) and printed circuit boards.""
""China Telecom Corporation Limited "",,,,""China Telecommunications Corporation (Chinese: 中国电信集团公司; literally: "China Telecommunications Group Corporation") is a Chinese state-owned telecommunication company. It is the largest fixed-line service and the third largest mobile telecommunication provider in the People's Republic of China. It has two major holding companies: China Telecom Corporation Limited (中国电信股份有限公司) and China Communications Services Corporation Limited (中国通信服务股份有限公司).""
""Cisco Systems, Inc. "",,,,""Cisco Systems, Inc. is an American multinational technology company headquartered in San Jose, California, that designs, manufactures and sells networking equipment. It is considered to be the biggest networking company in the world. The stock was added to the Dow Jones Industrial Average on June 8, 2009, and is also included in the S&P 500 Index, the Russell 1000 Index, NASDAQ-100 Index and the Russell 1000 Growth Stock Index.""
""Citrix Systems, Inc. "",,,,""Citrix Systems, Inc. is an American multinational software company founded in 1989 that provides server, application and desktop virtualization, networking, software-as-a-service (SaaS), and cloud computing technologies.
The company began by developing remote access products for Microsoft operating systems. It licensed source code from Microsoft and has been in partnership with the company throughout its history. Citrix came to prominence in the 1990s as a leader in thin client technology. Through several acquisitions in the mid-2000s, the company expanded into server and desktop virtualization, as well as cloud and Infrastructure as a Service.
Citrix currently services around 330,000 organizations worldwide and is based in Fort Lauderdale, Florida, in the Miami metropolitan area, and Santa Clara, California, with subsidiary operations in California and Massachusetts, and additional development centers in Canada, Germany, Denmark, the United Kingdom, India and Australia.""
""Commerce One "",,,,""Commerce One was a pioneering e-commerce company founded in 1994 as DistriVision in Walnut Creek, California.
The company was renamed Commerce One in 1997, and went public in 1999. The company tripled on opening day in July 1999. They were one of the darlings in the hot B2B (business-to-business) sector, and saw their stock soar from 20 to over 600 in early 2000, before it collapsed in the dot-com crash. In 2001, the company acquired Veo Systems from Asim Abdullah for $300 million.
In October 2002, the company announced that it planned to lay off 400 employees, which was 36% of its staff.
One of the company's technologies was SOX, an XML schema technology that influenced the development of the W3C's XML Schema language, and the Java Architecture for XML Binding (JAXB)
The company filed for Chapter 11 bankruptcy protection on Oct. 6 2004 [1]. In December 2004, a portion of its patent portfolio was sold by a bankruptcy court to JGR Acquisitions, a subsidiary of Novell, Inc., for $15.5 million.[2] The remaining business interests, including all remaining intellectual property rights to the software, together with a patent license from JGR, were sold to new investors that continued to operate the company as Commerce One.
Commerce One formally announced on February 7, 2006, that it had been acquired by Perfect Commerce  – a provider of On-Demand Supplier Relationship Management (SRM) Solutions and The Open Supplier Network. Commerce One, LLC is a wholly owned subsidiary of Perfect Commerce, LLC, a Virginia Limited Liability Company headquartered in Newport News, VA with an office in Paris, France.""
""Compaq Computer Corporation "",,,,""Compaq Computer Corporation was a company founded in 1982 that developed, sold, and supported computers and related products and services. Compaq produced some of the first IBM PC compatible computers, being the first company to legally reverse engineer the IBM Personal Computer. It rose to become the largest supplier of PC systems during the 1990s before being overtaken by HP in 2001. Struggling in the aftermath of the dot-com bubble bust, Compaq was acquired for US$25 billion by HP in 2002. The Compaq brand remained in use by HP for lower-end systems until 2013 when, without warning, the Compaq name was quietly discontinued.
The company was formed by Rod Canion, Jim Harris and Bill Murto—former Texas Instruments senior managers. Murto (SVP of Sales) departed Compaq in 1987, while Canion (President and CEO) and Harris (SVP of Engineering) left under a shakeup in 1991, which saw Eckhard Pfeiffer appointed President and CEO. Pfeiffer served through the 1990s. Ben Rosen provided the venture capital financing for the fledgling company and served as chairman of the board for 18 years from 1983 until September 28, 2000, when he retired and was succeeded by Michael Capellas, who served as the last Chairman and CEO until its merger with HP.
Prior to its takeover the company was headquartered in a facility in northwest unincorporated Harris County, Texas, that now continues as HP's largest United States facility.
^ The Compaq Portable was the first 100% compatible IBM computer clone..., oldcomputers.net
^ The Compaq computer is a full-function portable business computer that resembles the IBM PC in almost every way..., Byte review
^ Rivkin, Jan W. and Porter, Michael E. Matching Dell, Harvard Business School Case 9-799-158, June 6, 1999.
^ "Hewlett-Packard and Compaq Agree to Merge, Creating $87 Billion Global Technology Leader" (Press release). Hewlett-Packard. September 3, 2001. Retrieved October 4, 2008. 
^ "Hewlett-Packard in Deal to Buy Compaq for $25 Billion in Stock". The New York Times. 2001-09-04. Retrieved October 4, 2008. 
^ Chris Ziegler (2012-05-23). "'HP Compaq' branding to end next year, Compaq name will live on for 'basic computing at entry-level pricing'". The Verge. Retrieved 2012-11-16. 
^ "Compaq Names Michael Capellas Chairman". H41131.www4.hp.com. Retrieved 2012-08-26. 
^ "Compaq Appoints Michael D. Capellas President and Chief Executive Officer". H41131.www4.hp.com. Retrieved 2012-08-26.""
""Dell Computer Corporation "",,,,""Dell Inc. is an American privately owned multinational computer technology company based in Round Rock, Texas, United States, that develops, sells, repairs, and supports computers and related products and services. Eponymously named after its founder, Michael Dell, the company is one of the largest technological corporations in the world, employing more than 103,300 people worldwide.
Dell sells personal computers (PCs), servers, data storage devices, network switches, software, computer peripherals, HDTVs, cameras, printers, MP3 players, and electronics built by other manufacturers. The company is well known for its innovations in supply chain management and electronic commerce, particularly its direct-sales model and its "build-to-order" or "configure to order" approach to manufacturing—delivering individual PCs configured to customer specifications. Dell was a pure hardware vendor for much of its existence, but with the acquisition in 2009 of Perot Systems, Dell entered the market for IT services. The company has since made additional acquisitions in storage and networking systems, with the aim of expanding their portfolio from offering computers only to delivering complete solutions for enterprise customers.
Dell was listed at number 51 in the Fortune 500 list, until 2014. After going private in 2013, the newly confidential nature of its financial information prevents the company from being ranked by Fortune. In 2015, it was the third largest PC vendor in the world after Lenovo and HP. Dell is currently the #1 shipper of PC monitors in the world. Dell is the sixth largest company in Texas by total revenue, according to Fortune magazine. It is the second largest non-oil company in Texas – behind AT&T – and the largest company in the Greater Austin area. It was a publicly traded company (NASDAQ: DELL), as well as a component of the NASDAQ-100 and S&P 500, until it was taken private in a leveraged buyout which closed on October 30, 2013.
^ a b De La Merced, Michael J. (October 29, 2013). "Sale of Dell Closes, Moving Company Into Private Ownership". The New York Times. DealBook. Retrieved October 29, 2013. 
^ "Dell Company Profile". Retrieved July 28, 2010. 
^ "2015 annual results". Forbes. 
^ a b c "Form 10-K Annual Report Pursuant to Section 13 or 15(d) of the Securities Exchange Act of 1934 for the Fiscal Year Ended February 3, 2012 Commission File Number: 0-17017 Dell Inc.". i.dell.com. Dell Inc. March 13, 2012. Retrieved October 29, 2014. 
^ "What you don't know about Dell". Bloomberg BusinessWeek. November 2, 2003. Retrieved October 28, 2012. 
^ "Dell selling former site of North Carolina manufacturing plant". statesman.com. Retrieved April 27, 2013. 
^ "Dell company profile". Reuters Financial. Retrieved June 15, 2013. 
^ Carey, David (September 13, 2013). "Silver Lake Investors Said to See Dell as Mixed Blessing". Bloomberg. Retrieved January 9, 2014. 
^ "Dell – Fortune 500 2013 – Fortune". Fortune. Archived from the original on June 15, 2013. Retrieved July 11, 2013. 
^ "Gartner Says Worldwide PC Shipments Declined 8.3 Percent in Fourth Quarter of 2015". www.gartner.com. Retrieved 2016-02-17. 
^ "Dell Captures Top Position in Worldwide PC Monitors in the Second Quarter, Moves Ahead of Samsung for the First Time Since 2007, According to IDC – prUS24322313". Idc.com. September 19, 2013. Archived from the original on October 19, 2013. Retrieved January 9, 2014. 
^ "Fortune 500". CNN. 
^ "Fortune 500 2010: States: Texas Companies". CNN.""
""DiamondCluster International, Inc. "",,,,""Diamond Management & Technology Consultants (casually referred to as Diamond) was an independent management consulting firm founded in 1994, headquartered in Chicago, IL with satellite offices in Hartford, CT, New York City, Washington DC, London, and Mumbai. It was acquired by the British firm, PwC in 2010. Diamond was a smaller player among companies such as Mercer Management Consulting, Deloitte Consulting, and Accenture. The industry segments under which Diamond operated include consumer packaged goods, financial services, and health-care, among numerous others.
^""
""Digital Equipment Corporation "",,,,""Digital Equipment Corporation, also known as DEC and using the trademark Digital, was a major American company in the computer industry from the 1960s to the 1990s. It was a leading vendor of computer systems, including computers, software, and peripherals, and its PDP and successor VAX products were the most successful of all minicomputers in terms of sales.
From 1957 until 1992 its headquarters were located in a former wool mill in Maynard, Massachusetts (since renamed Clock Tower Place and now home to multiple companies). DEC was acquired in June 1998 by Compaq, which subsequently merged with Hewlett-Packard in May 2002. Some parts of DEC, notably the compiler business and the Hudson, Massachusetts facility, were sold to Intel.
Digital Equipment Corporation should not be confused with the unrelated companies Digital Research, Inc or Western Digital, although the latter once manufactured the LSI-11 chipsets used in DEC's low end PDP-11/03 computers.""
""EIS, Inc. "",,,,""Genuine Parts Company, (NYSE: GPC), is a service organization engaged in the distribution of automotive replacement parts, industrial replacement parts, office products and electrical/electronic materials. GPC serves numerous customers from more than 2,600 operations around the world and has approximately 39,000 employees.""
""EMC Corporation "",,,,""EMC Corporation (stylized as EMC²) is an American multinational corporation headquartered in Hopkinton, Massachusetts, United States. EMC sells data storage, information security, virtualization, analytics, cloud computing and other products and services that enable businesses to store, manage, protect, and analyze data. EMC's target markets include large companies and small- and medium-sized businesses across various vertical markets. The stock was added to the New York Stock Exchange on April 6, 1986, and is also listed on the S&P 500 index.
EMC has over 70,000 employees and is the world’s largest provider of data storage systems by market share, competing against NetApp, IBM, Hewlett-Packard, and Hitachi Data Systems (arranged in descending order of external data storage market share). Joseph M. Tucci has been EMC's chief executive since January 2001 and chairman of the board of directors since January 2006; he joined the company in 2000 as president and chief operating officer. Tucci is also chairman of the board of VMware Corporation and chairman of the board of Pivotal Software, which are partially owned by EMC. On October 12, 2015, Dell Inc. announced that it would acquire EMC in a cash-and-stock deal valued at $67 billion—the largest-ever acquisition in the technology industry.

""
""Ericsson "",,,,""Ericsson (Telefonaktiebolaget L. M. Ericsson) is a Swedish multinational corporation that provides communication technology and services. The company offers services, software and infrastructure in information and communications technology (ICT) for telecommunications operators, traditional telecommunications and Internet Protocol (IP) networking equipment, mobile and fixed broadband, operations and business support services, cable television, IPTV, video systems, and an extensive services operation. Ericsson had 35% market share in the 2G/3G/4G mobile network infrastructure market in 2012.
The company was founded in 1876 by Lars Magnus Ericsson; as of 2016 it is headquartered in Stockholm, Sweden. The company employs around 110,000 people and operates in around 180 countries. Ericsson holds over 37,000 granted patents as of May 2015, including many in wireless communications.""
""Ernst & Young "",,,,""Ernst & Young (trading as EY) is a multinational professional services firm headquartered in London, United Kingdom. It is one of the "Big Four" audit firms and is the third largest professional services firm in the world by aggregated revenue in 2014, after PwC and Deloitte.
The organization operates as a network of member firms which are separate legal entities in individual countries. It has 212,000 employees in over 700 offices around 150 countries in the world. It provides assurance (including financial audit), tax, consulting and advisory services to companies.
The firm dates back to 1849 with the founding of Harding & Pullein in England. The current firm was formed by a merger of Ernst & Whinney and Arthur Young & Co. in 1989. It was known as Ernst & Young until 2013, when it underwent a rebranding to EY. The acronym "EY" was already an informal name for the firm prior to its official adoption.
As of 2015, EY was the 11th largest privately owned organization in the United States.
^ a b "Facts & Figures". Ernst Young. 30 June 2011. Retrieved 8 December 2011. 
^ Mark Weinberger. Ernst & Young. Retrieved 23 April 2013.
^ John Ferraro. Ernst & Young. Retrieved 23 April 2013.
^ "EY reports 2015 global revenues up by 11.6%". Retrieved 15 September 2015. 
^ [1].
^ Aubin, Dena (2012-10-01). "Ernst's revenues rise 6.7 percent on advisory growth". Reuters. Retrieved 2013-07-03. 
^ Roxburgh, Helen (2012-12-13). "Tax and advisory boosts KPMG global revenues". Economia (ICAEW). Retrieved 2013-07-03. 
^ "EY at a glance". EY. Retrieved 2013-07-03. 
^ "A timeline of our history". EY. Retrieved 2013-07-03. 
^ Reddan, Fiona (2013-07-01). "Ernst & Young re-brands". The Irish Times. Retrieved 2013-07-03. 
^ "America's Largest Private Companies". Forbes. 18 December 2015. Retrieved 28 October 2016.""
""Forrester Research "",,,,""Forrester Research is an American independent technology and market research company that provides advice on existing and potential impact of technology, to its clients and the public. Forrester Research has five research centers in the US: Cambridge, Massachusetts; New York, New York; San Francisco, California; Washington, D.C.; and Dallas, Texas. It also has four European research centers in Amsterdam, Frankfurt, London, and Paris and four research centers in the APAC region in New Delhi, Singapore, Beijing, and Sydney . The firm has 27 sales locations worldwide. It offers a variety of services including syndicated research on technology as it relates to business, quantitative market research on consumer technology adoption as well as enterprise IT spending, research-based consulting and advisory services, events, workshops, teleconferences, and executive peer-networking programs.""
""Gartner Group "",,,,""Gartner, Inc. is an American research and advisory firm providing information technology related insight headquartered in Stamford, Connecticut, United States. It was known as Gartner Group, Inc until 2001.
Research provided by Gartner is targeted at CIOs and senior IT leaders, marketing leaders and supply chain leaders. Gartner clients include large corporations, government agencies, technology companies and the investment community. The company consists of Research, Executive Programs, Consulting and Events. Founded in 1979, Gartner has over 7,800 employees, including 1,280 in research, located in 85 countries.
Gartner uses Hype Cycles and Magic Quadrants for visualization of its market analysis results.""
""General Dynamics "",,,,""General Dynamics Corporation is an American aerospace and defense company. Formed by mergers and divestitures, it is the world's fifth-largest defense contractor based on 2012 revenues. General Dynamics is headquartered in West Falls Church, Fairfax County, Virginia.
The company has changed markedly in the post–Cold War era of defense consolidation. It has four main business segments: Marine Systems, Combat Systems, Information Systems Technology, and Aerospace. General Dynamics' former Fort Worth Division manufactured one of the Western world's most-produced jet fighters, the General Dynamics F-16 Fighting Falcon—until 1993, when production was sold to Lockheed. In 1999, the company re-entered the airframe business with its purchase of Gulfstream Aerospace.""
""General Electric "",,,,""General Electric (GE) is an American multinational conglomerate corporation incorporated in New York, and headquartered in Fairfield, Connecticut. As of 2015, the company operates through the following segments: Appliances, Power and Water, Oil and Gas, Energy Management, Aviation, Healthcare, Transportation and Capital which cater to the needs of Home Appliances, Financial services, Medical devices, Life Sciences, Pharmaceutical, Automotive, Software Development and Engineering industries.
In 2011, GE ranked among the Fortune 500 as the 6th-largest firm in the U.S. by gross revenue, and the 14th most profitable. As of 2012 the company was listed the fourth-largest in the world among the Forbes Global 2000, further metrics being taken into account. The Nobel Prize has twice been awarded to employees of General Electric: Irving Langmuir in 1932 and Ivar Giaever in 1973.
On January 13, 2016, it was announced that GE will be moving its corporate headquarters to the South Boston Waterfront neighborhood of Boston, Massachusetts. Some of the workers will arrive in the summer of 2016, and the full move will be completed by 2018.""
""Google Inc. "",,,,""Google is an American multinational technology company specializing in Internet-related services and products. These include online advertising technologies, search, cloud computing, and software. Most of its profits are derived from AdWords, an online advertising service that places advertising near the list of search results.
Google was founded by Larry Page and Sergey Brin while they were Ph.D. students at Stanford University. Together, they own about 14 percent of its shares and control 56 percent of the stockholder voting power through supervoting stock. They incorporated Google as a privately held company on September 4, 1998. An initial public offering followed on August 19, 2004. Its mission statement from the outset was "to organize the world's information and make it universally accessible and useful," and its unofficial slogan was "Don't be evil". In 2004, Google moved to its new headquarters in Mountain View, California, nicknamed the Googleplex. In August 2015, Google announced plans to reorganize its interests as a holding company called Alphabet Inc. When this restructuring took place on October 2, 2015, Google became Alphabet's leading subsidiary, as well as the parent for Google's Internet interests.
Rapid growth since incorporation has triggered a chain of products, acquisitions and partnerships beyond Google's core search engine (Google Search). It offers online productivity software (Google Docs) including email (Gmail), a cloud storage service (Google Drive) and a social networking service (Google+). Desktop products include applications for web browsing (Google Chrome), organizing and editing photos (Google Photos), and instant messaging (Hangouts). The company leads the development of the Android mobile operating system and the browser-only Chrome OS for a class of netbooks known as Chromebooks. Google has moved increasingly into communications hardware: it partners with major electronics manufacturers in the production of its "high-quality low-cost" Nexus devices. In 2012, a fiber-optic infrastructure was installed in Kansas City to facilitate a Google Fiber broadband service.
The corporation has been estimated to run more than one million servers in data centers around the world (as of 2007). It processes over one billion search requests and about 24 petabytes of user-generated data each day (as of 2009). In December 2013, Alexa listed google.com as the most visited website in the world. Numerous Google sites in other languages figure in the top one hundred, as do several other Google-owned sites such as YouTube and Blogger. Its market dominance has led to prominent media coverage, including criticism of the company over issues such as aggressive tax avoidance, search neutrality, copyright, censorship, and privacy.""
""HP Labs "",,,,""HP Labs is the exploratory and advanced research group for HP Inc. HP Labs operates its headquarters in Palo Alto, California, United States, and has research and development facilities in Bristol, UK. Researchers working at HP Labs are credited with the development of programmable desktop calculator, inkjet printing, and 3D graphics.
HP Labs was established on March 3, 1966 by HP founders Bill Hewlett and David Packard to create an organization focused on the future and not bound by day-to-day business concerns. It remained a division of the original Hewlett-Packard company until November 1, 2015, when it split into two companies; HP Inc. and Hewlett Packard Enterprise. With that split also came the split of Hewlett Packard Labs (formerly HP Laboratories).
Products and Solutions:
Laptops and Tablets
Desktops
Printers
Ink and Toner
Display and accessories
Business Solutions""
""Infineon Technologies "",,,,""Infineon Technologies AG is a German semiconductor manufacturer founded on 1 April 1999, when the semiconductor operations of the parent company Siemens AG were spun off to form a separate legal entity. As of 30 September 2013, Infineon has 26,725 employees worldwide. In fiscal year 2013, the company achieved sales of €3.843 billion.
On 1 May 2006, Infineon's Memory Products division was carved out as a distinct company called Qimonda AG, which at its height employed about 13,500 people worldwide. Qimonda was listed on the New York Stock Exchange until 2009.""
""Infusion "",,,,""Infusion is the process of extracting chemical compounds or flavors from plant material in a solvent such as water, oil or alcohol, by allowing the material to remain suspended in the solvent over time (a process often called steeping). An infusion is also the name for the resultant liquid. The process of infusion is distinct from decoction, which involves boiling the plant material, or percolation, in which the water passes through the material (as in a coffeemaker).""
""Intel Corporation "",,,,""Intel Corporation (better known as Intel, stylized as intel) is an American multinational technology company headquartered in Santa Clara, California. Intel is one of the world's largest and highest valued semiconductor chip makers, based on revenue. It is the inventor of the x86 series of microprocessors, the processors found in most personal computers. Intel supplies processors for computer system manufacturers such as Apple, Samsung, HP and Dell. Intel also makes motherboard chipsets, network interface controllers and integrated circuits, flash memory, graphics chips, embedded processors and other devices related to communications and computing.
Intel Corporation was founded on July 18, 1968 by semiconductor pioneers Robert Noyce and Gordon Moore and widely associated with the executive leadership and vision of Andrew Grove, Intel combines advanced chip design capability with a leading-edge manufacturing capability.
Intel was an early developer of SRAM and DRAM memory chips, which represented the majority of its business until 1981. Although Intel created the world's first commercial microprocessor chip in 1971, it was not until the success of the personal computer (PC) that this became its primary business. During the 1990s, Intel invested heavily in new microprocessor designs fostering the rapid growth of the computer industry. During this period Intel became the dominant supplier of microprocessors for PCs, and was known for aggressive and anti-competitive tactics in defense of its market position, particularly against Advanced Micro Devices (AMD), as well as a struggle with Microsoft for control over the direction of the PC industry.
Intel was ranked #56 on the 2015 rankings of the world's most valuable brands published by Millward Brown Optimor.
In June 2013, Intel unveiled its fourth generation of Intel Core processors (Haswell) in an event named Computex in Taipei.
The Open Source Technology Center at Intel hosts PowerTOP and LatencyTOP, and supports other open-source projects such as Wayland, Intel Array Building Blocks, Threading Building Blocks (TBB), and Xen.
Intel was conceived as portmanteau of the words integrated and electronics. The fact that "intel" is the term for intelligence information also made the name appropriate.""
""IBM Almaden Research Center "",,,,""The IBM Almaden Research Center is in Almaden Valley, San Jose, California, and is one of IBM's twelve worldwide research labs that form IBM Research. Its scientists perform basic and applied research in computer science, services, storage systems, physical sciences, and materials science and technology. The center opened in 1986, and continues the research started in San Jose more than fifty years ago. Nearly all of Almaden’s approximately 500 research employees are in technical functions and more than half of these hold Ph.D.s. The lab is home to ten IBM Fellows, ten IBM Distinguished Engineers, nine IBM Master Inventors and seventeen members of the IBM Academy of Technology.
Almaden is located at 650 Harry Road on nearly 700 acres (2.8 km2) of land in the hills above Silicon Valley. The site was chosen because of its close proximity to Stanford University, UC Santa Cruz, UC Berkeley and other collaborative academic institutions. Research at the lab is organized into four areas: Science and Technology, Computer Science, Storage Systems, and Service Research.
IBM opened its first West Coast research centre, the San Jose Research Laboratory in 1952, managed by Reynold B. Johnson. Amongst its first developments was the IBM 350, the first commercial moving head hard disk drive. Launched in 1956, this saw use in the IBM 305 RAMAC computer system. Subdivisions included the Advanced Systems Development Division. Directors of the center include hard disc drive developer Jack Harker.
Prompted by a need for additional space, the center moved to its present Almaden location in 1986.
Scientists at the IBM Almaden center have contributed to several scientific discoveries such as the development of photoresists and the quantum mirage effect.

""
""The IBM Toronto Software Lab "",,,,""The IBM Centers for Advanced Studies (CAS) are a group of research centres on the world that facilitate collaboration with university students and professors using IBM systems. There are 20 CAS located in various countries around the world. IBM's first CAS was established at the IBM Toronto Software Lab.
CAS is responsible for the annual CASCON.

""
""International Data Corporation (IDC) "",,,,""International Data Corporation (IDC), an American market research, analysis and advisory firm, specializes in information technology, telecommunications, and consumer technology, Software Development. A wholly owned subsidiary of the International Data Group (IDG). IDC has its headquarters in Framingham, Massachusetts, United States (U.S.). The corporation has approximately 70 offices grouped into Latin America, Middle East and Africa, Central and Eastern Europe, Europe, Asia/Pacific and the U.S.. It employs over 1,100 analysts, who provide consultancy in relation to technology opportunities and trends for over 110 countries.""
""Internet Corporation for Assigned Names and Numbers "",,,,""The Internet Corporation for Assigned Names and Numbers (ICANN /ˈaɪkæn/ EYE-kan) is a nonprofit organization that is responsible for coordinating the maintenance and methodologies of several databases, with unique identifiers, related to the namespaces of the Internet - and thereby, ensuring the network's stable and secure operation.
Most visibly, much of its work has concerned the Internet's global Domain Name System, including policy development for internationalization of the DNS system, introduction of new generic top-level domains (TLDs), and the operation of root name servers. The numbering facilities ICANN manages include the Internet Protocol address spaces for IPv4 and IPv6, and assignment of address blocks to regional Internet registries. ICANN also maintains registries of Internet protocol identifiers.
ICANN performs the actual technical maintenance work of the central Internet address pools and DNS Root registries pursuant to the IANA function contract.
ICANN's primary principles of operation have been described as helping preserve the operational stability of the Internet; to promote competition; to achieve broad representation of the global Internet community; and to develop policies appropriate to its mission through bottom-up, consensus-based processes.
ICANN was created on September 18, 1998, and incorporated on September 30, 1998 in the State of California. It is headquartered in the Playa Vista section of Los Angeles, California.""
""Iona Technologies "",,,,""IONA Technologies was an Irish software company. It was founded in 1991.
The company began life as a campus company in Trinity College, Dublin and maintained headquarters in Dublin, Boston and Tokyo.
The company specialized in distributed service-oriented architecture (SOA) infrastructure. IONA products connect systems and applications by creating a network of services without requiring a centralized server or creating an IT stack.
On 25 June 2008 it was announced that IONA would be acquired by Progress Software for about $162 million, which occurred shortly thereafter.
On 24 December 2012 Progress Software sold the IONA-related Orbix, Orbacus and Artix software product lines to Micro Focus International plc for $15 million.
The open-source group was later spun out into its own entity FuseSource Corp.. This group consisted of individuals and technologies involved in the various open-source projects and communities, including those that joined as part of the acquisition of LogicBlaze.
^ Campus Companies Programme Irish Scientist 1999
^ Irish Independent Thu, 24 Jan 2002
^ eWeek Wed, 25 June 2008
^ Micro Focus International plc 2012-12-24""
""Juniper Networks "",,,,""Juniper Networks is a multinational corporation headquartered in Sunnyvale, California that develops and markets networking products. Its products include routers, switches, network management software, network security products and software-defined networking technology.
Juniper was founded in 1996 by Pradeep Sindhu. The company received several rounds of funding from venture capitalists and telecommunications companies before going public in 1999. Juniper grew to $673 million in annual revenues by 2000. By 2001 it had a 37 percent share of the core routers market, challenging Cisco's once-dominant market-share. It grew to $4 billion in revenues by 2004 and $4.63 billion in 2014. Juniper appointed Kevin Johnson as CEO in 2008, Shaygan Kheradpir in 2013 and Rami Rahim in 2014.
Juniper Networks originally focused on core routers, which are used by internet service providers (ISPs) to perform IP address lookups and direct internet traffic. Through the acquisition of Unisphere in 2002, Juniper entered the market for edge routers, which are used by ISPs to route internet traffic to individual consumers. Juniper entered the IT security market with its own JProtect security toolkit in 2003, before acquiring security company NetScreen Technologies the following year. It entered the enterprise segment in the early 2000s, which accounted for one-third of revenues by 2005. As of 2014, Juniper has been focused on developing new software-defined networking products. However, in 2016, the company encountered some controversy under suspicion allegedly putting backdoors into its ScreenOS products.
^ a b c "Juniper Networks Financial Results". Juniper. Retrieved February 4, 2015. 
^ Juniper Networks: Form 10-K (PDF), retrieved February 6, 2015 
^ Duffy, Jim (June 7, 2010). "Cisco vs Juniper". Network World. Retrieved April 20, 2015. 
^""
""Lockheed Martin Corporation "",,,,""Lockheed Martin (NYSE: LMT) is an American global aerospace, defense, security and advanced technologies company with worldwide interests. It was formed by the merger of Lockheed Corporation with Martin Marietta in March 1995. It is headquartered in Bethesda, Maryland, in the Washington, DC, area. Lockheed Martin employs 116,000 people worldwide. Marillyn Hewson is the current President and Chief Executive Officer.
Lockheed Martin stands as one of the world's premier companies in the aerospace, defense, security, and technologies industry; it is the world's largest defense contractor based on revenue for fiscal year 2014. In 2013, 78% of Lockheed Martin's revenues came from military sales; it topped the list of US federal government contractors and received nearly 10% of the funds paid out by the Pentagon. In 2009 US government contracts accounted for $38.4 billion (85%), foreign government contracts $5.8 billion (13%), and commercial and other contracts for $900 million (2%).
Lockheed Martin operates in five business segments: Aeronautics, Information Systems & Global Solutions, Missiles and Fire Control, Mission Systems and Training, and Space Systems. The company received the Collier Trophy six times, including in 2001 for being part of developing the X-35/F-35B LiftFan Propulsion System, and most recently in 2006 for leading the team that developed the F-22 Raptor fighter jet. Lockheed Martin is currently developing the F-35 Lightning II and leads the international supply chain, leads the team for the development and implementation of technology solutions for the new USAF Space Fence (AFSSS replacement), and is the primary contractor for the development of the Orion Spacecraft command module. The company also invests in healthcare systems, renewable energy systems, intelligent energy distribution and compact nuclear fusion.""
""Macromedia "",,,,""Macromedia was an American graphics, multimedia and web development software company (1992–2005) headquartered in San Francisco, California that produced such products as Flash and Dreamweaver. Its rival, Adobe Systems, acquired Macromedia on December 3, 2005.
^ "Adobe to acquire Macromedia". Retrieved 2005-04-18.""
""Microsoft Corporation "",,,,""Microsoft Corporation /ˈmaɪkrəˌsɒft, -roʊ-, -ˌsɔːft/ (commonly referred to as Microsoft) is an American multinational technology company headquartered in Redmond, Washington, that develops, manufactures, licenses, supports and sells computer software, consumer electronics and personal computers and services. Its best known software products are the Microsoft Windows line of operating systems, Microsoft Office office suite, and Internet Explorer and Edge web browsers. Its flagship hardware products are the Xbox game consoles and the Microsoft Surface tablet lineup. It is the world's largest software maker by revenue, and one of the world's most valuable companies.
Microsoft was founded by Paul Allen and Bill Gates on April 4, 1975, to develop and sell BASIC interpreters for Altair 8800. It rose to dominate the personal computer operating system market with MS-DOS in the mid-1980s, followed by Microsoft Windows. The company's 1986 initial public offering, and subsequent rise in its share price, created three billionaires and an estimated 12,000 millionaires among Microsoft employees. Since the 1990s, it has increasingly diversified from the operating system market and has made a number of corporate acquisitions. In May 2011, Microsoft acquired Skype Technologies for $8.5 billion in its largest acquisition to date.
As of 2015, Microsoft is market dominant in both the IBM PC-compatible operating system (while it lost the majority of the overall operating system market to Android) and office software suite markets (the latter with Microsoft Office). The company also produces a wide range of other software for desktops and servers, and is active in areas including Internet search (with Bing), the video game industry (with the Xbox, Xbox 360 and Xbox One consoles), the digital services market (through MSN), and mobile phones (via the operating systems of Nokia's former phones and Windows Phone OS). In June 2012, Microsoft entered the personal computer production market for the first time, with the launch of the Microsoft Surface, a line of tablet computers.
With the acquisition of Nokia's devices and services division to form Microsoft Mobile Oy, the company re-entered the smartphone hardware market, after its previous attempt, Microsoft Kin, which resulted from their acquisition of Danger Inc.
Microsoft is a portmanteau of the words microcomputer and software.

""
""Motorola, Inc. "",,,,""Motorola, Inc. (/ˌmoʊtəˈroʊlə/) was an American multinational telecommunications company based in Schaumburg, Illinois, United States. After having lost $4.3 billion from 2007 to 2009, the company was divided into two independent public companies, Motorola Mobility and Motorola Solutions on January 4, 2011. Motorola Solutions is generally considered to be the direct successor to Motorola, Inc., as the reorganization was structured with Motorola Mobility being spun off.
Motorola designed and sold wireless network equipment such as cellular transmission base stations and signal amplifiers. Motorola's home and broadcast network products included set-top boxes, digital video recorders, and network equipment used to enable video broadcasting, computer telephony, and high-definition television. Its business and government customers consisted mainly of wireless voice and broadband systems (used to build private networks), and, public safety communications systems like Astro and Dimetra. These businesses (except for set-top boxes and cable modems) are now part of Motorola Solutions. Google sold Motorola Home (the former General Instrument cable businesses) to the Arris Group in December 2012 for US$2.35 billion.
Motorola's wireless telephone handset division was a pioneer in cellular telephones. Also known as the Personal Communication Sector (PCS) prior to 2004, it pioneered the "mobile phone" with DynaTAC, "flip phone" with the MicroTAC as well as the "clam phone" with the StarTAC in the mid-1990s. It had staged a resurgence by the mid-2000s with the RAZR, but lost market share in the second half of that decade. Later it focused on smartphones using Google's open-source Android mobile operating system. The first phone to use the newest version of Google's open source OS, Android 2.0, was released on November 2, 2009 as the Motorola Droid (the GSM version launched a month later, in Europe, as the Motorola Milestone).
The handset division (along with cable set-top boxes and cable modems) was later spun off into the independent Motorola Mobility. On May 22, 2012, Google CEO Larry Page announced that Google had closed on its deal to acquire Motorola Mobility. On January 29, 2014, Google CEO Larry Page announced that pending closure of the deal, Motorola Mobility would be acquired by Chinese technology company Lenovo for US$2.91 billion (subject to certain adjustments). On October 30, 2014, Lenovo finalized its purchase of Motorola Mobility from Google.""
""Motorola Mobility, Inc. "",,,,""Motorola Mobility LLC is a company owned by Lenovo that develops mobile devices. Headquartered in Chicago, Illinois, United States, the company was formed on January 4, 2011 by the split of Motorola Inc. into two separate companies; Motorola Mobility took on the company's consumer-oriented product lines, including its mobile phone business and its cable modems and set-top boxes for digital cable and satellite television services, while Motorola Solutions retained the company's enterprise-oriented product lines.
The company primarily manufactures smartphones and other mobile devices running the Android operating system developed by Google Inc.; in August 2011, only several months after the split, Google acquired Motorola Mobility for US$12.5 billion. Google's stated intent for the purchase was to gain control of Motorola Mobility's portfolio of patents, so it could adequately protect other Android vendors from lawsuits. The deal closed in May 2012, after which it also sold its cable modem and set-top box business to Arris Group. Under Google ownership, Motorola Mobility increased its focus on the entry-level smartphone market, introduced one of the first Android Wear smartwatches, and also began development on Project Ara, a platform for modular smartphones with interchangeable components.
Google's ownership of the company would be short-lived, as it announced in January 2014 that it would sell most of Motorola Mobility to Chinese computer technology firm Lenovo for $2.91 billion. The sale, which excluded all but 2000 of Motorola Mobility's patents and the team working on Project Ara (which became part of the main Android development staff), was completed on October 30, 2014. Lenovo disclosed an intent to use its purchase of Motorola Mobility as a way to expand into the U.S. smartphone market.
In January 2016, following the August 2015 merger of Lenovo's existing smartphone business with Motorola Mobility, it was announced that the company would begin to phase out the name Motorola as a public-facing brand, replacing it with the "Moto" brand used on most of its recent devices, and the "Vibe" brand that originated under Lenovo, its parent company. However, Motorola later clarified that the brand is not disappearing, but the marketing focus will be on its "Moto" (and Lenovo's "Vibe") brands, whereas the Motorola brand will appear on packaging and also through its licensees.""
""Motorola Solutions, Inc. "",,,,""Motorola Solutions, Inc. is an American data communications and telecommunications equipment provider that succeeded Motorola, Inc., following the spinoff of the mobile phone division into Motorola Mobility in 2011. The company is headquartered in Chicago, Illinois.
Motorola Solutions sold its enterprise mobility solutions business to Zebra Technologies in a transaction that was completed on October 27, 2014. It also previously had a Networks division, which it sold to Nokia Siemens Networks in a transaction that was completed on April 29, 2011. In the former Motorola parent company, it was the second largest division based on revenue.""
""National Instruments Corporation "",,,,""National Instruments Corporation, or NI, is an American company with international operation. Headquartered in Austin, Texas, it is a producer of automated test equipment and virtual instrumentation software. Common applications include data acquisition, instrument control and machine vision.
In 2012, the company sold products to more than 35,000 companies with revenues of $1.12 billion USD.""
""Nokia Corporation "",,,,""Nokia Corporation (Finnish: Nokia Oyj, Finnish pronunciation: [ˈnokiɑ], UK /ˈnɒkiə/, US /ˈnoʊkiə/) is a Finnish multinational communications and information technology company, founded in 1865. Nokia is headquartered in Espoo, Uusimaa, in the greater Helsinki metropolitan area. In 2014, Nokia employed 61,656 people across 120 countries, conducts sales in more than 150 countries and reported annual revenues of around €12.73 billion. Nokia is a public limited-liability company listed on the Helsinki Stock Exchange and New York Stock Exchange. It is the world's 274th-largest company measured by 2013 revenues according to the Fortune Global 500. The company is a component of the Euro Stoxx 50 stock market index.
The company currently focuses on large-scale telecommunications infrastructures, and technology development and licensing. Nokia is also a major contributor to the mobile telephony industry, having assisted in development of the GSM and LTE standards, and was, for a period, the largest vendor of mobile phones in the world. Nokia's dominance also extended into the smartphone industry through its Symbian platform, but it was soon overshadowed by the growing dominance of Apple's iPhone line and Android devices. Nokia eventually entered into a pact with Microsoft in 2011 to exclusively use its Windows Phone platform on future smartphones.
In September 2013, Microsoft announced that it would acquire Nokia's mobile phone business as part of an overall deal totaling €5.44 billion (US $7.17 billion). Stephen Elop, Nokia's former CEO, and several other executives joined the new Microsoft Mobile subsidiary of Microsoft as part of the deal, which was completed on April 25, 2014. Since the sale of its mobile phone business, Nokia began to focus more extensively on its telecommunications infrastructure business, marked by the divestiture of its Here Maps division.
In January 2016, Nokia acquired French telecommunications company Alcatel-Lucent to grow their innovation, network equipment, wireless technology and services.""
""Nortel Networks Corporation "",,,,""Nortel Networks Corporation, formerly known as Northern Telecom Limited, Northern Electric and sometimes known simply as Nortel, was a multinational telecommunications and data networking equipment manufacturer headquartered in Mississauga, Ontario, Canada. It was founded in Montreal, Quebec in 1895. At its height, Nortel accounted for more than a third of the total valuation of all the companies listed on the Toronto Stock Exchange (TSX), employing 94,500 people worldwide.
On January 14, 2009, Nortel filed for protection from its creditors in the United States, Canada, and the United Kingdom, in order to restructure its debt and financial obligations. In June 2009, the company announced it would cease operations and sell off all of its business units. The period of bankruptcy protection was extended to February 2, 2013. As part of the bankruptcy proceedings in the United States, Nortel Networks Inc. publishes monthly operating reports outlining cash receipts and disbursements.""
""Novell, Inc. "",,,,""Novell, Inc. /noʊˈvɛl/ is an American software and services company headquartered in Provo, Utah. It had been instrumental in making the Utah Valley a focus for technology and software development. Novell technology contributed to the emergence of local area networks, which displaced the dominant mainframe computing model and changed computing worldwide. Today, a primary focus of the company is on developing software for enterprise.
The company originally was an independent corporate entity until it was acquired as a wholly owned subsidiary by The Attachmate Group; this latter was acquired in 2014 by Micro Focus International, of which Novell is now a division.
^ https://www.novell.com/ContactsOffices/contacts_offices.jsp""
""NVIDIA Corporation "",,,,""NVIDIA Corporation (/ɪnˈvɪdiə/ in-VID-eeə) (commonly referred to as Nvidia, stylized as NVIDIA, nVIDIA or nvidia) began as an American technology company based in Santa Clara, California. Nvidia designs graphics processing units (GPUs) for the gaming market, as well as system on a chip units (SOCs) for the mobile computing and automotive market. Nvidia's primary GPU product line, labeled "GeForce", is in direct competition with Advanced Micro Devices' (AMD) "Radeon" products. Nvidia expanded its presence in the gaming industry with its handheld SHIELD Portable, SHIELD Tablet, and SHIELD Android TV.
Since 2014, Nvidia has shifted to become a platform company focused on four markets - Gaming, Professional Visualization, Data Centers and Auto.
In addition to GPU manufacturing, Nvidia provides parallel processing capabilities to researchers and scientists that allow them to efficiently run high-performance applications. They are deployed in supercomputing sites around the world. More recently, Nvidia has moved into the mobile computing market, where it produces Tegra mobile processors for smartphones and tablets, as well as vehicle navigation and entertainment systems. In addition to AMD, its competitors include Intel and Qualcomm.
^ a b c d e f g h "NVIDIA CORP 2014 Annual Report Form (10-K)" (XBRL). United States Securities and Exchange Commission. March 13, 2014. 
^ a b "NVIDIA CORP 2015 Q1 Quarterly Report Form (10-Q)" (XBRL). United States Securities and Exchange Commission. May 21, 2014. 
^ "The World Leader In Visual Computing". Nvidia. Retrieved February 1, 2016. 
^ Nvidia: The Way It's Meant To Be Played on YouTube
^ Clark, Don (August 4, 2011). "J.P. Morgan Shows Benefits from Chip Change". WSJ Digits Blog. Retrieved September 14, 2011. 
^ "Top500 Supercomputing Sites". Top500. Retrieved September 14, 2011. 
^ Burns, Chris. "2011 The Year of Nvidia dominating Android Superphones and tablets". SlashGear. Retrieved September 14, 2011. 
^ "Tegra Super Tablets". Nvidia. Retrieved September 14, 2011. 
^ "Tegra Super Phones". Nvidia. Retrieved September 14, 2011.""
""Sun Microsystems "",,,,""Sun Microsystems, Inc. was a company that sold computers, computer components, computer software, and information technology services and that created the Java programming language, Solaris Unix and the Network File System (NFS). Sun significantly evolved several key computing technologies, among them Unix, RISC processors, thin client computing, and virtualized computing. Sun was founded on February 24, 1982. At its height, Sun headquarters were in Santa Clara, California (part of Silicon Valley), on the former west campus of the Agnews Developmental Center.
On January 27, 2010, Sun was acquired by Oracle Corporation for US $7.4 billion, based on an agreement signed on April 20, 2009. The following month, Sun Microsystems, Inc. was merged with Oracle USA, Inc. to become Oracle America, Inc.
Sun products included computer servers and workstations built on its own RISC-based SPARC processor architecture as well as on x86-based AMD's Opteron and Intel's Xeon processors; storage systems; and a suite of software products including the Solaris operating system, developer tools, Web infrastructure software, and identity management applications. Other technologies include the Java platform, MySQL, and NFS. Sun was a proponent of open systems in general and Unix in particular, and a major contributor to open source software. Sun's main manufacturing facilities were located in Hillsboro, Oregon, and Linlithgow, Scotland.
^ "Company Info". Sun Microsystems. Retrieved 2006-12-04. 
^ "The Network is the Computer (YouTube video uploaded by Sun Microsystems)". YouTube. Retrieved December 23, 2007. 
^ "Churchill Club Presents: Scott McNealy in Conversation With Ed Zander". news release (Market Wire). February 17, 2011. Retrieved June 14, 2011. 
^ Stephen Shankland (January 27, 2010). "Oracle buys Sun, becomes hardware company". CNET News. Retrieved June 14, 2011. 
^ "Oracle has been renamed "Oracle America Inc."". Oracle Corporation. February 15, 2010. Retrieved June 14, 2011. 
^ Matt Lee (November 30, 2006). "Sun begins releasing Java under the GPL". Free Software Foundation. Retrieved June 14, 2011. FSF president and founder Richard Stallman said, "I think Sun has contributed more than any other company to the free software community in the form of software. It shows leadership. It's an example I hope others will follow."""
""Philips Semiconductors "",,,,""NXP Semiconductors N.V. is a global semiconductor manufacturer headquartered in Eindhoven, The Netherlands. The company employs approximately 45,000 people in more than 35 countries, including 11,200 engineers in 23 countries.NXP reported revenue of $6.1 billion in 2015, including one month of revenue contribution from recently acquired Freescale Semiconductor.
NXP is currently the fifth-largest non-memory semiconductor supplier globally, and the leading semiconductor supplier for the Secure Identification, Automotive and Digital Networking industries.The company was founded in 1953, with manufacturing and development in Nijmegen, Netherlands. Known then as Philips Semiconductors, the company was sold to a consortium of private equity investors in 2006, at which point the company's name was changed to NXP.
On August 6, 2010, NXP completed its IPO, with shares trading on NASDAQ under the ticker symbol NXPI. On December 23, 2013, NXP Semiconductors was added to the NASDAQ 100.Finally, on March 2, 2015, it was announced that NXP Semiconductors would merge with chip designer and manufacturer Freescale Semiconductor in a $40 billion US-dollar deal. The merger was closed on December 7, 2015.
NXP Semiconductors provides mixed signal and standard product solutions based on its security, identification, automotive, networking, RF, analog, and power management expertise. With an emphasis on security of the connected vehicle and the growing Internet of Things, the company's products are used in a wide range of "smart" automotive, identification, wired and wireless infrastructure, lighting, industrial, consumer, mobile and computing applications.
NXP is the co-inventor of near field communication (NFC) technology along with Sony and supplies NFC chip sets which enable mobile phones to be used to pay for goods, and store and exchange data securely. NXP manufactures chips for eGovernment applications such as electronic passports; RFID tags and labels; and transport and access management, with the chip set and contactless card for MIFARE used by many major public transit systems worldwide.
In addition, NXP manufactures automotive chips for in-vehicle networking, passive keyless entry and immobilization, and car radios. NXP invented the I²C interface over 30 years ago and is a supplier of I²C solutions. NXP is also a volume supplier of standard logic devices, and celebrated its 50 years in logic (via its history as both Signetics and Philips Semiconductors) in March 2012.
NXP currently owns more than 9,000 issued or pending patents.""
""NXP Semiconductors "",,,,""NXP Semiconductors N.V. is a global semiconductor manufacturer headquartered in Eindhoven, The Netherlands. The company employs approximately 45,000 people in more than 35 countries, including 11,200 engineers in 23 countries.NXP reported revenue of $6.1 billion in 2015, including one month of revenue contribution from recently acquired Freescale Semiconductor.
NXP is currently the fifth-largest non-memory semiconductor supplier globally, and the leading semiconductor supplier for the Secure Identification, Automotive and Digital Networking industries.The company was founded in 1953, with manufacturing and development in Nijmegen, Netherlands. Known then as Philips Semiconductors, the company was sold to a consortium of private equity investors in 2006, at which point the company's name was changed to NXP.
On August 6, 2010, NXP completed its IPO, with shares trading on NASDAQ under the ticker symbol NXPI. On December 23, 2013, NXP Semiconductors was added to the NASDAQ 100.Finally, on March 2, 2015, it was announced that NXP Semiconductors would merge with chip designer and manufacturer Freescale Semiconductor in a $40 billion US-dollar deal. The merger was closed on December 7, 2015.
NXP Semiconductors provides mixed signal and standard product solutions based on its security, identification, automotive, networking, RF, analog, and power management expertise. With an emphasis on security of the connected vehicle and the growing Internet of Things, the company's products are used in a wide range of "smart" automotive, identification, wired and wireless infrastructure, lighting, industrial, consumer, mobile and computing applications.
NXP is the co-inventor of near field communication (NFC) technology along with Sony and supplies NFC chip sets which enable mobile phones to be used to pay for goods, and store and exchange data securely. NXP manufactures chips for eGovernment applications such as electronic passports; RFID tags and labels; and transport and access management, with the chip set and contactless card for MIFARE used by many major public transit systems worldwide.
In addition, NXP manufactures automotive chips for in-vehicle networking, passive keyless entry and immobilization, and car radios. NXP invented the I²C interface over 30 years ago and is a supplier of I²C solutions. NXP is also a volume supplier of standard logic devices, and celebrated its 50 years in logic (via its history as both Signetics and Philips Semiconductors) in March 2012.
NXP currently owns more than 9,000 issued or pending patents.""
""Pixar Animation Studios "",,,,""Pixar Animation Studios, or simply Pixar (/ˈpɪksɑːr/), is an American computer animation film studio based in Emeryville, California. The studio is best known for its CGI-animated feature films created with RenderMan, its own implementation of the industry-standard RenderMan image-rendering application programming interface used to generate high-quality images. Pixar began in 1979 as the Graphics Group, part of the computer division of Lucasfilm before its spin-out as a corporation in 1986 with funding by Apple Inc. co-founder Steve Jobs, who became its majority shareholder. The Walt Disney Company bought Pixar in 2006 at a valuation of $7.4 billion, a transaction that resulted in Jobs becoming Disney's largest single shareholder at the time. Luxo Jr., a character from a 1986 Pixar short film of the same name, is the mascot for the studio.
Pixar has produced a total of 16 feature films, beginning with Toy Story (1995), which was the first-ever computer-animated feature film, and its most recent being The Good Dinosaur (2015). All 16 films have debuted with CinemaScore ratings of at least "A−," indicating positive receptions with audiences. The studio has also produced several short films. As of December 2015, its feature films have made over $9.5 billion worldwide, with an average worldwide gross of $593 million per film. Three of Pixar's films—Finding Nemo (2003), Toy Story 3 (2010), and Inside Out (2015)—are among the 50 highest-grossing films of all time, with Toy Story 3 being the third all-time highest animated film with a gross of $1.063 billion, behind Walt Disney Animation Studios' Frozen (2013) and Illumination Entertainment's Minions (2015), which grossed $1.276 billion and $1.159 billion respectively in their initial releases as of 2016. Thirteen of Pixar's films are among the 50 highest-grossing animated films.
The studio has earned fifteen Academy Awards, seven Golden Globe Awards, and eleven Grammy Awards, among many other awards and acknowledgments. Most of Pixar's films have been nominated for the Academy Award for Best Animated Feature, since its inauguration in 2001, with eight winning; this includes Finding Nemo, Toy Story 3, and Inside Out, along with The Incredibles (2004), Ratatouille (2007), WALL-E (2008), Up (2009), and Brave (2012). Monsters, Inc. (2001) and Cars (2006) are the only two films that were nominated for the award without winning it. Up and Toy Story 3 were also the second and third animated films to be nominated for the Academy Award for Best Picture, the first being Disney's Beauty and the Beast (1991). Cars 2 (2011), Monsters University (2013) and The Good Dinosaur (2015) are the only Pixar films to not be nominated for any Academy Awards. On September 6, 2009, executives John Lasseter, Brad Bird, Pete Docter, Andrew Stanton, and Lee Unkrich were presented with the Golden Lion for Lifetime Achievement by the biennial Venice Film Festival. This award was presented by Lucasfilm founder George Lucas.""
""Quest Software, Inc. "",,,,""Legacy of the Ancients is a fantasy role-playing video game published by Electronic Arts in 1987.""
""Rational Software Corporation "",,,,""Rational Machines was founded by Paul Levy and Mike Devlin in 1981 to provide tools to expand the use of modern software engineering practices, particularly explicit modular architecture and iterative development. Rational was sold for US$2.1 billion to IBM on February 20, 2003.""
""Research in Motion Limited "",,,,""BlackBerry Limited, formerly known as Research In Motion Limited (RIM), is a Canadian telecommunication and wireless equipment company best known to the general public as the developer of the BlackBerry brand of smartphones and tablets, but also well known worldwide as a provider of secure and high reliability software for industrial applications and mobile device management (MDM). QNX, a subsidiary of BlackBerry, is widely recognised as a leader in automotive software industry. BlackBerry's software and hardware products are used worldwide by various government's agencies and by car makers and industrial plants throughout the world, much of this activity being unseen by the public. The company is headquartered in Waterloo, Ontario, Canada. It was founded by Mike Lazaridis and Douglas Fregin in 1984. Mr. Fregin later left the company. In 1992 Mr. Lazaridis hired Jim Balsillie, and Mr. Lazaridis and Mr. Balsillie served as co-CEOs until January 22, 2012. In November 2013, John S. Chen took over as CEO. His strategy is to subcontract manufacturing to Foxconn, and to focus on software technology.
Originally a dominant innovative company in the smartphone market for business and government usage, with 41% U.S. market share in Q1 2010, the company's dominance in the U.S. smartphone market has in recent years declined precipitously, in part because of intense competition from Apple's iPhone and Google's Android. Due to such competition, the company's share in the U.S. personal consumer market has now been reduced to 1.2% in June 2015.
On September 23, 2013, the company signed a letter of intent to be acquired for US$4.7 billion, or US$9 per share, by a consortium led by Fairfax Financial which announced its intentions to take the company private. On November 4, 2013, the deal was scrapped in favor of a US$1 billion cash injection which, according to one analyst, represented the level of confidence BlackBerry's largest shareholder has in the company. The majority of BlackBerry's remaining value lies in its patent portfolio which has been valued at between US$2 billion and $3 billion.

^ a b c d e "Financial Tables". Research In Motion Investor Relations. Retrieved April 17, 2015. 
^ 
^ Staff writer (July 10, 2013). "Press release: BlackBerry Charts New Course by Officially Adopting its Iconic Brand Name". BlackBerry Limited. BlackBerry Limited. Retrieved July 20, 2013. 
^ 
^ 
^ G+M "BlackBerry deal bolsters Foxconn’s makeover gambit" Jim and Gupta, TAIPEI/SAN FRANCISCO — Reuters, Published Tuesday, Dec. 24 2013
^ Cyran, Robert; Larsen, Peter Thal (July 26, 2010). "BlackBerry’s Era May Be Ending". New York Times. Retrieved January 27, 2016. 
^ "comScore Reports June 2015 U.S. Smartphone Subscriber Market Share". comScore. August 7, 2015. Retrieved August 13, 2015. 
^ Reed, Brad (September 23, 2013). "BlackBerry Fairfax Financial buyout: $4.7 billion". BGR. Retrieved October 23, 2013. 
^ "Fairfax Financial scraps bid for BlackBerry; leads $1 billion cash infusion deal". November 4, 2013. 
^ Julianne Pepitone (September 24, 2013). "BlackBerry's valuable patents could spark a bidding war". CNN Money. Retrieved October 13, 2013.""
""SAS Institute, Inc. "",,,,""SAS Institute (or SAS, pronounced "sass") is an American developer of analytics software based in Cary, North Carolina. SAS develops and markets a suite of analytics software (also called SAS), which helps access, manage, analyze and report on data to aid in decision-making. The company is the world's largest privately held software business and its software is used by most of the Fortune 500.
SAS has developed a model workplace environment and benefits program designed to retain employees, allow them to focus on their work, and reduce operating costs. It provides on-site, subsidized or free healthcare, gyms, daycare and life counseling services.
SAS Institute started as a project at North Carolina State University to create a statistical analysis system (hence the proper name, Statistical Analysis System) that was originally used primarily by agricultural departments at universities in the late 1960s. It became an independent, private business led by current CEO James Goodnight and three other project leaders from the university in 1976. SAS grew from $10 million in revenues in 1980 to $1.1 billion by 2000. A larger proportion of these revenues are spent on research and development than at most other software companies, at one point more than double the industry average.
^ Lohr, Steve (November 21, 2009). "At a Software Powerhouse, the Good Life Is Under Siege". The New York Times. Retrieved September 27, 2011. 
^ 
^ 
^""
""Sega Corporation "",,,,""Sega Games Co., Ltd. (株式会社セガゲームス, Kabushiki gaisha Sega gēmusu), originally short for Service Games and officially styled as SEGA, is a Japanese multinational video game developer and publisher headquartered in Tokyo, Japan, with multiple offices around the world. Sega developed and manufactured numerous home video game consoles from 1983 to 2001, but the financial losses incurred from their Dreamcast console caused the company to restructure itself in 2001, and focus on providing software as a third-party developer from then on. Nonetheless, Sega remains the world's most prolific arcade producer, with over 500 games in over 70 franchises on more than 20 different arcade system boards since 1981.
Sega, along with their sub-studios, are known for their multi-million selling game franchises including Sonic the Hedgehog, Virtua Fighter, Phantasy Star, Yakuza, and Total War, amongst others. Sega's head offices are located in Ōta, Tokyo, Japan. Sega's North American division, Sega of America, is headquartered in Irvine, California, having moved there from San Francisco in 2015. Sega's European division, Sega Europe, is headquartered in the Brentford area of London, England.""
""Siemens AG "",,,,""Siemens AG (German pronunciation: [ˈziːmɛns]) is a German company headquartered in Berlin and Munich and the largest engineering company in Europe with branch offices abroad.
The principal divisions of the company are Industry, Energy, Healthcare, and Infrastructure & Cities, which represent the main activities of the company. The company is a prominent maker of medical diagnostics equipment and its medical health-care division, which generates about 12 percent of the company's total sales, is its second-most profitable unit, after the industrial automation division. The company is a component of the Euro Stoxx 50 stock market index. Siemens and its subsidiaries employ approximately 362,000 people worldwide and reported global revenue of around €71.9 billion in 2014 according to their annual report.""
""Silicon Graphics, Inc. "",,,,""Silicon Graphics, Inc. (later rebranded SGI, historically known as Silicon Graphics Computer Systems or SGCS) was an American manufacturer of high-performance computing solutions, including computer hardware and software. Founded in Mountain View, California in November 1981 by Jim Clark, its initial market was 3D graphics computer workstations, but its products, strategies and market positions developed significantly over time.
Early systems were based on the Geometry Engine that Clark and Marc Hannah had developed at Stanford University, and were derived from Clark's broader background in computer graphics. The Geometry Engine was the first very-large-scale integration (VLSI) implementation of a geometry pipeline, specialized hardware that accelerated the "inner-loop" geometric computations needed to display three-dimensional images. For much of its history, the company focused on 3D imaging and were a major supplier of both hardware and software in this market.
They reincorporated as a Delaware corporation in January 1990. Through the mid to late-1990s, the rapidly improving performance of commodity Wintel machines began to erode SGI's stronghold in the 3D market. The porting of Maya to other platforms is a major event in this process. SGI made several attempts to address this, including a disastrous move from their existing MIPS platforms to the Intel Itanium, as well as introducing their own Linux-based Intel IA-32 based workstations and servers that failed in the market. In the mid-2000s the company repositioned itself as a supercomputer vendor, a move that also failed.
On April 1, 2009, SGI filed for Chapter 11 bankruptcy protection and announced that it would sell substantially all of its assets to Rackable Systems, a deal finalized on May 11, 2009, with Rackable assuming the name "Silicon Graphics International". The remains of Silicon Graphics, Inc. became Graphics Properties Holdings, Inc.""
""Sony Corporation "",,,,""Sony Corporation (ソニー株式会社, Sonī Kabushiki Gaisha), commonly referred to as Sony /ˈsoʊniː/, is a Japanese multinational conglomerate corporation headquartered in Kōnan Minato, Tokyo, Japan. Its diversified business includes consumer and professional electronics, gaming, entertainment and financial services. The company is one of the leading manufacturers of electronic products for the consumer and professional markets. Sony is ranked 116th on the 2015 list of Fortune Global 500.
Sony Corporation is the electronics business unit and the parent company of the Sony Group, which is engaged in business through its four operating segments – electronics (including video games, network services and medical business), motion pictures, music and financial services. These make Sony one of the most comprehensive entertainment companies in the world. Sony's principal business operations include Sony Corporation (Sony Electronics in the U.S.), Sony Pictures Entertainment, Sony Computer Entertainment, Sony Music Entertainment, Sony Mobile Communications (formerly Sony Ericsson) and Sony Financial. Sony is among the Worldwide Top 20 Semiconductor Sales Leaders and as of 2013, the fourth-largest television manufacturer in the world, after Samsung Electronics, LG Electronics and TCL.
The Sony Group (ソニー・グループ, Sonī Gurūpu) is a Japan-based corporate group primarily focused on the Electronics (such as AV/IT products and components), Game (such as the PlayStation), Entertainment (such as motion pictures and music) and Financial Services (such as insurance and banking) sectors. The group consists of Sony Corporation (holding and electronics), Sony Computer Entertainment (games), Sony Pictures Entertainment (motion pictures), Sony Music Entertainment (music), Sony/ATV Music Publishing (music publishing), Sony Financial Holdings (financial services) and others.
The company's current slogan is BE MOVED. Their former slogans were make.believe (2009–2014) and like.no.other (2005–2014).
Sony is a member of the SMFG (Sumitomo Mitsui Financial Group) keiretsu, the successor to the Mitsui keiretsu to which it previously belonged.
^ a b "Sony Global – Corporate Information". Retrieved 13 June 2015. 
^ a b c d e f "Consolidated financial results for the fiscal year ended March 2015, Sony Corporation" (PDF). 
^ "Historical market caps of the largest Japanese companies". Retrieved 1 April 2015. 
^ "Access & Map." Sony Global. Retrieved 6 December 2011. "1–7–1 Konan Minato-ku, Tokyo 108-0075, Japan" – Map – Address in Japanese: "〒108-0075 東京都港区港南1–7–1"
^ Sony Corporate History (Japanese). Sony.co.jp. Retrieved 7 July 2011.
^ "Global 500 – Fortune". Fortune. 
^ Organization Data. Sony.net. Retrieved on 25 April 2012.
^ Business Overview, Annual Report 2010. (PDF) . Retrieved 7 July 2011.
^ Organization Data. Sony.net. Retrieved 7 July 2011.
^ Global market share held by LCD TV manufacturers from 2008 to 2013. Retrieved 26 February 2015.""
""SRI International "",,,,""SRI International (SRI) is an American nonprofit research institute headquartered in Menlo Park, California. The trustees of Stanford University established SRI in 1946 as a center of innovation to support economic development in the region.
The organization was founded as the Stanford Research Institute. SRI formally separated from Stanford University in 1970 and became known as SRI International in 1977. SRI describes its mission as creating world-changing solutions to make people safer, healthier, and more productive. It performs client-sponsored research and development for government agencies, commercial businesses, and private foundations. It also licenses its technologies, forms strategic partnerships, sells products, and creates spin-off companies.
SRI's annual revenue in 2014 was approximately $540 million. SRI's headquarters are located near the Stanford University campus. William A. Jeffrey has served as SRI's president and CEO since September 2014.
SRI employs about 2,100 people. Sarnoff Corporation, a wholly owned subsidiary of SRI since 1988, was fully integrated into SRI in January 2011.
SRI's focus areas include biomedical sciences, chemistry and materials, computing, Earth and space systems, economic development, education and learning, energy and environmental technology, security and national defense, as well as sensing and devices. SRI has received more than 4,000 patents and patent applications worldwide.""
""Standard Performance Evaluation Corporation "",,,,""The Standard Performance Evaluation Corporation (SPEC) is an American non-profit organization that aims to "produce, establish, maintain and endorse a standardized set" of performance benchmarks for computers.
SPEC was founded in 1988. SPEC benchmarks are widely used to evaluate the performance of computer systems; the test results are published on the SPEC website. Results are sometimes informally referred to as "SPECmarks" or just "SPEC".
SPEC evolved into an umbrella organization encompassing four diverse groups; Graphics and Workstation Performance Group (GWPG), the High Performance Group (HPG), the Open Systems Group (OSG) and the newest, the Research Group (RG). More details are on their website; [1].
^ "SPEC Frequently Asked Questions". Retrieved 15 March 2010. 
^ "The SPEC Organization". Retrieved 15 March 2010. 
^ "SPEC Membership". Retrieved 15 March 2010.""
""Taiwan Semiconductor Manufacturing Company, Limited "",,,,""Taiwan Semiconductor Manufacturing Company, Limited (TSMC; Chinese: 台灣積體電路製造公司; pinyin: Táiwān Jītǐ Diànlù Zhìzào Gōngsī), also known as Taiwan Semiconductor, is the world's largest dedicated independent (pure-play) semiconductor foundry, with its headquarters and main operations located in the Hsinchu Science and Industrial Park in Hsinchu, Taiwan.""
""Telcordia Technologies "",,,,""Telcordia Technologies, Inc. is an American subsidiary of the Swedish telecommunications company Ericsson. The company provides interconnection technology and clearinghouse solutions for numbering plan, routing, call billing, and technical standards coordination between competing telecommunications carriers. Telcordia's headquarters are located in Piscataway, New Jersey (U.S.) and it has several branches and subsidiaries in the Americas, Europe, and Asia.
Telcordia was formerly known as Bell Communications Research, Inc. or Bellcore. It was the telecommunication research and development company created as part of the break-up of the American Telephone and Telegraph Company (AT&T).
Since 2013, the company is doing business as iconectiv.""
""Texas Instruments Inc. "",,,,""Texas Instruments Inc. (TI) is an American electronics company that designs and makes semiconductors, which it sells to electronics designers and manufacturers globally. Headquartered in Dallas, Texas, United States, TI is the third largest manufacturer of semiconductors worldwide after Intel and Samsung, the second largest supplier of chips for cellular handsets after Qualcomm, and the largest producer of digital signal processors (DSPs) and analog semiconductors, among a wide range of other semiconductor products, including calculators, microcontrollers and multi-core processors. Texas Instruments is among the Top 20 Semiconductor producing companies in the world.
Texas Instruments was founded in 1951. It emerged after a reorganization of Geophysical Service, a company that manufactured equipment for use in the seismic industry as well as defense electronics. TI began research in transistors in the early 1950s and produced the world's first commercial silicon transistor. In 1954, Texas Instruments designed and manufactured the first transistor radio and Jack Kilby invented the integrated circuit in 1958 while working at TI's Central Research Labs. The company produced the first integrated circuit-based computer for the U.S. Air Force in 1961. TI researched infrared technology in the late 1950s and later made radar systems as well as guidance and control systems for both missiles and bombs. The hand-held calculator was introduced to the world by TI in 1967.
In the 1970s and 1980s the company focused on consumer electronics including digital clocks, watches, hand-held calculators, home computers as well as various sensors. In 1997, its defense business was sold to Raytheon. In 2007, Texas Instruments was awarded the Manufacturer of the Year for Global Supply Chain Excellence by World Trade magazine. Texas Instruments is considered to be one of the most ethical companies in the world.
After the acquisition of National Semiconductor in 2011, the company has a combined portfolio of nearly 45,000 analog products and customer design tools, making it the world's largest maker of analog technology components. In 2011, Texas Instruments ranked 175 in the Fortune 500. TI is made up of two main divisions: Semiconductors (SC) and Educational Technology (ET) of which Semiconductor products account for approximately 96% of TI's revenue.""
""Toshiba Corporation "",,,,""Toshiba Corporation (株式会社東芝, Kabushiki-gaisha Tōshiba, English /təˈʃiːbə, tɒ-, toʊ-/) (commonly referred to as Toshiba, stylized as TOSHIBA) is a Japanese multinational conglomerate corporation headquartered in Tokyo, Japan. Its diversified products and services include information technology and communications equipment and systems, electronic components and materials, power systems, industrial and social infrastructure systems, consumer electronics, household appliances, medical equipment, office equipment, lighting and logistics.
Toshiba was founded in 1938 as Tokyo Shibaura Electric K.K. through the merger of Shibaura Seisaku-sho (founded in 1875) and Tokyo Denki (founded in 1890). The company name was officially changed to Toshiba Corporation in 1978. Toshiba made a large number of corporate acquisitions during its history, including of Semp in 1977, of Westinghouse Electric LLC, a nuclear energy company in 2006, of Landis+Gyr in 2011, and of IBM's point-of-sale business in 2012.
Toshiba is organised into four business groupings: the Digital Products Group, the Electronic Devices Group, the Home Appliances Group and the Social Infrastructure Group. In 2010, Toshiba was the world's fifth-largest personal computer vendor measured by revenues (after Hewlett-Packard, Dell, Acer and Lenovo). In the same year, it was also the world's fourth-largest manufacturer of semiconductors by revenues (after Intel Corporation, Samsung Electronics and Texas Instruments).
Toshiba is listed on the Tokyo Stock Exchange, where it is a constituent of the Nikkei 225 and TOPIX indices, the Osaka Securities Exchange and the Nagoya Stock Exchange .
^ a b c d e f "Toshiba Financial Statements" (PDF). Toshiba Corporation. Retrieved 2014-05-10. 
^ Jones, Daniel (2003) [1917], Peter Roach, James Hartmann and Jane Setter, eds., English Pronouncing Dictionary, Cambridge: Cambridge University Press, ISBN 3-12-539683-2 
^ (Press Release) http://www.toshiba.co.jp/about/press/2006_02/pr0601.htm
^ "Gartner Says Worldwide PC Market Grew 13 Percent in 2007" (Press release). Economic Times. January 16, 2010. Retrieved January 18, 2010.""
""Virage Logic "",,,,""Virage Logic corporation, founded 1996, was a provider of both functional and physical semiconductor intellectual property (IP) for the design of complex integrated circuits. The company's highly differentiated product portfolio includes processor centric solutions, interface IP solutions, embedded SRAMs and NVMs, embedded memory test and repair, logic libraries, and memory development software.""
""Yahoo! Inc. "",,,,""Yahoo Inc. (styled as Yahoo!) is an American multinational technology company headquartered in Sunnyvale, California. It is globally known for its Web portal, search engine Yahoo! Search, and related services, including Yahoo! Directory, Yahoo! Mail, Yahoo! News, Yahoo! Finance, Yahoo! Groups, Yahoo! Answers, advertising, online mapping, video sharing, fantasy sports and its social media website. It is one of the most popular sites in the United States. According to third-party web analytics providers, Alexa and SimilarWeb, Yahoo! is the highest-read news and media website, with over 7 billion readers per month, being the fourth most visited website globally, as of June 2015. According to news sources, roughly 700 million people visit Yahoo websites every month. Yahoo itself claims it attracts "more than half a billion consumers every month in more than 30 languages."
Yahoo was founded by Jerry Yang and David Filo in January 1994 and was incorporated on March 2, 1995. Marissa Mayer, a former Google executive, serves as CEO and President of the company.
In January 2015, the company announced it planned to spin-off its stake in Alibaba Group in a separately listed company. In December 2015 it reversed this decision, opting instead to spin-off its internet business as a separate company.
^ https://info.yahoo.com/management-team
^ "Yahoo! Inc., VMware, Inc. Beat Earnings Estimates". ValueWalk. 
^ Yahoo! Inc. Form 10-K, Securities and Exchange Commission, February 28, 2015
^ "YHOO Annual Income Statement - Yahoo! Inc. Annual Financials". 
^ "YHOO Balance Sheet - Yahoo! Inc. Stock - Yahoo! Canada Finance". 
^ "Yahoo - Yahoo Reports Fourth Quarter and Full Year 2014 Results". 
^ "Yahoo - Yahoo Reports Second Quarter 2015 Results". 
^ a b "Yahoo.com Site Overview". Alexa.com. Alexa Internet. Retrieved February 27, 2016. 
^ Staff (2012). "yahoo.com". Quantcast – It's your audience. We just find it. Quantcast Corporation. Retrieved May 23, 2012. 
^ "Yahoo.com Analytics". SimilarWeb.com. Retrieved June 29, 2015. 
^ "Top 50 sites in the world for News And Media". SimilarWeb.com. Retrieved June 29, 2015. 
^ Swartz, Jon (November 7, 2011). "Yahoo's latest moves baffle some". USA Today (Washington DC). Retrieved July 22, 2012. 
^ "Canada Pension Plan mulls Yahoo! buy, report says". CBC News (Toronto). October 20, 2011. Retrieved July 22, 2012. 
^ "Yahoo!". LinkedIn.com. LinkedIn. 2012. Retrieved July 2, 2012. 
^ "Yahoo! celebrates 20th anniversary". Yahoo! News. 2015-03-01. Retrieved 2016-03-27. 
^ "At 20, Yahoo celebrates and looks ahead". Yahoo!. 2015-03-01. Retrieved 2016-03-27. 
^ Perlroth, Nicole (July 17, 2012). "Marissa Mayer hopes to improve user experience at Yahoo". The Times of India (New Delhi). Archived from the original on July 17, 2012. 
^ "Yahoo reverses Alibaba spin-off plan". BBC News. December 9, 2015. Retrieved December 29, 2015.""
""Zilog, Inc. "",,,,""Zilog, Inc., previously known as ZiLOG, is an American manufacturer of 8-bit and 16-bit microcontrollers. Its most famous product is the Z80 series, 8-bit microprocessors that were compatible with the Intel 8080 but less expensive to use. The Z80 was widely used during the 1980s in many popular home computers such as the TRS-80 and the ZX81. 16 and 32-bit processors were also produced by the company, but these did not see widespread use. From the 1990s the company focused primarily on the microcontroller market.
The name is an acronym for "Z (the last word in) integrated logic".
^ Zilog Inc 10-K, March 2009
^ IXYS Corporation 10-K, March 2013
^ Zilog Oral History Panel on the Founding of the Company and the Development of the Z80 Microprocessor""
""Computing Accreditation Commission "",,,,""CSAB, Inc., formerly called the Computing Sciences Accreditation Board, Inc., is a non-profit professional organization in the United States, focused on the quality of education in computing disciplines. The Association for Computing Machinery (ACM) and the IEEE Computer Society (IEEE-CS) are the member societies of CSAB. The Association for Information Systems (AIS) was a member society between 2002 and September 2009.
CSAB itself is a member society of ABET, to support the accreditation of several computing (related) disciplines:
It is leading for computer science, information systems, information technology, and software engineering
It is working together with other ABET member societies for computer engineering, information engineering technology, and biological engineering
Who is doing what:
For the disciplines where CSAB is leading, it develops the accreditation criteria and it educate the so-called Program Evaluators (PEVs).
But the accreditation activities themselves are conducted by the appropriate ABET accreditation commission. For computing this is the Computing Accreditation Commission (CAC).""
""American Mathematical Society "",,,,""The American Mathematical Society (AMS) is an association of professional mathematicians dedicated to the interests of mathematical research and scholarship, and serves the national and international community through its publications, meetings, advocacy and other programs.
The society is one of the four parts of the Joint Policy Board for Mathematics (JPBM) and a member of the Conference Board of the Mathematical Sciences (CBMS).""
""American National Standards Institute "",,,,""The American National Standards Institute (ANSI, /ˈænsiː/ AN-see) is a private non-profit organization that oversees the development of voluntary consensus standards for products, services, processes, systems, and personnel in the United States. The organization also coordinates U.S. standards with international standards so that American products can be used worldwide.
ANSI accredits standards that are developed by representatives of other standards organizations, government agencies, consumer groups, companies, and others. These standards ensure that the characteristics and performance of products are consistent, that people use the same definitions and terms, and that products are tested the same way. ANSI also accredits organizations that carry out product or personnel certification in accordance with requirements defined in international standards.
The organization's headquarters are in Washington, DC. ANSI's operations office is located in New York City. The ANSI annual operating budget is funded by the sale of publications, membership dues and fees, accreditation services, fee-based programs, and international standards programs.

""
""American Statistical Association "",,,,""The American Statistical Association (ASA) is the main professional organization for statisticians and related professionals in the United States. It was founded in Boston, Massachusetts on November 27, 1839, and is the second oldest continuously operating professional society in the US (only the Massachusetts Medical Society, founded in 1781, is older). The ASA services statisticians, quantitative scientists, and users of statistics across many academic areas and applications.

""
""Ames Research Center "",,,,""Ames Research Center (ARC), commonly known as NASA Ames, is a major NASA research center at Moffett Federal Airfield in California's Silicon Valley. Originally founded as the second National Advisory Committee for Aeronautics (NACA) laboratory, that agency was dissolved and its assets and personnel transferred to the newly created National Aeronautics and Space Administration (NASA) on October 1, 1958. NASA Ames is named in honor of Joseph Sweetman Ames, a physicist and one of the founding members of NACA. At last estimate NASA Ames has over US$3.0 billion in capital equipment, 2,300 research personnel and a US$860 million annual budget.
Ames was originally founded to conduct wind-tunnel research on the aerodynamics of propeller-driven aircraft; however, its role has expanded to encompass spaceflight and information technology. Ames plays a role in many NASA missions. It provides leadership in astrobiology; small satellites; robotic lunar exploration; the search for habitable planets; supercomputing; intelligent/adaptive systems; advanced thermal protection; and airborne astronomy. Ames also develops tools for a safer, more efficient national airspace. The center's current director is Eugene Tu.
The site is mission center for several key current missions (Kepler, the Lunar Crater Observation and Sensing Satellite (LCROSS) mission, Stratospheric Observatory for Infrared Astronomy (SOFIA), Interface Region Imaging Spectrograph) and a major contributor to the "new exploration focus" as a participant in the Orion crew exploration vehicle.

""
""Argonne National Laboratory "",,,,""Argonne National Laboratory is a science and engineering research national laboratory operated by UChicago Argonne LLC for the United States Department of Energy located near Lemont, Illinois, outside Chicago. It is the largest national laboratory by size and scope in the Midwest. Argonne was initially formed to carry out Enrico Fermi's work on nuclear reactors as part of the Manhattan Project, and it was designated as the first national laboratory in the United States on July 1, 1946. Today it maintains a broad portfolio in basic science research, energy storage and renewable energy, environmental sustainability, supercomputing, and national security.
UChicago Argonne, LLC, the operator of the laboratory, "brings together the expertise of the University of Chicago (the sole member of the LLC) with Jacobs Engineering Group Inc." Argonne is a part of the expanding Illinois Technology and Research Corridor. Argonne formerly ran a smaller facility called Argonne National Laboratory-West (or simply Argonne-West) in Idaho next to the Idaho National Engineering and Environmental Laboratory. In 2005, the two Idaho-based laboratories merged to become the Idaho National Laboratory.""
""Association for Information Systems "",,,,""The Association for Information Systems (AIS) is an international, not-for-profit, professional association with the stated mission to serve society through the advancement of knowledge and the promotion of excellence in the practice and study of information systems. Membership is made up primarily of academic educators, researchers, and institutions that specialize in information systems (IS) development, implementation, and evaluation. The association has members in more than 90 countries, and is led by a president who is annually elected from one of three world regions—the Americas, Europe and Africa, and Asia-Pacific—on a rotating basis. The governing Council is made up of elected functional vice-presidents and other officers and council members who are elected in the three world regions. The association organizes two annual conferences for IS researchers, educators, and students: The International Conference on Information Systems (I.C.I.S.), which alternates between the three world regions, and the Americas Conference For Information Systems (AMCIS), which is located at different sites in North, Central, and South America. The Association publishes academic journals including:
Journal of the Association for Information Systems (JAIS)
Scandinavian Journal of Information Systems (SJIS)
Revista Latinoamericana Y Del Caribe De La Associacion De Sistemas De Informacion (RELCASI)
Pacific Asia Journal of the Association for Information Systems (PAJAIS)
Journal of the Midwest Association for Information Systems (JMWAIS)
Journal of Information Technology Theory and Application (JITTA)
Communications of the Association for Information Systems (CAIS)
AIS Transactions on Replication Research (TRR)
AIS Transactions on Human-Computer Interaction (THCI)
Affiliated journals include:
Business & Information Systems Engineering (BISE)
Management Information Systems Quarterly (MISQ)
MIS Quarterly Executive (MISQe)
Information Systems Journal (ISJ)
Systèmes d'Information et Management (SIM)
Foundations and Trends in Information Systems (FnTIS)
Both AIS published titles and affiliated journals are included in the AIS eLibrary, which is accessible as a benefit of membership.[1]

""
""British Computer Society "",,,,""The British Computer Society is a professional body and a learned society that represents those working in Information Technology both in the United Kingdom and internationally. Established in 1957, in 2009 it rebranded as BCS — The Chartered Institute for IT, although this has not been reflected in a legal name change.""
""Clay Mathematics Institute "",,,,""The Clay Mathematics Institute (CMI) is a private, non-profit foundation, based in Peterborough, New Hampshire, United States. CMI's scientific activities are managed from the President's office in Oxford, United Kingdom. The institute is "dedicated to increasing and disseminating mathematical knowledge." It gives out various awards and sponsorships to promising mathematicians. The institute was founded in 1998 through the sponsorship of Boston businessman Landon T. Clay. Harvard mathematician Arthur Jaffe was the first president of CMI.
While the institute is best known for its Millennium Prize Problems, it carries out a wide range of activities, including a postdoctoral program (ten Clay Research Fellows are supported currently), conferences, workshops, and summer schools.""
""Charles Babbage Institute "",,,,""The Charles Babbage Institute is a research center at the University of Minnesota specializing in the history of information technology, particularly the history of digital computing, programming/software, and computer networking since 1935. The institute is named for Charles Babbage, the nineteenth-century English inventor of the programmable computer. The Institute is located in Elmer L. Andersen Library at the University of Minnesota Libraries in Minneapolis, Minnesota.""
""Computer History Museum "",,,,""The Computer History Museum (CHM) is a museum established in 1996 in Mountain View, California, US. The Museum is dedicated to preserving and presenting the stories and artifacts of the information age, and exploring the computing revolution and its impact on society.""
""Computing Community Consortium "",,,,""The Computing Community Consortium (CCC) is an organization whose goal is to catalyze and empower the U.S. computing research community to pursue audacious, high-impact research.
Established in 2006 through a cooperative agreement between the Computing Research Association (CRA) — representing over 220 North American academic departments, industrial research labs, and professional societies with computing research interests — and the U.S. National Science Foundation, the CCC provides:

a voice for the national computing research community. The CCC facilitates the development of a bold, multi-themed vision for computing research, and communicates that vision to a wide range of major stakeholders.

The CCC is governed by an 18-member Council. Susan Graham (U.C. Berkeley) serves as Chair. Ann Drobnis serves as staff Director.
The CCC is housed at CRA's headquarters in Washington, D.C., USA.""
""Computing Research Association "",,,,""The Computing Research Association (CRA) is a 501(c)3 non-profit association of North American academic departments of computer science, computer engineering, and related fields; laboratories and centers in industry, government, and academia engaging in basic computing research; and affiliated professional societies. CRA was formed in 1972 and is based in Washington, D.C., USA.

""
""Courant Institute of Mathematical Sciences "",,,,""The Courant Institute of Mathematical Sciences (CIMS) is an independent division of New York University (NYU) under the Faculty of Arts & Science that serves as a center for research and advanced training in computer science and mathematics. The Director of the Courant Institute directly reports to New York University's Provost and President and works closely with deans and directors of other NYU colleges and divisions respectively. The Courant Institute is named after Richard Courant, one of the founders of the Courant Institute and also a mathematics professor at New York University from 1936 to 1972.
The Courant Institute is considered one of the leading and most prestigious mathematics schools and mathematical sciences research centers in the world. It is ranked #1 in applied mathematical research in US, #5 in citation impact worldwide, and #12 in citation worldwide. On the Faculty Scholarly Productivity Index, it is ranked #3 with an index of 1.84. It is also known for its extensive research in pure mathematical areas, such as partial differential equations, probability and geometry, as well as applied mathematical areas, such as computational biology, computational neuroscience, and mathematical finance. The Mathematics Department of the Institute has 18 members of the United States National Academy of Sciences (more than any other mathematics department in the U.S.) and five members of the National Academy of Engineering. Four faculty members have been awarded the National Medal of Science, one was honored with the Kyoto Prize, and nine have received career awards from the National Science Foundation. Courant Institute professors Peter Lax, S. R. Srinivasa Varadhan, Mikhail Gromov, Louis Nirenberg won the 2005, 2007, 2009 and 2015 Abel Prize respectively for their research in partial differential equations, probability and geometry. Louis Nirenberg also received the Chern Medal in 2010, and Subhash Khot won the Nevanlinna Prize in 2014.
The undergraduate programs and graduate programs at the Courant Institute are run independently by the Institute, and formally associated with the NYU College of Arts and Science and NYU Graduate School of Arts and Science respectively.
^ "NYU > Courant Institute > About > Courant and NYU". Cims.nyu.edu. Retrieved 2011-08-06. 
^ "Search: Sciences Programs | US News". US News. Retrieved 2015-05-06. 
^ "Faculty Scholarly Productivity Index". The Chronicle. 2007-11-16. Retrieved 2011-08-06. 
^ Foderaro, Lisa W. (2009-06-01). "Complex Math, Simple Sum: 3 Awards in 5 Years". The New York Times.""
""Defense Advanced Research Projects Agency "",,,,""The Defense Advanced Research Projects Agency (DARPA) is an agency of the U.S. Department of Defense responsible for the development of emerging technologies for use by the military.
DARPA was created in February 1958 as the Advanced Research Projects Agency (ARPA) by President Dwight D. Eisenhower. Its purpose was to formulate and execute research and development projects to expand the frontiers of technology and science, with the aim to reach beyond immediate military requirements. The administration was responding to the Soviet launching of Sputnik 1 in 1957, and DARPA's mission was to ensure U.S. military technology would be more sophisticated than that of the nation's potential enemies.
The name of the organization changed several times from its founding name ARPA: DARPA (March 1972), ARPA (February 1993), and DARPA (March 1996).
DARPA is independent from other military research and development and reports directly to senior Department of Defense management. DARPA has ca. 240 personnel (13 in management, close to 140 technical) directly managing a $3 billion budget.
DARPA-funded projects have provided significant technologies that influenced many non-military fields, such as computer networking and graphical user interfaces in information technology.""
""Eclipse Foundation "",,,,""The Eclipse Foundation is a not-for-profit, member supported corporation that acts as the steward of Eclipse, an open source community working to build a development platform consisting of the frameworks, tools and run-times needed for "building, deploying and managing software across the lifecycle." The most well-known of the Eclipse projects is the Eclipse platform, a multi-language software development environment and IDE.
The Eclipse Foundation's stated aim is to cultivate both the community and "an ecosystem of complementary products and services." It is considered a "third generation" open-source organisation.""
""Electronic Frontier Foundation "",,,,""The Electronic Frontier Foundation (EFF) is an international non-profit digital rights group based in San Francisco in the United States.
EFF provides funds for legal defense in court, presents amicus curiae briefs, defends individuals and new technologies from what it considers abusive legal threats, works to expose government malfeasance, provides guidance to the government and courts, organizes political action and mass mailings, supports some new technologies which it believes preserve personal freedoms and online civil liberties, maintains a database and web sites of related news and information, monitors and challenges potential legislation that it believes would infringe on personal liberties and fair use, and solicits a list of what it considers patent abuses with intentions to defeat those that it considers without merit.""
""European Network and Information Security Agency "",,,,""The European Union Agency for Network and Information Security - self-designation ENISA from the abbreviation of its original name - is an agency of the European Union. It is fully operational since September 1, 2005. It has its seat in Heraklion, Crete (Greece).
ENISA was created in 2004 by EU Regulation No 460/2004 under the name of European Network and Information Security Agency. The new basic regulation is EU Regulation No 526/2013 referencing it as the European Union Agency for Network and Information Security (ENISA). The objective of ENISA is to improve network and information security in the European Union. The agency has to contribute to the development of a culture of network and information security for the benefit of the citizens, consumers, enterprises and public sector organisations of the European Union, and consequently will contribute to the smooth functioning of the EU Internal Market.
ENISA assists the Commission, the Member States and, consequently, the business community in meeting the requirements of network and information security, including present and future EU legislation. ENISA ultimately strives to serve as a centre of expertise for both Member States and EU Institutions to seek advice on matters related to network and information security.
ENISA supported 2010-, 2012- and Cyber Europe 2014 pan-European cybersecurity exercises.

""
""European Telecommunications Standards Institute "",,,,""The European Telecommunications Standards Institute (ETSI) is an independent, not-for-profit, standardization organization in the telecommunications industry (equipment makers and network operators) in Europe, headquartered in Sophia-Antipolis, France, with worldwide projection. ETSI produces globally-applicable standards for Information and Communications Technologies (ICT), including fixed, mobile, radio, converged, broadcast and internet technologies.
ETSI was created by CEPT in 1988 and is officially recognized by the European Commission and the EFTA secretariat. Based in Sophia Antipolis (France), ETSI is officially responsible for standardization of Information and Communication Technologies (ICT) within Europe.
ETSI publishes between 2,000 and 2,500 standards every year. Since its establishment in 1988, it produced over 30,000. These include the standards that enable key global technologies such as GSM cell phone system, 3G, 4G, DECT, TETRA professional mobile radio system, and Short Range Device requirements including LPD radio, smart cards and many more standards success stories.
Significant ETSI technical committees and Industry Specification Groups (ISGs) include SmartM2M (for machine-to-machine communications), Intelligent Transport Systems, Network Functions Virtualisation, Cyber Security, Electronic Signatures and Infrastructures etc. ETSI inspired the creation of, and is a partner in, 3GPP and oneM2M. All technical committees, working and industry specification groups are accessible via the ETSI Portal
ETSI technology clusters provide a simple, easy to grasp overview of ETSI’s activities in ICT standardization. Each technology cluster represents a major component of a global ICT architecture and covers the work of a number of ETSI technical committees and working groups that share a common technological scope and vision. The work of a single Technical Committee may be represented in several clusters. Clusters facilitate easy identification of an area of interest based on business relevance or application domain rather than purely on specific technical work areas.
In 2013, ETSI's budget exceeded €23 million, with contributions coming from members, commercial activities like sale of documents, plug-tests and fora hosting (i.e. the hosting of forums), contract work and partner funding.
ETSI is a founding partner organization of the Global Standards Collaboration initiative.

""
""Federal Communications Commission "",,,,""The Federal Communications Commission (FCC) is an independent agency of the United States government, created by Congressional statute (see 47 U.S.C. § 151 and 47 U.S.C. § 154) to regulate interstate communications by radio, television, wire, satellite, and cable in all 50 states, the District of Columbia and U.S. territories. The FCC works towards six goals in the areas of broadband, competition, the spectrum, the media, public safety and homeland security, and modernizing itself.
The FCC was formed by the Communications Act of 1934 to replace the radio regulation functions of the Federal Radio Commission. The FCC took over wire communication regulation from the Interstate Commerce Commission. The FCC's mandated jurisdiction covers the 50 states, the District of Columbia, and Political divisions of the United States. The FCC also provides varied degrees of cooperation, oversight, and leadership for similar communications bodies in other countries of North America. The FCC is funded entirely by regulatory fees. It has an estimated fiscal-2016 budget of US$388 million. It has 1,720 federal employees.
^ "Employee Profile at the FCC". FCC. 30 Jul 2013. Retrieved 31 Dec 2014. 
^ a b 2016 Budget Estimate FCC Budget Estimates. FCC.
^ 
^ "2008 Performance and Accountability Report" (PDF). Federal Communications Commission. September 2008.""
""Free Software Foundation "",,,,""The Free Software Foundation (FSF) is a 501(c)(3) non-profit organization founded by Richard Stallman on 4 October 1985 to support the free software movement, which promotes the universal freedom to study, distribute, create, and modify computer software, with the organization's preference for software being distributed under copyleft ("share alike") terms, such as with its own GNU General Public License. The FSF was incorporated in Massachusetts, USA, where it is also based.
From its founding until the mid-1990s, FSF's funds were mostly used to employ software developers to write free software for the GNU Project. Since the mid-1990s, the FSF's employees and volunteers have mostly worked on legal and structural issues for the free software movement and the free software community.
Consistent with its goals, only free software is used on the FSF's computers.""
""Test Technology Technical Council "",,,,""Vishwani D. Agrawal (born December 7, 1957) is the James J. Danaher Professor of Electrical and Computer Engineering at Auburn University. He has over forty decades of industry and university experience, including working at Bell Labs, Murray Hill, NJ, Rutgers University, TRW and IIT, Delhi. He is well known as a cofounder and long-term mentor of the International Conference on VLSI Design held annually in India since 1985.""
""International Game Developers Association "",,,,""International Game Developers Association (IGDA) is the professional association for over 12,000 video and computer game developers worldwide. It is incorporated in the United States as a non-profit organization. Its stated mission is "To advance the careers and enhance the lives of game developers by connecting members with their peers, promoting professional development, and advocating on issues that affect the developer community."
In recognition of the wide-ranging, multidisciplinary nature of interactive entertainment, there are supposedly no barriers to entry.""
""International Organization for Standardization "",,,,""The International Organization for Standardization (ISO) is an international standard-setting body composed of representatives from various national standards organizations.
Founded on 23 February 1947, the organization promotes worldwide proprietary, industrial and commercial standards. It is headquartered in Geneva, Switzerland, and as of 2015 works in 196 countries.
It was one of the first organizations granted general consultative status with the United Nations Economic and Social Council.""
""ITU "",,,,""The International Telecommunication Union (ITU; French: Union Internationale des Télécommunications), originally the International Telegraph Union (French: Union Télégraphique Internationale), is a specialized agency of the United Nations (UN) that is responsible for issues that concern information and communication technologies.
The ITU coordinates the shared global use of the radio spectrum, promotes international cooperation in assigning satellite orbits, works to improve telecommunication infrastructure in the developing world, and assists in the development and coordination of worldwide technical standards. The ITU is active in areas including broadband Internet, latest-generation wireless technologies, aeronautical and maritime navigation, radio astronomy, satellite-based meteorology, convergence in fixed-mobile phone, Internet access, data, voice, TV broadcasting, and next-generation networks.
ITU also organizes worldwide and regional exhibitions and forums, such as ITU TELECOM WORLD, bringing together representatives of government and the telecommunications and ICT industry to exchange ideas, knowledge and technology.
ITU, based in Geneva, Switzerland, is a member of the United Nations Development Group. ITU has been an intergovernmental public-private partnership organization since its inception. Its membership includes 193 Member States and around 700 public and private sector companies as well as international and regional telecommunication entities, known as Sector Members and Associates, which undertake most of the work of each Sector.""
""Isaac Newton Institute for Mathematical Sciences "",,,,""The Isaac Newton Institute for Mathematical Sciences is an international research institute for mathematics and its many applications at the University of Cambridge. It is named after one of the university's most illustrious figures, the mathematician and natural philosopher Sir Isaac Newton and occupies buildings adjacent to the Cambridge Centre for Mathematical Sciences.""
""JISC "",,,,""Jisc (formerly the Joint Information Systems Committee) is a United Kingdom non-departmental public body whose role is to support post-16 and higher education, and research, by providing relevant and useful advice, digital resources and network and technology services, while researching and developing new technologies and ways of working. It is funded by a combination of the UK further and higher education funding bodies, and individual higher education institutions.""
""Jet Propulsion Laboratory "",,,,""The Jet Propulsion Laboratory (JPL) is a federally funded research and development center and NASA field center located in La Cañada Flintridge, California and Pasadena, California, United States.
The JPL is managed by the nearby California Institute of Technology (Caltech) for NASA. The laboratory's primary function is the construction and operation of planetary robotic spacecraft, though it also conducts Earth-orbit and astronomy missions. It is also responsible for operating NASA's Deep Space Network.
Among the laboratory's current major active projects are the Mars Science Laboratory mission (which includes the Curiosity rover), the Cassini–Huygens mission orbiting Saturn, the Mars Exploration Rover Opportunity, the Mars Reconnaissance Orbiter, the Dawn mission to the dwarf planet Ceres and asteroid Vesta, the Juno spacecraft en route to Jupiter, the NuSTAR X-ray telescope, and the Spitzer Space Telescope. They are also responsible for managing the JPL Small-Body Database, and provides physical data and lists of publications for all known small Solar System bodies.
The JPL's Space Flight Operations Facility and Twenty-Five-Foot Space Simulator are designated National Historic Landmarks.""
""Langley Research Center "",,,,""Langley Research Center (LaRC) is the oldest of NASA's field centers, located in Hampton, Virginia, United States. It directly borders Poquoson, Virginia and Langley Field. LaRC focuses primarily on aeronautical research, though the Apollo lunar lander was flight-tested at the facility and a number of high-profile space missions have been planned and designed on-site.
Established in 1917 by the National Advisory Committee for Aeronautics, the Center currently devotes two-thirds of its programs to aeronautics, and the rest to space. LaRC researchers use more than 40 wind tunnels to study improved aircraft and spacecraft safety, performance, and efficiency. Between 1958 and 1963, when NASA started Project Mercury, LaRC served as the main office of the Space Task Group, with the office being transferred to the Manned Spacecraft Center (now the Lyndon B. Johnson Space Center) in Houston in 1962–63.
The current director is David E. Bowles.""
""Lawrence Berkeley National Laboratory "",,,,""The Lawrence Berkeley National Laboratory (LBNL or LBL), commonly referred to as Berkeley Lab, is a United States national laboratory located in the Berkeley Hills near Berkeley, California that conducts scientific research on behalf of the United States Department of Energy (DOE). It is managed and operated by the University of California, The laboratory overlooks University of California, Berkeley's main campus.""
""Lawrence Livermore National Laboratory "",,,,""Lawrence Livermore National Laboratory (LLNL) is a federal research facility in Livermore, California, founded by the University of California in 1952. A Federally Funded Research and Development Center (FFRDC), it is primarily funded by the United States Department of Energy (DOE) and managed and operated by Lawrence Livermore National Security, LLC (LLNS), a partnership of the University of California, Bechtel, Babcock & Wilcox, URS, and Battelle Memorial Institute in affiliation with the Texas A&M University System. The laboratory was honored in 2012 by having the synthetic chemical element livermorium named after it.""
""Linux Professional Institute "",,,,""The Linux Professional Institute Inc. (LPI) is a non-profit organization that provides vendor-independent professional certification for Linux system administrators and programmers.
The Linux Professional Institute Certifications (LPIC) provide a qualification that can be used to indicate that someone is competent at a certain level.""
""London Mathematical Society "",,,,""The London Mathematical Society (LMS) is one of the United Kingdom's learned societies for mathematics (the others being the Royal Statistical Society (RSS) and the Institute of Mathematics and its Applications (IMA)).""
""Los Alamos National Laboratory "",,,,""Los Alamos National Laboratory (or LANL; previously known at various times as Project Y, Los Alamos Laboratory, and Los Alamos Scientific Laboratory) is the only laboratory in the United States where classified work towards the design of nuclear weapons has been undertaken besides the Lawrence Livermore National Laboratory. LANL is a United States Department of Energy national laboratory, managed and operated by Los Alamos National Security (LANS), located in Los Alamos, New Mexico. The laboratory is one of the largest science and technology institutions in the world. It conducts multidisciplinary research in fields such as national security, space exploration, renewable energy, medicine, nanotechnology, and supercomputing.""
""Personal Robots Group "",,,,""Cynthia Lynn Breazeal (born November 15, 1967 in Albuquerque, New Mexico) is an Associate Professor of Media Arts and Sciences at the Massachusetts Institute of Technology, where she is the director of the Personal Robots Group (formerly the Robotic Life Group) at the MIT Media Laboratory. She is best known for her work in robotics where she is recognized as a pioneer of social robotics and human–robot interaction.
^ "Cynthia Breazeal – Roboticist". National Academy of Sciences. Women's Adventures in Science. Retrieved April 17, 2010.""
""Max Planck Institutes "",,,,""The Max Planck Society for the Advancement of Science (German: Max-Planck-Gesellschaft zur Förderung der Wissenschaften e. V.; abbreviated MPG) is a formally independent non-governmental and non-profit association of German research institutes founded in 1911 as the Kaiser Wilhelm Society and renamed the Max Planck Society in 1948 in honor of its former president, theoretical physicist Max Planck. The society is funded by the federal and state governments of Germany as well as other sources.
According to its primary goal Max Planck Society supports fundamental research in the natural, life and social sciences, the arts and humanities in its 83 (as of January 2014) Max Planck Institutes. The society has a total staff of approximately 17,000 permanent employees, including 5,470 scientists, plus around 4,600 non-tenured scientists and guests. Society budget for 2015 was about €1.7 billion.
The Max Planck Institutes focus on excellence in research. The Max Planck Society has a world-leading reputation as a science and technology research organization, with 33 Nobel Prizes awarded to their scientists, and are generally regarded as the foremost basic research organization in Europe and the world. In 2013, the Nature Publishing Index placed the Max Planck institutes fifth worldwide in terms of research published in Nature journals (after Harvard, MIT, Stanford and the US NIH). In terms of total research volume (unweighted by citations or impact), the Max Planck Society is only outranked by the Chinese Academy of Sciences, the Russian Academy of Sciences and Harvard University. The Thomson Reuters-Science Watch website placed the Max Planck Society as the second leading research organization worldwide following Harvard University, in terms of the impact of the produced research over science fields.
The Max Planck Society and its predecessor Kaiser Wilhelm Society hosted several renowned scientists in their fields, including Otto Hahn, Werner Heisenberg, and Albert Einstein, to name a few.
^ a b c d e f g 
^ a b c d e f g 
^ a b 
^ Nature Publishing Index - 2013 Global Top 200, Nature Publishing Group
^ The titans: Institutional rankings by output and citations, Times Higher Education, 17 September 2009
^ http://archive.sciencewatch.com/inter/ins/09/09Top20Overall/""
""Moving Picture Experts Group "",,,,""The Moving Picture Experts Group (MPEG) is a working group of authorities that was formed by ISO and IEC to set standards for audio and video compression and transmission. It was established in 1988 by the initiative of Hiroshi Yasuda (Nippon Telegraph and Telephone) and Leonardo Chiariglione, group Chair since its inception. The first MPEG meeting was in May 1988 in Ottawa, Canada. As of late 2005, MPEG has grown to include approximately 350 members per meeting from various industries, universities, and research institutions. MPEG's official designation is ISO/IEC JTC 1/SC 29/WG 11 – Coding of moving pictures and audio (ISO/IEC Joint Technical Committee 1, Subcommittee 29, Working Group 11).""
""Mozilla Foundation "",,,,""The Mozilla Foundation is a non-profit organization that exists to support and collectively lead the open source Mozilla project. Founded in July 2003, the organization sets the policies that govern development, operates key infrastructure and controls Mozilla trademarks and copyrights. It owns a taxable subsidiary: the Mozilla Corporation, which employs many Mozilla developers and coordinates releases of the Mozilla Firefox web browser and Mozilla Thunderbird email client. The subsidiary is 100% owned by the parent, and therefore follows the same non-profit principles. The Mozilla Foundation was founded by the Netscape-affiliated Mozilla Organization. The organization is currently based in the Silicon Valley city of Mountain View, California, United States.
The Mozilla Foundation describes itself as "a non-profit organization that promotes openness, innovation and participation on the Internet." The Mozilla Foundation is guided by the Mozilla Manifesto, which lists 10 principles which Mozilla believes "are critical for the Internet to continue to benefit the public good as well as commercial aspects of life."""
""National Academy of Engineering "",,,,""The National Academy of Engineering (NAE) is a private non-profit organization in the United States. The National Academy of Engineering is part of the National Academies of Sciences, Engineering, and Medicine, which also includes the National Academy of Science (NAS), the National Academy of Medicine and the National Research Council.
New members are elected by current members, based on their distinguished and continuing achievements in original research. The election process for new members is conducted annually. The NAE is autonomous in its administration and in the selection of its members, sharing with the rest of the National Academies the role of advising the federal government. The NAE operates engineering programs aimed at meeting national needs, encourages education and research, and recognizes the superior achievements of engineers. The current president is Dr. C. Daniel Mote, Jr.""
""National Academy of Sciences "",,,,""The National Academy of Sciences (NAS) is a private non-profit organization in the United States. The National Academy of Sciences is part of the National Academies of Sciences, Engineering, and Medicine, which also includes the National Academy of Engineering (NAE), the National Academy of Medicine and the National Research Council.
As a national academy, new members of the organization are elected annually by current members, based on their distinguished and continuing achievements in original research. Election to the National Academy is one of the highest honors in U.S. science. Members serve pro bono as "advisers to the nation" on science, engineering, and medicine. The group holds a congressional charter under Title 36 of the United States Code.
Founded in 1863 as a result of an Act of Congress that was approved by Abraham Lincoln, the NAS is charged with providing independent, objective advice to the nation on matters related to science and technology. … to provide scientific advice to the government "whenever called upon" by any government department. The Academy receives no compensation from the government for its services.

""
""National Institute of Standards and Technology "",,,,""The National Institute of Standards and Technology (NIST), known between 1901 and 1988 as the National Bureau of Standards (NBS), is a measurement standards laboratory, also known as a National Metrological Institute (NMI), which is a non-regulatory agency of the United States Department of Commerce. The institute's official mission is to:

Promote U.S. innovation and industrial competitiveness by advancing measurement science, standards, and technology in ways that enhance economic security and improve our quality of life.

NIST had an operating budget for fiscal year 2007 (October 1, 2006-September 30, 2007) of about $843.3 million. NIST's 2009 budget was $992 million, and it also received $610 million as part of the American Recovery and Reinvestment Act. NIST employs about 2,900 scientists, engineers, technicians, and support and administrative personnel. About 1,800 NIST associates (guest researchers and engineers from American companies and foreign countries) complement the staff. In addition, NIST partners with 1,400 manufacturing specialists and staff at nearly 350 affiliated centers around the country. NIST publishes the Handbook 44 that provides the "Specifications, tolerances, and other technical requirements for weighing and measuring devices".
^ "2016 Appropriations Increase NIST Funding 166 percent". NIST. 2016. Retrieved 2016-01-13. 
^ 
^ NIST General Information. Retrieved on August 21, 2010.
^ "NIST Budget, Planning and Economic Studies". National Institute of Standards and Technology. October 5, 2010. Retrieved October 6, 2010.""
""National Science Foundation "",,,,""The National Science Foundation (NSF) is a United States government agency that supports fundamental research and education in all the non-medical fields of science and engineering. Its medical counterpart is the National Institutes of Health. With an annual budget of about US$7.0 billion (fiscal year 2012), the NSF funds approximately 24% of all federally supported basic research conducted by the United States' colleges and universities. In some fields, such as mathematics, computer science, economics and the social sciences, the NSF is the major source of federal backing.
The NSF's director, deputy director, and the 24 members of the National Science Board (NSB) are appointed by the President of the United States, and confirmed by the United States Senate. The director and deputy director are responsible for administration, planning, budgeting and day-to-day operations of the foundation, while the NSB meets six times a year to establish its overall policies. The current NSF director, confirmed in March 2014, is astronomer France A. Córdova, former president of Purdue University.
^ "Visit NSF". 
^ 
^ 
^ Morello, Lauren (March 12, 2014). "US Senate approves France Córdova to lead NSF". Nature. Retrieved March 18, 2014.""
""Oak Ridge National Laboratory "",,,,""Oak Ridge National Laboratory (ORNL) is an American multiprogram science and technology national laboratory managed for the United States Department of Energy (DOE) by UT-Battelle. ORNL is the largest science and energy national laboratory in the Department of Energy system by acreage and by annual budget. ORNL is located in Oak Ridge, Tennessee, near Knoxville. ORNL's scientific programs focus on materials, neutron science, energy, high-performance computing, systems biology and national security.
ORNL partners with the state of Tennessee, universities and industries to solve challenges in energy, advanced materials, manufacturing, security and physics.
The laboratory is home to several of the world's top supercomputers including the world's second most powerful supercomputer ranked by the TOP500, Titan, and is a leading neutron science and nuclear energy research facility that includes the Spallation Neutron Source and High Flux Isotope Reactor. ORNL hosts the Center for Nanophase Materials Sciences, the BioEnergy Science Center, and the Consortium for Advanced Simulation of Light-Water Reactors.""
""Object Management Group "",,,,""The Object Management Group (OMG) is an international, open membership, not-for-profit technology standards consortium. OMG Task Forces develop enterprise integration standards for a wide range of technologies and industries. OMG modeling standards enable visual design, execution and maintenance of software and other processes. Originally aimed at standardizing distributed object-oriented systems, the company now focuses on modeling (programs, systems and business processes) and model-based standards.""
""OSGi Alliance "",,,,""The OSGi Alliance, formerly known as the Open Services Gateway initiative, is an open standards organization founded in March 1999 that originally specified and continues to maintain the OSGi standard.
The OSGi specification describes a modular system and a service platform for the Java programming language that implements a complete and dynamic component model, something that does not exist in standalone Java/VM environments. Applications or components, coming in the form of bundles for deployment, can be remotely installed, started, stopped, updated, and uninstalled without requiring a reboot; management of Java packages/classes is specified in great detail. Application life cycle management is implemented via APIs that allow for remote downloading of management policies. The service registry allows bundles to detect the addition of new services, or the removal of services, and adapt accordingly.
The OSGi specifications have evolved beyond the original focus of service gateways, and are now used in applications ranging from mobile phones to the open-source Eclipse IDE. Other application areas include automobiles, industrial automation, building automation, PDAs, grid computing, entertainment, fleet management and application servers.

""
""Operational Research Society "",,,,""The Operational Research Society or The OR Society is an international learned society in the field of operational research (O.R.), with more than 2,500 members (2011). It has its headquarters in Birmingham, England.""
""Open Source Initiative "",,,,""The Open Source Initiative (OSI) is an organization dedicated to promoting open-source software.
The organization was founded in February 1998 by Bruce Perens and Eric S. Raymond, part of a group inspired by the Netscape Communications Corporation publishing the source code for its flagship Netscape Communicator product. Later, in August 1998, the organization added a board of directors.
Raymond was president from its founding until February 2005, followed briefly by Russ Nelson and then Michael Tiemann. In May 2012, the new board elected Simon Phipps as president and in May 2015 Allison Randal was elected as president when Phipps stepped down in preparation for the 2016 end of his Board term.

""
""Pacific Northwest National Laboratory "",,,,""Pacific Northwest National Laboratory (PNNL) is one of the United States Department of Energy national laboratories, managed by the Department of Energy's Office of Science. The main campus of the laboratory is in Richland, Washington.
PNNL scientists conduct basic and applied research and development to strengthen U.S. scientific foundations for fundamental research and innovation; prevent and counter acts of terrorism through applied research in information analysis, cyber security, and the nonproliferation of weapons of mass destruction; increase the U.S. energy capacity and reduce dependence on imported oil; and reduce the effects of human activity on the environment. PNNL has been operated by Battelle Memorial Institute since 1965.""
""Royal Statistical Society "",,,,""The Royal Statistical Society (RSS) is a British learned society for statistics, a professional body for statisticians, and a charity which promotes statistics for the public good.

""
""San Diego Supercomputer Center "",,,,""The San Diego Supercomputer Center (SDSC) is an organized research unit of the University of California, San Diego (UCSD). Physically, SDSC is located on the east end of Eleanor Roosevelt College on the campus of UCSD.
Founded in 1985, SDSC describes its mission as "developing and using technology to advance science." SDSC is primarily funded by the National Science Foundation (NSF) and pursues research in the areas of high performance computing, grid computing, computational biology, geoinformatics, computational physics, computational chemistry, data management, scientific visualization, and computer networking. SDSC is internationally recognized for its contribution to computational biosciences and computational approaches to earth sciences and genomics. SDSC is especially known for its role in the creation and maintenance of the Protein Data Bank, the George E. Brown, Jr. Network for Earthquake Engineering Simulation Cyberinfrastructure Center (NEESit), cyberinfrastructure for the geosciences (GEON), and the Tree of Life Project (TOL) .
SDSC is one of the four original sites involved in the TeraGrid project along with National Center for Supercomputing Applications (NCSA), Argonne National Laboratory, and the Center for Advanced Computing Research (CACR).
SDSC developed the Rocks cluster computing environment, and is a pioneer in data management software development, including the storage resource broker (SRB).
Currently, the director of SDSC is Michael L. Norman, professor of physics at UCSD, who succeeds Francine Berman, a noted pioneer in grid computing. He was named director in September 2010, having been interim director for more than a year.
SDSC is home to the Performance Modeling and Characterization (PMaC) laboratory, whose mission is to bring scientific rigor to the prediction and understanding of factors affecting the performance of current and projected High Performance Computing (HPC) platforms. PMaC is funded by the Department of Energy (SciDac PERC research grant), the Department of Defense (Navy DSRC PET program), DARPA, and the National Science Foundation. Allan E. Snavely founded the PMaC laboratory in 2001.
In 2009 a combined team from SDSC and Lawrence Berkeley National Labs led by Allan Snavely won the prestigious Data Challenge competition held in Portland Oregon, at SC09, the annual premier conference in High Performance Computing, Networking, Storage and Analysis for their design of a new kind of supercomputer that makes extensive use of flash memory and nicknamed "Dash". Dash is a prototype for a much larger system nicknamed "Gordon" that the team will deploy at SDSC in 2011 with more than 256 TB of flash memory. 
SDSC is also home to the Center for Applied Internet Data Analysis (CAIDA), and the Computational and Applied Statistics Laboratory (CASL). CAIDA is a collaboration of government, research, and commercial entities working together to improve the Internet.
^ "UC Regents Policy 2307".""
""Semiconductor Industry Association "",,,,""The Semiconductor Industry Association (SIA) is a trade association and lobbying group founded in 1977 that represents the United States semiconductor industry. It is located in Washington, D.C.
One of the main achievements of the SIA was the creation of the first National Technology Roadmap for Semiconductors, in the early 1990s. A chart from this roadmap, reproduced below, outlined the progress of the semiconductor industry over the next 15 years.

""
""Software Engineering Institute "",,,,""The Carnegie Mellon Software Engineering Institute (SEI) is a federally funded research and development center headquartered on the campus of Carnegie Mellon University in Pittsburgh, Pennsylvania, United States. The SEI also has offices in Washington, DC and Los Angeles, California. The SEI operates with major funding from the U.S. Department of Defense. The SEI also works closely with industry and academia through research collaborations.
On November 14, 1984, the U.S. Department of Defense elected Carnegie Mellon University as the host site of the Software Engineering Institute. The institute was founded with an initial allocation of $6 million, with another $97 million to be allocated in the subsequent five years. The SEI's contract with the Department of Defense is subject to review and renewal every five years.
The SEI program of work is conducted in several principal areas: cybersecurity, software assurance, software engineering and acquisition, and component capabilities critical to the Department of Defense.
^ O'Toole, James. "CMU Wins Software War." Pittsburgh Post Gazette, November 15, 1984. https://news.google.com/newspapers?id=RYRIAAAAIBAJ&sjid=mm4DAAAAIBAJ&pg=4978%2C3800166
^ http://www.sei.cmu.edu/research/index.cfm""
""SLAC National Accelerator Laboratory "",,,,""SLAC National Accelerator Laboratory, originally named Stanford Linear Accelerator Center, is a United States Department of Energy National Laboratory operated by Stanford University under the programmatic direction of the U.S. Department of Energy Office of Science and located in Menlo Park, California.
The SLAC research program centers on experimental and theoretical research in elementary particle physics using electron beams and a broad program of research in atomic and solid-state physics, chemistry, biology, and medicine using synchrotron radiation.""
""Systems Research Center "",,,,""The Systems Research Center (SRC) was a research laboratory created by Digital Equipment Corporation (DEC) in 1984, in Palo Alto, California.
DEC SRC was founded by a group of computer scientists, led by Robert Taylor, who left the Computer Science Laboratory (CSL) of Xerox PARC after an internal power struggle. SRC survived the takeover of DEC by Compaq in 1998. When Compaq was acquired by Hewlett-Packard in 2002, SRC was merged with other HP corporate research labs.
After Taylor's retirement, the lab was directed by Roy Levin and then by Lyle Ramshaw.
Some of the important developments made at SRC include the Modula-3 programming language; the snoopy cache, used in the first multiprocessor workstation, the Firefly, built from MicroVAX 78032 microprocessors; the first multi-threaded Unix system, Taos; the first user interface editor; early networked window systems, Trestle. Among the researchers at SRC, there are Butler Lampson, Chuck Thacker, and Leslie Lamport, all recipients of the ACM A.M. Turing Award.
A later inhabitant of this building is A9.com, a research part of Amazon.com.""
""The Open Group "",,,,""The Open Group is a vendor- and technology-neutral industry consortium, currently with over four hundred member organizations. It was formed in 1996 when X/Open merged with the Open Software Foundation. Services provided include strategy, management, innovation and research, standards, certification, and test development.
The Open Group is most famous as the certifying body for the UNIX trademark, and its publication of the Single UNIX Specification technical standard, which extends the POSIX standards and is the official definition of a UNIX system. The Open Group also develops and manages the TOGAF standard, which is an industry standard enterprise architecture framework.
The Open Group members include a range of IT buyers and vendors as well as government agencies, for example Capgemini, Fujitsu, Oracle, Hitachi, HP, Orbus Software, IBM, Kingdee, NEC, SAP, US Department of Defense, NASA and others.""
""Transaction Processing Performance Council "",,,,""Transaction Processing Performance Council (TPC) is a non-profit organization founded in 1988 to define transaction processing and database benchmarks and to disseminate objective, verifiable TPC performance data to the industry. TPC benchmarks are used in evaluating the performance of computer systems; the results are published on the TPC web site.""
""Viewpoints Research Institute "",,,,""Started by Alan Kay, Viewpoints Research Institute (VPRI) is a nonprofit public benefit organization incorporated in 2001 to improve "powerful ideas education" for the world's children and to advance the state of systems research and personal computing. Many of the institute's themes co-evolved with the inventions of networked personal computers, graphical user interfaces and dynamic object-oriented programming.
Its globally dispersed research group comes from a tradition of whole systems design developed by ARPA in the sixties and Xerox PARC in the seventies. Its ideology is motivated by user-centered systems design. Using this vantage point, the group invents computing technologies, content, curriculum, graphical user interfaces (GUIs), programming languages, implementation systems and processor and memory structures.""
""Wikimedia Foundation, Inc. "",,,,""The Wikimedia Foundation (WMF) is an American non-profit and charitable organization headquartered in San Francisco, California, that operates many wikis. The foundation is mostly known for hosting Wikipedia, the Internet encyclopedia, as well as Wiktionary, Wikiquote, Wikibooks, Wikisource, Wikimedia Commons, Wikispecies, Wikinews, Wikiversity, Wikidata, Wikivoyage, Wikimedia Incubator, and Meta-Wiki. It also owned the now-defunct Nupedia.
The Wikimedia organization was founded in 2003 by Jimmy Wales, co-founder of Wikipedia, as a way to fund Wikipedia and its sister projects through non-profit means.
As of 2015, the foundation employs over 280 people, with annual revenues in excess of US$75 million. Lila Tretikov presently leads the foundation as its executive director, though on 26 February 2016 she announced her resignation effective 31 March, as a result of the WMF's controversial Knowledge Engine project. Patricio Lorente is chairman of the board.
^ Cbrown1023. "Board of Trustees". Wikimedia Foundation. Retrieved July 22, 2015. 
^ a b c 
^ "Staff and contractors page (Wikimedia Foundation website)". Wikimedia Foundation. Retrieved August 7, 2015. 
^ Neate, Rupert (October 7, 2008). "Wikipedia founder Jimmy Wales goes bananas". The Daily Telegraph. Retrieved October 25, 2009. The encyclopedia's huge fan base became such a drain on Bomis's resources that Mr Wales, and co-founder Larry Sanger, thought of a radical new funding model – charity. 
^ 
^ Hern, Alex (February 26, 2016). "Head of Wikimedia resigns over search engine plans". The Guardian. 
^ "Online-Enzyklopädie: Chefin der Wikipedia-Stiftung tritt zurück". Spiegel Online. February 26, 2016. 
^ "Wikipedia editors force trustee to resign". Irish Examiner. January 28, 2016.""
""World Intellectual Property Organization "",,,,""The World Intellectual Property Organization (WIPO) is one of the 17 specialized agencies of the United Nations.
WIPO was created in 1967 "to encourage creative activity, to promote the protection of intellectual property throughout the world."
WIPO currently has 188 member states, administers 26 international treaties, and is headquartered in Geneva, Switzerland. The current Director-General of WIPO is Francis Gurry, who took office on October 1, 2008. 186 of the UN Members as well as the Holy See and Niue are Members of WIPO. Non-members are the states of Marshall Islands, Federated States of Micronesia, Nauru, Palau, Solomon Islands, South Sudan and Timor-Leste. Palestine has observer status.

""
""World Wide Web Consortium "",,,,""The World Wide Web Consortium (W3C) is the main international standards organization for the World Wide Web (abbreviated WWW or W3).
Founded and currently led by Tim Berners-Lee, the consortium is made up of member organizations which maintain full-time staff for the purpose of working together in the development of standards for the World Wide Web. As of 4 November 2015, the World Wide Web Consortium (W3C) has 408 members.
The W3C also engages in education and outreach, develops software and serves as an open forum for discussion about the Web.""
""A. van Wijngaarden "",,,,""Adriaan van Wijngaarden (2 November 1916 – 7 February 1987) was a Dutch mathematician and computer scientist, who is considered by many to have been the founding father of informatica (computer science) in the Netherlands.
Even though he was trained as an engineer, van Wijngaarden would emphasize and promote the mathematical aspects of computing, first in numerical analysis, then in programming languages and finally in design principles of programming languages.""
""Ada Lovelace "",,,,""Augusta Ada King, Countess of Lovelace (née Byron; 10 December 1815 – 27 November 1852) was an English mathematician and writer, chiefly known for her work on Charles Babbage's early mechanical general-purpose computer, the Analytical Engine. Her notes on the engine include what is recognised as the first algorithm intended to be carried out by a machine. As a result, she is often regarded as the first computer programmer.
Ada Lovelace was the only legitimate child of the poet George Lord Byron and his wife Anne Isabella Milbanke ("Annabella"), Lady Wentworth. All Byron's other children were born out of wedlock to other women. Byron separated from his wife a month after Ada was born and left England forever four months later, eventually dying of disease in the Greek War of Independence when Ada was eight years old. Her mother remained bitter towards Lord Byron and promoted Ada's interest in mathematics and logic in an effort to prevent her from developing what she saw as the insanity seen in her father, but Ada remained interested in him despite this (and was, upon her eventual death, buried next to him at her request). Often ill, she spent most of her childhood sick. Ada married William Lord King in 1835. King was made Earl of Lovelace in 1838, and she became Lady Lovelace.
Her educational and social exploits brought her into contact with scientists such as Andrew Crosse, Sir David Brewster, Charles Wheatstone, Michael Faraday and the author Charles Dickens, in which she used to further her education. Ada described her approach as "poetical science" and herself as an "Analyst (& Metaphysician)".
As a teenager, her mathematical talents led her to an ongoing working relationship and friendship with fellow British mathematician Charles Babbage, also known as 'the father of computers', and in particular, Babbage's work on the Analytical Engine. Lovelace first met him in June 1833, through their mutual friend, and her private tutor, Mary Somerville. Between 1842 and 1843, Ada translated an article by Italian military engineer Luigi Menabrea on the engine, which she supplemented with an elaborate set of notes, simply called Notes. These notes contain what many consider to be the first computer program—that is, an algorithm designed to be carried out by a machine. Lovelace's notes are important in the early history of computers. She also developed a vision of the capability of computers to go beyond mere calculating or number-crunching, while many others, including Babbage himself, focused only on those capabilities. Her mind-set of "poetical science" led her to ask questions about the Analytical Engine (as shown in her notes) examining how individuals and society relate to technology as a collaborative tool.
She died of uterine cancer in 1852 at the age of 36.

""
""Alan Cooper "",,,,""Alan Cooper (born June 3, 1952) is an American software designer and programmer. Widely recognized as the “Father of Visual Basic," Cooper is also known for his books About Face: The Essentials of Interaction Design and The Inmates Are Running the Asylum: Why High-Tech Products Drive Us Crazy and How to Restore the Sanity. As founder of Cooper, a leading interaction design consultancy, he created the Goal-Directed design methodology and pioneered the use of personas as practical interaction design tools to create high-tech products.""
""Alan Curtis Kay "",,,,""Alan Curtis Kay (born May 17, 1940 ) is an American computer scientist. He has been elected a Fellow of the American Academy of Arts and Sciences, the National Academy of Engineering, and the Royal Society of Arts. He is best known for his pioneering work on object-oriented programming and windowing graphical user interface design.
He is the president of the Viewpoints Research Institute, and an Adjunct Professor of Computer Science at the University of California, Los Angeles. He is also on the advisory board of TTI/Vanguard. Until mid-2005, he was a Senior Fellow at HP Labs, a Visiting Professor at Kyoto University, and an Adjunct Professor at the Massachusetts Institute of Technology (MIT). After 10 years at Xerox PARC, Kay became Atari's chief scientist for three years.
Kay is also a former professional jazz guitarist, composer, and theatrical designer, and an amateur classical pipe organist.""
""Alan Mathison Turing "",,,,""Alan Mathison Turing OBE FRS (/ˈtjʊərɪŋ/; 23 June 1912 – 7 June 1954) was a pioneering English computer scientist, mathematician, logician, cryptanalyst and theoretical biologist. He was highly influential in the development of theoretical computer science, providing a formalisation of the concepts of algorithm and computation with the Turing machine, which can be considered a model of a general purpose computer. Turing is widely considered to be the father of theoretical computer science and artificial intelligence.
During the Second World War, Turing worked for the Government Code and Cypher School (GC&CS) at Bletchley Park, Britain's codebreaking centre. For a time he led Hut 8, the section responsible for German naval cryptanalysis. He devised a number of techniques for breaking German ciphers, including improvements to the pre-war Polish bombe method and an electromechanical machine that could find settings for the Enigma machine. Turing played a pivotal role in cracking intercepted coded messages that enabled the Allies to defeat the Nazis in many crucial engagements, including the Battle of the Atlantic; it has been estimated that this work shortened the war in Europe by as many as two to four years.
After the war, he worked at the National Physical Laboratory, where he designed the ACE, among the first designs for a stored-program computer. In 1948 Turing joined Max Newman's Computing Machine Laboratory at the Victoria University of Manchester, where he helped develop the Manchester computers and became interested in mathematical biology. He wrote a paper on the chemical basis of morphogenesis, and predicted oscillating chemical reactions such as the Belousov–Zhabotinsky reaction, first observed in the 1960s.
Turing was prosecuted in 1952 for homosexual acts, when such behaviour was still a criminal act in the UK. He accepted treatment with DES (chemical castration) as an alternative to prison. Turing died in 1954, 16 days before his 42nd birthday, from cyanide poisoning. An inquest determined his death as suicide, but it has been noted that the known evidence is equally consistent with accidental poisoning. In 2009, following an Internet campaign, British Prime Minister Gordon Brown made an official public apology on behalf of the British government for "the appalling way he was treated". Queen Elizabeth II granted him a posthumous pardon in 2013.""
""Allen Kent "",,,,""Allen Kent (October 24, 1921 – May 1, 2014) was an information scientist.""
""Allen Newell "",,,,""Allen Newell (/ˈnuːəl, ˈnjuː-/; March 19, 1927 – July 19, 1992) was a researcher in computer science and cognitive psychology at the RAND Corporation and at Carnegie Mellon University’s School of Computer Science, Tepper School of Business, and Department of Psychology. He contributed to the Information Processing Language (1956) and two of the earliest AI programs, the Logic Theory Machine (1956) and the General Problem Solver (1957) (with Herbert A. Simon). He was awarded the ACM's A.M. Turing Award along with Herbert A. Simon in 1975 for their basic contributions to artificial intelligence and the psychology of human cognition.""
""Andrew S. Grove "",,,,""Andrew Stephen "Andy" Grove (born András István Gróf; 2 September 1936 – 21 March 2016) was a Hungarian-born American businessman, engineer, author and a science pioneer in the semiconductor industry. He escaped from Communist-controlled Hungary at the age of 20 and moved to the United States where he finished his education. He was one of the founders and the CEO of Intel Corporation, helping transform the company into the world's largest manufacturer of semiconductors.
As a result of his work at Intel, along with his books and professional articles, Grove had a considerable influence on the management of modern electronics manufacturing industries worldwide. He has been called the "guy who drove the growth phase" of Silicon Valley. Steve Jobs, when he was considering returning to be Apple's CEO, called Grove, who was someone he "idolized," for his personal advice. In 1997 Time magazine chose him "Man of the Year", for being "the person most responsible for the amazing growth in the power and the innovative potential of microchips.". One source notes that by his accomplishments at Intel alone, he "merits a place alongside the great business leaders of the 20th century."
In 2000, he was diagnosed with Parkinson's disease and was a contributor to several foundations that sponsor research towards a cure. He died at his home on March 21, 2016; the cause of death was unspecified.""
""Barbara Liskov "",,,,""Barbara Liskov (born November 7, 1939 as Barbara Jane Huberman) is an American computer scientist who is an institute professor at the Massachusetts Institute of Technology and Ford Professor of Engineering in its School of Engineering's electrical engineering and computer science department. She is a Turing award winner and has developed the Liskov substitution principle.""
""Barry W. Boehm "",,,,""Barry W. Boehm (born 1935) is an American software engineer, distinguished professor  of computer science, industrial and systems engineering; the TRW Professor of Software Engineering; and founding director of the Center for Systems and Software Engineering at the University of Southern California. He is known for his many contributions to the area of software engineering.""
""Bill Gates "",,,,""William Henry "Bill" Gates III (born October 28, 1955) is an American business magnate, entrepreneur, philanthropist, investor, and computer programmer. In 1975, Gates and Paul Allen co-founded Microsoft, which became the world's largest PC software company. During his career at Microsoft, Gates held the positions of chairman, CEO and chief software architect, and was the largest individual shareholder until May 2014. Gates has authored and co-authored several books.
Starting in 1987, Gates was included in the Forbes list of the world's wealthiest people and was the wealthiest from 1995 to 2007, again in 2009, and has been since 2014. Between 2009 and 2014, his wealth doubled from US$40 billion to more than US$82 billion. Between 2013 and 2014, his wealth increased by US$15 billion. Gates is currently the wealthiest person in the world with a net worth of US$77.5 billion.
Gates is one of the best-known entrepreneurs of the personal computer revolution. Gates has been criticized for his business tactics, which have been considered anti-competitive, an opinion that has in some cases been upheld by numerous court rulings. Later in his career Gates pursued a number of philanthropic endeavors, donating large amounts of money to various charitable organizations and scientific research programs through the Bill & Melinda Gates Foundation, established in 2000.
Gates stepped down as chief executive officer of Microsoft in January 2000. He remained as chairman and created the position of chief software architect for himself. In June 2006, Gates announced that he would be transitioning from full-time work at Microsoft to part-time work, and full-time work at the Bill & Melinda Gates Foundation. He gradually transferred his duties to Ray Ozzie (chief software architect) and Craig Mundie (chief research and strategy officer). Ozzie later left the company. Gates's last full-time day at Microsoft was June 27, 2008. He stepped down as chairman of Microsoft, in February 2014, taking on a new post as technology adviser to support newly appointed CEO Satya Nadella.
^ "#1 Bill Gates". Forbes. Retrieved January 2, 2016. 
^ Manes 1994, p. 11.
^ "Bill Gates (American computer programmer, businessman, and philanthropist)". Retrieved March 20, 2013. 
^ Sheridan, Patrick (May 2, 2014). "Bill Gates no longer Microsoft's biggest shareholder". CNN Money. Retrieved August 22, 2014. 
^ MSFT (Holdings), NASDAQ 
^ MSFT (Symbol), NASDAQ 
^ Wahba, Phil (September 17, 2008). "Bill Gates tops US wealth list 15 years continuously". Reuters. Archived from the original on September 16, 2012. Retrieved November 6, 2008. 
^ "William 'Bill' Gates", Forbes (profile), retrieved December 2014 
^ "The World's Billionaires". Forbes. Retrieved November 30, 2014. 
^ "Iceland". International Monetary Fund. Retrieved October 2014. 
^ "Forbes magazine profile on Bill Gates". Forbes. 2015. Retrieved May 1, 2015. 
^ Manes 1994, p. 459.
^ Lesinski 2006, p. 96.
^ Einstein, David (2000-01-13). "Gates steps down as Microsoft CEO". Forbes. Retrieved 2016-01-21. 
^ "Microsoft Chairman Gates to leave day-to-day role. - Jun. 16, 2006". money.cnn.com. Retrieved 2016-01-21. 
^ a b "Bill Gates | Development of Information and Knowledge Management". www.tlu.ee. Retrieved 2016-01-21. 
^""
""Bill Joy "",,,,""William Nelson "Bill" Joy (born November 8, 1954) is an American computer scientist. Joy co-founded Sun Microsystems in 1982 along with Vinod Khosla, Scott McNealy and Andreas von Bechtolsheim, and served as chief scientist at the company until 2003. He played an integral role in the early development of BSD UNIX while a graduate student at Berkeley, and he is the original author of the vi text editor. He also wrote the 2000 essay "Why the Future Doesn't Need Us", in which he expressed deep concerns over the development of modern technologies.""
""Bill Millard "",,,,""William "Bill" Millard was the founder of IMS Associates, makers of the IMSAI series of computers and the electronics retailer ComputerLand. He is credited as the "father" of modern computer retailing. He has also been called one of the world's most elusive tax exiles.
William H. Millard worked for IBM and, later, as the head of data processing for the city and county of San Francisco. In 1969, together with his wife, Millard started a software publisher company called Systems Dynamics, which went bankrupt in 1972.
In 1973, Millard founded IMS Associates, which is most famous for IMSAI 8080 microcomputer first shipped in late 1975. By 1977, IMSAI's product line included printers, terminals, floppy diskettes and software. To finance rapidly growing operations, IMSAI pledged 20% of its stock as convertible note in exchange for $250,000 from investment firm Marriner & Co.
In 1976, in partnership with John Martin-Musumeci, IMS launched a successful computer reseller franchise ComputerLand. In 1982, ComputerLand's sales reached over $400 million and by 1984 the venture reached over $1 billion in revenue.
Legal troubles from the failure of IMS, centered largely on a convertible note from the Marriner partnership that was later sold to a group of investors, led to a lawsuit in which Millard lost a substantial portion of his stake in ComputerLand. In the late 1980s, Millard relinquished control of ComputerLand. In 1987, he sold ComputerLand to E.M. Warburg, Pincus & Co. for about $200 million.
He and his family moved to Saipan where he removed himself from the public view.
In September 2011, after 20 years, he was found living in the Cayman Islands.""
""Bjarne Stroustrup "",,,,""Bjarne Stroustrup (Danish: [ˈbjɑːnə ˈsdʁʌʊ̯ˀsdʁɔb]; born 30 December 1950) is a Danish computer scientist, most notable for the creation and development of the widely used C++ programming language. He is a Distinguished Research Professor and holds the College of Engineering Chair in Computer Science at Texas A&M University, is a visiting professor at Columbia University, and works at Morgan Stanley as a Managing Director in New York. 

""
""Bruce Schneier "",,,,""Bruce Schneier (/ˈʃnaɪər/; born January 15, 1963) is an American cryptographer, computer security and privacy specialist, and writer. He is the author of several books on general security topics, computer security and cryptography.
Schneier is a fellow at the Berkman Center for Internet & Society at Harvard Law School, a program fellow at the New America Foundation's Open Technology Institute and the CTO of Resilient Systems. He is also a contributing writer for The Guardian news organization.""
""C. A. R. Hoare "",,,,""Sir Charles Antony Richard Hoare FRS FREng (born 11 January 1934), commonly known as Tony Hoare or C. A. R. Hoare, is a British computer scientist. He developed the sorting algorithm quicksort in 1959/1960. He also developed Hoare logic for verifying program correctness, and the formal language Communicating Sequential Processes (CSP) to specify the interactions of concurrent processes (including the dining philosophers problem) and the inspiration for the occam programming language.

""
""Charles Babbage "",,,,""Charles Babbage KH FRS (/ˈbæbɪdʒ/; 26 December 1791 – 18 October 1871) was an English polymath. A mathematician, philosopher, inventor and mechanical engineer, Babbage is best remembered for originating the concept of a programmable computer.
Considered by some to be a "father of the computer", Babbage is credited with inventing the first mechanical computer that eventually led to more complex designs. His varied work in other fields has led him to be described as "pre-eminent" among the many polymaths of his century.
Parts of Babbage's uncompleted mechanisms are on display in the London Science Museum. In 1991, a perfectly functioning difference engine was constructed from Babbage's original plans. Built to tolerances achievable in the 19th century, the success of the finished engine indicated that Babbage's machine would have worked.""
""Charles Tandy "",,,,""Charles David Tandy (15 May 1918 – 4 November 1978) was the Chairman of the Board, President, and Chief Executive Officer of the Tandy Corporation.""
""Claude Elwood Shannon "",,,,""Claude Elwood Shannon (April 30, 1916 – February 24, 2001) was an American mathematician, electrical engineer, and cryptographer known as "the father of information theory".
Shannon is famous for having founded information theory with a landmark paper that he published in 1948. He is perhaps equally well known for founding both digital computer and digital circuit design theory in 1937, when, as a 21-year-old master's degree student at the Massachusetts Institute of Technology (MIT), he wrote his thesis demonstrating that electrical applications of Boolean algebra could construct any logical, numerical relationship. Shannon contributed to the field of cryptanalysis for national defense during World War II, including his basic work on codebreaking and secure telecommunications.""
""Clive Sinclair "",,,,""Sir Clive Marles Sinclair (born 30 July 1940) is an English entrepreneur and inventor, most commonly known for his work in consumer electronics in the late 1970s and early 1980s.
After spending several years as assistant editor of Instrument Practice, Sinclair founded Sinclair Radionics in 1961, where he produced the first slim-line electronic pocket calculator in 1972 (the Sinclair Executive). Sinclair later moved into the production of home computers and produced the Sinclair ZX80, the UK's first mass-market home computer for less than £100, and later, with Sinclair Research, the ZX81 and ZX Spectrum; the latter is widely recognised for its importance in the early days of the British home computer industry.
Knighted in 1983, Sinclair formed Sinclair Vehicles and released the Sinclair C5, a battery electric vehicle that was a commercial failure. Since then Sinclair has concentrated on personal transport, including the A-bike, a folding bicycle for commuters that weighs 5.5 kilograms (12 lb) and folds down small enough to be carried on public transport.

""
""David Hilbert "",,,,""David Hilbert (German: [ˈdaːvɪt ˈhɪlbɐt]; 23 January 1862 – 14 February 1943) was a German mathematician. He is recognized as one of the most influential and universal mathematicians of the 19th and early 20th centuries. Hilbert discovered and developed a broad range of fundamental ideas in many areas, including invariant theory and the axiomatization of geometry. He also formulated the theory of Hilbert spaces, one of the foundations of functional analysis.
Hilbert adopted and warmly defended Georg Cantor's set theory and transfinite numbers. A famous example of his leadership in mathematics is his 1900 presentation of a collection of problems that set the course for much of the mathematical research of the 20th century.
Hilbert and his students contributed significantly to establishing rigor and developed important tools used in modern mathematical physics. Hilbert is known as one of the founders of proof theory and mathematical logic, as well as for being among the first to distinguish between mathematics and metamathematics.""
""Donald Ervin Knuth "",,,,""Donald Ervin Knuth (/kəˈnuːθ/ kə-NOOTH; born January 10, 1938) is an American computer scientist, mathematician, and professor emeritus at Stanford University.
He is the author of the multi-volume work The Art of Computer Programming. Knuth has been called the "father of the analysis of algorithms". He contributed to the development of the rigorous analysis of the computational complexity of algorithms and systematized formal mathematical techniques for it. In the process he also popularized the asymptotic notation. In addition to fundamental contributions in several branches of theoretical computer science, Knuth is the creator of the TeX computer typesetting system, the related METAFONT font definition language and rendering system, and the Computer Modern family of typefaces.
As a writer and scholar, Knuth created the WEB and CWEB computer programming systems designed to encourage and facilitate literate programming, and designed the MIX/MMIX instruction set architectures. As a member of the academic and scientific community, Knuth is strongly opposed to the policy of granting software patents. He has expressed his disagreement directly to both the United States Patent and Trademark Office and European Patent Organization.
^ a b Donald Knuth at the Mathematics Genealogy Project
^ 
^ "Professor Donald Knuth ForMemRS". London: Royal Society. Archived from the original on 2015-11-17. 
^ Knuth, Donald Ervin. "Frequently Asked Questions". Home page. Stanford University. Retrieved 2010-11-02. 
^ Knuth, Donald Ervin, Home page, Stanford University, retrieved 2005-03-16 
^ The Art of Computer Programming, Stanford University .
^ Karp, Richard M. (February 1986), "Combinatorics, Complexity, and Randomness", Communications of the ACM (New York, NY, USA: ACM) 29 (2): 98–109, doi:10.1145/5657.5658 
^ Knuth, Donald Ervin, Curriculum vitae, Stanford University .
^ "Professor Donald Knuth's Thinking Against Software Patents" (PDF), Notices (article) (The American Mathematical Society), March 2002 .
^ Knuth, Donald Ervin, Against software patents (PDF) (Letters)  to the patent offices in the USA and Europe.""
""Douglas R. Hofstadter "",,,,""Douglas Richard Hofstadter (born February 15, 1945) is an American professor of cognitive science whose research focuses on the sense of "I", consciousness, analogy-making, artistic creation, literary translation, and discovery in mathematics and physics. He is best known for his book Gödel, Escher, Bach: An Eternal Golden Braid, first published in 1979. It won both the Pulitzer Prize for general non-fiction and a National Book Award (at that time called The American Book Award) for Science. His 2007 book I Am a Strange Loop won the Los Angeles Times Book Prize for Science and Technology.""
""Edmund Callis Berkeley "",,,,""Edmund Callis Berkeley (February 22, 1909 – March 7, 1988) was an American computer scientist who co-founded the Association for Computing Machinery (ACM) in 1947. His 1949 book Giant Brains, or Machines That Think popularized cognitive images of early computers. He was also a social activist who worked to achieve conditions that might minimize the threat of nuclear war.

""
""Gary McGraw "",,,,""Gary McGraw is an American computer scientist, author, and researcher.""
""Georg Scheutz "",,,,""Pehr (Per) Georg Scheutz (23 September 1785 – 22 May 1873) was a 19th-century Swedish lawyer, translator, and inventor, who is now best known for his pioneering work in computer technology.""
""Gordon Bell "",,,,""C. Gordon Bell (born August 19, 1934) is an American electrical engineer and manager. An early employee of Digital Equipment Corporation (DEC) 1960–1966, Bell designed several of their PDP machines and later became Vice President of Engineering 1972-1983, overseeing the development of the VAX. Bell's later career includes entrepreneur, investor, founding Assistant Director of NSF's Computing and Information Science and Engineering Directorate 1986-1987, and researcher emeritus at Microsoft Research, 1995–2015.""
""Grace Murray Hopper "",,,,""Grace Brewster Murray Hopper (December 9, 1906 – January 1, 1992), née Grace Brewster Murray, was an American computer scientist and United States Navy Rear Admiral. She was one of the first programmers of the Harvard Mark I computer in 1944, invented the first compiler for a computer programming language, and was one of those who popularized the idea of machine-independent programming languages which led to the development of COBOL, one of the first high-level programming languages.
Owing to her accomplishments and her naval rank, she is sometimes referred to as "Amazing Grace". The U.S. Navy Arleigh Burke class guided-missile destroyer USS Hopper (DDG-70) is named for her, as is the Cray XE6 "Hopper" supercomputer at NERSC.""
""H. D. Goode "",,,,""Thomas Goode snr. JP. (May 1816 – 25 October 1882) (Goode sounds like 'good') was a South Australian merchant closely associated with the Murray River town of Goolwa.""
""Harry H. Goode "",,,,""Harry H. Goode (June 30, 1909 – October 30, 1960) was an American computer engineer and systems engineer and professor at the University of Michigan. He is known as co-author of the book Systems Engineering from 1957, which is one of the earliest significant books directly related to systems engineering.

""
""Herbert A. Simon "",,,,""Herbert Alexander Simon (June 15, 1916 – February 9, 2001), a Nobel laureate, was an American political scientist, economist, sociologist, psychologist, and computer scientist whose research ranged across the fields of cognitive psychology, cognitive science, computer science, public administration, economics, management, philosophy of science, sociology, and political science, unified by studies of decision-making. With almost a thousand highly cited publications, he was one of the most influential social scientists of the twentieth century. For many years he held the post of Richard King Mellon Professor at Carnegie Mellon University
Simon was among the founding fathers of several of today's important scientific domains, including artificial intelligence, information processing, decision-making, problem-solving, organization theory, complex systems, and computer simulation of scientific discovery.
He coined the terms bounded rationality and satisficing, and was among the earliest to analyze the architecture of complexity and to propose a preferential attachment mechanism to explain power law distributions.
He also received many top-level honors later in life. These include: becoming a fellow of the American Academy of Arts and Sciences in 1959; election to the National Academy of Sciences in 1967; APA Award for Distinguished Scientific Contributions to Psychology (1969);the ACM's Turing Award for making "basic contributions to artificial intelligence, the psychology of human cognition, and list processing" (1975); the Nobel Memorial Prize in Economics "for his pioneering research into the decision-making process within economic organizations" (1978); the National Medal of Science (1986); the APA's Award for Outstanding Lifetime Contributions to Psychology (1993); ACM fellow (1994); and IJCAI Award for Research Excellence (1995). Simon is currently, as of 2016 the most cited person in Artificial Intelligence and Cognitive Psychology on Google Scholar. 
As a testament to his interdisciplinary approach, Simon was affiliated with such varied Carnegie Mellon departments as the School of Computer Science, Tepper School of Business, departments of Philosophy, Social and Decision Sciences, and Psychology. Simon received an honorary Doctor of Political science degree from University of Pavia in 1988 and an honorary Doctor of Laws (LL.D.) degree from Harvard University in 1990.""
""Herman H. Goldstine "",,,,""Herman Heine Goldstine (September 13, 1913 – June 16, 2004) was a mathematician and computer scientist, who was one of the original developers of ENIAC, the first of the modern electronic digital computers.""
""Herman Hollerith "",,,,""Herman Hollerith (February 29, 1860 – November 17, 1929) was an American inventor who developed an electromechanical punched card tabulator to assist in summarizing information and, later, accounting. He was the founder of The Tabulating Machine Company that was consolidated in 1911 with three other companies to form the Computing-Tabulating-Recording Company, later renamed IBM. Hollerith is regarded as one of the seminal figures in the development of data processing. His invention of the punched card tabulating machine marks the beginning of the era of semiautomatic data processing systems, and his concept dominated that landscape for nearly a century.

""
""Ivar Jacobson "",,,,""Ivar Hjalmar Jacobson (born 1939) is a Swedish computer scientist and software engineer, known as major contributor to UML, Objectory, Rational Unified Process (RUP), aspect-oriented software development and Essence.""
""James Gosling "",,,,""James Arthur Gosling, OC (born May 19, 1955) is a Canadian computer scientist, best known as the father of the Java programming language.
^ 
^ James Gosling at the Mathematics Genealogy Project
^ List of publications from Microsoft Academic Search
^ James Gosling's publications indexed by the DBLP Bibliography Server at the University of Trier""
""J. Presper Eckert "",,,,""John Adam Presper "Pres" Eckert, Jr. (April 9, 1919 – June 3, 1995) was an American electrical engineer and computer pioneer. With John Mauchly he invented the first general-purpose electronic digital computer (ENIAC), presented the first course in computing topics (the Moore School Lectures), founded the Eckert–Mauchly Computer Corporation, and designed the first commercial computer in the U.S., the UNIVAC, which incorporated Eckert's invention of the mercury delay line memory.""
""J. Richard Buchi "",,,,""Julius Richard Büchi (1924–1984) was a Swiss logician and mathematician.
He received his Dr. sc. nat. in 1950 at the ETH Zürich under supervision of Paul Bernays and Ferdinand Gonseth. Shortly afterwards he went to Purdue University, Lafayette, Indiana. He and his first student Lawrence Landweber had a major influence on the development of theoretical computer science.
Together with his friend Saunders Mac Lane, a student of Paul Bernays as well, Büchi published numerous celebrated works. He invented what is now known as the Büchi automaton, a finite state automaton accepting certain collections of infinite words known as omega-regular languages. The "n squares' problem", known also as Büchi's problem, is an open problem from number theory, closely related to Hilbert's tenth problem. One of his students was Lawrence Landweber who received his doctorate at Purdue University in 1967.""
""Jim Blinn "",,,,""James F. Blinn (born 1949) is an American computer scientist who first became widely known for his work as a computer graphics expert at NASA's Jet Propulsion Laboratory (JPL), particularly his work on the pre-encounter animations for the Voyager project, his work on the Carl Sagan Cosmos documentary series and the research of the Blinn–Phong shading model.""
""John Cocke "",,,,""John Cocke (May 30, 1925 – July 16, 2002) was an American computer scientist recognized for his large contribution to computer architecture and optimizing compiler design. He is considered by many to be "the father of RISC architecture."
He attended Duke University, where he received his Bachelor's degree in Mechanical Engineering in 1946 and his Ph.D. in Mathematics in 1956. Cocke spent his entire career as an industrial researcher for IBM, from 1956 to 1992.
Perhaps the project where his innovations were most noted was in the IBM 801 minicomputer, where his realization that matching the design of the architecture's instruction set to the relatively simple instructions actually emitted by compilers could allow high performance at a low cost.
He is one of the inventors of the CYK algorithm (C for Cocke). He was also involved in the pioneering speech recognition and machine translation work at IBM in the 1970s and 1980s, and is credited by Frederick Jelinek with originating the idea of using a trigram language model for speech recognition.
Cocke was appointed IBM Fellow in 1972. He won the Eckert-Mauchly Award in 1985, ACM Turing Award in 1987, the National Medal of Technology in 1991 and the National Medal of Science in 1994, The Franklin Institute's Certificate of Merit in 1996, the Seymour Cray Computer Science and Engineering Award in 1999, and the The Benjamin Franklin Medal in 2000.
In 2002, he was made a Fellow of the Computer History Museum "for his development and implementation of reduced instruction set computer architecture and program optimization technology."
He was born in Charlotte, North Carolina and died in Valhalla, New York.

""
""John Sculley "",,,,""John Sculley III (born April 6, 1939) is an American businessman, entrepreneur and investor in high-tech startups. Sculley was vice-president (1970–1977) and president of Pepsi-Cola (1977–1983), until he became chief executive officer of Apple Inc. on April 8, 1983, a position he held until leaving in 1993. In May 1987, Sculley was named Silicon Valley's top-paid executive, with an annual salary of US$2.2 million.
Sales at Apple increased from $800 million to $8 billion under Sculley's management, although many attribute his success to the fact that Sculley joined the company just when Steve Jobs' visions and Steve Wozniak's creations had become highly lucrative. However, his stint at Apple remains controversial due to his departure from founder Steve Jobs's sales structure, particularly regarding Sculley's decision to compete with IBM in selling computers to the same types of customers. Others say that the "two clashed over management styles and priorities, Jobs focusing on future innovation and Sculley more on current product lines and profitability." But Sculley ultimately was forced to step down as Apple CEO because he was opposed to licensing Macintosh software and was talking to Goldman-Sachs about splitting Apple into two companies. When Sculley left in May 1993, Apple had $2 billion in cash and $200 million in debt.
Sculley is recognized worldwide as an expert in marketing, in part because of his early successes at PepsiCo, notably his introduction of the Pepsi Challenge, which allowed the company to gain market share from primary rival Coca Cola. He used similar marketing strategies throughout the 1980s and 1990s at Apple to mass-market Macintosh personal computers, and today he continues to speak and write about disruptive marketing strategies. Sculley is currently invested in and involved with a number of high-tech start-up companies, including 3CInteractive, XL Marketing (now rebranded as Zeta Interactive), Inflexion Point, Mobeam, OpenPeak, x10 Credit, Pivot Acquisition Corp., nextSource, and WorldMate.
Jeff Daniels portrayed Sculley in the film Steve Jobs, Matthew Modine portrayed him in the 2013 film Jobs, and Allan Royal portrayed him in the 1999 TNT film Pirates of Silicon Valley.
^ Beverage Industry - Google Books. Books.google.ca (2011-11-15). Retrieved on 2013-10-11.
^ Malone, Infinite Loop, pg 412.
^ The New York Times: "Company News – Visionary Apple Chairman Moves On"
^ The New York Times: "New Strategy Set by Apple"
^ "Sculley quic bio at end of article". "MacRumors. 2013-03-18. Retrieved 2014-04-14. 
^ Isaacson, Walter (2011). Steve Jobs. New York, NY: Simon & Schuster. p. 149. ISBN 978-1-4516-4853-9. 
^ "January 2014". Zeta Interactive. Retrieved 2014-04-14.""
""John Von Neumann "",,,,""John von Neumann (/vɒn ˈnɔɪmən/; Hungarian: Neumann János Lajos (Hungarian pronunciation: [ˈnɒjmɒn ˈjaːnoʃ ˈlɒjoʃ]; December 28, 1903 – February 8, 1957) was a Hungarian-American pure and applied mathematician, physicist, inventor, computer scientist, and polymath. He made major contributions to a number of fields, including mathematics (foundations of mathematics, functional analysis, ergodic theory, geometry, topology, and numerical analysis), physics (quantum mechanics, hydrodynamics, fluid dynamics and quantum statistical mechanics), economics (game theory), computing (Von Neumann architecture, linear programming, self-replicating machines, stochastic computing), and statistics.
He was a pioneer of the application of operator theory to quantum mechanics, in the development of functional analysis, a principal member of the Manhattan Project and the Institute for Advanced Study in Princeton (as one of the few originally appointed), and a key figure in the development of game theory and the concepts of cellular automata, the universal constructor and the digital computer. He published 150 papers in his life; 60 in pure mathematics, 20 in physics, and 60 in applied mathematics. His last work, an unfinished manuscript written while in the hospital, was later published in book form as The Computer and the Brain.
Von Neumann's mathematical analysis of the structure of self-replication preceded the discovery of the structure of DNA. In a short list of facts about his life he submitted to the National Academy of Sciences, he stated "The part of my work I consider most essential is that on quantum mechanics, which developed in Göttingen in 1926, and subsequently in Berlin in 1927–1929. Also, my work on various forms of operator theory, Berlin 1930 and Princeton 1935–1939; on the ergodic theorem, Princeton, 1931–1932."
During World War II he worked on the Manhattan Project with J. Robert Oppenheimer and Edward Teller, developing the mathematical models behind the explosive lenses used in the implosion-type nuclear weapon. After the war, he served on the General Advisory Committee of the United States Atomic Energy Commission, and later as one of its commissioners. He was a consultant to a number of organizations, including the United States Air Force, the Armed Forces Special Weapons Project, and the Lawrence Livermore National Laboratory. Along with theoretical physicist Edward Teller, mathematician Stanislaw Ulam, and others, he worked out key steps in the nuclear physics involved in thermonuclear reactions and the hydrogen bomb.
^ Dempster, M. A. H. (February 2011). "Benoit B. Mandelbrot (1924–2010): a father of Quantitative Finance" (PDF). Quantitative Finance 11 (2): 155–156. doi:10.1080/14697688.2011.552332.""
""John W. Mauchly "",,,,""John William Mauchly (August 30, 1907 – January 8, 1980) was an American physicist who, along with J. Presper Eckert, designed ENIAC, the first general purpose electronic digital computer, as well as EDVAC, BINAC and UNIVAC I, the first commercial computer made in the United States.
Together they started the first computer company, the Eckert-Mauchly Computer Corporation (EMCC), and pioneered fundamental computer concepts including the stored program, subroutines, and programming languages. Their work, as exposed in the widely read First Draft of a Report on the EDVAC (1945) and as taught in the Moore School Lectures (1946), influenced an explosion of computer development in the late 1940s all over the world.""
""Jon Kleinberg "",,,,""Jon Michael Kleinberg is an American computer scientist and the Tisch University Professor of Computer Science at Cornell University known for his work in algorithms and networks. He is a recipient of the Nevanlinna Prize by the International Mathematical Union.""
""Kent Beck "",,,,""Kent Beck (born 1961) is an American software engineer and the creator of Extreme Programming, a software development methodology which eschews rigid formal specification for a collaborative and iterative design process. Beck was one of the 17 original signatories of the Agile Manifesto, the founding document for agile software development. Extreme and Agile methods are closely associated with Test Driven Development, of which Beck is perhaps the leading proponent.
Beck pioneered software design patterns as well as the commercial application of Smalltalk. He wrote the SUnit unit testing framework for Smalltalk, which spawned the xUnit series of frameworks, notably JUnit for Java, which Beck wrote with Erich Gamma. Beck popularized CRC cards with Ward Cunningham, the inventor of the wiki.
Beck attended the University of Oregon between 1979 and 1987, receiving B.S. and M.S. degrees in computer and information science. He lives near Medford, Oregon and works at Facebook.""
""Kurt Godel "",,,,""Kurt Friedrich Gödel (/ˈkɜːrt ˈgɜːdəl/; German: [ˈkʊʁt ˈɡøːdəl]; April 28, 1906 – January 14, 1978) was an Austrian, and later American, logician, mathematician, and philosopher. Considered with Aristotle and Gottlob Frege to be one of the most significant logicians in history, Gödel made an immense impact upon scientific and philosophical thinking in the 20th century, a time when others such as Bertrand Russell, A. N. Whitehead, and David Hilbert were pioneering the use of logic and set theory to understand the foundations of mathematics.
Gödel published his two incompleteness theorems in 1931 when he was 25 years old, one year after finishing his doctorate at the University of Vienna. The first incompleteness theorem states that for any self-consistent recursive axiomatic system powerful enough to describe the arithmetic of the natural numbers (for example Peano arithmetic), there are true propositions about the naturals that cannot be proved from the axioms. To prove this theorem, Gödel developed a technique now known as Gödel numbering, which codes formal expressions as natural numbers.
He also showed that neither the axiom of choice nor the continuum hypothesis can be disproved from the accepted axioms of set theory, assuming these axioms are consistent. The former result opened the door for mathematicians to assume the axiom of choice in their proofs. He also made important contributions to proof theory by clarifying the connections between classical logic, intuitionistic logic, and modal logic.

""
""Marian Smoluchowski "",,,,""Marian Smoluchowski (Polish: [ˈmarjan smɔluˈxɔfski]; 28 May 1872 - 5 September 1917) was an ethnic Polish scientist in the Austro-Hungarian Empire. He was a pioneer of statistical physics and an avid mountaineer.""
""Mark D. Weiser "",,,,""Mark D. Weiser (July 23, 1952 – April 27, 1999) was a chief scientist at Xerox PARC in the United States. Weiser is widely considered to be the father of ubiquitous computing, a term he coined in 1988.""
""Marvin Lee Minsky "",,,,""Marvin Lee Minsky (August 9, 1927 – January 24, 2016) was an American cognitive scientist in the field of artificial intelligence (AI), co-founder of the Massachusetts Institute of Technology's AI laboratory, and author of several texts on AI and philosophy.""
""Maurice V. Wilkes "",,,,""Sir Maurice Vincent Wilkes FRS, FREng, DFBCS (26 June 1913 – 29 November 2010) was a British computer scientist credited with several important developments in computing. At the time of his death, Wilkes was an Emeritus Professor of the University of Cambridge. He received a number of distinctions: he was a knight bachelor, Distinguished Fellow of the British Computer Society, a Fellow of the Royal Academy of Engineering and a Fellow of the Royal Society.
^ Maurice Wilkes from the Association for Computing Machinery (ACM) Digital Library
^ Maurice Wilkes at the Mathematics Genealogy Project
^ Kay, Michael Howard (1976). Data independence in database management systems (PhD thesis). University of Cambridge. 
^ Leslie, Ian Malcolm (1983). Extending the local area network (PhD thesis). University of Cambridge. 
^ Wegner, Peter (1968). Programming Languages, Information Structures, and Machine Organization (PhD thesis). University College London. 
^ Wheeler, David John (1951). Automatic Computing With EDSAC (PhD thesis). University of Cambridge. (subscription required)
^ Wilkes, M. V. (1975). "Early computer developments at Cambridge: The EDSAC". Radio and Electronic Engineer 45 (7): 332. doi:10.1049/ree.1975.0063. 
^ Wilkes, Maurice (1951). "The EDSAC Computer". Proceedings of the Review of Electronic Digital Computers: 79. doi:10.1109/AFIPS.1951.13. 
^ 
^ Wilkes, M. V. (1969). "The Growth of Interest in Microprogramming: A Literature Survey". ACM Computing Surveys 1 (3): 139. doi:10.1145/356551.356553. 
^ a b Campbell-Kelly, M. (2014). "Sir Maurice Vincent Wilkes 26 June 1913 -- 29 November 2010". Biographical Memoirs of Fellows of the Royal Society. doi:10.1098/rsbm.2013.0020. 
^ Wilkes, M. V. (1996). "Computers then and now---part 2". Proceedings of the 1996 ACM 24th annual conference on Computer science - CSC '96. p. 115. doi:10.1145/228329.228342. ISBN 0897918282. 
^ Maurice V. Wilkes 2001 Fellow
^ "Father of British computing Sir Maurice Wilkes dies". BBC News. 30 November 2010. Retrieved 18 January 2011. 
^ Maurice Wilkes's publications indexed by the DBLP Bibliography Server at the University of Trier
^ List of publications from Microsoft Academic Search
^ http://ei.cs.vt.edu/~history/Wilkes.html Biography of Maurice Wilkes
^ http://www.guardian.co.uk/technology/2010/nov/30/sir-maurice-wilkes-obituary Obituary in The Guardian
^ http://www.independent.co.uk/news/obituaries/maurice-wilkes-visionary-and-pioneering-doyen-of-british-computing-2147811.html Obituary in The Independent
^ http://www.telegraph.co.uk/news/obituaries/technology-obituaries/8171435/Professor-Sir-Maurice-Wilkes.html Obituary in The Telegraph
^ Wilkes, M. V. (1985). Memoirs of a computer pioneer. Cambridge, Mass: MIT Press. ISBN 0-262-23122-0. 
^ Automatic Digital Computers. John Wiley & Sons, New York, 1956, 305 pages, QA76.W5 1956.
^ Wilkes, Maurice (1966). A short introduction to numerical analysis. Cambridge, UK: Cambridge University Press. ISBN 0-521-09412-7.""
""Peter J. Denning "",,,,""Peter James Denning (born January 6, 1942) is an American computer scientist and writer. He is best known for pioneering work in virtual memory, especially for inventing the working-set model for program behavior, which addressed thrashing in operating systems and became the reference standard for all memory management policies. He is also known for his works on principles of operating systems, operational analysis of queueing network systems, design and implementation of CSNET, the ACM digital library, codifying the great principles of computing, and most recently for the book The Innovator's Way, on innovation as a set of learnable practices.
^ "NPS vita for Peter J. Denning". 
^ The Innovator's Way: Essential Practices for Successful Innovation, MIT Press (2010)""
""Robert R. Everett "",,,,""The Defense Science Board (DSB) is a committee of civilian experts appointed to advise the U.S. Department of Defense on scientific and technical matters. It was established in 1956 on the recommendation of the second Hoover Commission.""
""Seymour Papert "",,,,""Seymour Aubrey Papert (born February 29, 1928) is an MIT mathematician, computer scientist, and educator. He is one of the pioneers of artificial intelligence, and co-inventor, with Wally Feurzeig, of the Logo programming language.""
""Steve Jobs "",,,,""Steven Paul "Steve" Jobs (/ˈdʒɒbz/; February 24, 1955 – October 5, 2011) was an American information technology entrepreneur and inventor. He was the co-founder, chairman, and chief executive officer (CEO) of Apple Inc.; CEO and largest shareholder of Pixar Animation Studios; a member of The Walt Disney Company's board of directors following its acquisition of Pixar; and founder, chairman, and CEO of NeXT Inc. Jobs is widely recognized as a pioneer of the microcomputer revolution of the 1970s, along with Apple co-founder Steve Wozniak. Shortly after his death, Jobs's official biographer, Walter Isaacson, described him as a "creative entrepreneur whose passion for perfection and ferocious drive revolutionized six industries: personal computers, animated movies, music, phones, tablet computing, and digital publishing."
Jobs's countercultural lifestyle and philosophy was a product of the time and place of his upbringing. Jobs was adopted at birth in San Francisco, and raised in a hotbed of counterculture, the San Francisco Bay Area during the 1960s. As a senior at Homestead High School in Cupertino, California, his two closest friends were the older engineering student (and Homestead High alumnus) Wozniak and his girlfriend, the artistically inclined and countercultural Homestead High junior Chrisann Brennan. Jobs and Wozniak bonded over their mutual fascination with Jobs's musical idol Bob Dylan, discussing his lyrics and collecting bootleg reel-to-reel tapes of Dylan's concerts. Jobs later dated Joan Baez who notably had a prior relationship with Dylan. Jobs briefly attended Reed College in 1972 before dropping out. He then decided to travel through India in 1974 and to study Zen Buddhism. Jobs's declassified FBI report says an acquaintance knew that Jobs used illegal drugs in college including marijuana and LSD. Jobs told a reporter once that taking LSD was “one of the two or three most important things” he did in his life.
Jobs co-founded Apple in 1976 to sell Wozniak's Apple I personal computer. The duo gained fame and wealth a year later for the Apple II, one of the first highly successful mass-produced personal computers. In 1979, after a tour of Xerox PARC, Jobs saw the commercial potential of the Xerox Alto, which was mouse-driven and had a graphical user interface (GUI). This led to development of the unsuccessful Apple Lisa in 1983, followed by the very successful Macintosh in 1984. In addition to being the first mass-produced computer with a GUI, the Macintosh instigated the sudden rise of the desktop publishing industry in 1985 with the addition of the Apple LaserWriter, the first laser printer to feature vector graphics. Following a long power struggle, Jobs was forced out of Apple in 1985.
After leaving Apple, Jobs took a few of its members with him to found NeXT, a computer platform development company specializing in state-of-the-art computers for higher-education and business markets. In addition, Jobs helped to initiate the development of the visual effects industry when he funded the spinout of the computer graphics division of George Lucas's company Lucasfilm in 1986. The new company, Pixar, would eventually produce the first fully computer-animated film, Toy Story—an event made possible in part because of Jobs's financial support.
In 1997, Apple purchased NeXT, allowing Jobs to become the former's CEO once again. He would return the company, which was on the verge of bankruptcy, back to profitability. Beginning in 1997 with the "Think different" advertising campaign, Jobs worked closely with designer Jonathan Ive to develop a line of products that would have larger cultural ramifications: the iMac, iTunes, Apple Stores, the iPod, the iTunes Store, the iPhone, the App Store, and the iPad. Mac OS was also revamped into Mac OS X, based on NeXT's NeXTSTEP platform.
Jobs was diagnosed with a pancreatic neuroendocrine tumor in 2003 and died of respiratory arrest related to the tumor on October 5, 2011.
^ "The Walt Disney Company and Affiliated Companies –  board of directors". The Walt Disney Company. Retrieved October 2, 2009. 
^ a b Isaacson, Walter (2011). Steve Jobs. Simon & Schuster. 
^ D'Onfro, Jillian (March 22, 2015). "Why execs from other companies wanted to meet with Steve Jobs on Fridays". Business Insider. Retrieved 2015-09-11. 
^ Foremski, Tom. "The Steve Jobs way: Exploring the intersection of psychedelics and technology | ZDNet". ZDNet. Retrieved 2016-02-24. 
^ a b "The Steve Jobs Nobody Knew". Rolling Stone. Retrieved 2016-02-24. 
^ a b "New Steve Jobs Bio Reveals Details of His Relationships With Bob Dylan, Bono". Rolling Stone. Retrieved 2016-02-24. 
^ "Here's How Zen Meditation Changed Steve Jobs' Life And Sparked A Design Revolution". Business Insider. Retrieved 2016-02-24. 
^ Tsukayama, Hayley (2012-02-09). "Steve Jobs’s unflattering FBI file mentions drug use, 2.65 GPA". The Washington Post. ISSN 0190-8286. Retrieved 2016-02-26. 
^ Palmer, Brian (2011-10-06). "Did Dropping Acid Make Steve Jobs More Creative?". Slate. ISSN 1091-2339. Retrieved 2016-02-24. 
^ Swaine, Michael and Paul Frieberger. Fire in the Valley: The Birth and Death of the Personal Computer, 3rd Edition, Dallas: Pragmatic Bookshelf, 2014: 310
^ Smith, Alvy Ray. "Pixar Founding Documents". Alvy Ray Smith Homepage. Archived from the original on April 27, 2005. Retrieved January 11, 2011.""
""Thomas J. Watson "",,,,""Thomas John Watson Sr. (February 17, 1874 – June 19, 1956) was an American businessman. He served as the chairman and CEO of International Business Machines (IBM) and oversaw the company's growth into an international force from 1914 to 1956. Watson developed IBM's management style and corporate culture from John Henry Patterson's training at NCR. He turned the company into a highly-effective selling organization, based largely on punched card tabulating machines. A leading self-made industrialist, he was one of the richest men of his time and was called the world's greatest salesman when he died in 1956.""
""Thomas Watson, Jr. "",,,,""Thomas John Watson Jr. (January 14, 1914 – December 31, 1993) was an American businessman, political figure, and philanthropist. He was the 2nd president of IBM (1952–1971), the 11th national president of the Boy Scouts of America (1964–1968), and the 16th United States Ambassador to the Soviet Union (1979–1981). He received many honors during his lifetime, including being awarded the Presidential Medal of Freedom by Lyndon B. Johnson in 1964. Watson was called "the greatest capitalist in history" and one of "100 most influential people of the 20th century".""
""Vannevar Bush "",,,,""Vannevar Bush (/væˈniːvɑːr/ van-NEE-var; March 11, 1890 – June 28, 1974) was an American engineer, inventor and science administrator, who during World War II headed the U.S. Office of Scientific Research and Development (OSRD), through which almost all wartime military R&D was carried out, including initiation and early administration of the Manhattan Project. He is also known in engineering for his work on analog computers, for founding Raytheon, and for the memex, a hypothetical adjustable microfilm viewer with a structure analogous to that of hypertext. In 1945, Bush published the essay "As We May Think" in which he predicted that "wholly new forms of encyclopedias will appear, ready made with a mesh of associative trails running through them, ready to be dropped into the memex and there amplified". The memex influenced generations of computer scientists, who drew inspiration from its vision of the future. He was chiefly responsible for the movement that led to the creation of the National Science Foundation.
For his master's thesis, Bush invented and patented a "profile tracer", a mapping device for assisting surveyors. It was the first of a string of inventions. He joined the Department of Electrical Engineering at Massachusetts Institute of Technology (MIT) in 1919, and founded the company now known as Raytheon in 1922. Starting in 1927, Bush constructed a differential analyzer, an analog computer with some digital components that could solve differential equations with as many as 18 independent variables. An offshoot of the work at MIT by Bush and others was the beginning of digital circuit design theory. Bush became vice president of MIT and dean of the MIT School of Engineering in 1932, and president of the Carnegie Institution of Washington in 1938.
Bush was appointed to the National Advisory Committee for Aeronautics (NACA) in 1938, and soon became its chairman. As chairman of the National Defense Research Committee (NDRC), and later director of OSRD, Bush coordinated the activities of some six thousand leading American scientists in the application of science to warfare. Bush was a well-known policymaker and public intellectual during World War II, when he was in effect the first presidential science advisor. As head of NDRC and OSRD, he initiated the Manhattan Project, and ensured that it received top priority from the highest levels of government. In Science, The Endless Frontier, his 1945 report to the President of the United States, Bush called for an expansion of government support for science, and he pressed for the creation of the National Science Foundation.""
""ActionScript "",,,,""ActionScript is an object-oriented programming language originally developed by Macromedia Inc. (since merged into Adobe Systems). It is a derivation of HyperTalk, the scripting language for HyperCard. It is now a dialect of ECMAScript (meaning it is a superset of the syntax and semantics of the language more widely known as JavaScript), though it originally arose as a sibling, both being influenced by HyperTalk.
ActionScript is used primarily for the development of websites and software targeting the Adobe Flash Player platform, used on Web pages in the form of embedded SWF files.
ActionScript 3 is also used with Adobe AIR system for the development of desktop and mobile applications. The language itself is open-source in that its specification is offered free of charge and both an open source compiler (as part of Apache Flex) and open source virtual machine (Mozilla Tamarin) are available.
ActionScript is also used with Scaleform GFx for the development of 3D video game user interfaces and HUDs.""
""ActiveX "",,,,""ActiveX is a software framework created by Microsoft that adapts its earlier Component Object Model (COM) and Object Linking and Embedding (OLE) technologies for content downloaded from a network, particularly in the context of the World Wide Web. It was introduced in 1996 and is commonly used in its Windows operating system. In principle it is not dependent on Microsoft Windows, but in practice, most ActiveX controls require being run on Microsoft Windows. Most also require the client to be running on Intel x86 hardware, because they contain compiled code.""
""Ada language "",,,,""Ada is a structured, statically typed, imperative, wide-spectrum, and object-oriented high-level computer programming language, extended from Pascal and other languages. It has built-in language support for design-by-contract, extremely strong typing, explicit concurrency, offering tasks, synchronous message passing, protected objects, and non-determinism. Ada improves code safety and maintainability by using the compiler to find errors in favor of runtime errors. Ada is an international standard; the current version (known as Ada 2012) is defined by ISO/IEC 8652:2012.
Ada was originally designed by a team led by Jean Ichbiah of CII Honeywell Bull under contract to the United States Department of Defense (DoD) from 1977 to 1983 to supersede the hundreds of programming languages then used by the DoD. Ada was named after Ada Lovelace (1815–1852), who is credited with being the first computer programmer.""
""ANSI C "",,,,""ANSI C, ISO C and Standard C refer to the successive standards for the C programming language published by the American National Standards Institute (ANSI) and the International Organization for Standardization (ISO). Historically, the names referred specifically to the original and best-supported version of the standard (known as C89 or C90). Software developers writing in C are encouraged to conform to the standards, as doing so aids portability between compilers.""
""Ajax "",,,,""Ajax (also AJAX; /ˈeɪdʒæks/; short for asynchronous JavaScript and XML) is a set of web development techniques using many web technologies on the client-side to create asynchronous Web applications. With Ajax, web applications can send data to and retrieve from a server asynchronously (in the background) without interfering with the display and behavior of the existing page. By decoupling the data interchange layer from the presentation layer, Ajax allows for web pages, and by extension web applications, to change content dynamically without the need to reload the entire page. Despite the name, the use of XML is not required (JSON is often used in the AJAJ variant), and the requests do not need to be asynchronous.
Ajax is not a technology, but a group of technologies. HTML and CSS can be used in combination to mark up and style information. The DOM is accessed with JavaScript to dynamically display – and allow the user to interact with – the information presented. JavaScript and the XMLHttpRequest object provide a method for exchanging data asynchronously between browser and server to avoid full page reloads.""
""AppleScript "",,,,""AppleScript is a scripting language created by Apple Inc. and built into Macintosh operating systems since System 7. The term "AppleScript" may refer to the scripting system itself, or to an individual script written in the AppleScript language.
AppleScript is primarily a scripting language developed by Apple to do Inter-Application Communication (IAC) using AppleEvents. AppleScript is related to, but different from, AppleEvents. AppleEvents is designed to exchange data between and control other applications in order to automate repetitive tasks. AppleScript has some limited processing abilities of its own, in addition to sending and receiving AppleEvents to applications. AppleScript can do basic calculation, and can do intricate text processing – and it is extensible, allowing the addition of scripting additions which add new functions to the language itself. Mainly, however, AppleScript relies on the functionality of applications and processes to handle complex tasks. As a structured command language, AppleScript can be compared to Unix shells; the Microsoft Windows Script Host; or IBM REXX in its functionality, but it is unique from all three. Essential to its functionality is the fact that Macintosh applications publish "dictionaries" of addressable objects and operations.
AppleScript has some elements of object-oriented programming, particularly in the construction of script objects, and natural language programming tendencies in its syntax, but does not strictly conform to either category.""
""AutoCAD "",,,,""AutoCAD is a commercial software application for 2D and 3D computer-aided design (CAD) and drafting — available since 1982 as a desktop application and since 2010 as a mobile, web- and cloud-based app marketed as AutoCAD 360.
Developed and marketed by Autodesk, AutoCAD was first released in December 1982, running on microcomputers with internal graphics controllers. Prior to the introduction of AutoCAD, most commercial CAD programs ran on mainframe computers or minicomputers, with each CAD operator (user) working at a separate graphics terminal.
AutoCAD is used across a wide range of industries, by architects, project managers, engineers, graphic designers, and other professionals. It is supported by 750 training centers worldwide as of 1994.
As Autodesk's flagship product, by March 1986 AutoCAD had become the most ubiquitous CAD program worldwide.""
""BASIC "",,,,""BASIC (an acronym for Beginner's All-purpose Symbolic Instruction Code) is a family of general-purpose, high-level programming languages whose design philosophy emphasizes ease of use.
In 1964, John G. Kemeny and Thomas E. Kurtz designed the original BASIC language at Dartmouth College in New Hampshire. They wanted to enable students in fields other than science and mathematics to use computers. At the time, nearly all use of computers required writing custom software, which was something only scientists and mathematicians tended to learn.
Versions of BASIC became widespread on microcomputers in the mid-1970s and 1980s. Microcomputers usually shipped with BASIC, often in the machine's firmware. Having an easy-to-learn language on these early personal computers allowed small business owners, professionals, hobbyists, and consultants to develop custom software on computers they could afford.
BASIC remains popular in many dialects and in new languages influenced by BASIC, such as Microsoft's Visual Basic. In 2006, 59% of developers for the .NET Framework used Visual Basic .NET as their only programming language.

""
""Bluetooth "",,,,""Bluetooth is a wireless technology standard for exchanging data over short distances (using short-wavelength UHF radio waves in the ISM band from 2.4 to 2.485 GHz) from fixed and mobile devices, and building personal area networks (PANs). Invented by telecom vendor Ericsson in 1994, it was originally conceived as a wireless alternative to RS-232 data cables. It can connect several devices, overcoming problems of synchronization.
Bluetooth is managed by the Bluetooth Special Interest Group (SIG), which has more than 25,000 member companies in the areas of telecommunication, computing, networking, and consumer electronics. The IEEE standardized Bluetooth as IEEE 802.15.1, but no longer maintains the standard. The Bluetooth SIG oversees development of the specification, manages the qualification program, and protects the trademarks. A manufacturer must make a device meet Bluetooth SIG standards to market it as a Bluetooth device. A network of patents apply to the technology, which are licensed to individual qualifying devices.""
""Business Process Execution Language "",,,,""The Web Services Business Process Execution Language (WS-BPEL), commonly known as BPEL (Business Process Execution Language), is an OASIS standard executable language for specifying actions within business processes with web services. Processes in BPEL export and import information by using web service interfaces exclusively.

""
""CDMA systems "",,,,""Qualcomm code-excited linear prediction (QCELP), also known as Qualcomm PureVoice, is a speech codec developed in 1994 by Qualcomm to increase the speech quality of the IS-96A codec earlier used in CDMA networks. It was later replaced with EVRC since it provides better speech quality with fewer bits. The two versions, QCELP8 and QCELP13, operate at 8 and 13 kilobits per second (Kbit/s) respectively.
In CDMA systems, a QCELP vocoder converts a sound signal into a signal transmissible within a circuit. In wired systems, voice signals are generally sampled at 8 kHz (that is, 8,000 sample values per second) and then encoded by 8-bit quantization for each sample value. Such a system transmits at 64 kbit/s, an expensive rate in a wireless system. A QCELP vocoder with variable rates can reduce the rate enough to fit a wireless system by coding the information more efficiently. In particular, it can change its own coding rates based on the speaker's volume or pitch; a louder or higher-pitched voice requires a higher rate.""
""Cell Broadband Engine "",,,,""Cell is a multi-core microprocessor microarchitecture that combines a general-purpose Power Architecture core of modest performance with streamlined coprocessing elements which greatly accelerate multimedia and vector processing applications, as well as many other forms of dedicated computation.
It was developed by Sony, Toshiba, and IBM, an alliance known as "STI". The architectural design and first implementation were carried out at the STI Design Center in Austin, Texas over a four-year period beginning March 2001 on a budget reported by Sony as approaching US$400 million. Cell is shorthand for Cell Broadband Engine Architecture, commonly abbreviated CBEA in full or Cell BE in part.
The first major commercial application of Cell was in Sony's PlayStation 3 game console. Mercury Computer Systems has a dual Cell server, a dual Cell blade configuration, a rugged computer, and a PCI Express accelerator board available in different stages of production. Toshiba had announced plans to incorporate Cell in high definition television sets, but seems to have abandoned the idea. Exotic features such as the XDR memory subsystem and coherent Element Interconnect Bus (EIB) interconnect appear to position Cell for future applications in the supercomputing space to exploit the Cell processor's prowess in floating point kernels.
The Cell architecture includes a memory coherence architecture that emphasizes power efficiency, prioritizes bandwidth over low latency, and favors peak computational throughput over simplicity of program code. For these reasons, Cell is widely regarded as a challenging environment for software development. IBM provides a Linux-based development platform to help developers program for Cell chips. The architecture will not be widely used unless it is adopted by the software development community. However, Cell's strengths may make it useful for scientific computing regardless of its mainstream success.""
""CMOS "",,,,""Complementary metal–oxide–semiconductor (CMOS) /ˈsiːmɒs/ is a technology for constructing integrated circuits. CMOS technology is used in microprocessors, microcontrollers, static RAM, and other digital logic circuits. CMOS technology is also used for several analog circuits such as image sensors (CMOS sensor), data converters, and highly integrated transceivers for many types of communication. In 1963, while working for Fairchild Semiconductor, Frank Wanlass patented CMOS (US patent 3,356,858).
CMOS is also sometimes referred to as complementary-symmetry metal–oxide–semiconductor (or COS-MOS). The words "complementary-symmetry" refer to the fact that the typical design style with CMOS uses complementary and symmetrical pairs of p-type and n-type metal oxide semiconductor field effect transistors (MOSFETs) for logic functions.
Two important characteristics of CMOS devices are high noise immunity and low static power consumption. Since one transistor of the pair is always off, the series combination draws significant power only momentarily during switching between on and off states. Consequently, CMOS devices do not produce as much waste heat as other forms of logic, for example transistor–transistor logic (TTL) or NMOS logic, which normally have some standing current even when not changing state. CMOS also allows a high density of logic functions on a chip. It was primarily for this reason that CMOS became the most used technology to be implemented in VLSI chips.
The phrase "metal–oxide–semiconductor" is a reference to the physical structure of certain field-effect transistors, having a metal gate electrode placed on top of an oxide insulator, which in turn is on top of a semiconductor material. Aluminium was once used but now the material is polysilicon. Other metal gates have made a comeback with the advent of high-k dielectric materials in the CMOS process, as announced by IBM and Intel for the 45 nanometer node and beyond.""
""CORBA "",,,,""The Common Object Request Broker Architecture (CORBA) is a standard defined by the Object Management Group (OMG) designed to facilitate the communication of systems that are deployed on diverse platforms. CORBA enables collaboration between systems on different operating systems, programming languages, and computing hardware. CORBA has many of the same design goals as object-oriented programming: encapsulation and reuse. CORBA uses an object-oriented model although the systems that use CORBA do not have to be object-oriented. CORBA is an example of the distributed object paradigm.""
""Datalog "",,,,""Datalog is a declarative logic programming language that syntactically is a subset of Prolog. It is often used as a query language for deductive databases. In recent years, Datalog has found new application in data integration, information extraction, networking, program analysis, security, and cloud computing.
Its origins date back to the beginning of logic programming, but it became prominent as a separate area around 1977 when Hervé Gallaire and Jack Minker organized a workshop on logic and databases. David Maier is credited with coining the term Datalog.
^ Huang, Green, and Loo, "Datalog and Emerging applications", SIGMOD 2011 (PDF), UC Davis .
^ Gallaire, Hervé; Minker, John ‘Jack’, eds. (1978), "Logic and Data Bases, Symposium on Logic and Data Bases, Centre d'études et de recherches de Toulouse, 1977", Advances in Data Base Theory, New York: Plenum Press, ISBN 0-306-40060-X .
^ Abiteboul, Serge; Hull, Richard; Vianu, Victor, Foundations of databases, p. 305 .""
""DNS (Domain Name System) "",,,,""Zeroshell is a small Linux distribution for servers and embedded systems which aims to provide network services. As its name implies, its administration relies on a web-based graphical interface. There is no need to use a shell to administer and configure it. Zeroshell is available as Live CD and CompactFlash images, and VMware virtual machines.""
""DOS "",,,,""DOS /dɒs/, short for disk operating system, is an acronym for several computer operating systems that are operated by using the command line.
MS-DOS dominated the IBM PC compatible market between 1981 and 1995, or until about 2000 including the partially MS-DOS-based Microsoft Windows (95, 98, and Millennium Edition). "DOS" is used to describe the family of several very similar command-line systems, including MS-DOS, PC DOS, DR-DOS, FreeDOS, ROM-DOS, and PTS-DOS.
In spite of the common usage, none of these systems were simply named "DOS" (a name given only to an unrelated IBM mainframe operating system in the 1960s). A number of unrelated, non-x86 microcomputer disk operating systems had "DOS" in their names, and are often referred to simply as "DOS" when discussing machines that use them (e.g. AmigaDOS, AMSDOS, ANDOS, Apple DOS, Atari DOS, Commodore DOS, CSI-DOS, ProDOS, and TRSDOS). While providing many of the same operating system functions for their respective computer systems, programs running under any one of these operating systems would not run under others.

""
""Dreamweaver "",,,,""Adobe Dreamweaver is a proprietary web development tool developed by Adobe Systems. Dreamweaver was created by Macromedia in 1997, and was maintained by them until Macromedia was acquired by Adobe Systems in 2005.
Adobe Dreamweaver is available for OS X and for Windows.
Following Adobe's acquisition of the Macromedia product suite, releases of Dreamweaver subsequent to version 8.0 have been more compliant with W3C standards. Recent versions have improved support for Web technologies such as CSS, JavaScript, and various server-side scripting languages and frameworks including ASP (ASP JavaScript, ASP VBScript, ASP.NET C#, ASP.NET VB), ColdFusion, Scriptlet, and PHP.""
""DSL"",,,,""Digital subscriber line (DSL; originally digital subscriber loop) is a family of technologies that are used to transmit digital data over telephone lines. In telecommunications marketing, the term DSL is widely understood to mean asymmetric digital subscriber line (ADSL), the most commonly installed DSL technology, for Internet access. DSL service can be delivered simultaneously with wired telephone service on the same telephone line. This is possible because DSL uses higher frequency bands for data. On the customer premises, a DSL filter on each non-DSL outlet blocks any high-frequency interference to enable simultaneous use of the voice and DSL services.
The bit rate of consumer DSL services typically ranges from 256 kbit/s to over 100 Mbit/s in the direction to the customer (downstream), depending on DSL technology, line conditions, and service-level implementation. Bit rates of 1 Gbit/s have been reached in trials, but most homes are likely to be limited to 500-800 Mbit/s. In ADSL, the data throughput in the upstream direction (the direction to the service provider) is lower, hence the designation of asymmetric service. In symmetric digital subscriber line (SDSL) services, the downstream and upstream data rates are equal. Researchers at Bell Labs have reached speeds of 10 Gbit/s, while delivering 1 Gbit/s symmetrical broadband access services using traditional copper telephone lines. These higher speeds are lab results, however. A 2012 survey found that "DSL continues to be the dominant technology for broadband access" with 364.1 million subscribers worldwide.
^ The Next Generation of DSL Can Pump 1Gbps Through Copper Phone Lines, Gizmodo, 18 December 2013, Andrew Tarantola
^ Alcatel-Lucent sets broadband speed record using copper, Phys.org, 10 July 2014, Nancy Owano
^ Researchers get record broadband speeds out of old-school copper wire, Engadget, 10 July 2014, Matt Brian
^ "BT G.fast". Gfastnews.com. 2014-06-21. Retrieved 2014-11-21. 
^ http://point-topic.com/wp-content/uploads/2013/02/Sample-Report-Global-Broadband-Statistics-Q2-2012.pdf""
""DVDs "",,,,""DVD ( "digital versatile disc" or "digital video disc") is a digital optical disc storage format invented and developed by Philips, Sony, Toshiba, and Panasonic in 1995. The medium can store any kind of digital data and is widely used for software and other computer files as well as video programs watched using DVD players. DVDs offer higher storage capacity than compact discs while having the same dimensions.
Pre-recorded DVDs are mass-produced using molding machines that physically stamp data onto the DVD. Such discs are a form of DVD-ROMs, because data can only be read and not written or erased. Blank recordable DVD discs (DVD-R and DVD+R) can be recorded once using a DVD recorder and then function as a DVD-ROM. Rewritable DVDs (DVD-RW, DVD+RW, and DVD-RAM) can be recorded and erased many times.
DVDs are used in DVD-Video consumer digital video format and in DVD-Audio consumer digital audio format as well as for authoring DVD discs written in a special AVCHD format to hold high definition material (often in conjunction with AVCHD format camcorders). DVDs containing other types of information may be referred to as DVD data discs.""
""Eclipse "",,,,""An eclipse is an astronomical event that occurs when an astronomical object is temporarily obscured, either by passing into the shadow of another body or by having another body pass between it and the viewer. An eclipse is a type of syzygy.
The term eclipse is most often used to describe either a solar eclipse, when the Moon's shadow crosses the Earth's surface, or a lunar eclipse, when the Moon moves into the Earth's shadow. However, it can also refer to such events beyond the Earth–Moon system: for example, a planet moving into the shadow cast by one of its moons, a moon passing into the shadow cast by its host planet, or a moon passing into the shadow of another moon. A binary star system can also produce eclipses if the plane of the orbit of its constituent stars intersects the observer's position.""
""Ethernet "",,,,""Ethernet /ˈiːθərnɛt/ is a family of computer networking technologies commonly used in local area networks (LANs) and metropolitan area networks (MANs). It was commercially introduced in 1980 and first standardized in 1983 as IEEE 802.3, and has since been refined to support higher bit rates and longer link distances. Over time, Ethernet has largely replaced competing wired LAN technologies such as token ring, FDDI and ARCNET.
The original 10BASE5 Ethernet uses coaxial cable as a shared medium, while the newer Ethernet variants use twisted pair and fiber optic links in conjunction with hubs or switches. Over the course of its history, Ethernet data transfer rates have been increased from the original 2.94 megabits per second (Mbit/s) to the latest 100 gigabits per second (Gbit/s), with 400 Gbit/s expected by late 2017. The Ethernet standards comprise several wiring and signaling variants of the OSI physical layer in use with Ethernet.
Systems communicating over Ethernet divide a stream of data into shorter pieces called frames. Each frame contains source and destination addresses, and error-checking data so that damaged frames can be detected and discarded; most often, higher-layer protocols trigger retransmission of lost frames. As per the OSI model, Ethernet provides services up to and including the data link layer.
Since its commercial release, Ethernet has retained a good degree of backward compatibility. Features such as the 48-bit MAC address and Ethernet frame format have influenced other networking protocols. The primary alternative for some uses of contemporary LANs is Wi-Fi, a wireless protocol standardized as IEEE 802.11.
^ Ralph Santitoro (2003). "Metro Ethernet Services –  A Technical Overview" (PDF). mef.net. Retrieved 2016-01-09. 
^ "IEEE 802.3 'Standard for Ethernet' Marks 30 Years of Innovation and Global Market Growth" (Press release). IEEE. June 24, 2013. Retrieved January 11, 2014. 
^ Xerox (August 1976). "Alto: A Personal Computer System Hardware Manual" (PDF). Xerox. p. 37. Retrieved 25 August 2015. 
^ "Adopted Timeline" (PDF). IEEE 802.3bs Task Force. 2014-05-19. Retrieved 2015-02-24. 
^ Charles M. Kozierok (2005-09-20). "Data Link Layer (Layer 2)". tcpipguide.com. Retrieved 2016-01-09. 
^ Joe Jensen (2007-10-26). "802.11 g: Pros & Cons of a Wireless Network in a Business Environment". networkbits.net. Retrieved 2016-01-09.""
""Excel "",,,,""ExCeL London (Exhibition Centre London) (often referred to as the ExCeL Exhibition Centre) is an exhibitions and international convention centre in the London Borough of Newham. It is located on a 100-acre (0.40 km2) site on the northern quay of the Royal Victoria Dock in London Docklands, between Canary Wharf and London City Airport. The ICC Auditorium also hosted Miss World 2014 on December 14, 2014.""
""CD-ROMs "",,,,""A CD-ROM /ˌsiːˌdiːˈrɒm/ is a pre-pressed optical compact disc which contains data. The name is an acronym which stands for "Compact Disc Read-Only Memory". Computers can read CD-ROMs, but cannot write to CD-ROMs which are not writable or erasable.
Until the mid-2000s, CD-ROMs were popularly used to distribute software for computers and video game consoles. Some CDs, called enhanced CDs, hold both computer data and audio with the latter capable of being played on a CD player, while data (such as software or digital video) is only usable on a computer (such as ISO 9660 format PC CD-ROMs).
The Yellow Book is the technical standard that defines the format of CD-ROMs. One of a set of color-bound books that contain the technical specifications for all CD formats, the Yellow Book, created by Sony and Philips in 1988, was the first extension of Compact Disc Digital Audio. It adapted the format to hold any form of digital data.""
""Facebook "",,,,""Facebook is a corporation and online social networking service headquartered in Menlo Park, California, in the United States. Its website was launched on February 4, 2004, by Mark Zuckerberg with his Harvard College roommates and fellow students Eduardo Saverin, Andrew McCollum, Dustin Moskovitz and Chris Hughes. The founders had initially limited the website's membership to Harvard students, but later expanded it to colleges in the Boston area, the Ivy League, and Stanford University. It gradually added support for students at various other universities and later to high-school students. Since 2006, anyone who is at least 13 years old was allowed to become a registered user of the website, though the age requirement may be higher depending on applicable local laws. Its name comes from the face book directories often given to American university students.
After registering to use the site, users can create a user profile, add other users as "friends", exchange messages, post status updates and photos, share videos, use various apps and receive notifications when others update their profiles. Additionally, users may join common-interest user groups, organized by workplace, school or college, or other characteristics, and categorize their friends into lists such as "People From Work" or "Close Friends". Also users can complain or block unpleasant people. Facebook had over 1.59 billion monthly active users as of August 2015. Because of the large volume of data users submit to the service, Facebook has come under scrutiny for their privacy policies. Facebook, Inc. held its initial public offering in February 2012 and began selling stock to the public three months later, reaching an original peak market capitalization of $104 billion. On July 13, 2015, Facebook became the fastest company in the Standard & Poor's 500 Index to reach a market cap of $250 billion. Following its Q3 earnings call in 2015, Facebook's market cap soared past $300 billion.""
""Internet Explorer "",,,,""Internet Explorer (formerly Microsoft Internet Explorer and Windows Internet Explorer, commonly abbreviated IE or MSIE) is a series of graphical web browsers developed by Microsoft and included as part of the Microsoft Windows line of operating systems, starting in 1995. It was first released as part of the add-on package Plus! for Windows 95 that year. Later versions were available as free downloads, or in service packs, and included in the Original Equipment Manufacturer (OEM) service releases of Windows 95 and later versions of Windows.
Internet Explorer was one of the most widely used web browsers, attaining a peak of about 95% usage share during 2002 and 2003. This came after it managed to win the first browser war against Netscape, which was the dominant browser in the 1990s. Its usage share has since declined with the launch of Firefox (2004) and Google Chrome (2008), and with the growing popularity of operating systems such as OS X, Linux, iOS and Android that do not run Internet Explorer. Estimates for Internet Explorer's overall market share range from 17.11% to 51.59% or by StatCounter's numbers ranked 3rd, just after Firefox (or even as low as 10.3% when counting all platforms, then after Safari), as of September 2015 (browser market share is notoriously difficult to calculate). Microsoft spent over US$100 million per year on Internet Explorer in the late 1990s, with over 1,000 people working on it by 1999.
Versions of Internet Explorer for other operating systems have also been produced, including an Xbox 360 version called Internet Explorer for Xbox and an embedded OEM version called Pocket Internet Explorer, later rebranded Internet Explorer Mobile made for Windows Phone, Windows CE, and previously, based on Internet Explorer 7 for Windows Mobile. It remains in development alongside the desktop versions. Internet Explorer for Mac and Internet Explorer for UNIX (Solaris and HP-UX) have been discontinued.
On March 17, 2015, Microsoft announced that Microsoft Edge will replace Internet Explorer as the default browser on its Windows 10 devices. This effectively makes Internet Explorer 11 the last release. Internet Explorer will, however, remain on some versions of Windows 10 primarily for enterprise purposes. Starting January 12, 2016, only the most recent version of Internet Explorer on each operating system is supported. Support varies based on the operating system's technical capabilities and its support lifecycle.
The browser has been scrutinized throughout its development for use of third-party technology (such as the source code of Spyglass Mosaic, used without royalty in early versions) and security and privacy vulnerabilities, and the United States and the European Union have alleged that integration of Internet Explorer with Windows has been to the detriment of fair browser competition.""
""Firefox "",,,,""Mozilla Firefox (or simply Firefox) is a free and open-source web browser developed by the Mozilla Foundation and its subsidiary, the Mozilla Corporation. Firefox is available for Windows, OS X and Linux operating systems, with its mobile versions available for Android, and Firefox OS; where all of these versions use the Gecko layout engine to render web pages, which implements current and anticipated web standards, but an additional version released in late 2015 – Firefox for iOS has also been made available – that doesn't use Gecko.
Firefox was created in 2002, under the name "Phoenix" by the Mozilla community members who wanted a standalone browser rather than the Mozilla Application Suite bundle. Even during its beta phase, Firefox proved to be popular by its testers and was praised for its speed, security and add-ons compared to Microsoft's then-dominant Internet Explorer 6. Firefox was released in November 2004, and was highly successful with 60 million downloads within nine months, which was the first time that Internet Explorer's dominance was challenged. Firefox is considered the spiritual successor of Netscape Navigator, as the Mozilla community was created by Netscape in 1998 before their acquisition by AOL.
As of January 2016, Firefox has between 9% and 16% of worldwide usage as a "desktop" browser, making it the second most popular web browser. Firefox is the most popular browser with Samoa, Germany, Eritrea and Cuba at 61.05%, 38.36%, 79.39% and 85.93% of the market share, respectively. It is also the most popular desktop browser in many other African, and a few Asian countries. According to Mozilla, as of December 2014 there were half a billion Firefox users around the world. With Internet Explorer declining, Firefox reached second place in February 2016, as a desktop browser.""
""Flickr "",,,,""Flickr (pronounced "flicker") is an image hosting and video hosting website and web services suite that was created by Ludicorp in 2004 and acquired by Yahoo in 2005. In addition to being a popular website for users to share and embed personal photographs, and effectively an online community, the service is widely used by photo researchers and by bloggers to host images that they embed in blogs and social media.
The Verge reported in March 2013 that Flickr had a total of 87 million registered members and more than 3.5 million new images uploaded daily. In August 2011 the site reported that it was hosting more than 6 billion images and this number continues to grow steadily according to reporting sources. Photos and videos can be accessed from Flickr without the need to register an account but an account must be made in order to upload content onto the website. Registering an account also allows users to create a profile page containing photos and videos that the user has uploaded and also grants the ability to add another Flickr user as a contact. For mobile users, Flickr has official mobile apps for iOS, Android, and PlayStation Vita, operating systems, and an optimised mobile website.
^ "An Amazing 8 Years - Flickr Blog". Flickr. Retrieved 2014-02-10. 
^ "Flickr.com Site Info". Alexa Internet. Retrieved 2016-03-08. 
^ Terdiman, Daniel (2004-12-09). "Photo Site a Hit With Bloggers". Wired. Retrieved 2008-08-28. Flickr enables users to post photos from nearly any camera phone or directly from a PC. It also allows users to post photos from their accounts or from their cameras to most widely used blog services. The result is that an increasing number of bloggers are regularly posting photos from their Flickr accounts. 
^ "The man behind Flickr on making the service 'awesome again'". The Verge. 2013-03-20. Retrieved 2013-08-29. 
^ Parfeni, Lucian (2011-08-05). "Flickr Boasts 6 Billion Photo Uploads". Softpedia. Retrieved 2012-03-01. 
^ "Flickr for iPhone, iPod touch and iPad on the iTunes App Store". Retrieved 2013-07-24. 
^ "Official Flickr App for Android". Retrieved 2013-07-24. 
^ Pollicino, Joe (2012-02-22). "'Select' PS Vita Apps Hit the US PlayStation Store: Netflix, LiveTweet and Flickr". Engadget. Retrieved 2013-07-24. 
^ "Help: Using Flickr on your phone". Flickr. Retrieved 2014-04-04.""
""Fortran "",,,,""Fortran (formerly FORTRAN, derived from "Formula Translation") is a general-purpose, imperative programming language that is especially suited to numeric computation and scientific computing. Originally developed by IBM in the 1950s for scientific and engineering applications, Fortran came to dominate this area of programming early on and has been in continuous use for over half a century in computationally intensive areas such as numerical weather prediction, finite element analysis, computational fluid dynamics, computational physics and computational chemistry. It is a popular language for high-performance computing and is used for programs that benchmark and rank the world's fastest supercomputers.
Fortran encompasses a lineage of versions, each of which evolved to add extensions to the language while usually retaining compatibility with prior versions. Successive versions have added support for structured programming and processing of character-based data (FORTRAN 77), array programming, modular programming and generic programming (Fortran 90), high performance Fortran (Fortran 95), object-oriented programming (Fortran 2003) and concurrent programming (Fortran 2008).""
""FreeBSD "",,,,""FreeBSD is a free Unix-like operating system descended from Research Unix via the Berkeley Software Distribution (BSD). Although for legal reasons FreeBSD cannot use the Unix trademark, it is a direct descendant of BSD, which was historically also called "BSD Unix" or "Berkeley Unix". The first version of FreeBSD was released in 1993, and today FreeBSD is the most widely used open-source BSD distribution, accounting for more than three-quarters of all installed systems running open-source BSD derivatives.
FreeBSD has similarities with Linux, with two major differences in scope and licensing: FreeBSD maintains a complete operating system, i.e. the project delivers kernel, device drivers, userland utilities and documentation, as opposed to Linux delivering a kernel and drivers only and relying on third-parties for system software; and FreeBSD source code is generally released under a permissive BSD license as opposed to the copyleft GPL used by Linux.
The FreeBSD project includes a security team overseeing all software shipped in the base distribution. A wide range of additional third-party applications may be installed using the pkgng package management system or the FreeBSD Ports, or by directly compiling source code. Due to its permissive licensing terms, much of FreeBSD's code base has become an integral part of other operating systems such as Juniper JUNOS and Apple's OS X.
Since 2004, the New York City BSD Users Group database provides dmesg information from a collection of computers (laptops, workstations, single-board computers, embedded systems, virtual machines, etc.) running FreeBSD.""
""Google Earth "",,,,""Google Earth is a virtual globe, map and geographical information program that was originally called EarthViewer 3D created by Keyhole, Inc, a Central Intelligence Agency (CIA) funded company acquired by Google in 2004 (see In-Q-Tel). It maps the Earth by the superimposition of images obtained from satellite imagery, aerial photography and geographic information system (GIS) onto a 3D globe. It was originally available with three different licenses, but has since been reduced to just two: Google Earth (a free version with limited function) and Google Earth Pro, which is now free (it previously cost $399 a year) and is intended for commercial use. The third original option, Google Earth Plus, has been discontinued.
The product, re-released as Google Earth in 2005, is available for use on personal computers running Windows 2000 and above, Mac OS X 10.3.9 and above, Linux kernel: 2.6 or later (released on June 12, 2006), and FreeBSD. Google Earth is also available as a browser plugin which was released on May 28, 2008. It was also made available for mobile viewers on the iPhone OS on October 28, 2008, as a free download from the App Store, and is available to Android users as a free app in the Google Play store. In addition to releasing an updated Keyhole based client, Google also added the imagery from the Earth database to their web-based mapping software, Google Maps. The release of Google Earth in June 2005 to the public caused a more than tenfold increase in media coverage on virtual globes between 2004 and 2005, driving public interest in geospatial technologies and applications. As of October 2011, Google Earth has been downloaded more than a billion times.
Google Earth displays satellite images of varying resolution of the Earth's surface, allowing users to see things like cities and houses looking perpendicularly down or at an oblique angle (see also bird's eye view). The degree of resolution available is based somewhat on the points of interest and popularity, but most land (except for some islands) is covered in at least 15 meters of resolution. Maps showing a visual representation of Google Earth coverage Melbourne, Victoria, Australia; Las Vegas, Nevada, USA; and Cambridge, Cambridgeshire, United Kingdom include examples of the highest resolution, at 15 cm (6 inches). Google Earth allows users to search for addresses for some countries, enter coordinates, or simply use the mouse to browse to a location.
For large parts of the surface of the Earth only 2D images are available, from almost vertical photography. Viewing this from an oblique angle, there is perspective in the sense that objects which are horizontally far away are seen smaller, like viewing a large photograph, not quite like a 3D view.
For other parts of the surface of the Earth, 3D images of terrain and buildings are available. Google Earth uses digital elevation model (DEM) data collected by NASA's Shuttle Radar Topography Mission (SRTM). This means one can view almost the entire earth in three dimensions. Since November 2006, the 3D views of many mountains, including Mount Everest, have been improved by the use of supplementary DEM data to fill the gaps in SRTM coverage.
Some people use the applications to add their own data, making them available through various sources, such as the Bulletin Board Systems (BBS) or blogs mentioned in the link section below. Google Earth is able to show various kinds of images overlaid on the surface of the earth and is also a Web Map Service client. Google Earth supports managing three-dimensional Geospatial data through Keyhole Markup Language (KML).""
""Google "",,,,""Google is an American multinational technology company specializing in Internet-related services and products. These include online advertising technologies, search, cloud computing, and software. Most of its profits are derived from AdWords, an online advertising service that places advertising near the list of search results.
Google was founded by Larry Page and Sergey Brin while they were Ph.D. students at Stanford University. Together, they own about 14 percent of its shares and control 56 percent of the stockholder voting power through supervoting stock. They incorporated Google as a privately held company on September 4, 1998. An initial public offering followed on August 19, 2004. Its mission statement from the outset was "to organize the world's information and make it universally accessible and useful," and its unofficial slogan was "Don't be evil". In 2004, Google moved to its new headquarters in Mountain View, California, nicknamed the Googleplex. In August 2015, Google announced plans to reorganize its interests as a holding company called Alphabet Inc. When this restructuring took place on October 2, 2015, Google became Alphabet's leading subsidiary, as well as the parent for Google's Internet interests.
Rapid growth since incorporation has triggered a chain of products, acquisitions and partnerships beyond Google's core search engine (Google Search). It offers online productivity software (Google Docs) including email (Gmail), a cloud storage service (Google Drive) and a social networking service (Google+). Desktop products include applications for web browsing (Google Chrome), organizing and editing photos (Google Photos), and instant messaging (Hangouts). The company leads the development of the Android mobile operating system and the browser-only Chrome OS for a class of netbooks known as Chromebooks. Google has moved increasingly into communications hardware: it partners with major electronics manufacturers in the production of its "high-quality low-cost" Nexus devices. In 2012, a fiber-optic infrastructure was installed in Kansas City to facilitate a Google Fiber broadband service.
The corporation has been estimated to run more than one million servers in data centers around the world (as of 2007). It processes over one billion search requests and about 24 petabytes of user-generated data each day (as of 2009). In December 2013, Alexa listed google.com as the most visited website in the world. Numerous Google sites in other languages figure in the top one hundred, as do several other Google-owned sites such as YouTube and Blogger. Its market dominance has led to prominent media coverage, including criticism of the company over issues such as aggressive tax avoidance, search neutrality, copyright, censorship, and privacy.""
""Google Maps "",,,,""Google Maps is a desktop web mapping service developed by Google. It offers satellite imagery, street maps, 360° panoramic views of streets (Street View), real-time traffic conditions (Google Traffic), and route planning for traveling by foot, car, bicycle (in beta), or public transportation.
Google Maps began as a C++ desktop program designed by Lars and Jens Eilstrup Rasmussen at Where 2 Technologies. In October 2004, the company was acquired by Google, which converted it into a web application. After additional acquisitions of a geospatial data visualization company and a realtime traffic analyzer, Google Maps was launched in February 2005. The service's front end utilizes JavaScript, XML, and Ajax. Google Maps offers an API that allows maps to be embedded on third-party websites, and offers a locator for urban businesses and other organizations in numerous countries around the world. Google Map Maker allows users to collaboratively expand and update the service's mapping worldwide.
Google Maps' satellite view is a "top-down" view; most of the high-resolution imagery of cities is aerial photography taken from aircraft flying at 800 to 1,500 feet (240 to 460 m), while most other imagery is from satellites. Much of the available satellite imagery is no more than three years old and is updated on a regular basis. Google Maps uses a close variant of the Mercator projection, and therefore cannot accurately show areas around the poles.
The current redesigned version of the desktop application was made available in 2013, alongside the "classic" (pre-2013) version. Google Maps for mobile was released in September 2008 and features GPS turn-by-turn navigation. In August 2013, it was determined to be the world's most popular app for smartphones, with over 54% of global smartphone owners using it at least once.
In 2012, Google reported of having over 7,100 employees and contractors directly working in mapping.
^ "What is the Google Maps API?". 
^ "Blurry or Outdated Imagery". 
^ How Often is Google Maps and Google Earth Updated?. Technicamix.com (2011-10-18). Retrieved on 2013-11-24.
^ "Google+ Smartphone App Popularity". Business Insider. Retrieved 2013-09-06. 
^ Carlson, Nicholas. "To Do What Google Does In Maps, Apple Would Have To Hire 7,000 People". Business Insider Australia. Retrieved 2016-03-06.""
""Google Scholar "",,,,""Google Scholar is a freely accessible web search engine that indexes the full text or metadata of scholarly literature across an array of publishing formats and disciplines. Released in beta in November 2004, the Google Scholar index includes most peer-reviewed online journals of Europe and America's largest scholarly publishers, plus scholarly books and other non-peer reviewed journals. While Google does not publish the size of Google Scholar's database, third-party researchers estimated it to contain roughly 160 million documents as of May 2014 and an earlier statistical estimate published in PLOS ONE using a Mark and recapture method estimated approximately 80-90% coverage of all articles published in English.
Google Scholar is similar in function to the freely available CiteSeerX and getCITED. It also resembles the subscription-based tools, Elsevier's Scopus and Thomson Reuters' Web of Science.""
""GSM "",,,,""GSM (Global System for Mobile Communications, originally Groupe Spécial Mobile), is a standard developed by the European Telecommunications Standards Institute (ETSI) to describe the protocols for second-generation (2G) digital cellular networks used by mobile phones, first deployed in Finland in July 1991. As of 2014 it has become the default global standard for mobile communications - with over 90% market share, operating in over 219 countries and territories.
2G networks developed as a replacement for first generation (1G) analog cellular networks, and the GSM standard originally described a digital, circuit-switched network optimized for full duplex voice telephony. This expanded over time to include data communications, first by circuit-switched transport, then by packet data transport via GPRS (General Packet Radio Services) and EDGE (Enhanced Data rates for GSM Evolution or EGPRS).
Subsequently, the 3GPP developed third-generation (3G) UMTS standards followed by fourth-generation (4G) LTE Advanced standards, which do not form part of the ETSI GSM standard.
"GSM" is a trademark owned by the GSM Association. It may also refer to the (initially) most common voice codec used, Full Rate.

""
""HDTV "",,,,""High-definition television (HDTV) provides a resolution that is substantially higher than that of standard-definition television.
HDTV may be transmitted in various formats:
1080p: 1920×1080p: 2,073,600 pixels (~2.07 megapixels) per frame
1080i: 1920×1080i: 1,036,800 pixels (~1.04 MP) per field or 2,073,600 pixels (~2.07 MP) per frame
Some countries also use a non-standard CEA resolution, such as 1440×1080i: 777,600 pixels (~0.78 MP) per field or 1,555,200 pixels (~1.56 MP) per frame

720p: 1280×720p: 921,600 pixels (~0.92 MP) per frame
The letter "p" here stands for progressive scan, while "i" indicates interlaced.
When transmitted at two megapixels per frame, HDTV provides about five times as many pixels as SD (standard-definition television).""
""HP-UX "",,,,""HP-UX (from "Hewlett Packard Unix") is Hewlett Packard Enterprise's proprietary implementation of the Unix operating system, based on UNIX System V (initially System III) and first released in 1984. Recent versions support the HP 9000 series of computer systems, based on the PA-RISC processor architecture, and HP Integrity systems, based on Intel's Itanium architecture.
Earlier versions of HP-UX supported the HP Integral PC and HP 9000 Series 200, 300, and 400 computer systems based on the Motorola 68000 series of processors, as well as the HP 9000 Series 500 computers based on HP's proprietary FOCUS processor architecture.
HP-UX was the first Unix to offer access control lists for file access permissions as an alternative to the standard Unix permissions system. HP-UX was also among the first Unix systems to include a built-in logical volume manager. HP has had a long partnership with Veritas Software, and uses VxFS as the primary file system.
It is one of five commercial operating systems that have versions certified to The Open Group's UNIX 03 standard. (The others are OS X, Solaris, Inspur K-UX and AIX.)""
""InfiniBand "",,,,""InfiniBand (abbreviated IB), a computer-networking communications standard used in high-performance computing, features very high throughput and very low latency. It is used for data interconnect both among and within computers. InfiniBand is also utilized as either a direct, or switched interconnect between servers and storage systems, as well as an interconnect between storage systems.
As of 2014 it was the most commonly used interconnect in supercomputers. Mellanox and Intel manufacture InfiniBand host bus adapters and network switches, and in February 2016 it was reported that Oracle Corporation had engineered its own Infiniband switch units and server adapter chips for use in its own product lines and by third parties. Mellanox IB cards are available for Solaris, RHEL, SLES, Windows, HP-UX, VMware ESX. It is designed to be scalable and uses a switched fabric network topology.
As an interconnect, IB competes with Ethernet and proprietary technologies such as Cray's SeaStar.
Alternative network transport technologies include Fibre Channel and Ethernet.
The technology is promoted by the InfiniBand Trade Association.""
""IP Multimedia Subsystem "",,,,""The IP Multimedia Subsystem or IP Multimedia Core Network Subsystem (IMS) is an architectural framework for delivering IP multimedia services. Historically, mobile phones have provided voice call services over a switched-circuit-style network, rather than strictly over an IP packet-switched network. Alternative methods of delivering voice or other multimedia services over IP have become available on smartphones (e.g. VoIP or Skype), but they have not become standardized across the industry. IMS is an architectural framework to provide such standardization.
IMS was originally designed by the wireless standards body 3rd Generation Partnership Project (3GPP), as a part of the vision for evolving mobile networks beyond GSM. Its original formulation (3GPP Rel-5) represented an approach to delivering "Internet services" over GPRS. This vision was later updated by 3GPP, 3GPP2 and ETSI TISPAN by requiring support of networks other than GPRS, such as Wireless LAN, CDMA2000 and fixed lines.
To ease the integration with the Internet, IMS uses IETF protocols wherever possible, e.g., SIP (Session Initiation Protocol). According to the 3GPP, IMS is not intended to standardize applications, but rather to aid the access of multimedia and voice applications from wireless and wireline terminals, i.e., to create a form of fixed-mobile convergence (FMC). This is done by having a horizontal control layer that isolates the access network from the service layer. From a logical architecture perspective, services need not have their own control functions, as the control layer is a common horizontal layer. However, in implementation this does not necessarily map into greater reduced cost and complexity.
Alternative and overlapping technologies for access and provisioning of services across wired and wireless networks include combinations of Generic Access Network, soft switches and "naked" SIP.
Since it is becoming increasingly easier to access content and contacts using mechanisms outside the control of traditional wireless/fixed operators, the interest of IMS is being challenged.
Examples of global standards based on IMS are MMTel which is the basis for Voice over LTE (VoLTE) and Rich Communication Services(RCS) which is also known as joyn or Advanced Messaging.""
""iPhone "",,,,""iPhone (/ˈaɪfoʊn/ EYE-fohn) is a line of smartphones designed and marketed by Apple Inc. They run Apple's iOS mobile operating system. The first generation iPhone was released on June 29, 2007; the most recent iPhone model is the iPhone SE, which was unveiled at a special event on March 22, 2016.
The user interface is built around the device's multi-touch screen, including a virtual keyboard. The iPhone has Wi-Fi and can connect to cellular networks. An iPhone can shoot video (though this was not a standard feature until the iPhone 3GS), take photos, play music, send and receive email, browse the web, send and receive text messages, follow GPS navigation, record notes, perform mathematical calculations, and receive visual voicemail. Other functions—video games, reference works, social networking, etc.—can be enabled by downloading application programs (‘apps’); as of October 2013, the App Store offered more than one million apps by Apple and third parties and is ranked as the world's largest mobile software distribution network of its kind (by number of currently available applications).
There are nine generations of iPhone models, each accompanied by one of the nine major releases of iOS. The original 1st-generation iPhone was a GSM phone and established design precedents, such as a button placement that has persisted throughout all releases and a screen size maintained for the next four iterations. The iPhone 3G added 3G cellular network capabilities and A-GPS location. The iPhone 3GS added a faster processor and a higher-resolution camera that could record video at 480p. The iPhone 4 featured a higher-resolution 960×640 "Retina Display", a VGA front-facing camera for video calling and other apps, and a 5-megapixel rear-facing camera with 720p video capture. The iPhone 4S upgrades to an 8-megapixel camera with 1080p video recording, a dual-core A5 processor, and a natural language voice control system called Siri. iPhone 5 features the dual-core A6 processor, increases the size of the Retina display to 4 inches, introduces LTE support and replaces the 30-pin dock connector with an all-digital Lightning connector. The iPhone 5C features the same A6 chip as the iPhone 5, along with a new backside-illuminated FaceTime camera and a new casing made of polycarbonate. The iPhone 5S features the dual-core 64-bit A7 processor, an updated camera with a larger aperture and dual-LED flash, and the Touch ID fingerprint scanner, integrated into the home button, and fitness tracking facilities. The iPhone 6 and iPhone 6 Plus further increased screen size, measuring at 4.7 inches and 5.5 inches, respectively. In addition, they also feature a new A8 chip and M8 motion coprocessor. As of 2013, the iPhone 3GS had the longest production run, 1,181 days; followed by the iPhone 4, produced for 1,174 days.
The resounding sales of the iPhone, at the time, have been credited with reshaping the smartphone industry, and helping make Apple one of the world's most valuable publicly traded companies by 2011. Almost all modern smartphones have replicated the iPhone design of a slate format with a touchscreen interface.
In late 2014, JP Morgan estimated "iPhone percentage of the worldwide smartphone install base has been around 15% since late 2012" being far behind the dominant Android-based smartphones. In a few mature market countries such as Japan, the iPhone has a majority, an exception to Android's dominance, and Australia where Android is rapidly approaching parity. In March 2014, sales of the iPhone brand had reached 500 million devices. In the last quarter of 2014, there were 74.5 million iPhones sold, a record, compared to 51.0 million in the last quarter of 2013. Tim Cook revealed at the Apple Watch conference on March 9, 2015, that Apple had sold a total of 700 million iPhones to date.""
""iPod "",,,,""The iPod is a line of portable media players and multi-purpose pocket computers designed and marketed by Apple Inc. The first line was released on October 23, 2001, about 8½ months after iTunes (Macintosh version) was released. The most recent iPod redesigns were announced on July 15, 2015. There are three current versions of the iPod: the ultra-compact iPod Shuffle, the compact iPod Nano and the touchscreen iPod Touch.
Like other digital music players, iPods can serve as external data storage devices. Storage capacity varies by model, ranging from 2 GB for the iPod Shuffle to 128 GB for the iPod Touch (previously 160 GB for the iPod Classic, which is now discontinued).
Apple's iTunes software (and other alternative software) can be used to transfer music, photos, videos, games, contact information, e-mail settings, Web bookmarks, and calendars, to the devices supporting these features from computers using certain versions of Apple Macintosh and Microsoft Windows operating systems.
Before the release of iOS 5, the iPod branding was used for the media player included with the iPhone and iPad, a combination of the Music and Videos apps on the iPod Touch. As of iOS 5, separate apps named "Music" and "Videos" are standardized across all iOS-powered products. While the iPhone and iPad have essentially the same media player capabilities as the iPod line, they are generally treated as separate products. During the middle of 2010, iPhone sales overtook those of the iPod.
In mid-2015, a new model of the iPod Touch was announced by Apple, and was officially released on the Apple store on July 15, 2015. The sixth generation iPod Touch includes a wide variety of spec improvements such as the upgraded A8 processor and higher-quality screen. The core is over 5 times faster than previous models and is built to be roughly on par with the iPhone 5S. It is available in 5 different colors: Space grey, pink, gold, silver and Product (red).
^ Apple Inc. "iTunes system requirements. Apple iTunes software runs on Mac OS X 10.3.9 or OS X 10.4.9 or later and on Microsoft Windows XP (Service Pack 2) or later". Retrieved May 28, 2008. 
^ McKillop, Ross (July 8, 2007). "10 Alternatives to iTunes for managing your iPod". Simple Help. Retrieved May 28, 2008. 
^ "APPLE IOS 5 IPHONE / IPAD HANDS-ON (screenshot of iPhone home screen with iOS 5 preview)". Boy Genius Report. June 6, 2011. Retrieved June 6, 2011. 
^ Horace Dediu (October 21, 2010). "Turning up the volume: iPhone overtakes iPod". asymco. Retrieved October 25, 2013. 
^ Cunningham, Andrew (July 18, 2015). "iPod Touch performance preview: 500% better CPU, 900% faster graphics". arstechnica. Retrieved July 18, 2015. 
^ Biggs, Tim (July 16, 2015). "New iPod Touch is the entry level Apple Music device". The Sydney Morning Herald Digital Life. Retrieved July 18, 2015.""
""IPTV "",,,,""Internet Protocol television (IPTV) is a system through which television services are delivered using the Internet protocol suite over a packet-switched network such as a LAN or the Internet, instead of being delivered through traditional terrestrial, satellite signal, and cable television formats. Unlike downloaded media, IPTV offers the ability to stream the media in smaller batches, directly from the source. As a result, a client media player can begin playing the data (such as a movie) before the entire file has been transmitted. This is known as streaming media.
IPTV services may be classified into three main groups:
Live television, with or without interactivity related to the current TV show;
Time-shifted television: catch-up TV (replays a TV show that was broadcast hours or days ago), start-over TV (replays the current TV show from its beginning);
Video on demand (VOD): browse a catalog of videos, not related to TV programming.
IPTV is distinguished from Internet television by its ongoing standardization process (e.g., European Telecommunications Standards Institute) and preferential deployment scenarios in subscriber-based telecommunications networks with high-speed access channels into end-user premises via set-top boxes or other customer-premises equipment.""
""IPv6 "",,,,""Internet Protocol version 6 (IPv6) is the most recent version of the Internet Protocol (IP), the communications protocol that provides an identification and location system for computers on networks and routes traffic across the Internet. IPv6 was developed by the Internet Engineering Task Force (IETF) to deal with the long-anticipated problem of IPv4 address exhaustion. IPv6 is intended to replace IPv4.
Every device on the Internet is assigned an IP address for identification and location definition. With the rapid growth of the Internet after commercialization in the 1990s, it became evident that far more addresses than the IPv4 address space has available were necessary to connect new devices in the future. By 1998, the Internet Engineering Task Force (IETF) had formalized the successor protocol. IPv6 uses a 128-bit address, theoretically allowing 2128, or approximately 7038340000000000000♠3.4×1038 addresses. The actual number is slightly smaller, as multiple ranges are reserved for special use or completely excluded from use. The total number of possible IPv6 address is more than 7028790000000000000♠7.9×1028 times as many as IPv4, which uses 32-bit addresses and provides approximately 4.3 billion addresses. The two protocols are not designed to be interoperable, complicating the transition to IPv6. However, several IPv6 transition mechanisms have been devised to permit communication between IPv4 and IPv6 hosts.
IPv6 provides other technical benefits in addition to a larger addressing space. In particular, it permits hierarchical address allocation methods that facilitate route aggregation across the Internet, and thus limit the expansion of routing tables. The use of multicast addressing is expanded and simplified, and provides additional optimization for the delivery of services. Device mobility, security, and configuration aspects have been considered in the design of the protocol.
IPv6 addresses are represented as eight groups of four hexadecimal digits with the groups being separated by colons, for example 2001:0db8:0000:0042:0000:8a2e:0370:7334, but methods to abbreviate this full notation exist.""
""JavaScript "",,,,""JavaScript (/ˈdʒɑːvəˌskrɪpt/) is a high-level, dynamic, untyped, and interpreted programming language. It has been standardized in the ECMAScript language specification. Alongside HTML and CSS, it is one of the three essential technologies of World Wide Web content production; the majority of websites employ it and it is supported by all modern Web browsers without plug-ins. JavaScript is prototype-based with first-class functions, making it a multi-paradigm language, supporting object-oriented, imperative, and functional programming styles. It has an API for working with text, arrays, dates and regular expressions, but does not include any I/O, such as networking, storage, or graphics facilities, relying for these upon the host environment in which it is embedded.
Despite some naming, syntactic, and standard library similarities, JavaScript and Java are otherwise unrelated and have very different semantics. The syntax of JavaScript is actually derived from C, while the semantics and design are influenced by the Self and Scheme programming languages.
JavaScript is also used in environments that are not Web-based, such as PDF documents, site-specific browsers, and desktop widgets. Newer and faster JavaScript virtual machines (VMs) and platforms built upon them have also increased the popularity of JavaScript for server-side Web applications. On the client side, JavaScript has been traditionally implemented as an interpreted language, but more recent browsers perform just-in-time compilation. It is also used in game development, the creation of desktop and mobile applications, and server-side network programming with runtime environments such as Node.js.

""
""JPEG "",,,,""JPEG (/ˈdʒeɪpɛɡ/ JAY-peg) is a commonly used method of lossy compression for digital images, particularly for those images produced by digital photography. The degree of compression can be adjusted, allowing a selectable tradeoff between storage size and image quality. JPEG typically achieves 10:1 compression with little perceptible loss in image quality.
JPEG compression is used in a number of image file formats. JPEG/Exif is the most common image format used by digital cameras and other photographic image capture devices; along with JPEG/JFIF, it is the most common format for storing and transmitting photographic images on the World Wide Web. These format variations are often not distinguished, and are simply called JPEG.
The term "JPEG" is an abbreviation for the Joint Photographic Experts Group, which created the standard. The MIME media type for JPEG is image/jpeg, except in older Internet Explorer versions, which provides a MIME type of image/pjpeg when uploading JPEG images. JPEG files usually have a filename extension of .jpg or .jpeg.
JPEG/JFIF supports a maximum image size of 65,535×65,535 pixels, hence up to 4 gigapixels (for an aspect ratio of 1:1).""
""LAPACK "",,,,""LAPACK (Linear Algebra Package) is a standard software library for numerical linear algebra. It provides routines for solving systems of linear equations and linear least squares, eigenvalue problems, and singular value decomposition. It also includes routines to implement the associated matrix factorizations such as LU, QR, Cholesky and Schur decomposition. LAPACK was originally written in FORTRAN 77, but moved to Fortran 90 in version 3.2 (2008). The routines handle both real and complex matrices in both single and double precision.
LAPACK was designed as the successor to the linear equations and linear least-squares routines of LINPACK and the eigenvalue routines of EISPACK. LINPACK, written in the 1970s and 1980s, was designed to run on the then-modern vector computers with shared memory. LAPACK, in contrast, was designed to effectively exploit the caches on modern cache-based architectures, and thus can run orders of magnitude faster than LINPACK on such machines, given a well-tuned BLAS implementation. LAPACK has also been extended to run on distributed-memory systems in later packages such as ScaLAPACK and PLAPACK.
LAPACK is licensed under a three-clause BSD style license, a permissive free software license with few restrictions.""
""Linux "",,,,""Linux (pronounced /ˈlɪnəks/ LIN-əks or, less frequently, /ˈlaɪnəks/ LYN-əks) is a Unix-like and mostly POSIX-compliant computer operating system (OS) assembled under the model of free and open-source software development and distribution. The defining component of Linux is the Linux kernel, an operating system kernel first released on 5 October 1991 by Linus Torvalds. The Free Software Foundation uses the name GNU/Linux to describe the operating system, which has led to some controversy.
Linux was originally developed as a free operating system for personal computers based on the Intel x86 architecture, but has since been ported to more computer hardware platforms than any other operating system. Because of the dominance of Android on smartphones, Linux has the largest installed base of all general-purpose operating systems. Linux, in its original form, is also the leading operating system on servers and other big iron systems such as mainframe computers and virtually all fastest supercomputers, but is used on only around 1.6% of desktop computers when not including Chrome OS, which has about 5% of the overall and nearly 20% of the sub-$300 notebook sales. Linux also runs on embedded systems, which are devices whose operating system is typically built into the firmware and is highly tailored to the system; this includes smartphones and tablet computers running Android and other Linux derivatives, TiVo and similar DVR devices, network routers, facility automation controls, televisions, video game consoles, and smartwatches.
The development of Linux is one of the most prominent examples of free and open-source software collaboration. The underlying source code may be used, modified and distributed — commercially or non-commercially — by anyone under the terms of its respective licenses, such as the GNU General Public License. Typically, Linux is packaged in a form known as a Linux distribution, for both desktop and server use. Some of the popular mainstream Linux distributions are Debian, Ubuntu, Linux Mint, Fedora, openSUSE, Arch Linux and Gentoo, together with commercial Red Hat Enterprise Linux and SUSE Linux Enterprise Server distributions. Linux distributions include the Linux kernel, supporting utilities and libraries, and usually a large amount of application software to fulfill the distribution's intended use.
Distributions oriented toward desktop use typically include a windowing system, such as X11, Mir or a Wayland implementation, and an accompanying desktop environment, such as GNOME or the KDE Software Compilation; some distributions may also include a less resource-intensive desktop, such as LXDE or Xfce. Distributions intended to run on servers may omit all graphical environments from the standard install, and instead include other software to set up and operate a solution stack such as LAMP. Because Linux is freely redistributable, anyone may create a distribution for any intended use.
^ Linux Online (2008). "Linux Logos and Mascots". Archived from the original on August 15, 2010. Retrieved August 11, 2009. 
^ GNU Userland
^ Unix Fundamentals
^ Operating Systems
^ The Linux FAQ: The X Window System
^ What is the difference between Gnome, KDE, Xfce & LXDE
^ "The Linux Kernel Archives: Frequently asked questions". kernel.org. September 2, 2014. Retrieved September 4, 2015. 
^ "U.S. Reg No: 1916230". United States Patent and Trademark Office. Retrieved April 1, 2006. 
^ "Re: How to pronounce Linux?". Newsgroup: comp.os.linux. 23 April 1992. Usenet: 1992Apr23.123216.22024@klaava.Helsinki.FI. Retrieved January 9, 2007. 
^ a b Free On-Line Dictionary of Computing (June 2006). "Linux". Retrieved September 15, 2009. 
^ Safalra (14 April 2007). "Pronunciation of 'Linux'". Safalra's Website. Retrieved September 15, 2009. 
^ "Conflicts between ISO/IEC 9945 (POSIX) and the Linux Standard Base". opengroup.org. 2003-07-29. Retrieved 2014-04-27. 
^ Eckert, Jason W. (2012). Linux+ Guide to Linux Certification (Third ed.). Boston, Massachusetts: Cengage Learning. p. 33. ISBN 978-1111541538. Retrieved April 14, 2013. The shared commonality of the kernel is what defines Linux; the differing OSS applications that can interact with the common kernel are what differentiate Linux distributions. 
^ Linus Benedict Torvalds (5 October 1991). "Free minix-like kernel sources for 386-AT". Newsgroup: comp.os.minix. Retrieved September 30, 2011. 
^ "What Is Linux: An Overview of the Linux Operating System". Linux Foundation. April 3, 2009. Retrieved August 15, 2011. 
^ 
^ "Linux and the GNU System". Gnu.org. Retrieved 1 September 2013. 
^ Barry Levine (26 August 2013). "Linux' 22th [sic] Birthday Is Commemorated - Subtly - by Creator". Simpler Media Group, Inc. Retrieved 10 May 2015. Originally developed for Intel x86-based PCs, Torvalds’ “hobby” has now been released for more hardware platforms than any other OS in history. 
^ "NetMarketShare:Mobile/Tablet Operating System Market Share". 
^ Computerworld, Patrick Thibodeau. "IBM's newest mainframe is all Linux". Retrieved February 22, 2009. 
^ Lyons, Daniel (March 15, 2005). "Linux rules supercomputers". Forbes. Retrieved February 22, 2007. 
^ "Desktop Operating System Market Share". Netmarketshare.com. Retrieved November 23, 2015. 
^ http://gs.statcounter.com/chart.php?bar=1&device=Desktop&device_hidden=desktop&multi-device=true&statType_hidden=os&region_hidden=ww&granularity=monthly&statType=Operating%20System&region=Worldwide&fromInt=201510&toInt=201510&fromMonthYear=2015-10&toMonthYear=2015-10&csv=1
^ Steven J. Vaughan-Nichols. "Chromebook shipments leap by 67 percent". ZDNet. Retrieved September 29, 2015. 
^ Linux Devices (January 2010). "Trolltech rolls "complete" Linux smartphone stack". Archived from the original on 2012-05-25. Retrieved September 29, 2009. 
^ "Sony Open Source Code Distribution Service". Sony Electronics. Retrieved October 8, 2011. 
^ "Sharp Liquid Crystal Television Instruction Manual" (PDF). Sharp Electronics. p. 24. Retrieved October 8, 2011. 
^ IBM (October 2001). "Linux Watch (WatchPad)". Retrieved June 18, 2015.""
""Lotus Notes "",,,,""IBM Notes (formerly Lotus Notes; see branding, below) and IBM Domino (formerly Lotus Domino) are the client and server, respectively, of a collaborative client-server software platform sold by IBM.
IBM Notes provides business collaboration functions, such as email, calendars, to-do lists, contacts management, teamrooms, discussion forums, file sharing, microblogging, instant messaging, blogs, and user directories. IBM Notes can also be used with other IBM Domino applications and databases. IBM Notes 9 Social Edition removed integration with the office software package IBM Lotus Symphony, which had been integrated with the IBM Lotus Notes client in versions 8.x.
Lotus Development Corporation originally developed "Lotus Notes" in 1989. IBM bought the Lotus corporation in 1995 and it became known as the Lotus Development division of IBM. As of 2015 it forms part of the IBM Software and Systems Group under the name "IBM Collaboration Solutions".
IBM Notes is a desktop workflow application, commonly used in corporate environments for email but can also be used to access databases such as document libraries and custom applications.""
""Macintosh OS "",,,,""Mac OS is a series of graphical user interface–based operating systems developed by Apple Inc. for their Macintosh line of computer systems.
The original operating system was first introduced in 1984 as being integral to the original Macintosh, and referred to as the "System". Referred to by its major revision starting with "System 6" and "System 7", Apple rebranded version 7.6 as "Mac OS" as part of their Macintosh clone program in 1996. The Macintosh, specifically its system software, is credited with having popularized the early graphical user interface concept.
There are two architectural legacies of Mac OS. Up to major revision 9, from 1984 to 2000, it is historically known as Classic Mac OS. Major revision 10, from 2001 to present, is branded OS X (originally referred to as Mac OS X). Both legacies share a general interface design, and there has been some overlap of application frameworks for compatibility; but the two systems have different origins and use deeply different architectures.""
""Macintosh "",,,,""The Macintosh (/ˈmækᵻntɒʃ/ MAK-in-tosh; branded as Mac since 1998) is a series of personal computers (PCs) designed, developed, and marketed by Apple Inc. Steve Jobs introduced the original Macintosh computer on January 24, 1984. This was the first mass-market personal computer featuring an integral graphical user interface and mouse. This first model was later renamed to "Macintosh 128k" for uniqueness amongst a populous family of subsequently updated models which are also based on Apple's same proprietary architecture. Since 1998, Apple has largely phased out the Macintosh name in favor of "Mac", though the product family has been nicknamed "Mac" or "the Mac" since the development of the first model.
The Macintosh, however, was expensive, which hindered its ability to be competitive in a market already dominated by the Commodore 64 for consumers, as well as the IBM Personal Computer and its accompanying clone market for businesses. Macintosh systems still found success in education and desktop publishing and kept Apple as the second-largest PC manufacturer for the next decade. In the 1990s, improvements in the rival Wintel platform, notably with the introduction of Windows 3.0, then Windows 95, gradually took market share from the more expensive Macintosh systems. The performance advantage of 68000-based Macintosh systems was eroded by Intel's Pentium, and in 1994 Apple was relegated to third place as Compaq became the top PC manufacturer. Even after a transition to the superior PowerPC-based Power Macintosh (later renamed the PowerMac, in line with the PowerBook series) line in 1994, the falling prices of commodity PC components and the release of Windows 95 saw the Macintosh user base decline.
In 1998, after the return of Steve Jobs, Apple consolidated its multiple consumer-level desktop models into the all-in-one iMac G3, which became a commercial success and revitalized the brand. Since their transition to Intel processors in 2006, the complete lineup is entirely based on said processors and associated systems. Its current lineup comprises three desktops (the all-in-one iMac, entry-level Mac mini, and the Mac Pro tower graphics workstation), and four laptops (the MacBook, MacBook Air, MacBook Pro, and MacBook Pro with Retina display). Its Xserve server was discontinued in 2011 in favor of the Mac Mini and Mac Pro.
Apple also develops the operating system for the Mac, currently OS X version 10.11 "El Capitan". Macs are currently capable of running non-Apple operating systems such as Linux, OpenBSD, and Microsoft Windows with the aid of Boot Camp or third-party software. Apple does not license OS X for use on non-Apple computers, though it did license previous versions of Mac OS through their Macintosh clone program from 1995 to 1997.""
""Mathematica "",,,,""Mathematica is a symbolic mathematical computation program, sometimes called a computer algebra program, used in many scientific, engineering, mathematical, and computing fields. It was conceived by Stephen Wolfram and is developed by Wolfram Research of Champaign, Illinois. The Wolfram Language is the programming language used in Mathematica.""
""MATLAB "",,,,""MATLAB (matrix laboratory) is a multi-paradigm numerical computing environment and fourth-generation programming language. A proprietary programming language developed by MathWorks, MATLAB allows matrix manipulations, plotting of functions and data, implementation of algorithms, creation of user interfaces, and interfacing with programs written in other languages, including C, C++, Java, Fortran and Python.
Although MATLAB is intended primarily for numerical computing, an optional toolbox uses the MuPAD symbolic engine, allowing access to symbolic computing abilities. An additional package, Simulink, adds graphical multi-domain simulation and model-based design for dynamic and embedded systems.
In 2004, MATLAB had around one million users across industry and academia. MATLAB users come from various backgrounds of engineering, science, and economics.

""
""Mobile IP "",,,,""Mobile IP (or MIP) is an Internet Engineering Task Force (IETF) standard communications protocol that is designed to allow mobile device users to move from one network to another while maintaining a permanent IP address. Mobile IP for IPv4 is described in IETF RFC 5944, and extensions are defined in IETF RFC 4721. Mobile IPv6, the IP mobility implementation for the next generation of the Internet Protocol, IPv6, is described in RFC 6275.""
""WiMAX "",,,,""WiMAX (Worldwide Interoperability for Microwave Access) is a family of wireless communications standards initially designed to provide 30 to 40 megabit-per-second data rates, with the 2011 update providing up to 1 Gbit/s for fixed stations. The name "WiMAX" was created by the WiMAX Forum, which was formed in June 2001 to promote conformity and interoperability of the standard. The forum describes WiMAX as "a standards-based technology enabling the delivery of last mile wireless broadband access as an alternative to cable and DSL". IEEE 802.16m or WirelessMAN-Advanced is a candidate for the 4G, in competition with the LTE Advanced standard.""
""MP3 "",,,,""MPEG-1 or MPEG-2 Audio Layer III, more commonly referred to as MP3, is an audio coding format for digital audio which uses a form of lossy data compression. It is a common audio format for consumer audio streaming or storage, as well as a de facto standard of digital audio compression for the transfer and playback of music on most digital audio players.
The use of lossy compression is designed to greatly reduce the amount of data required to represent the audio recording and still sound like a faithful reproduction of the original uncompressed audio for most listeners. An MP3 file that is created using the setting of 128 kbit/s will result in a file that is about 1/11 the size of the CD file created from the original audio source (44,100 samples per second × 16 bits per sample × 2 channels = 1,411,200 bit/s; MP3 compressed at 128 kbit/s: 128,000 bit/s [1 k = 1,000, not 1024, because it is a bit rate]. Ratio: 1,411,200/128,000 = 11.025). An MP3 file can also be constructed at higher or lower bit rates, with higher or lower resulting quality.
The compression works by reducing the accuracy of certain parts of a sound that are considered to be beyond the auditory resolution ability of most people. This method is commonly referred to as perceptual coding. It uses psychoacoustic models to discard or reduce precision of components less audible to human hearing, and then records the remaining information in an efficient manner.
MP3 was designed by the Moving Picture Experts Group (MPEG) as part of its MPEG-1 standard and later extended in the MPEG-2 standard. The first subgroup for audio was formed by several teams of engineers at Fraunhofer IIS, University of Hannover, AT&T-Bell Labs, Thomson-Brandt, CCETT, and others. MPEG-1 Audio (MPEG-1 Part 3), which included MPEG-1 Audio Layer I, II and III was approved as a committee draft of ISO/IEC standard in 1991, finalised in 1992 and published in 1993 (ISO/IEC 11172-3:1993). Backwards compatible MPEG-2 Audio (MPEG-2 Part 3) with additional bit rates and sample rates was published in 1995 (ISO/IEC 13818-3:1995).
^ 
^ 
^ 
^ a b 
^ a b c 
^ a b 
^ 
^ 
^ 
^ 
^ 
^ 
^""
""MS-DOS "",,,,""MS-DOS (/ˌɛmɛsˈdɒs/ EM-es-DOSS; acronym for Microsoft Disk Operating System) is an operating system for x86-based personal computers mostly developed by Microsoft. It was the most commonly used member of the DOS family of operating systems, and was the main operating system for IBM PC compatible personal computers during the 1980s to the mid-1990s, when it was gradually superseded by operating systems offering a graphical user interface (GUI), in various generations of the graphical Microsoft Windows operating system by Microsoft Corporation.
MS-DOS resulted from a request in 1981 by IBM for an operating system to use in its IBM PC range of personal computers. Microsoft quickly bought the rights to 86-DOS from Seattle Computer Products, and began work on modifying it to meet IBM's specification. IBM licensed and released it in August 1981 as PC DOS 1.0 for use in their PCs. Although MS-DOS and PC DOS were initially developed in parallel by Microsoft and IBM, in subsequent years the two products diverged and became different from each other, with recognizable differences in compatibility, syntax, and capabilities.
During its life, several competing products were released for the x86 platform, and MS-DOS went through eight versions, until development ceased in 2000. Initially MS-DOS was targeted at Intel 8086 processors running on computer hardware using floppy disks to store and access not only the operating system, but application software and user data as well. Progressive version releases delivered support for other mass storage media in ever greater sizes and formats, along with added feature support for newer processors and rapidly evolving computer architectures. Ultimately it was the key product in Microsoft's growth from a programming languages company to a diverse software development firm, providing the company with essential revenue and marketing resources. It was also the underlying basic operating system on which early versions of Windows ran as a GUI. It is a flexible operating system, and consumes negligible installation space.

""
""MySQL "",,,,""MySQL (officially pronounced as /maɪ ˌɛskjuːˈɛl/ "My S-Q-L",) is an open-source relational database management system (RDBMS); in July 2013, it was the world's second most widely used RDBMS, and the most widely used open-source client–server model RDBMS. It is named after co-founder Michael Widenius's daughter, My. The SQL abbreviation stands for Structured Query Language. The MySQL development project has made its source code available under the terms of the GNU General Public License, as well as under a variety of proprietary agreements. MySQL was owned and sponsored by a single for-profit firm, the Swedish company MySQL AB, now owned by Oracle Corporation. For proprietary use, several paid editions are available, and offer additional functionality.
MySQL is a popular choice of database for use in web applications, and is a central component of the widely used LAMP open-source web application software stack (and other "AMP" stacks). LAMP is an acronym for "Linux, Apache, MySQL, Perl/PHP/Python". Free-software open-source projects that require a full-featured database management system often use MySQL. Applications that use the MySQL database include: TYPO3, MODx, Joomla, WordPress, phpBB, MyBB, Drupal and other software. MySQL is also used in many high-profile, large-scale websites, including Google (though not for searches), Facebook, Twitter, Flickr, and YouTube.
On all platforms except Windows, MySQL ships with no GUI tools to administer MySQL databases or manage data contained within the databases. Users may use the included command line tools, or install MySQL Workbench via a separate download. Many third party GUI tools are also available.""
""NetWare "",,,,""NetWare is a computer network operating system developed by Novell, Inc. It initially used cooperative multitasking to run various services on a personal computer, using the IPX network protocol. The final update release was version 6.5SP8 of May 2009; Netware is no longer on Novell's product list. NetWare 6.5SP8 General Support ended in 2010, with Extended Support until the end of 2015, and Self Support until the end of 2017. The replacement is Open Enterprise Server.
The original NetWare product in 1983, supported clients running both CP/M and MS-DOS, ran over a proprietary star network topology and was based on a Novell-built file server using the Motorola 68000 processor, but the company soon moved away from building its own hardware, and NetWare became hardware-independent, running on any suitable Intel-based IBM PC compatible system, and a wide range of network cards. From the beginning NetWare implemented a number of features inspired by mainframe and minicomputer systems that were not available in its competitors.
In the early 1990s, Novell introduced separate cheaper networking products, unrelated to classic NetWare. These were NetWare Lite 1.0 (NWL), and later Personal NetWare 1.0 (PNW) in 1993.
In 1993, the main product line took a dramatic turn when Version 4 introduced NetWare Directory Services (NDS), a global directory service similar to the Active Directory that Microsoft would release seven years later. This, along with a new e-mail system, GroupWise, application configuration suite, ZENworks, and security product BorderManager were all targeted at the needs of large enterprises.
By 2000, however, Microsoft was taking more of Novell's customer base and Novell increasingly looked to a future based on a Linux kernel. The successor to NetWare, Open Enterprise Server (OES), released in March 2005, offered all the services previously hosted by NetWare v6.5, but on a SUSE Linux Enterprise Server; the NetWare kernel remained an option until OES 11 in late 2011.""
""OFDM System "",,,,""Orthogonal frequency-division multiplexing (OFDM) is a method of encoding digital data on multiple carrier frequencies. OFDM has developed into a popular scheme for wideband digital communication, used in applications such as digital television and audio broadcasting, DSL Internet access, wireless networks, powerline networks, and 4G mobile communications.
OFDM is a frequency-division multiplexing (FDM) scheme used as a digital multi-carrier modulation method. A large number of closely spaced orthogonal sub-carrier signals are used to carry data on several parallel data streams or channels. Each sub-carrier is modulated with a conventional modulation scheme (such as quadrature amplitude modulation or phase-shift keying) at a low symbol rate, maintaining total data rates similar to conventional single-carrier modulation schemes in the same bandwidth.
The primary advantage of OFDM over single-carrier schemes is its ability to cope with severe channel conditions (for example, attenuation of high frequencies in a long copper wire, narrowband interference and frequency-selective fading due to multipath) without complex equalization filters. Channel equalization is simplified because OFDM may be viewed as using many slowly modulated narrowband signals rather than one rapidly modulated wideband signal. The low symbol rate makes the use of a guard interval between symbols affordable, making it possible to eliminate intersymbol interference (ISI) and utilize echoes and time-spreading (on analogue TV these are visible as ghosting and blurring, respectively) to achieve a diversity gain, i.e. a signal-to-noise ratio improvement. This mechanism also facilitates the design of single frequency networks (SFNs), where several adjacent transmitters send the same signal simultaneously at the same frequency, as the signals from multiple distant transmitters may be combined constructively, rather than interfering as would typically occur in a traditional single-carrier system.

""
""OpenMP "",,,,""OpenMP (Open Multi-Processing) is an application programming interface (API) that supports multi-platform shared memory multiprocessing programming in C, C++, and Fortran, on most platforms, processor architectures and operating systems, including Solaris, AIX, HP-UX, Linux, OS X, and Windows. It consists of a set of compiler directives, library routines, and environment variables that influence run-time behavior.
OpenMP is managed by the nonprofit technology consortium OpenMP Architecture Review Board (or OpenMP ARB), jointly defined by a group of major computer hardware and software vendors, including AMD, IBM, Intel, Cray, HP, Fujitsu, Nvidia, NEC, Red Hat, Texas Instruments, Oracle Corporation, and more.
OpenMP uses a portable, scalable model that gives programmers a simple and flexible interface for developing parallel applications for platforms ranging from the standard desktop computer to the supercomputer.
An application built with the hybrid model of parallel programming can run on a computer cluster using both OpenMP and Message Passing Interface (MPI), or more transparently through the use of OpenMP extensions for non-shared memory systems.""
""Oracle Database "",,,,""Oracle Database (commonly referred to as Oracle RDBMS or simply as Oracle) is an object-relational database management system produced and marketed by Oracle Corporation.
Larry Ellison and his two friends and former co-workers, Bob Miner and Ed Oates, started a consultancy called Software Development Laboratories (SDL) in 1977. SDL developed the original version of the Oracle software. The name Oracle comes from the code-name of a CIA-funded project Ellison had worked on while previously employed by Ampex.
^ Dietrich, Mike (22 July 2014). "Oracle Database 12.1.0.2 is available!!!". Retrieved 2 February 2015. 
^ Lextrait, Vincent (January 2010). "The Programming Languages Beacon, v10.0". Retrieved 14 March 2010. 
^ "What's New". Retrieved 29 November 2010. 
^ "Welcome to Larryland". Guardian. Retrieved 2009-12-19.""
""PDF "",,,,""The Portable Document Format (PDF) is a file format used to present documents in a manner independent of application software, hardware, and operating systems. Each PDF file encapsulates a complete description of a fixed-layout flat document, including the text, fonts, graphics, and other information needed to display it. In 1991, Adobe Systems' co-founder John Warnock outlined a system called "Camelot" that developed into PDF.
Adobe Systems made the PDF specification available free of charge in 1993. PDF was a proprietary format controlled by Adobe, until it was officially released as an open standard on July 1, 2008, and published by the International Organization for Standardization as ISO 32000-1:2008, at which time control of the specification passed to an ISO Committee of volunteer industry experts. In 2008, Adobe published a Public Patent License to ISO 32000-1 granting royalty-free rights for all patents owned by Adobe that are necessary to make, use, sell, and distribute PDF compliant implementations. However, there are still some proprietary technologies defined only by Adobe, such as Adobe XML Forms Architecture and JavaScript for Acrobat, which are referenced by ISO 32000-1 as normative and indispensable for the application of the ISO 32000-1 specification. These proprietary technologies are not standardized and their specification is published only on Adobe’s website. The ISO committee is actively standardizing many of these as part of ISO 32000-2.""
""Pentium "",,,,""Pentium is a brand used for a series of x86-compatible microprocessors produced by Intel since 1993. In its current form, Pentium processors are considered entry-level products that Intel rates as "two stars", meaning that they are above the low-end Atom and Celeron series but below the faster Core i3, i5 and i7 lines as well as the high-end Xeon processors.
The current Pentium processors have only the name in common with the early ones, and are in fact based on the Intel Core architecture, typically implemented by lowering the clock frequency and disabling some features, such as hyper-threading, virtualization and, partly, L3 cache.
The name Pentium is originally derived from the Greek word pente (πέντε), meaning "five" (a reference to Intel's fifth-generation microarchitecture employed on the first Pentium processors), with the Latin ending -ium.""
""Photoshop "",,,,""Adobe Photoshop is a raster graphics editor developed and published by Adobe Systems for Windows and OS X.
Photoshop was created in 1988 by Thomas and John Knoll. Since then, it has become the de facto industry standard in raster graphics editing, such that the word "photoshop" has become a verb as in "to Photoshop an image," "photo shopping" and "photoshop contest", though Adobe discourages such use. It can edit and compose raster images in multiple layers and supports masks, alpha compositing and several color models including RGB, CMYK, Lab color space, spot color and duotone. Photoshop has vast support for graphic file formats but also uses its own PSD and PSB file formats which support all the aforementioned features. In addition to raster graphics, it has limited abilities to edit or render text, vector graphics (especially through clipping path), 3D graphics and video. Photoshop's featureset can be expanded by Photoshop plug-ins, programs developed and distributed independently of Photoshop that can run inside it and offer new or enhanced features.
Photoshop's naming scheme was initially based on version numbers. However, in October 2002, following the introduction of Creative Suite branding, each new version of Photoshop was designated with "CS" plus a number; e.g., the eighth major version of Photoshop was Photoshop CS and the ninth major version was Photoshop CS2. Photoshop CS3 through CS6 were also distributed in two different editions: Standard and Extended. In June 2013, with the introduction of Creative Cloud branding, Photoshop's licensing scheme was changed to that of software as a service and the "CS" suffixes were replaced with "CC". Historically, Photoshop was bundled with additional software such as Adobe ImageReady, Adobe Fireworks, Adobe Bridge, Adobe Device Central and Adobe Camera RAW.
Alongside Photoshop, Adobe also develops and publishes Photoshop Elements, Photoshop Lightroom, Photoshop Express and Photoshop Touch. Collectively, they are branded as "The Adobe Photoshop Family". It is currently a licensed software.""
""Pocket PC "",,,,""A Pocket PC (P/PC, PPC), also known by Microsoft as a 'Windows Mobile Classic device', is a hardware specification for a handheld-sized smartphone or personal digital assistant (PDA), that runs the Windows Mobile operating system. It has some of the abilities of modern desktop PCs.
As of 2010, thousands of applications exist for handhelds adhering to the Microsoft Pocket PC specification, many of which are freeware. Some of these devices are also mobile phones. Microsoft-compliant Pocket PCs can be used with many add-ons such as GPS receivers, barcode readers, RFID readers, and cameras.
In 2007, with the advent of Windows Mobile 6, Microsoft dropped the name Pocket PC in favor of a new naming scheme:
Windows Mobile Classic (before, Pocket PC): devices without an integrated phone;
Windows Mobile Professional: devices with an integrated phone and a touch screen;
Windows Mobile Standard: devices without a touch screen.
But in 2010, even Windows Mobile devices were discontinued in favor of Windows Phone devices.""
""PowerPoint "",,,,""Microsoft PowerPoint is a slide show presentation program currently developed by Microsoft. PowerPoint initially named "Presenter", was created by Forethought Inc.. Microsoft's version of PowerPoint was officially launched on May 22, 1990, as a part of the Microsoft Office suite. PowerPoint is useful for helping develop the slide-based presentation format, and is currently one of the most commonly-used presentation programs available.""
""Prolog "",,,,""Prolog is a general purpose logic programming language associated with artificial intelligence and computational linguistics.
Prolog has its roots in first-order logic, a formal logic, and unlike many other programming languages, Prolog is declarative: the program logic is expressed in terms of relations, represented as facts and rules. A computation is initiated by running a query over these relations.
The language was first conceived by a group around Alain Colmerauer in Marseille, France, in the early 1970s and the first Prolog system was developed in 1972 by Colmerauer with Philippe Roussel.
Prolog was one of the first logic programming languages, and remains the most popular among such languages today, with several free and commercial implementations available. The language has been used for theorem proving, expert systems, as well as its original intended field of use, natural language processing. Modern Prolog environments support creating graphical user interfaces, as well as administrative and networked applications.
Prolog is well-suited for specific tasks that benefit from rule-based logical queries such as searching databases, voice control systems, and filling templates.""
""QuarkXPress "",,,,""QuarkXPress is a computer application for creating and editing complex page layouts in a WYSIWYG (What You See Is What You Get) environment. It runs on Mac OS X and Windows. It was first released by Quark, Inc. in 1987 and is still owned and published by them.
The most recent version, QuarkXPress 2015 (internal version number 11.0), allows publishing in English ("International and U.S.") and 36 other languages, including Arabic, Chinese, Japanese, Portuguese, German, Korean, Russian, French, and Spanish.
QuarkXPress is used by individual designers and large publishing houses to produce a variety of layouts, from single-page flyers to the multi-media projects required for magazines, newspapers, catalogs, and the like. QuarkXPress once dominated the market for page layout software, with over 95% market share among professional users. As of 2010, one publisher estimated that US market share has fallen to below 25% and Adobe InDesign has become the market leader, although QuarkXPress still had significant marketshare.""
""QuickBooks "",,,,""QuickBooks is an accounting software package developed and marketed by Intuit. QuickBooks products are geared mainly toward small and medium-sized businesses and offer on-premises accounting applications as well as cloud based versions that accept business payments, manage and pay bills, and payroll functions.""
""Quicken "",,,,""Quicken is a personal finance management tool developed by Intuit, Inc. On March 3, 2016, Intuit announced plans to sell Quicken to H.I.G. Capital. Terms of the sale were not disclosed.
Different (and incompatible) versions of Quicken run on Windows and Macintosh systems. Previous versions ran on DOS. There are several versions of Quicken for Windows, including Quicken Starter, Quicken Deluxe, Quicken Rental Property Manager, Quicken Premier, and Quicken Home & Business, as well as Quicken for Mac. Since 2008, each version has tended to have the release year in the product name (e.g., Quicken Basic 2008); before then, versions were numbered (e.g., Quicken 8 for DOS).
Quicken's major marketplace is North America, and most of the software sold is specialized for the United States and Canadian marketplace and user base. But the core functions can often be used more widely, regardless of country; and versions have been tailored for a variety of marketplaces, including Australia, Germany, Hong Kong, India, New Zealand, the Philippines, and Singapore. Development of the UK-specific version of Quicken was stopped in January 2005, with sales and support ending shortly afterwards. There were also versions for Argentina, Brazil, Chile, Colombia, Costa Rica, Denmark, Ecuador, France, Mexico, the Netherlands, Spain, Sweden, Switzerland, Uruguay, and Venezuela.""
""QuickTime "",,,,""QuickTime is an extensible multimedia framework developed by Apple Inc., capable of handling various formats of digital video, picture, sound, panoramic images, and interactivity. The classic version of QuickTime is available for Windows Vista and later, as well as Mac OS X Leopard and later operating systems. A more recent version, QuickTime X, is currently available on Mac OS X Snow Leopard and newer.
As of the Mac OS X Lion, the underlying media framework for Quicktime, QTKit, is deprecated in favor of a newer graphics framework, AV Foundation. The QuickTime X player however is still included with the new releases of the OS.""
""Ruby on Rails "",,,,""Ruby on Rails, or simply Rails, is a web application framework written in Ruby under MIT License. Rails is a model–view–controller (MVC) framework, providing default structures for a database, a web service, and web pages. It encourages and facilitates the use of web standards such as JSON or XML for data transfer, and HTML, CSS and JavaScript for display and user interfacing. In addition to MVC, Rails emphasizes the use of other well-known software engineering patterns and paradigms, including convention over configuration (CoC), don't repeat yourself (DRY), and the active record pattern.""
""Samba "",,,,""Samba (Portuguese pronunciation: [ˈsɐ̃bɐ]) is a Brazilian musical genre and dance style, with its roots in Africa via the West African slave trade and African religious traditions, particularly Angola and the Congo. Although there were various forms of samba in Brazil in the form of various popular rhythms and regional dances that originated from the drumming, samba as music genre is seen as a musical expression of urban Rio de Janeiro, then the capital of Imperial Brazil.
It is recognized around the world as a symbol of Brazil and the Brazilian Carnival. Considered one of the most popular Brazilian cultural expressions, samba has become an icon of Brazilian national identity. The Bahian Samba de Roda (dance circle), which became a UNESCO Heritage of Humanity in 2005, is the main root of the samba carioca, the samba that is played and danced in Rio de Janeiro.

The modern samba that emerged at the beginning of the 20th century is predominantly in a 2/4 tempo varied with the conscious use of a sung chorus to a batucada rhythm, with various stanzas of declaratory verses. Traditionally, the samba is played by strings (cavaquinho and various types of guitar) and various percussion instruments such as tamborim. Influenced by American orchestras in vogue since the Second World War and the cultural impact of US music post-war, samba began to use trombones, trumpets, choros, flutes, and clarinets.
In addition to distinct rhythms and meters, samba brings a whole historical culture of food, varied dances (miudinho, coco, samba de roda, and pernada), parties, clothes such as linen shirts, and the Naif painting of established names such as Nelson Sargento, Guilherme de Brito, and Heitor dos Prazeres. Anonymous community artists, including painters, sculptors, designers, and stylists, make the clothes, costumes, carnival floats, and cars, opening the doors of schools of samba. There is also a great tradition of ballroom samba in Brazil, with many styles. Samba de Gafieira is the style more famous in Rio de Janeiro, where common people used to go to the gafieira parties since the 1930s, and where the moves and identity of this dance has emerged, getting more and more different from its African, European, Argentinian and Cuban origins and influences.
The Samba National Day is celebrated on December 2. The date was established at the initiative of Luis Monteiro da Costa, an Alderman of Salvador, in honor of Ary Barroso. He composed "Na Baixa do Sapateiro" even though he had never been in Bahia. Thus 2 December marked the first visit of Ary Barroso to Salvador. Initially, this day was celebrated only in Salvador, but eventually it turned into a national holiday.
Samba is a local style in Southeastern Brazil and Northeast Brazil, especially in Rio de Janeiro, São Paulo, Salvador and Recife. Its importance as Brazil's national music transcends region, however; samba schools, samba musicians and carnival organizations centered around the performance of samba exist in every region of the country, even though other musical styles prevail in various regions (for instance, in Southern Brazil, Center-West Brazil, and all of the Brazilian countryside, Sertanejo, or Brazilian country music, is the most popular style). Since Rio de Janeiro is the most popular Brazilian city worldwide, usually samba is used to identify Brazilians as part of the same national culture.""
""SharePoint "",,,,""SharePoint is a web application platform in the Microsoft Office server suite. Launched in 2001, SharePoint combines various functions which are traditionally separate applications: intranet, extranet, content management, document management, personal cloud, enterprise social networking, enterprise search, business intelligence, workflow management, web content management, and an enterprise application store. SharePoint servers have traditionally been deployed for internal use in mid-size businesses and large departments alongside Microsoft Exchange, Skype for Business, and Office Web Apps. However, Microsoft's 'Office 365' software as a service offering (which includes a version of SharePoint) has led to increased usage of SharePoint in smaller organizations.
While Office 365 provides SharePoint as a service, installing SharePoint on premises typically requires multiple virtual machines, at least two separate physical servers, and is a somewhat significant installation and configuration effort. The software is based on an n-tier service oriented architecture. Enterprise application software (for example, email servers, ERP, BI and CRM products) often either requires or integrates with elements of SharePoint. As an application platform, SharePoint provides central management, governance, and security controls. The SharePoint platform manages Internet Information Services (IIS) via form-based management tooling.
Since the release of SharePoint 2013, Microsoft's primary channel for distribution of SharePoint has been Office 365, where the product is continuously being upgraded. New versions are released every few years, and represent a supported snapshot of the cloud software. Microsoft currently has three tiers of pricing for SharePoint 2013, including a free version (whose future is currently uncertain). SharePoint 2013 is also resold through a cloud model by many third-party vendors. The next on-premises release is SharePoint 2016, expected to have increased hybrid cloud integration.""
""Simulink "",,,,""Simulink, developed by MathWorks, is a graphical programming environment for modeling, simulating and analyzing multidomain dynamic systems. Its primary interface is a graphical block diagramming tool and a customizable set of block libraries. It offers tight integration with the rest of the MATLAB environment and can either drive MATLAB or be scripted from it. Simulink is widely used in automatic control and digital signal processing for multidomain simulation and Model-Based Design.""
""Smalltalk "",,,,""Smalltalk is an object-oriented, dynamically typed, reflective programming language. Smalltalk was created as the language to underpin the "new world" of computing exemplified by "human–computer symbiosis." It was designed and created in part for educational use, more so for constructionist learning, at the Learning Research Group (LRG) of Xerox PARC by Alan Kay, Dan Ingalls, Adele Goldberg, Ted Kaehler, Scott Wallace, and others during the 1970s.
The language was first generally released as Smalltalk-80. Smalltalk-like languages are in continuing active development and have gathered loyal communities of users around them. ANSI Smalltalk was ratified in 1998 and represents the standard version of Smalltalk.""
""Short Message Service "",,,,""Short Message Service (SMS) is a text messaging service component of phone, Web, or mobile communication systems. It uses standardized communications protocols to allow fixed line or mobile phone devices to exchange short text messages.
SMS was the most widely used data application, with an estimated 3.5 billion active users, or about 80% of all mobile phone subscribers at the end of 2010. The term "SMS" is used for both the user activity and all types of short text messaging in many parts of the world. SMS is also employed in direct marketing, known as SMS marketing. As of September 2014, global SMS messaging business is said to be worth over USD 100 billion, and SMS accounts for almost 50 percent of all the revenue generated by mobile messaging.
SMS as used on modern handsets originated from radio telegraphy in radio memo pagers using standardized phone protocols. These were defined in 1985 as part of the Global System for Mobile Communications (GSM) series of standards as a means of sending messages of up to 160 characters to and from GSM mobile handsets. Though most SMS messages are mobile-to-mobile text messages, support for the service has expanded to include other mobile technologies, such as ANSI CDMA networks and Digital AMPS, as well as satellite and landline networks.""
""SPARC "",,,,""SPARC (from "scalable processor architecture") is a RISC instruction set architecture (ISA) developed by Sun Microsystems and introduced in mid-1987.
SPARC is a registered trademark of SPARC International, Inc., an organization established in 1989 to promote the SPARC architecture, manage SPARC trademarks, and provide conformance testing. Implementations of the original 32-bit SPARC architecture were initially designed and used in Sun's Sun-4 workstation and server systems, replacing their earlier Sun-3 systems based on the Motorola 68000 series of processors. Later, SPARC processors were used in SMP and CC-NUMA servers produced by Sun, Solbourne and Fujitsu, among others, and designed for 64-bit operation.
SPARC International was intended to open the SPARC architecture to make a larger ecosystem for the design, which has been licensed to several manufacturers, including Texas Instruments, Atmel, Cypress Semiconductor, and Fujitsu. As a result of SPARC International, the SPARC architecture is fully open, non-proprietary and royalty free.
In March 2006, the complete design of Sun's UltraSPARC T1 microprocessor was released in open-source form at OpenSPARC.net and named the OpenSPARC T1. In 2007 the design of Sun's UltraSPARC T2 microprocessor was also released in open-source form as OpenSPARC T2.
The most recent commercial iterations of the SPARC processor design are the Fujitsu Laboratories Ltd.'s 34 core SPARC64 XIfx 2.2 GHz of 1.1 TFLOPS introduced 2015, which is used in the supercomputer PRIMEHPC FX100; the Fujitsu Laboratories Ltd.'s 16 core SPARC64 X+ introduced in 2014, running at 3.2 GHz; the 32 core SPARC M7 introduced by Oracle Corporation in October 2015 running at 4.133 GHz.""
""Secure Sockets Layer "",,,,""Transport Layer Security (TLS) and its predecessor, Secure Sockets Layer (SSL), both of which are frequently referred to as 'SSL', are cryptographic protocols designed to provide communications security over a computer network. Several versions of the protocols are in widespread use in applications such as web browsing, email, Internet faxing, instant messaging, and voice-over-IP (VoIP). Major web sites use TLS to secure all communications between their servers and web browsers.
The primary goal of the TLS protocol is to provide privacy and data integrity between two communicating computer applications. When secured by TLS, connections between a client (e.g. a web browser) and a server (e.g. wikipedia.org) will have one or more of the following properties:
The connection is private because symmetric cryptography is used to encrypt the data transmitted. The keys for this symmetric encryption are generated uniquely for each connection and are based on a secret negotiated at the start of the session (see TLS handshake protocol). The server and client negotiate the details of which encryption algorithm and cryptographic keys to use before the first byte of data is transmitted (see Algorithm). The negotiation of a shared secret is both secure (the negotiated secret is unavailable to eavesdroppers and cannot be obtained, even by an attacker who places himself in the middle of the connection) and reliable (no attacker can modify the communications during the negotiation without being detected).
The identity of the communicating parties can be authenticated using public-key cryptography. This authentication can be made optional, but is generally required for at least one of the parties (typically the server).
The connection is reliable because each message transmitted includes a message integrity check using a message authentication code to prevent undetected loss or alteration of the data during transmission.
In addition to the properties above, careful configuration of TLS can provide additional privacy-related properties such as forward secrecy, ensuring that any future disclosure of encryption keys cannot be used to decrypt any TLS communications recorded in the past.
TLS supports many different methods for exchanging keys, encrypting data, and authenticating message integrity (see Algorithm). As a result, secure configuration of TLS involves many configurable parameters, and not all choices provide all of the privacy-related properties described in the list above (see authentication and key exchange table, cipher security table, and data integrity table).
Attempts have been made to subvert aspects of the communications security that TLS seeks to provide and the protocol has been revised several times to address these security threats (see Security). Web browsers have also been revised by their developers to defend against potential security weaknesses after these were discovered (see TLS/SSL support history of web browsers.)
The TLS protocol is composed of two layers: the TLS record protocol and the TLS handshake protocol.
TLS is a proposed Internet Engineering Task Force (IETF) standard, first defined in 1999 and updated in RFC 5246 (August 2008) and RFC 6176 (March 2011). It is based on the earlier SSL specifications (1994, 1995, 1996) developed by Netscape Communications for adding the HTTPS protocol to their Navigator web browser.

""
""Skype "",,,,""Skype (/ˈskaɪp/) is an application that provides video chat and voice call services. Users may exchange such digital documents as images, text, video and any others, and may transmit both text and video messages. Skype allows the creation of video conference calls. Skype is available for Microsoft Windows, Macintosh, or Linux, as well as Android, Blackberry, and both Apple and Windows smartphones and tablets. Skype is based on a freemium model. Much of the service is free, but Skype Credit or a subscription is required to call a landline or a mobile phone number. At the end of 2010, there were over 660 million worldwide users, with over 300 million estimated active each month as of August 2015. At one point in February 2012, there were thirty four million users concurrently online on Skype.
First released in August 2003, Skype was created by Swedish Niklas Zennström and Danish Janus Friis, in cooperation with Ahti Heinla, Priit Kasesalu, and Jaan Tallinn, Estonians who developed the backend that was also used in the music-sharing application Kazaa. In September 2005, eBay acquired Skype for $2.6 billion. In September 2009, Silver Lake, Andreessen Horowitz and the Canada Pension Plan Investment Board announced the acquisition of 65% of Skype for $1.9 billion from eBay, which attributed to the enterprise a market value of $2.92 billion. Microsoft bought Skype in May 2011 for $8.5 billion. Its Skype division headquarters are in Luxembourg, but most of the development team and 44% of all the division's employees are still situated in Tallinn and Tartu, Estonia.
Skype allows users to communicate over the Internet by voice using a microphone, by video by using a webcam, as well as with instant messaging. Skype-to-Skype calls to other users are free of charge, while calls to landline telephones and mobile phones (over traditional telephone networks) are charged via a debit-based user account system called Skype Credit. Some network administrators have banned Skype on corporate, government, home, and education networks, citing such reasons as inappropriate usage of resources, excessive bandwidth usage, and security concerns.
Skype originally featured a hybrid peer-to-peer and client–server system. Skype has been powered entirely by Microsoft-operated supernodes since May 2012. The 2013 mass surveillance disclosures revealed that Microsoft had granted intelligence agencies unfettered access to supernodes and Skype communication content.""
""Transport Layer Security "",,,,""Transport Layer Security (TLS) and its predecessor, Secure Sockets Layer (SSL), both of which are frequently referred to as 'SSL', are cryptographic protocols designed to provide communications security over a computer network. Several versions of the protocols are in widespread use in applications such as web browsing, email, Internet faxing, instant messaging, and voice-over-IP (VoIP). Major web sites use TLS to secure all communications between their servers and web browsers.
The primary goal of the TLS protocol is to provide privacy and data integrity between two communicating computer applications. When secured by TLS, connections between a client (e.g. a web browser) and a server (e.g. wikipedia.org) will have one or more of the following properties:
The connection is private because symmetric cryptography is used to encrypt the data transmitted. The keys for this symmetric encryption are generated uniquely for each connection and are based on a secret negotiated at the start of the session (see TLS handshake protocol). The server and client negotiate the details of which encryption algorithm and cryptographic keys to use before the first byte of data is transmitted (see Algorithm). The negotiation of a shared secret is both secure (the negotiated secret is unavailable to eavesdroppers and cannot be obtained, even by an attacker who places himself in the middle of the connection) and reliable (no attacker can modify the communications during the negotiation without being detected).
The identity of the communicating parties can be authenticated using public-key cryptography. This authentication can be made optional, but is generally required for at least one of the parties (typically the server).
The connection is reliable because each message transmitted includes a message integrity check using a message authentication code to prevent undetected loss or alteration of the data during transmission.
In addition to the properties above, careful configuration of TLS can provide additional privacy-related properties such as forward secrecy, ensuring that any future disclosure of encryption keys cannot be used to decrypt any TLS communications recorded in the past.
TLS supports many different methods for exchanging keys, encrypting data, and authenticating message integrity (see Algorithm). As a result, secure configuration of TLS involves many configurable parameters, and not all choices provide all of the privacy-related properties described in the list above (see authentication and key exchange table, cipher security table, and data integrity table).
Attempts have been made to subvert aspects of the communications security that TLS seeks to provide and the protocol has been revised several times to address these security threats (see Security). Web browsers have also been revised by their developers to defend against potential security weaknesses after these were discovered (see TLS/SSL support history of web browsers.)
The TLS protocol is composed of two layers: the TLS record protocol and the TLS handshake protocol.
TLS is a proposed Internet Engineering Task Force (IETF) standard, first defined in 1999 and updated in RFC 5246 (August 2008) and RFC 6176 (March 2011). It is based on the earlier SSL specifications (1994, 1995, 1996) developed by Netscape Communications for adding the HTTPS protocol to their Navigator web browser.

""
""Twitter "",,,,""Twitter (/ˈtwɪtər/) is an online social networking service that enables users to send and read short 140-character messages called "tweets".
Registered users can read and post tweets, but those who are unregistered can only read them. Users access Twitter through the website interface, SMS or mobile device app. Twitter Inc. is based in San Francisco and has more than 25 offices around the world.
Twitter was created in March 2006 by Jack Dorsey, Evan Williams, Biz Stone, and Noah Glass and launched in July 2006. The service rapidly gained worldwide popularity, with more than 100 million users posting 340 million tweets a day in 2012. The service also handled 1.6 billion search queries per day. In 2013, Twitter was one of the ten most-visited websites and has been described as "the SMS of the Internet". As of May 2015, Twitter has more than 500 million users, out of which more than 332 million are active.""
""UMTS "",,,,""The Universal Mobile Telecommunications System (UMTS) is a third generation mobile cellular system for networks based on the GSM standard. Developed and maintained by the 3GPP (3rd Generation Partnership Project), UMTS is a component of the International Telecommunications Union IMT-2000 standard set and compares with the CDMA2000 standard set for networks based on the competing cdmaOne technology. UMTS uses wideband code division multiple access (W-CDMA) radio access technology to offer greater spectral efficiency and bandwidth to mobile network operators.
UMTS specifies a complete network system, which includes the radio access network (UMTS Terrestrial Radio Access Network, or UTRAN), the core network (Mobile Application Part, or MAP) and the authentication of users via SIM (subscriber identity module) cards.
The technology described in UMTS is sometimes also referred to as Freedom of Mobile Multimedia Access (FOMA) or 3GSM.
Unlike EDGE (IMT Single-Carrier, based on GSM) and CDMA2000 (IMT Multi-Carrier), UMTS requires new base stations and new frequency allocations.""
""Usenet "",,,,""Usenet is a worldwide distributed discussion system. It was developed from the general-purpose UUCP dial-up network architecture. Tom Truscott and Jim Ellis conceived the idea in 1979, and it was established in 1980. Users read and post messages (called articles or posts, and collectively termed news) to one or more categories, known as newsgroups. Usenet resembles a bulletin board system (BBS) in many respects and is the precursor to Internet forums that are widely used today. Usenet can be superficially regarded as a hybrid between email and web forums. Discussions are threaded, as with web forums and BBSs, though posts are stored on the server sequentially.
One notable difference between a BBS or web forum and Usenet is the absence of a central server and dedicated administrator. Usenet is distributed among a large, constantly changing conglomeration of servers that store and forward messages to one another in so-called news feeds. Individual users may read messages from and post messages to a local server operated by a commercial usenet provider, their Internet service provider, university, employer, or their own server.""
""VHDL "",,,,""VHDL (VHSIC Hardware Description Language) is a hardware description language used in electronic design automation to describe digital and mixed-signal systems such as field-programmable gate arrays and integrated circuits. VHDL can also be used as a general purpose parallel programming language.""
""VMware "",,,,""VMware, Inc. is an American company that provides cloud and virtualization software and services, and claims to be the first to successfully virtualize the x86 architecture commercially. Founded in 1998, VMware is based in Palo Alto, California. In 2004 it was acquired by and became a subsidiary of EMC Corporation, then on August 14, 2007, EMC sold 15% of the company in a New York Stock Exchange IPO. The company trades under the symbol VMW.
VMware's desktop software runs on Microsoft Windows, Linux, and Mac OS X, while its enterprise software hypervisors for servers, VMware ESX and VMware ESXi, are bare-metal hypervisors that run directly on server hardware without requiring an additional underlying operating system.
^ "Form 10-K, Annual Report for Fiscal Year ended December 31, 2014" (PDF). VMware.com. 
^ "VMware leader in virtualization market". 
^ Lohr, Steve (2009-08-31). "VMware market share more than 80%". The New York Times. Retrieved 2010-05-27. 
^ "VMware, Hyper-V virtualization leave others in the dust". 
^ "Understanding full virtualization, paravirtualization, and hardware assist" (PDF). 2007-10-15. Retrieved 2014-12-11. 
^ "Investor FAQs". Retrieved 2014-12-11. 
^ "ESX Server Architecture". Vmware.com. Archived from the original on January 31, 2009. Retrieved 2009-10-22.""
""VoiceXML "",,,,""VoiceXML (VXML) is a digital document standard for specifying interactive media and voice dialogs between humans and computers. It is used for developing audio and voice response applications, such as banking systems and automated customer service portals. VoiceXML applications are developed and deployed in a manner analogous to how a web browser interprets and visually renders the Hypertext Markup Language (HTML) it receives from a web server. VoiceXML documents are interpreted by a voice browser and in common deployment architectures, users interact with voice browsers via the public switched telephone network (PSTN).
The VoiceXML document format is based on Extensible Markup Language (XML). It is a standard developed by the World Wide Web Consortium (W3C).""
""VRML "",,,,""VRML (Virtual Reality Modeling Language, pronounced vermal or by its initials, originally—before 1995—known as the Virtual Reality Markup Language) is a standard file format for representing 3-dimensional (3D) interactive vector graphics, designed particularly with the World Wide Web in mind. It has been superseded by X3D.
^ Paul Festa and John Borland (May 19, 2005). "Is a 3D web more than just empty promises?". CNET News.com.""
""Wifi "",,,,""Wi-Fi or WiFi is a technology that allows electronic devices to connect to a wireless LAN (WLAN) network, mainly using the 2.4 gigahertz (12 cm) UHF and 5 gigahertz (6 cm) SHF ISM radio bands. Access to a WLAN is usually password protected, but may be open, which allows any device within its range to access the resources of the WLAN network.
The Wi-Fi Alliance defines Wi-Fi as any "wireless local area network" (WLAN) product based on the Institute of Electrical and Electronics Engineers' (IEEE) 802.11 standards. However, the term "Wi-Fi" is used in general English as a synonym for "WLAN" since most modern WLANs are based on these standards. "Wi-Fi" is a trademark of the Wi-Fi Alliance. The "Wi-Fi Certified" trademark can only be used by Wi-Fi products that successfully complete Wi-Fi Alliance interoperability certification testing.
Devices which can use Wi-Fi technology include personal computers, video-game consoles, smartphones, digital cameras, tablet computers and digital audio players. Wi-Fi compatible devices can connect to the Internet via a WLAN network and a wireless access point. Such an access point (or hotspot) has a range of about 20 meters (66 feet) indoors and a greater range outdoors. Hotspot coverage can be as small as a single room with walls that block radio waves, or as large as many square kilometres achieved by using multiple overlapping access points.

Wi-Fi is less secure than wired connections, such as Ethernet, precisely because an intruder does not need a physical connection. Web pages that use TLS are secure, but unencrypted internet access can easily be detected by intruders. Because of this, Wi-Fi has adopted various encryption technologies. The early encryption WEP proved easy to break. Higher quality protocols (WPA, WPA2) were added later. An optional feature added in 2007, called Wi-Fi Protected Setup (WPS), had a serious flaw that allowed an attacker to recover the router's password. The Wi-Fi Alliance has since updated its test plan and certification program to ensure all newly certified devices resist attacks.""
""Wikipedia "",,,,""Wikipedia (/ˌwɪkᵻˈpiːdiə/ or /ˌwɪkiˈpiːdiə/ WIK-i-PEE-dee-ə) is a free-access, free-content Internet encyclopedia, supported and hosted by the non-profit Wikimedia Foundation. Those who can access the site can edit most of its articles. Wikipedia is ranked among the ten most popular websites, and constitutes the Internet's largest and most popular general reference work.
Jimmy Wales and Larry Sanger launched Wikipedia on January 15, 2001. Sanger coined its name, a portmanteau of wiki and encyclopedia. Initially only in English, Wikipedia quickly became multilingual as it developed similar versions in other languages, which differ in content and in editing practices. The English Wikipedia is now one of 291 Wikipedia editions and is the largest with 5,116,887 articles (having reached 5,000,000 articles in November 2015). There is a grand total, including all Wikipedias, of over 38 million articles in over 250 different languages. As of February 2014, it had 18 billion page views and nearly 500 million unique visitors each month.
A peer review of 42 science articles found in both Encyclopædia Britannica and Wikipedia was published in Nature in 2005, and found that Wikipedia's level of accuracy approached Encyclopedia Britannica's. Criticisms of Wikipedia include claims that it exhibits systemic bias, presents a mixture of "truths, half truths, and some falsehoods", and that in controversial topics it is subject to manipulation and spin.""
""Windows "",,,,""Microsoft Windows (or simply Windows) is a metafamily of graphical operating systems developed, marketed, and sold by Microsoft. It consists of several families of operating systems, each of which cater to a certain sector of the computing industry. Active Windows families include Windows NT, Windows Embedded and Windows Phone; these may encompass subfamilies, e.g. Windows Embedded Compact (Windows CE) or Windows Server. Defunct Windows families include Windows 9x and Windows Mobile.
Microsoft introduced an operating environment named Windows on November 20, 1985, as a graphical operating system shell for MS-DOS in response to the growing interest in graphical user interfaces (GUIs). Microsoft Windows came to dominate the world's personal computer market with over 90% market share, overtaking Mac OS, which had been introduced in 1984. However, since 2012, because of the massive growth of smartphones, Windows sells less than Android, which became the most popular operating system in 2014, when counting all of the computing platforms each operating system runs on; in 2014, the number of Windows devices sold were less than 25% of Android devices sold. However, comparisons across different markets are not fully relevant; and for personal computers, Windows is still the most popular operating system.
As of March 2016, the most recent version of Windows for personal computers, tablets, smartphones and embedded devices is Windows 10. The most recent versions for server computers is Windows Server 2012 R2. A specialized version of Windows runs on the Xbox One game console.""
""Windows environment "",,,,""Windows Preinstallation Environment (also known as Windows PE and WinPE) is a lightweight version of Windows used for the deployment of PCs, workstations, and servers, or troubleshooting an operating system while it is offline. It is intended to replace MS-DOS boot disks and can be booted via USB flash drive, PXE, iPXE, CD-ROM, or hard disk. Traditionally used by large corporations and OEMs (to preinstall Windows client operating systems on PCs during manufacturing), it is now widely available free of charge via the Windows Automated Installation Kit (WAIK).
^ "Network-booting Windows PE". Retrieved 2012-09-18.""
""WordNet "",,,,""WordNet is a lexical database for the English language. It groups English words into sets of synonyms called synsets, provides short definitions and usage examples, and records a number of relations among these synonym sets or their members. WordNet can thus be seen as a combination of dictionary and thesaurus. While it is accessible to human users via a web browser, its primary use is in automatic text analysis and artificial intelligence applications. The database and software tools have been released under a BSD style license and are freely available for download from the WordNet website. Both the lexicographic data (lexicographer files) and the compiler (called grind) for producing the distributed database are available.""
""X-Window "",,,,""The X Window System (X11, or shortened to simply X, and sometimes informally X-Windows) is a windowing system for bitmap displays, common on UNIX-like computer operating systems.
X provides the basic framework for a GUI environment: drawing and moving windows on the display device and interacting with a mouse and keyboard. X does not mandate the user interface — this is handled by individual programs. As such, the visual styling of X-based environments varies greatly; different programs may present radically different interfaces.
X originated at the Massachusetts Institute of Technology (MIT) in 1984. The protocol has been version 11 (hence "X11") since September 1987. The X.Org Foundation leads the X project, with the current reference implementation, X.Org Server, available as free and open source software under the MIT License and similar permissive licenses.""
""X3D "",,,,""X3D should not be confused with 3DXML, which is a proprietary 3D file format.
X3D is a royalty-free ISO standard XML-based file format for representing 3D computer graphics. It is successor to the Virtual Reality Modeling Language (VRML). X3D features extensions to VRML (e.g. CAD, Geospatial, Humanoid animation, NURBS etc.), the ability to encode the scene using an XML syntax as well as the Open Inventor-like syntax of VRML97, or binary formatting, and enhanced application programming interfaces (APIs).
The X3D extension supports multi-stage and multi-texture rendering; it also supports shading with lightmap and normalmap. Starting in 2010, X3D has supported deferred rendering architecture. Now X3D can import SSAO, CSM and Realtime Environment Reflection/Lighting. The user can also use optimizations including BSP/QuadTree/OctTree or culling in the X3D scene.
X3D can work with other open source standards including XML, DOM and XPath.""
""XILINX "",,,,""Xilinx, Inc. (/ˈzaɪlɪŋks/ ZY-lingks) is an American technology company, primarily a supplier of programmable logic devices. It is known for inventing the field-programmable gate array (FPGA) and as the first semiconductor company with a fabless manufacturing model.
Founded in Silicon Valley in 1984, the company is headquartered in San Jose, California, with additional offices in Longmont, Colorado; Dublin, Ireland; Singapore; Hyderabad, India; Beijing, China; Shanghai, China; Brisbane, Australia and Tokyo, Japan.
Major FPGA product families include Virtex (high-performance), Kintex (mid-range) and Artix (low-cost), and the retired Spartan (low-cost) series. Major computer software includes Xilinx ISE and Vivado Design Suite.""
""YouTube "",,,,""YouTube is a video-sharing website headquartered in San Bruno, California, United States. The service was created by three former PayPal employees in February 2005. In November 2006, it was bought by Google for US$1.65 billion. YouTube now operates as one of Google's subsidiaries. The site allows users to upload, view, rate, share, and comment on videos, and it makes use of WebM, H.264/MPEG-4 AVC, and Adobe Flash Video technology to display a wide variety of user-generated and corporate media video. Available content includes video clips, TV clips, music videos, movie trailers, and other content such as video blogging, short original videos, and educational videos.
Most of the content on YouTube has been uploaded by individuals, but media corporations including CBS, the BBC, Vevo, Hulu, and other organizations offer some of their material via YouTube, as part of the YouTube partnership program. Unregistered users can watch videos, and registered users can upload videos to their channels. Videos considered to contain potentially offensive content are available only to registered users affirming themselves to be at least 18 years old.""
""ZigBee "",,,,""ZigBee is an IEEE 802.15.4-based specification for a suite of high-level communication protocols used to create personal area networks with small, low-power digital radios.
The technology defined by the ZigBee specification is intended to be simpler and less expensive than other wireless personal area networks (WPANs), such as Bluetooth or Wi-Fi. Applications include wireless light switches, electrical meters with in-home-displays, traffic management systems, and other consumer and industrial equipment that requires short-range low-rate wireless data transfer.
Its low power consumption limits transmission distances to 10–100 meters line-of-sight, depending on power output and environmental characteristics. ZigBee devices can transmit data over long distances by passing data through a mesh network of intermediate devices to reach more distant ones. ZigBee is typically used in low data rate applications that require long battery life and secure networking (ZigBee networks are secured by 128 bit symmetric encryption keys.) ZigBee has a defined rate of 250 kbit/s, best suited for intermittent data transmissions from a sensor or input device.
ZigBee was conceived in 1998, standardized in 2003, and revised in 2006. The name refers to the waggle dance of honey bees after their return to the beehive.""
